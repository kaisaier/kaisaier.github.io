<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[my_ahk]]></title>
    <url>%2Fblog%2F2019%2F08%2F19%2Fmy_autohotkey_script%2F</url>
    <content type="text"><![CDATA[. . . 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237;;==================================================================;;;;=========================CapsLock&apos;s Stuff=========================;;;;==================================================================;;;不用看注释, 懒得删, 代码重新写了, 所以注释都是错的SetCapsLockState, AlwaysOffCapsLock::Send, &#123;ESC&#125; ; Vimer&apos;s love Capslock = &#123;ESC&#125;;=====================================================================o; CapsLock Switcher: ;|;---------------------------------o-----------------------------------o; CapsLock + ` | &#123;CapsLock&#125; ;|;---------------------------------o-----------------------------------oCapsLock &amp; `:: ;|GetKeyState, CapsLockState, CapsLock, T ;|if CapsLockState = D ;| SetCapsLockState, AlwaysOff ;|else ;| SetCapsLockState, AlwaysOn ;|KeyWait, `` ;|return ;|;---------------------------------------------------------------------o ;;=============================Navigator============================||;===========================;U = PageDown;===========================;H = LeftCapsLock &amp; h::if getkeystate(&quot;alt&quot;) = 0Send, &#123;Left&#125;elseSend, +&#123;Left&#125;return;===========================;J = DownCapsLock &amp; j::if getkeystate(&quot;alt&quot;) = 0Send, &#123;Down&#125;elseSend, +&#123;Down&#125;return;===========================;K = UPCapsLock &amp; k::if getkeystate(&quot;alt&quot;) = 0Send, &#123;Up&#125;elseSend, +&#123;Up&#125;return;===========================;L = RightCapsLock &amp; l::if getkeystate(&quot;alt&quot;) = 0Send, &#123;Right&#125;elseSend, +&#123;Right&#125;return; CapsLock &amp; m::; if getkeystate(&quot;alt&quot;) = 0; Send, &#123;Home&#125;; else; Send, +&#123;Home&#125;; returnCapsLock &amp; ,::if getkeystate(&quot;alt&quot;) = 0Send, &#123;Home&#125;elseSend, +&#123;Home&#125;return;===========================;I = HomeCapsLock &amp; .::if getkeystate(&quot;alt&quot;) = 0Send, &#123;End&#125;elseSend, +&#123;End&#125;returnCapsLock &amp; u::if getkeystate(&quot;alt&quot;) = 0Send, ^zelseSend, ^yreturn; CapsLock &amp; r::; ; if getkeystate(&quot;alt&quot;) = 0; ; Send, ^y; ; ; else; Send, &#123;Ins&#125;; returnCapsLock &amp; y::if getkeystate(&quot;alt&quot;) = 0Send, ^c; else; Send, returnCapsLock &amp; p::if getkeystate(&quot;alt&quot;) = 0Send, ^v; else; Send, returnCapsLock &amp; b::if getkeystate(&quot;alt&quot;) = 0Send, ^&#123;Left&#125;elseSend, +^&#123;Left&#125;returnCapsLock &amp; w::if getkeystate(&quot;alt&quot;) = 0Send, ^&#123;Right&#125;elseSend, +^&#123;Right&#125;returnCapsLock &amp; i::if getkeystate(&quot;alt&quot;) = 0Send, ^&#123;Left&#125;elseSend, +^&#123;Left&#125;returnCapsLock &amp; o::if getkeystate(&quot;alt&quot;) = 0Send, ^&#123;Right&#125;elseSend, +^&#123;Right&#125;returnCapsLock &amp; `;::if getkeystate(&quot;alt&quot;) = 0Send, _elseSend, -returnCapsLock &amp; &apos;::if getkeystate(&quot;alt&quot;) = 0Send, =elseSend, +=returnCapsLock &amp; /::if getkeystate(&quot;alt&quot;) = 0Send, \elseSend, +\returnCapsLock &amp; 9:: if getkeystate(&quot;alt&quot;) = 0Send, [elseSend, &#123;&#123;&#125;returnCapsLock &amp; 0:: if getkeystate(&quot;alt&quot;) = 0Send, ]elseSend, &#123;&#125;&#125;returnCapsLock &amp; n:: if getkeystate(&quot;alt&quot;) = 0Send, ^&#123;BS&#125;elseSend, +&#123;Home&#125;&#123;Del&#125;returnCapsLock &amp; m:: if getkeystate(&quot;alt&quot;) = 0Send, ^&#123;Del&#125;elseSend, +&#123;End&#125;&#123;Del&#125;returnCapsLock &amp; d:: if getkeystate(&quot;alt&quot;) = 0Send, &#123;Del&#125;elseSend, ^&#123;Del&#125;return; ;=============================Deletor==============================||; CapsLock &amp; p:: Send, &#123;Del&#125; ; , = Del char after; CapsLock &amp; .:: Send, ^&#123;Del&#125; ; . = Del word after; CapsLock &amp; /:: Send, +&#123;End&#125;&#123;Del&#125; ; / = Del all after; CapsLock &amp; m:: Send, &#123;BS&#125; ; m = Del char before; ; CapsLock &amp; n:: Send, ^&#123;BS&#125; ; n = Del word before; ; CapsLock &amp; b:: Send, +&#123;Home&#125;&#123;Del&#125; ; b = Del all before; ; ;;============================Special Char==========================||; CapsLock &amp; &apos;:: Send, = ; &apos; = =; CapsLock &amp; `;:: Send, &#123;Enter&#125; ; ; = Enter; CapsLock &amp; &#123;:: Send, +9 ; &#123; = ( ; CapsLock &amp; &#125;:: Send, +0; ; &#125; = ); CapsLock &amp; `:: Send, +`` ; Shift; CapsLock &amp; 4:: Send, +4; CapsLock &amp; 5:: Send, +5; CapsLock &amp; 6:: Send, +6; CapsLock &amp; 7:: Send, +7; CapsLock &amp; 8:: Send, +8; CapsLock &amp; 9:: Send, +9; CapsLock &amp; 0:: Send, +0; CapsLock &amp; -:: Send, +-; CapsLock &amp; =:: Send, +=; CapsLock &amp; \:: Send, +=; ;;============================Editor================================||; CapsLock &amp; z:: Send, ^z ; Z = Cancel; CapsLock &amp; x:: Send, ^x ; X = Cut; CapsLock &amp; c:: Send, ^c ; C = Copy; CapsLock &amp; v:: Send, ^v ; V = Paste; CapsLock &amp; a:: Send, ^a ; A = Select All; CapsLock &amp; y:: Send, ^y ; Y = Redo; ;;===========================Controller=============================||; CapsLock &amp; s:: Send, ^&#123;Tab&#125; ; Switch Tag S = &#123;Ctr + Tab&#125;; CapsLock &amp; w:: Send, ^w ; Close Tag W = &#123;Ctr + W&#125;; CapsLock &amp; q:: Send, !&#123;F4&#125; ; Close Window Q = &#123;Alt + F4&#125;; CapsLock::Send, &#123;ESC&#125; ; Vimer&apos;s love Capslock = &#123;ESC&#125;; ;;=========================Application==============================||; CapsLock &amp; d:: Send, !d ; Dictionary D = &#123;Alt + D&#125;; CapsLock &amp; f:: Send, !f ; Everything F = &#123;Alt + F&#125;; CapsLock &amp; g:: Send, !g ; Reversed G = &#123;Alt + G&#125;; CapsLock &amp; e:: Run http://cn.bing.com/ ; Run Explore E = &#123;Explore&#125;; CapsLock &amp; r:: Run Powershell ; Run Powersh R = &#123;Powershell&#125;; CapsLock &amp; t:: Run C:\Program Files (x86)\Notepad++\notepad++.exe ; Run Notepad++ T = &#123;Text Editor&#125;;;==================================================================;;;;=========================CapsLock&apos;s Stuff=========================;;;;==================================================================;;]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>AutoHotKey</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Surface使用优化]]></title>
    <url>%2Fblog%2F2019%2F08%2F13%2Fsurface_optimization%2F</url>
    <content type="text"><![CDATA[亮度修复方法:一劳永逸的办法注册表修改键值： win+R，输入regedit，回车 导航到HKEY_LOCAL_MACHINE\SYSTEM\ControlSet001\Control\Class{4d36e968-e325-11ce-bfc1-08002be10318}\0000 或者 0001 的右边找到FeatureTestControl 双击键值，将9240更改为9250，重启即可。看不懂的，原文详细如下： . . . 网速慢修复方法: Press Windows button. Search for Regedit. Open it Navigate to the following path: HKEY_LOCAL_MACHINE\SYSTEM\ControlSet001\Services\mrvlpcie8897 Find the item labeled “TXAMSDU.” Double tap and modify the value from 1 to 0 Restart the machine link: https://www.windowscentral.com/surface-pro-4-slow-wi-fi-fix 解决掉TF卡问题在解决之前搜了很多方法.包括什么高性能 更新驱动 改注册表 都不行 或许有的人因为兼容问题通过以上的方法可以解决.但是我依然还存在这个情况. 后来经过研究发现是插口断电导致的 现在已经解决 此贴分享给与我一样情况的小伙伴们 : 设备管理器-通用串行总线控制器-Realtek USB 3.0 Card Reader-更新驱动程序-浏览我的计算机查找驱动-让我从计算机上可用驱动列表查找-USB大容量设备(这个步骤把SD卡接口驱动改为USB驱动, 然后你会发现原来的 Realtek USB 3.0 Card Reader 变成了 USB大容量存储设备 设备管理器-通用串行总线控制器-USB大容量存储设备-属性-电源管理-允许计算机关闭此设备以节省电源(不勾选)]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>Surface</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于SIGPIPE和SIGHUP]]></title>
    <url>%2Fblog%2F2019%2F08%2F12%2Fabout_sighup_sigpipe%2F</url>
    <content type="text"><![CDATA[SIGHUP 信号 在介绍 SIGHUP 信号之前，先来了解两个概念：进程组和会话。 . . . 进程组 进程组就是一系列相互关联的进程集合，系统中的每一个进程也必须从属于某一个进程组；每个进程组中都会有一个唯一的 ID(process group id)，简称 PGID；PGID 一般等同于进程组的创建进程的 Process ID，而这个进进程一般也会被称为进程组先导 (process group leader)，同一进程组中除了进程组先导外的其他进程都是其子进程； 进程组的存在，方便了系统对多个相关进程执行某些统一的操作，例如，我们可以一次性发送一个信号量给同一进程组中的所有进程。 会话 会话（session）是一个若干进程组的集合，同样的，系统中每一个进程组也都必须从属于某一个会话；一个会话只拥有最多一个控制终端（也可以没有），该终端为会话中所有进程组中的进程所共用。一个会话中前台进程组只会有一个，只有其中的进程才可以和控制终端进行交互；除了前台进程组外的进程组，都是后台进程组；和进程组先导类似，会话中也有会话先导 (session leader) 的概念，用来表示建立起到控制终端连接的进程。在拥有控制终端的会话中，session leader 也被称为控制进程(controlling process)，一般来说控制进程也就是登入系统的 shell 进程(login shell)； 执行睡眠后台进程 sleep 50 &amp; 之后，通过 ps 命令查看该进程及 shell 信息如上图： PPID 指父进程 id； PID 指进程 id； PGID 指进程组 id SID 指会话 id； TTY 指会话的控制终端设备； COMMAND 指进程所执行的命令 TPGID 指前台进程组的 PGID。 SIGHUP 信号的触发及默认处理 在对会话的概念有所了解之后，我们现在开始正式介绍一下 SIGHUP 信号，SIGHUP 信号在 用户终端连接 (正常或非正常) 结束 时发出, 通常是在终端的控制进程结束时, 通知同一 session 内的各个作业, 这时它们与控制终端不再关联. 系统对 SIGHUP 信号的默认处理是终止收到该信号的进程。所以若程序中没有捕捉该信号，当收到该信号时，进程就会退出。 SIGHUP 会在以下 3 种情况下被发送给相应的进程： 终端关闭时，该信号被发送到 session 首进程以及作为 job 提交的进程（即用 &amp; 符号提交的进程）； session 首进程退出时，该信号被发送到该 session 中的前台进程组中的每一个进程； 若父进程退出导致进程组成为孤儿进程组，且该进程组中有进程处于停止状态（收到 SIGSTOP 或 SIGTSTP 信号），该信号会被发送到该进程组中的每一个进程。 例如：在我们登录 Linux 时，系统会分配给登录用户一个终端 (Session)。在这个终端运行的所有程序，包括前台进程组和后台进程组，一般都属于这个 Session。当用户退出 Linux 登录时，前台进程组和后台有对终端输出的进程将会收到 SIGHUP 信号。这个信号的默认操作为终止进程，因此前台进 程组和后台有终端输出的进程就会中止。 此外，对于与终端脱离关系的守护进程，正常情况下是永远都收不到这个信号的, 所以可以人为的发SIGHUP信号给她用于通知它做一些想要的自定义的操作, 比较常见的如重新读取配置文件操作。 比如 xinetd 超级服务程序。 当 xinetd 程序在接收到 SIGHUP 信号之后调用 hard_reconfig 函数，它将循环读取 / etc/xinetd.d / 目录下的每个子配置文件，并检测其变化。如果某个正在运行的子服务的配置文件被修改以停止服务，则 xinetd 主进程讲给该子服务进程发送 SIGTERM 信号来结束它。如果某个子服务的配置文件被修改以开启服务，则 xinetd 将创建新的 socket 并将其绑定到该服务对应的端口上。 SIGPIPE 在网络编程中，SIGPIPE 这个信号是很常见的。当往一个写端关闭的管道或 socket 连接中连续写入数据时会引发 SIGPIPE 信号, 引发 SIGPIPE 信号的写操作将设置 errno 为 EPIPE。在 TCP 通信中，当通信的双方中的一方 close 一个连接时，若另一方接着发数据，根据 TCP 协议的规定，会收到一个 RST 响应报文，若再往这个服务器发送数据时，系统会发出一个 SIGPIPE 信号给进程，告诉进程这个连接已经断开了，不能再写入数据。 测试程序如下：简单的测试程序，函数未加错误判断 server.c 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364#include &lt;stdio.h&gt;#include &lt;string.h&gt;#include &lt;sys/socket.h&gt;#include &lt;arpa/inet.h&gt;#include &lt;unistd.h&gt;#include &lt;errno.h&gt;#include &lt;signal.h&gt;#define port 8888void handle(int sig)&#123; printf("SIGPIPE : %d\n",sig);&#125;void mysendmsg(int fd)&#123; // 写入第一条消息 char* msg1 = "first msg"; int n = write(fd, msg1, strlen(msg1)); if(n &gt; 0) //成功写入第一条消息,server 接收到 client 发送的 RST &#123; printf("success write %d bytes\n", n); &#125; // 写入第二条消息,触发SIGPIPE char* msg2 = "second msg"; n = write(fd, msg2, strlen(msg2)); if(n &lt; 0) &#123; printf("write error: %s\n", strerror(errno)); &#125;&#125;int main()&#123; signal(SIGPIPE , handle); //注册信号捕捉函数 struct sockaddr_in server_addr; bzero(&amp;server_addr, sizeof(server_addr)); server_addr.sin_family = AF_INET; server_addr.sin_addr.s_addr = htonl(INADDR_ANY); server_addr.sin_port = htons(port); int listenfd = socket(AF_INET , SOCK_STREAM , 0); bind(listenfd, (struct sockaddr *)&amp;server_addr, sizeof(server_addr)); listen(listenfd, 128); int fd = accept(listenfd, NULL, NULL); if(fd &lt; 0) &#123; perror("accept"); exit(1); &#125; mysendmsg(fd); return 0;&#125; client.c123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051#include&lt;stdio.h&gt;#include&lt;stdlib.h&gt;#include&lt;errno.h&gt;#include&lt;string.h&gt;#include&lt;sys/types.h&gt;#include&lt;netinet/in.h&gt;#include&lt;sys/socket.h&gt;#include&lt;sys/wait.h&gt;#include&lt;arpa/inet.h&gt;#include&lt;unistd.h&gt;#define PORT 8888#define MAX 1024int main()&#123; char buf[MAX] = &#123;'0'&#125;; int sockfd; int n; socklen_t slen; slen = sizeof(struct sockaddr); struct sockaddr_in seraddr; bzero(&amp;seraddr,sizeof(seraddr)); seraddr.sin_family = AF_INET; seraddr.sin_port = htons(PORT); seraddr.sin_addr.s_addr = htonl(INADDR_ANY); //socket() if((sockfd = socket(AF_INET,SOCK_STREAM,0)) == -1) &#123; perror("socket"); exit(-1); &#125; //connect() if(connect(sockfd,(struct sockaddr *)&amp;seraddr,slen) == -1) &#123; perror("connect"); exit(-1); &#125; int ret = shutdown(sockfd , SHUT_RDWR); if(ret &lt; 0) &#123; perror("shutdown perror"); &#125; return 0;&#125; 运行结果 此外，因为 SIGPIPE 信号的默认行为是结束进程，而我们绝对不希望因为写操作的错误而导致程序退出，尤其是作为服务器程序来说就更恶劣了。所以我们应该对这种信号加以处理，在这里，介绍两种处理 SIGPIPE 信号的方式： 给 SIGPIPE 设置 SIG_IGN 信号处理函数，忽略该信号:signal(SIGPIPE, SIG_IGN);前文说过，引发 SIGPIPE 信号的写操作将设置 errno 为 EPIPE,。所以，第二次往关闭的 socket 中写入数据时, 会返回 - 1, 同时 errno 置为 EPIPE. 这样，便能知道对端已经关闭，然后进行相应处理，而不会导致整个进程退出. 使用 send 函数的 MSG_NOSIGNAL 标志来禁止写操作触发 SIGPIPE 信号。send(sockfd , buf , size , MSG_NOSIGNAL);同样，我们可以根据 send 函数反馈的 errno 来判断 socket 的读端是否已经关闭。此外，我们也可以通过 IO 复用函数来检测管道和 socket 连接的读端是否已经关闭。以 POLL 为例，当 socket 连接被对方关闭时，socket 上的 POLLRDHUP 事件将被触发。]]></content>
      <categories>
        <category>NP</category>
      </categories>
      <tags>
        <tag>NP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTP安全性和幂等性]]></title>
    <url>%2Fblog%2F2019%2F07%2F11%2Fhttp_safety_idempotence%2F</url>
    <content type="text"><![CDATA[GET操作是安全的。 所谓安全是指不管进行多少次操作，资源的状态都不会改变。 比如我用GET浏览文章，不管浏览多少次，那篇文章还在那，没有变化。当然，你可能说每浏览一次文章，文章的浏览数就加一，这不也改变了资源的状态么？这并不矛盾，因为这个改变不是GET操作引起的，而是用户自己设定的服务端逻辑造成的。 . . . PUT，DELETE操作是幂等的。 所谓幂等是指不管进行多少次操作，结果都一样。 比如我用PUT修改一篇文章，然后在做同样的操作，每次操作后的结果并没有不同，DELETE也是一样。顺便说一句，因为GET操作是安全的，所以它自然也是幂等的。 POST操作既不是安全的，也不是幂等的，比如常见的POST重复加载问题：当我们多次发出同样的POST请求后，其结果是创建出了若干的资源。安全和幂等的意义在于：当操作没有达到预期的目标时，我们可以不停的重试，而不会对资源产生副作用。从这个意义上说，POST操作往往是有害的，但很多时候我们还是不得不使用它。 还有一点需要注意的就是，创建操作可以使用POST，也可以使用PUT，区别在于POST 是作用在一个集合资源之上的（/uri），而PUT操作是作用在一个具体资源之上的（/uri/xxx），再通俗点说，如果URL可以在客户端确定，那么就使用PUT，如果是在服务端确定，那么就使用POST，比如说很多资源使用数据库自增主键作为标识信息，而创建的资源的标识信息到底是什么只能由服务端提供，这个时候就必须使用POST。 关于GET POST 的混淆先说相同点，只有了解了相同点之后才能理解为什么会发生混淆。两者都能向服务器发送数据，提交的“内容”[注1]的格式相同，都是 param1=value1&amp;param2=value2&amp;.... get 和 post 区别如字面，一个是get（获取），一个是post（发送）。 get用来告诉服务器需要获取哪些内容（uri+query），向静态页面（uri）请求则直接返回文件内容给浏览器，向一个动态页面请求时可以提供查询参数（query）以获得相应内容。 post用来向服务器提交内容，主要是为了提交，而不是为了请求内容，就是说post的初衷并不要求服务器返回内容[注2]，只是提交内容让服务器处理（主要是存储或者处理之后再存储）。 get和post出现混淆是因为对提交的数据处理方法的滥用造成的，数据是无辜的。 混淆之一：将get提交的用来查询的字段当作是存储数据存入了服务器端文件或者数据库。然后就误以为get是用来提交用于存储的数据的。 混淆之二：编写脚本在服务器端通过处理post提交的数据并返回内容。只要有数据，就能用来进行判断，脚本怎写是程序员的事，而不在乎数据来源的形式（post、get，或者是自己预设值的常量）。这点功能上确实没问题，只是背离的其初始目的而已。 由于都是要传送数据，且数据格式相同（即使数据格式不同，只要能提取出相应数据）。使用的时候难免出现张冠李戴，将get数据用来存储、将post数据用来检索返回数据。但是二者还是有区别的（主要是根据其用途而“人为”造成的），get的长度限制在2048字节（由浏览器和服务器限制的，这是目前IE的数据，曾经是1024字节），很大程度上限制了get用来传递“存储数据”的数据的能力，所以还是老老实实用来做检索吧；post则无此限制（只是HTTP协议规范没有进行大小限制，但受限于服务器的处理能力），因此对于大的数据（一般来说需要存储的数据可能会比较大，比2048字节大）的传递有天然的优势，谁让它是 nature born post 呢。 get提交的数据是放在url里，目的是灵活的向服务其提交检索请求，可以在地址栏随时修改数据以变更需要获取的内容，比如直接修改分页的编号就跳到另外一个分页了（当然也可能是 404）。post提交的数据放在http请求的正文里，目的在于提交数据并用于服务器端的存储，而不允许用户过多的更改相应数据（主要是相对于在url 修改要麻烦很多，url的修改只要点击地址栏输入字符就可以了），除非是专门跑来编辑数据的。 PS：post和get的安全性在传输的层面上区别不大，但是采用url提交数据的get方式容易被人肉眼看到，或者出现在历史纪录里，还是可能被肉眼看到，都是一些本地的问题。 注1：我强调的是内容，至于http协议中的get和post的格式大家有兴趣就自己看看吧。 注2：get方式主要是为了获得预期内容，即uri+query相同时所得到的内容应该是相同的。而post主要是提交内容，至于是否有必要返回页面可能只是出于用户体验，比如注册时返回你的注册id，但是如果只是返回一个“您已注册成功”的相同页面（即使你post的数据不一样）也没什么好奇怪的。注3：关于这个“人为”，不是那么贴切，get和post还是有技术层面的区别的。但是从表象上看暂且这么说吧，毕竟二者的混淆也是“人为”的。 POST GET 本质区别一般在浏览器中输入网址访问资源都是通过GET方式；在FORM提交中，可以通过Method指定提交方式为GET或者POST，默认为GET提交 Http定义了与服务器交互的不同方法，最基本的方法有4种，分别是GET，POST，PUT，DELETE URL 全称是资源描述符，我们可以这样认为：一个URL地址，它用于描述一个网络上的资源，而HTTP中的GET，POST，PUT，DELETE就对应着对这个资源的查 ，改 ，增 ，删 4个操作。到这里，大家应该有个大概的了解了，GET一般用于获取/查询 资源信息，而POST一般用于更新 资源信息(个人认为这是GET和POST的本质区别，也是协议设计者的本意，其它区别都是具体表现形式的差异 )。 根据HTTP规范，GET用于信息获取，而且应该是安全的和幂等的 。 所谓安全的意味着该操作用于获取信息而非修改信息。换句话说，GET请求一般不应产生副作用。就是说，它仅仅是获取资源信息，就像数据库查询一样，不会修改，增加数据，不会影响资源的状态。 注意：这里安全的含义仅仅是指是非修改信息。 幂等的意味着对同一URL的多个请求应该返回同样的结果。这里我再解释一下幂等 这个概念: 幂等 （idempotent、idempotence）是一个数学或计算机学概念，常见于抽象代数中。 幂等有以下几种定义：对于单目运算，如果一个运算对于在范围内的所有的一个数多次进行该运算所得的结果和进行一次该运算所得的结果是一样的，那么我们就称该运算是幂等的。比如绝对值运算就是一个例子，在实数集中，有abs(a) = abs(abs(a)) 。 对于双目运算，则要求当参与运算的两个值是等值的情况下，如果满足运算结果与参与运算的两个值相等，则称该运算幂等，如求两个数的最大值的函数，有在在实数集中幂等，即max(x,x) = x 。 看完上述解释后，应该可以理解GET幂等的含义了。 但在实际应用中，以上2条规定并没有这么严格。引用别人文章的例子：比如，新闻站点的头版不断更新。虽然第二次请求会返回不同的一批新闻，该操作仍然被认为是安全的和幂等的，因为它总是返回当前的新闻。从根本上说，如果目标是当用户打开一个链接时，他可以确信从自身的角度来看没有改变资源即可。 根据HTTP规范，POST表示可能修改变服务器上的资源的请求 。 继续引用上面的例子：还是新闻以网站为例，读者对新闻发表自己的评论应该通过POST实现，因为在评论提交后站点的资源已经不同了，或者说资源被修改了。 上面大概说了一下HTTP规范中，GET和POST的一些原理性的问题。但在实际的做的时候，很多人却没有按照HTTP规范去做，导致这个问题的原因有很多，比如说： 很多人贪方便，更新资源时用了GET，因为用POST必须要到FORM（表单），这样会麻烦一点。 对资源的增，删，改，查操作，其实都可以通过GET/POST完成，不需要用到PUT和DELETE。 另外一个是，早期的但是Web MVC框架设计者们并没有有意识地将URL当作抽象的资源来看待和设计 。还有一个较为严重的问题是传统的Web MVC框架基本上都只支持GET和POST两种HTTP方法，而不支持PUT和DELETE方法。 转自: https://286.iteye.com/blog/1420713]]></content>
      <categories>
        <category>NP</category>
      </categories>
      <tags>
        <tag>HTTP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[etcd的restful接口]]></title>
    <url>%2Fblog%2F2019%2F07%2F10%2Fetcd_api_note%2F</url>
    <content type="text"><![CDATA[. . . etcd api接口 基本操作api: https://github.com/coreos/etcd/blob/6acb3d67fbe131b3b2d5d010e00ec80182be4628/Documentation/v2/api.md 集群配置api:&nbsp;https://github.com/coreos/etcd/blob/6acb3d67fbe131b3b2d5d010e00ec80182be4628/Documentation/v2/members_api.md 鉴权认证api:&nbsp;https://github.com/coreos/etcd/blob/6acb3d67fbe131b3b2d5d010e00ec80182be4628/Documentation/v2/auth_api.md 配置项：https://github.com/coreos/etcd/blob/master/Documentation/op-guide/configuration.md https://coreos.com/etcd/docs/latest/runtime-configuration.html https://coreos.com/etcd/docs/latest/clustering.html https://coreos.com/etcd/docs/latest/runtime-configuration.html https://coreos.com/etcd/docs/latest/ https://coreos.com/etcd/docs/latest/admin_guide.html#disaster-recovery 采用标准的restful 接口，支持http 和 https 两种协议。 运行单机etcd服务 1 ./bin/etcd &nbsp; 监听localhost和从IANA分配的端口，2379用于同client通讯，2389用于server与server直接的通讯。 &nbsp; 获取版本 &nbsp;/version 1 [root@vStack ~]# curl http://127.0.0.1:2379/version | python -m json.tool 2 % Total % Received % Xferd Average Speed Time Time Time Current 3 Dload Upload Total Spent Left Speed 4 100 44 100 44 0 0 14093 0 --:--:-- --:--:-- --:--:-- 22000 5 { 6 "etcdcluster": "2.3.0", 7 "etcdserver": "2.3.7" 8 } &nbsp;&nbsp; etcd 的基本API是一个分层的key空间。key空间由通常被称为"nodes"（节点）的keys和目录组成。 对datastore的访问，即通过 /version/keys&nbsp;端点(endpoint) 访问key空间。 1. PUT 为etcd存储的键赋值， 即创建 message 键值，赋值为"Hello world" 1 [root@vStack ~]# curl http://127.0.0.1:2379/v2/keys/message -X PUT -d value="Hello world" | python -m json.tool 2 % Total % Received % Xferd Average Speed Time Time Time Current 3 Dload Upload Total Spent Left Speed 4 100 119 100 102 100 17 38230 6371 --:--:-- --:--:-- --:--:-- 51000 5 { 6 "action": "set", 7 "node": { 8 "createdIndex": 30, 9 "key": "/message", 10 "modifiedIndex": 30, 11 "value": "Hello world" 12 } 13 } Response body返回值中的： action： &nbsp; 请求接口进行的动作名称。 通过 http &nbsp;PUT方法修改node.key的值，对应的action值为："set&ldquo;。 &nbsp;PUT方法中，请求body中存在 prevExist=true时， action为update；&nbsp;prevExist=false时，action为create；&nbsp;其他为set。 node.createIndex: &nbsp;etcd每次变化时创建的，唯一的，单调递增的、整数值作为索引。这个特定的索引值反映了在etcd状态成员里创建了一个给定key。除了用户请求外，etcd内部运行（如启动服务，重启服务、集群信息变化：添加、删除、同步服务等）也可能会因为对节点有变动而引起该值的变化。所以即使我们首次请求，此值也不是从1开始。update、get&nbsp;action不引起 node.createIndex值的变化。 &nbsp;&nbsp; &nbsp; &nbsp;&nbsp;node.key: &nbsp;在请求的HTTP路径中，作为操作对象key。etcd使用一个类似文件系统的方式来反映键值存储的内容， 因此所有的key都是以&lsquo;/&rsquo;开始 。 &nbsp; &nbsp; &nbsp; node.modifiedIndex: &nbsp;像&nbsp;node.createIndex, 这个属性也是etcd的索引。 引起这个值变化的Actions包括：set，delete，update，create，compareAndSwap 和 compareAndDelete。因为 get 和 watchcommands 在存储中不修改状态，所以这两个action不会修改mode.modifiedIndex值， 也不会修改 node.createIndex的值。 重启服务等也会修改此属性值。 &nbsp; &nbsp; &nbsp;&nbsp;node.value: &nbsp;处理完请求后的key值。 在上面的实例中，成功请求后，修改节点的值为 Hello world。 &nbsp; &nbsp; &nbsp; &nbsp; Response header返回值中： 在responses中包括一些的HTTP 的headers部，在header中提供了一些关于etcd集群的全部信息，集群提供服务请求。 1 X-Etcd-Cluster-Id: 7e27652122e8b2ae 2 X-Etcd-Index: 93 3 X-Raft-Index: 223696 4 X-Raft-Term: 8 &nbsp; &nbsp; X-Etcd-Cluster-Id: &nbsp;etcd 集群id。 X-Etcd-Index： &nbsp;当前etcd的索引，像前面的解释。当在key空间进行watch时，watch开始时，X-Etcd-Index是当前etcd的索引值，这意味着watched事件可能发生在X-Etcd-Index之后。 X-Raft-Index： 与X-Etcd-Index索引类似，是raft协议的索引。 X-Raft-Term： &nbsp;是一个在集群中发生master election时，将增长的整数。如果这个值增长的非常快，需要调优这个election超时。详见&nbsp;tuning&nbsp;部分。 &nbsp; 2. GET 查询etcd某个键存储的值 [root@vStack ~]# curl http://127.0.0.1:2379/v2/keys/message | python -m json.tool % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 102 100 102 0 0 64110 0 --:--:-- --:--:-- --:--:-- 99k { "action": "get", "node": { "createdIndex": 19, "key": "/message", "modifiedIndex": 19, "value": "Hello world" } } &nbsp; 3. PUT 修改键值：与创建新值几乎相同，但是反馈时会有一个prevNode值反应了修改前存储的内容。 -d value=xxxx [root@vStack ~]# curl http://127.0.0.1:2379/v2/keys/message -X PUT -d value="RECREATE" | python -m json.tool % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 202 100 188 100 14 57108 4252 --:--:-- --:--:-- --:--:-- 62666 { "action": "set", "node": { "createdIndex": 33, "key": "/message", "modifiedIndex": 33, "value": "RECREATE" }, "prevNode": { "createdIndex": 32, "key": "/message", "modifiedIndex": 32, "value": "Hello world" } } Respone中新的字段 "prevNode", 这个字段表示当前请求完成前的请求节点的状态。 prevNode的格式与node相同， 在访问的节点没有前面状态时将被忽略。 &nbsp; &nbsp; 4. DELETE 删除一个值&nbsp; 1 [root@vStack ~]# curl http://127.0.0.1:2379/v2/keys/message -X DELETE | python -m json.tool 2 % Total % Received % Xferd Average Speed Time Time Time Current 3 Dload Upload Total Spent Left Speed 4 100 177 100 177 0 0 73261 0 --:--:-- --:--:-- --:--:-- 172k 5 { 6 "action": "delete", 7 "node": { 8 "createdIndex": 19, 9 "key": "/message", 10 "modifiedIndex": 29 11 }, 12 "prevNode": { 13 "createdIndex": 19, 14 "key": "/message", 15 "modifiedIndex": 28, 16 "value": "test createIndex" 17 } 18 } &nbsp; 5. PUT 对一个键进行定时删除：etcd中对键进行定时删除，设定一个ttl值，当这个值到期时键就会被删除。反馈的内容会给出expiration项告知超时时间，ttl项告知设定的时长。 &nbsp; &nbsp; 在设定一个key时，设定其ttl（time to live), ttl时间后，自动删除。 -d&nbsp;ttl=xxx 1 [root@vStack ~]# curl http://127.0.0.1:2379/v2/keys/foo -XPUT -d value=bar -d ttl=5 | python -m json.tool 2 % Total % Received % Xferd Average Speed Time Time Time Current 3 Dload Upload Total Spent Left Speed 4 100 159 100 144 100 15 60453 6297 --:--:-- --:--:-- --:--:-- 72000 5 { 6 "action": "set", 7 "node": { 8 "createdIndex": 34, 9 "expiration": "2016-04-23T12:01:57.992249507Z", 10 "key": "/foo", 11 "modifiedIndex": 34, 12 "ttl": 5, 13 "value": "bar" 14 } 15 } &nbsp; 在repsonse中有两个新的字段： expiration：key的有效截至日期。 ttl: &nbsp; &nbsp; &nbsp; key的ttl值，单位秒。 &nbsp; &nbsp; &nbsp;注意|： key只有被cluster header设定过期，如果一个memeber 脱离的集群，它里面的key将没有过期，直到重新加入后才有过期功能。 &nbsp; 6. PUT 取消定时删除任务 -d ttl= 1 [root@vStack ~]# curl http://127.0.0.1:2379/v2/keys/foo -XPUT -d value=bar -d ttl= -d prevExist=true | python -m json.tool 2 % Total % Received % Xferd Average Speed Time Time Time Current 3 Dload Upload Total Spent Left Speed 4 100 254 100 225 100 29 98944 12752 --:--:-- --:--:-- --:--:-- 219k 5 { 6 "action": "update", 7 "node": { 8 "createdIndex": 38, 9 "key": "/foo", 10 "modifiedIndex": 39, 11 "value": "bar" 12 }, 13 "prevNode": { 14 "createdIndex": 38, 15 "expiration": "2016-04-23T12:07:05.415596297Z", 16 "key": "/foo", 17 "modifiedIndex": 38, 18 "ttl": 78, 19 "value": "bar" 20 } 21 } &nbsp; 7. PUT 刷新key的 ttl&nbsp; ttl 到删除key和重新设置ttl，都会触发watcher。通过在请求的body中增加 refresh=true，更新ttl(必须存在)，不引起触发watcher事件。 -d refresh=true 1 [root@vStack ~]# curl http://127.0.0.1:2379/v2/keys/message -XPUT -d ttl=100 -d refresh=true | python -m json.tool 2 % Total % Received % Xferd Average Speed Time Time Time Current 3 Dload Upload Total Spent Left Speed 4 100 304 100 284 100 20 46973 3307 --:--:-- --:--:-- --:--:-- 56800 5 { 6 "action": "set", 7 "node": { 8 "createdIndex": 145, 9 "expiration": "2016-12-28T06:58:20.426383304Z", 10 "key": "/message", 11 "modifiedIndex": 145, 12 "ttl": 100, 13 "value": "" 14 }, 15 "prevNode": { 16 "createdIndex": 144, 17 "expiration": "2016-12-28T06:57:55.628682326Z", 18 "key": "/message", 19 "modifiedIndex": 144, 20 "ttl": 76, 21 "value": "" 22 } 23 } &nbsp; &nbsp; 8. GET 对键值修改进行监控：etcd提供的这个API通过long polling(轮询)让用户可以监控一个值或者递归式(recursive=true 在url path中作为参数)地监控一个目录及其子目录的值，当目录或值发生变化时，etcd会主动通知。 ？wait=true &nbsp; &nbsp; 监听当前节点 &nbsp; &nbsp; &nbsp; ？recursive=true&nbsp; 递归监听当前节点和子目录 &nbsp; &nbsp; &nbsp; ？waitIndex=xxx 监听过去已经发生的。过去值的查询或监听， 必选与wait一起使用。 1 [root@vStack ~]# curl 'http://127.0.0.1:2379/v2/keys/message?wait=true&amp;waitIndex=2230' | python -m json.tool 2 % Total % Received % Xferd Average Speed Time Time Time Current 3 Dload Upload Total Spent Left Speed 4 100 183 0 183 0 0 131k 0 --:--:-- --:--:-- --:--:-- 178k 5 { 6 "action": "set", 7 "node": { 8 "createdIndex": 2230, 9 "key": "/message", 10 "modifiedIndex": 2230, 11 "value": "123" 12 }, 13 "prevNode": { 14 "createdIndex": 2229, 15 "key": "/message", 16 "modifiedIndex": 2229, 17 "value": "123" 18 } 19 } &nbsp; watch 一个ttl自删除的key时，收到如下 &ldquo;expire&rdquo; action。 1 [root@vStack ~]# curl http://127.0.0.1:2379/v2/keys/message?wait=true | python -m json.tool 2 % Total % Received % Xferd Average Speed Time Time Time Current 3 Dload Upload Total Spent Left Speed 4 100 215 0 215 0 0 19 0 --:--:-- 0:00:10 --:--:-- 45 5 { 6 "action": "expire", 7 "node": { 8 "createdIndex": 2223, 9 "key": "/message", 10 "modifiedIndex": 2224 11 }, 12 "prevNode": { 13 "createdIndex": 2223, 14 "expiration": "2016-12-28T09:25:00.028597482Z", 15 "key": "/message", 16 "modifiedIndex": 2223, 17 "value": "" 18 } 19 } &nbsp; &nbsp; 9. GET 对过去的键值操作进行查询：类似上面提到的监控，在其基础上指定过去某次修改的索引编号，就可以查询历史操作。默认可查询的历史记录为1000条。 ？ waitIndex=xxx &nbsp;&nbsp;监听过去已经发生的。 这个在确保在watch命令中，没有丢失事件非常有用。例如：我们反复watch 我们得到节点的 modifiedIndex+1。 因为 node 的modifiedIndex的值是不连续，如果waitIndex的值没有相应modifiedIndex，返回最大的modifedIndex的节点信息。&nbsp;如果大于节点中所有的modifiedIndex，等待，直到节点的modifiedIndex值大于等于waitIndex的值。 即使删除key后，也可以查询历史数据。 store中有一个全局的currentIndex，每次变更，index会加1.然后每个event都会关联到currentIndex. 当客户端调用watch接口（参数中增加 wait参数）时，如果请求参数中有waitIndex，并且waitIndex 小于 currentIndex，则从 EventHistroy 表中查询index小于等于waitIndex，并且和watch key 匹配的 event，如果有数据，则直接返回。如果历史表中没有或者请求没有带 waitIndex，则放入WatchHub中，每个key会关联一个watcher列表。 当有变更操作时，变更生成的event会放入EventHistroy表中，同时通知和该key相关的watcher。 注意： 1. 必须与 wait 一起使用； &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;2. curl 中url需要使用引号。 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;3. etcd 仅仅保留系统中所有key最近的1000条event，建议将获取到的response发送到另一个线程处理，而不是处理response而阻塞watch。 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;4. 如果watch超出了etcd保存的最近1000条，建议get后使用response header中的&nbsp;X-Etcd-Index&nbsp;+ 1进行重新watch，而不是使用node中的modifiedIndex+1. 因为 &nbsp;X-Etcd-Index&nbsp; 永远大于等于modifiedIndex， 使用modifiedIndex可能会返回401错误码，同样超出。 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;5. long polling可能会被服务器关闭，如超时或服务器关闭。导致仅仅收到header 200OK，body为空，此时应重新watch。 1 [root@vStack ~]# curl 'http://127.0.0.1:2379/v2/keys/foo?wait=true&amp;waitIndex=2' | python -m json.tool 2 % Total % Received % Xferd Average Speed Time Time Time Current 3 Dload Upload Total Spent Left Speed 4 100 144 0 144 0 0 102k 0 --:--:-- --:--:-- --:--:-- 140k 5 { 6 "action": "set", 7 "node": { 8 "createdIndex": 34, 9 "expiration": "2016-04-23T12:01:57.992249507Z", 10 "key": "/foo", 11 "modifiedIndex": 34, 12 "ttl": 5, 13 "value": "bar" 14 } 15 } &nbsp;如果超出了etcd保留的最近1000条，返回 401错误码 1 [root@vStack ~]# curl 'http://127.0.0.1:2379/v2/keys/message?wait=true&amp;waitIndex=8' | python -m json.tool 2 % Total % Received % Xferd Average Speed Time Time Time Current 3 Dload Upload Total Spent Left Speed 4 100 154 100 154 0 0 56163 0 --:--:-- --:--:-- --:--:-- 150k 5 { 6 "cause": "the requested history has been cleared [1186/8]", 7 "errorCode": 401, 8 "index": 2185, 9 "message": "The event in requested index is outdated and cleared" 10 } &nbsp; 10. PUT&nbsp;创建目录 1 [root@vStack ~]# curl http://127.0.0.1:2379/v2/keys/dir -XPUT -d dir=true | python -m json.tool 2 % Total % Received % Xferd Average Speed Time Time Time Current 3 Dload Upload Total Spent Left Speed 4 100 95 100 87 100 8 21260 1955 --:--:-- --:--:-- --:--:-- 29000 5 { 6 "action": "set", 7 "node": { 8 "createdIndex": 63, 9 "dir": true, 10 "key": "/dir", 11 "modifiedIndex": 63 12 } 13 } &nbsp; 11. GET 列出目录下所有的节点信息，最后以/结尾(不是必须的)。还可以通过recursive参数递归列出所有子目录信息。 没有recursive，返回第二级。后面不在返回。 1 [root@vStack ~]# curl http://127.0.0.1:2379/v2/keys/dir1/ | python -m json.tool 2 % Total % Received % Xferd Average Speed Time Time Time Current 3 Dload Upload Total Spent Left Speed 4 100 167 100 167 0 0 65234 0 --:--:-- --:--:-- --:--:-- 83500 5 { 6 "action": "get", 7 "node": { 8 "createdIndex": 67, 9 "dir": true, 10 "key": "/dir1", 11 "modifiedIndex": 67, 12 "nodes": [ 13 { 14 "createdIndex": 67, 15 "dir": true, 16 "key": "/dir1/dir2", 17 "modifiedIndex": 67 18 } 19 ] 20 } 21 } &nbsp; 12. POST 自动在目录下创建有序键。在对创建的目录使用POST参数，会自动在该目录下创建一个以global etcd index值为键的值，这样就相当于根据创建时间的先后进行了严格排序。该API对分布式队列这类场景非常有用。 1 [root@vStack ~]# curl http://127.0.0.1:2379/v2/keys/queue -XPOST -d value=Job1 | python -m json.tool 2 % Total % Received % Xferd Average Speed Time Time Time Current 3 Dload Upload Total Spent Left Speed 4 100 127 100 117 100 10 31655 2705 --:--:-- --:--:-- --:--:-- 39000 5 { 6 "action": "create", 7 "node": { 8 "createdIndex": 47, 9 "key": "/queue/00000000000000000047", 10 "modifiedIndex": 47, 11 "value": "Job1" 12 } 13 } &nbsp; 13. GET&nbsp;按顺序列出所有创建的有序键 ？ sorted=true ? recursive=true 1 [root@vStack ~]# curl -s 'http://127.0.0.1:2379/v2/keys/queue?sorted=true' | python -m json.tool 2 { 3 "action": "get", 4 "node": { 5 "createdIndex": 46, 6 "dir": true, 7 "key": "/queue", 8 "modifiedIndex": 46, 9 "nodes": [ 10 { 11 "createdIndex": 46, 12 "key": "/queue/00000000000000000046", 13 "modifiedIndex": 46, 14 "value": "" 15 }, 16 { 17 "createdIndex": 47, 18 "key": "/queue/00000000000000000047", 19 "modifiedIndex": 47, 20 "value": "Job1" 21 }, 22 { 23 "createdIndex": 48, 24 "key": "/queue/00000000000000000048", 25 "modifiedIndex": 48, 26 "value": "aaaa" 27 }, 28 { 29 "createdIndex": 49, 30 "key": "/queue/00000000000000000049", 31 "modifiedIndex": 49, 32 "value": "aaaa" 33 }, 34 { 35 "createdIndex": 50, 36 "key": "/queue/00000000000000000050", 37 "modifiedIndex": 50, 38 "value": "aaaa" 39 }, 40 { 41 "createdIndex": 51, 42 "key": "/queue/00000000000000000051", 43 "modifiedIndex": 51, 44 "value": "aaaa" 45 } 46 ] 47 } 48 } &nbsp; 14. DELETE&nbsp;删除目录：默认情况下只允许删除空目录，如果要删除有内容的目录需要加上recursive=true参数。 ？dir=true 删除目录&nbsp; ？recursive=true &nbsp;删除非空目录 &nbsp; &nbsp; &nbsp; 删除非空目录必须使用 recursive=true 参数，删除空目录，dir=true或recursive=true至少有一个。 1 [root@vStack ~]# curl 'http://127.0.0.1:2379/v2/keys/dir1?dir=true' -XDELETE | python -m json.tool 2 % Total % Received % Xferd Average Speed Time Time Time Current 3 Dload Upload Total Spent Left Speed 4 100 77 100 77 0 0 38557 0 --:--:-- --:--:-- --:--:-- 77000 5 { 6 "cause": "/dir1", 7 "errorCode": 108, 8 "index": 67, 9 "message": "Directory not empty" 10 } 11 [root@vStack ~]# curl 'http://127.0.0.1:2379/v2/keys/dir1?dir=true&amp;recursive=true' -XDELETE | python -m json.tool 12 % Total % Received % Xferd Average Speed Time Time Time Current 13 Dload Upload Total Spent Left Speed 14 100 166 100 166 0 0 62032 0 --:--:-- --:--:-- --:--:-- 83000 15 { 16 "action": "delete", 17 "node": { 18 "createdIndex": 67, 19 "dir": true, 20 "key": "/dir1", 21 "modifiedIndex": 68 22 }, 23 "prevNode": { 24 "createdIndex": 67, 25 "dir": true, 26 "key": "/dir1", 27 "modifiedIndex": 67 28 } 29 } &nbsp; 15. PUT 创建定时删除的目录：就跟定时删除某个键类似。如果目录因为超时被删除了，其下的所有内容也自动超时删除。 &nbsp; &nbsp; &nbsp; 如果目录存在，创建时，返回 102 错误码 &nbsp; &nbsp; &nbsp; -d ttl=xx 1 [root@vStack ~]# curl http://127.0.0.1:2379/v2/keys/dir -XPUT -d ttl=30 -d dir=true | python -m json.tool 2 % Total % Received % Xferd Average Speed Time Time Time Current 3 Dload Upload Total Spent Left Speed 4 100 157 100 142 100 15 22873 2416 --:--:-- --:--:-- --:--:-- 28400 5 { 6 "action": "set", 7 "node": { 8 "createdIndex": 52, 9 "dir": true, 10 "expiration": "2016-04-23T13:37:51.502289114Z", 11 "key": "/dir", 12 "modifiedIndex": 52, 13 "ttl": 30 14 } 15 } &nbsp; 16. PUT 设置刷新目录超时时间 &nbsp;&nbsp;开始创建时，没有设置ttl， 或刷新已设置ttl的目录的ttl的值。 -d ttl=xxx 设置或刷新的ttl值。 ttl为空是，取消ttl。 -d prevExist=true &nbsp; &nbsp;必选参数，否者报错102错误码 会触发watcher事件。 1 [root@vStack ~]# curl http://127.0.0.1:2379/v2/keys/dir -XPUT -d ttl=30 -d dir=true -d prevExist=true | python -m json.tool 2 % Total % Received % Xferd Average Speed Time Time Time Current 3 Dload Upload Total Spent Left Speed 4 100 304 100 274 100 30 60392 6612 --:--:-- --:--:-- --:--:-- 91333 5 { 6 "action": "update", 7 "node": { 8 "createdIndex": 56, 9 "dir": true, 10 "expiration": "2016-04-23T13:42:56.395923381Z", 11 "key": "/dir", 12 "modifiedIndex": 61, 13 "ttl": 30 14 }, 15 "prevNode": { 16 "createdIndex": 56, 17 "dir": true, 18 "expiration": "2016-04-23T13:42:46.225222674Z", 19 "key": "/dir", 20 "modifiedIndex": 56, 21 "ttl": 20 22 } 23 } 当ttl时间到后，watcher将收到一个"expire" action. &nbsp; 1 [root@vStack ~]# curl http://127.0.0.1:2379/v2/keys/dir?wait=true | python -m json.tool 2 % Total % Received % Xferd Average Speed Time Time Time Current 3 Dload Upload Total Spent Left Speed 4 100 207 0 207 0 0 16 0 --:--:-- 0:00:12 --:--:-- 43 5 { 6 "action": "expire", 7 "node": { 8 "createdIndex": 2219, 9 "key": "/dir", 10 "modifiedIndex": 2220 11 }, 12 "prevNode": { 13 "createdIndex": 2219, 14 "dir": true, 15 "expiration": "2016-12-28T09:22:35.853484071Z", 16 "key": "/dir", 17 "modifiedIndex": 2219 18 } 19 } &nbsp;&nbsp; 17. 创建一个隐藏节点：命名时名字以下划线_开头的key或目录，默认就是隐藏键。 &nbsp; &nbsp; &nbsp; list目录下时，将不显示。可以显示使用。 1 [root@vStack ~]# curl http://127.0.0.1:2379/v2/keys/_message -XPUT -d value="Hello hidden world" | python -m json.tool 2 % Total % Received % Xferd Average Speed Time Time Time Current 3 Dload Upload Total Spent Left Speed 4 100 134 100 110 100 24 46948 10243 --:--:-- --:--:-- --:--:-- 107k 5 { 6 "action": "set", 7 "node": { 8 "createdIndex": 69, 9 "key": "/_message", 10 "modifiedIndex": 69, 11 "value": "Hello hidden world" 12 } 13 } &nbsp;&nbsp; 注意： &nbsp; &nbsp; &nbsp; 1. api url 区分大小写，包括其中的参数。&nbsp; 2. 如果key存在，通过 curl http://IP:PORT/v2/keys/message001&nbsp;-XPUT -d dir=true , 将会把key调整为dir属性，value值为None； 增加 -d prevExist=false，将报105错误码。 修改为dir后，无法在恢复为key。 &nbsp; &nbsp; &nbsp;3. 不能对一个dir进行赋值，即&nbsp;curl http://127.0.0.1:2379/v2/keys/message001 -XPUT -d value=123 &nbsp; &nbsp;， 返回错误码 102， &ldquo;Not a file&rdquo; &nbsp; &nbsp; &nbsp;4. key相当于文件系统中的文件，可以赋值即向文件写内容。dir相当于文件系统的目录或路径，内容包括dir和key， 即文件系统中的目录和文件。 &nbsp; &nbsp; &nbsp;5. 在api url中的path，体现了存储结构。如果目录不存在，直接创建。如：curl http://127.0.0.1:2379/v2/keys/fst/sec/thr -XPUT -d value=123 &nbsp;中的fst、sec会自动创建为dir。&nbsp; &nbsp; &nbsp; &nbsp;6. 创建dir与key的区别，即在 curl的body中是否有 dir=true，有即为dir, 否认则key； dir存在时，value无效。 创建key时，value可以不存在。 &nbsp; &nbsp; &nbsp;7. 不能在key下创建dir或可以，否者报错误码：104，&ldquo;Not a directory&rdquo; &nbsp; &nbsp; &nbsp;8. 目录不能重复创建，即&nbsp;curl -v http://127.0.0.1:2379/v2/keys/message -XPUT -d dir=true &nbsp;如果 message 目录已经已经存在，返回错误码：102，&nbsp;&ldquo;Not a file&rdquo; &nbsp; &nbsp; &nbsp;9. 删除一个非空目录，返回错误码：102. 通过在url中增加 recursive=true 参数，可以参数非空目录。 &nbsp; Statistics &nbsp;统计接口 etcd 集群记录大量的统计数据，包括：延时(latency)，带宽和正常运行时间。统计功能通过统计端点(/stats)去理解一个集群的内部健康状态。 An etcd cluster keeps track of a number of statistics including latency, bandwidth and uptime. These are exposed via the statistics endpoint to understand the internal health of a cluster. Leader Statistics 领导点统计 &nbsp; 1 [root@localhost testectd]# curl http://127.0.0.1:2379/v2/stats/self | python -m json.tool 2 % Total % Received % Xferd Average Speed Time Time Time Current 3 Dload Upload Total Spent Left Speed 4 119 357 119 357 0 0 274k 0 --:--:-- --:--:-- --:--:-- 348k 5 { 6 "id": "45b967575ff25cb2", 7 "leaderInfo": { 8 "leader": "45b967575ff25cb2", 9 "startTime": "2016-12-29T20:15:13.811259537+08:00", 10 "uptime": "8m19.603722077s" 11 }, 12 "name": "infra0", 13 "recvAppendRequestCnt": 18, 14 "sendAppendRequestCnt": 3670, 15 "sendBandwidthRate": 123950.52498801574, 16 "sendPkgRate": 7.5456304767920797, 17 "startTime": "2016-12-29T20:14:29.300999352+08:00", 18 "state": "StateLeader" 19 } &nbsp; &nbsp; 1 [root@localhost testectd]# curl http://127.0.0.1:2379/v2/stats/leader | python -m json.tool 2 % Total % Received % Xferd Average Speed Time Time Time Current 3 Dload Upload Total Spent Left Speed 4 132 398 132 398 0 0 133k 0 --:--:-- --:--:-- --:--:-- 388k 5 { 6 "followers": { 7 "3c828782a67e0043": { 8 "counts": { 9 "fail": 1211, 10 "success": 0 11 }, 12 "latency": { 13 "average": 0, 14 "current": 0, 15 "maximum": 0, 16 "minimum": 9.2233720368547758e+18, 17 "standardDeviation": 0 18 } 19 }, 20 "b26f1b9a6c735437": { 21 "counts": { 22 "fail": 0, 23 "success": 3231 24 }, 25 "latency": { 26 "average": 0.0073246419065304607, 27 "current": 0.0032520000000000001, 28 "maximum": 1.713633, 29 "minimum": 0.0012520000000000001, 30 "standardDeviation": 0.035654606550540036 31 } 32 } 33 }, 34 "leader": "45b967575ff25cb2" 35 } &nbsp; Memeber API 1.&nbsp;List members 返回http 200 OK response，显示在 etcd 集群中的所有成员。 1 [root@vStack ~]# curl http://192.168.10.150:2379/v2/members | python -m json.tool 2 % Total % Received % Xferd Average Speed Time Time Time Current 3 Dload Upload Total Spent Left Speed 4 100 138 100 138 0 0 73287 0 --:--:-- --:--:-- --:--:-- 134k 5 { 6 "members": [ 7 { 8 "clientURLs": [ 9 "http://192.168.10.150:2379" 10 ], 11 "id": "8e9e05c52164694d", 12 "name": "default", 13 "peerURLs": [ 14 "http://localhost:2380" 15 ] 16 } 17 ] 18 } &nbsp; [root@vStack ~]# curl http://127.0.0.1:2379/v2/members | python -m json.tool % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 181 100 181 0 0 138k 0 --:--:-- --:--:-- --:--:-- 176k { "members": [ { "clientURLs": [ "http://localhost:2379", "http://localhost:4001" ], "id": "ce2a822cea30bfca", "name": "default", "peerURLs": [ "http://localhost:2380", "http://localhost:7001" ] } ] } &nbsp; 1 [root@vStack ~]# curl http://192.168.10.150:2379/v2/members | python -m json.tool 2 % Total % Received % Xferd Average Speed Time Time Time Current 3 Dload Upload Total Spent Left Speed 4 100 227 100 227 0 0 116k 0 --:--:-- --:--:-- --:--:-- 221k 5 { 6 "members": [ 7 { 8 "clientURLs": [], 9 "id": "755ef544f1926e2e", 10 "name": "", 11 "peerURLs": [ 12 "http://127.0.0.1:2380" 13 ] 14 }, 15 { 16 "clientURLs": [ 17 "http://192.168.10.150:2379" 18 ], 19 "id": "8e9e05c52164694d", 20 "name": "default", 21 "peerURLs": [ 22 "http://localhost:2380" 23 ] 24 } 25 ] 26 } &nbsp; 2. Add a member 成功时返回 HTTP 201 response 状态码，及新建入成员的信息，对新加入的成员生成一个成员id。 失败时，返回失败状态的字符描述。 &nbsp;Returns an HTTP 201 response code and the representation of added member with a newly generated a memberID when successful. Returns a string describing the failure condition when unsuccessful. If the POST body is malformed an HTTP 400 will be returned. If the member exists in the cluster or existed in the cluster at some point in the past an HTTP 409 will be returned. If any of the given peerURLs exists in the cluster an HTTP 409 will be returned. If the cluster fails to process the request within timeout an HTTP 500 will be returned, though the request may be processed later. &nbsp; 1 curl http://10.0.0.10:2379/v2/members -XPOST \ 2 -H "Content-Type: application/json" -d '{"peerURLs":["http://10.0.0.10:2380"]}' &nbsp; 1.&nbsp;&nbsp;需要在header中设置&nbsp;Content-Type: application/json， 否则会报 405 错误&nbsp;Unsupported Media Type 2. &nbsp;如果已经存在相同的peerURLs，直接返回当前存在相同peerURLs的member。 &nbsp; &nbsp; &nbsp; 3. 如果添加一个无法使用的peerURLs，导致服务挂掉，无法操作。重启也无法使用。解决方法删除物理文件，但这个会删除记录的数据，导致持久数据的就丢失。需要进一步寻求解决方法。 集群信息会记录到持久化信息文件中，重启问题依旧。除非使用不同的name或改变数据目录。 &nbsp; 3. Delete a member 从集群中删除一个memeber。 member ID 必须是一个64位整数的16位编码的字符串。成功时，返回 204 状态码和没有内容。失败时，返回404状态码和字符描述的失败情况。 从集群中删除一个不存在的member，返回500错误。集群处理失败请求，包括超时，返回一个500错误码。即使请求可能后面会处理。 1 [root@localhost testectd]# curl http://192.168.10.150:2379/v2/members/2ae1ee131894262b -XDELETE | python -m json.tool 2 % Total % Received % Xferd Average Speed Time Time Time Current 3 Dload Upload Total Spent Left Speed 4 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0 5 No JSON object could be decoded &nbsp; 1. 删除成员后，etcd使用的data-dir必须被删除。如下是删除最后一个member，etcd给出的输出，服务退出。 2016-12-29 16:10:59.544409 E | etcdserver: the member has been permanently removed from the cluster 2016-12-29 16:10:59.544480 I | etcdserver: the data-dir used by this member must be removed. 2. 通过etcdctl 删除一个成员后，服务会退出。通过 etcdctl重新加入，显示为unstart。 如果需要重新加入集群，先用命令加入，再启动，否则启动时报&nbsp;the member has been permanently removed from the cluster 加入后，启动前，需要删除其存储的数据(member id发生了改变，会将使用磁盘记录的id，与新加入的ID不一致)。并设置&nbsp;--initial-cluster-state existing 不能设置为 new 注意 cluster版本要一致。cluster{"etcdserver":"3.0.15","etcdcluster":"3.0.0"} 可以。 &nbsp; {"etcdserver":"2.3.7","etcdcluster":"2.3.0"} 出现：&nbsp;{"etcdserver":"3.0.15","etcdcluster":"2.3.0"} &nbsp;在集群系统中出现不同版本的member 以上删除重新加入的操作，高版本的可以，单低版本的不支持，报&nbsp; failed to find member 3c828782a67e0043 in cluster 34b660d543ad1445 无法发现其他member &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 即 在集群中，成员的版本不同。低版本的成员失败退出，重启启动可以重新加入集群。通过接口，被动从集群中移除，再次加入，只能停止所有的成员，删除其中磁盘数据，重新构建。导致数据的丢失，如何恢复？&nbsp;高版本的成员没有此问题。 &nbsp; 即 在集群中，成员的版本不同。集群版本降低为 低版本 如：{"etcdserver":"3.0.15","etcdcluster":"2.3.0"}， 低版本的成员退出后，集群版本升级为高版本：{"etcdserver":"3.0.15","etcdcluster":"3.0.0"}&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 其版本的首先启动时，使用--initial-cluster-state new； &nbsp;高版本的在启动时&nbsp;--initial-cluster-state&nbsp;existing 会报 集群版本不兼容。需要使用&nbsp;--initial-cluster-state new。 再次启动高版本的可以使用&nbsp;existing 。 &nbsp;构建集群时，采用就低版本，在高版本加入时，需要使用&nbsp;--initial-cluster-state new 或 不设置， 使用existing ，报集群不兼容。 &nbsp; &nbsp; 1 [root@centos7mini etcd]# ./etcdctl member remove 45b967575ff25cb2 2 Removed member 45b967575ff25cb2 from cluster 3 4 [root@centos7mini etcd]# ./etcdctl member add infra0 http://192.168.10.150:2380 5 Added member named infra0 with ID 700fb7bf97791e71 to cluster 6 7 ETCD_NAME="infra0" 8 ETCD_INITIAL_CLUSTER="infra3=http://192.168.10.184:2380,infra0=http://192.168.10.150:2380,infra1=http://192.168.10.231:2380" 9 ETCD_INITIAL_CLUSTER_STATE="existing" 10 11 [root@centos7mini etcd]# ./etcdctl member list 12 3c828782a67e0043: name=infra3 peerURLs=http://192.168.10.184:2380 clientURLs=http://192.168.10.184:2379 isLeader=true 13 700fb7bf97791e71[unstarted]: peerURLs=http://192.168.10.150:2380 14 b26f1b9a6c735437: name=infra1 peerURLs=http://192.168.10.231:2380 clientURLs=http://192.168.10.231:2379 isLeader=false 15 [root@centos7mini etcd]# &nbsp; 4. Change the peer urls of a member 修改已存在成员的peer urls。成员ID必须是一个64位整数的16进制显示的字符串。成功时，返回204状态码，空内容。失败时，返回失败状态字符描述。 修改的成员不存在，返回400 错误码。 如果提供的peerlURL存在，将返回409错误码。500错误： 集群处理超时。 1 [root@localhost etcd-v3.0.15-linux-amd64]# ./etcdctl member list 2 3c828782a67e0043: name=infra3 peerURLs=http://192.168.10.184:2380 clientURLs=http://192.168.10.184:2379 isLeader=false 3 45b967575ff25cb2: name=infra0 peerURLs=http://192.168.10.150:2380 clientURLs=http://192.168.10.150:2379 isLeader=true 4 b26f1b9a6c735437: name=infra1 peerURLs=http://192.168.10.231:2380 clientURLs=http://192.168.10.231:2379 isLeader=false 5 [root@localhost etcd-v3.0.15-linux-amd64]# 6 [root@localhost etcd-v3.0.15-linux-amd64]# curl http://192.168.10.150:2379/v2/members/b26f1b9a6c735437 -XPUT -H "Content-Type: application/json" -d '{"peerURLs":["http://127.0.0.1:2380"]}' 7 [root@localhost etcd-v3.0.15-linux-amd64]# 8 [root@localhost etcd-v3.0.15-linux-amd64]# ./etcdctl member list 9 3c828782a67e0043: name=infra3 peerURLs=http://192.168.10.184:2380 clientURLs=http://192.168.10.184:2379 isLeader=false 10 45b967575ff25cb2: name=infra0 peerURLs=http://192.168.10.150:2380 clientURLs=http://192.168.10.150:2379 isLeader=true 11 b26f1b9a6c735437: name=infra1 peerURLs=http://127.0.0.1:2380 clientURLs=http://192.168.10.231:2379 isLeader=false 12 [root@localhost etcd-v3.0.15-linux-amd64]# &nbsp; &nbsp; &nbsp; &nbsp; 在启动etcd设置&nbsp;--listen-client-urls 值时，请将localhost:2379或127.0.0.1:2379 设置，否者 本地etcdctl会报错如下 1 [root@centos7mini etcd]# ./etcdctl member list 2 Error: client: etcd cluster is unavailable or misconfigured 3 error #0: dial tcp 127.0.0.1:4001: getsockopt: connection refused 4 error #1: dial tcp 127.0.0.1:2379: getsockopt: connection refused &nbsp; &nbsp; 一个节点断开后，成为candicate，向其他member发起vote，重新选准 master/leader。 2016-12-29 19:43:15.767905 I | raft: b26f1b9a6c735437 became candidate at term 1462016-12-29 19:43:15.767932 I | raft: b26f1b9a6c735437 received vote from b26f1b9a6c735437 at term 1462016-12-29 19:43:15.767961 I | raft: b26f1b9a6c735437 [logterm: 101, index: 688] sent vote request to 45b967575ff25cb2 at term 1462016-12-29 19:43:17.266905 I | raft: b26f1b9a6c735437 is starting a new election at term 146 &nbsp; 将一个memeber加入两个集群时，出现 cluster id 匹配问题。以下是静态创建cluster。 1 # etcd --name infra1 --initial-advertise-peer-urls http://192.168.10.231:2380 \ 2 --listen-peer-urls http://192.168.10.231:2380 \ 3 --listen-client-urls http://192.168.10.231:2379,http://127.0.0.1:2379 \ 4 --advertise-client-urls http://192.168.10.231:2379 \ 5 --initial-cluster-token etcd-cluster-1 \ 6 --initial-cluster infra0=http://192.168.10.150:2380,infra1=http://192.168.10.231:2380,infra3=http://192.168.10.184:2380 \ 7 --initial-cluster-state new 8 9 # ./etcd --debug --name infra3 --initial-advertise-peer-urls http://192.168.10.184:2380 \ 10 --listen-peer-urls http://192.168.10.184:2380 --initial-cluster infra3=http://192.168.10.184:2380 \ 11 --listen-client-urls http://192.168.10.184:2379 --advertise-client-urls http://192.168.10.184:2379 \ 12 --initial-cluster-state new --initial-cluster-token etcd-cluster-1 &nbsp; &nbsp;2016-12-30 11:47:30.939730 E | rafthttp: request sent was ignored (cluster ID mismatch: remote[3c828782a67e0043]=625ac7f9082c643, local=34b660d543ad1445) 2016-12-30 11:47:30.977766 E | rafthttp: request sent was ignored (cluster ID mismatch: remote[3c828782a67e0043]=625ac7f9082c643, local=34b660d543ad1445) &nbsp; &nbsp; 如下建立集群后，--debug 提示：150上的iptables防火墙导致。 2016-12-30 12:07:19.479241 D | rafthttp: failed to dial 45b967575ff25cb2 on stream Message (dial tcp 192.168.10.150:2380: getsockopt: no route to host)2016-12-30 12:07:19.914614 D | rafthttp: failed to dial 45b967575ff25cb2 on stream MsgApp v2 (dial tcp 192.168.10.150:2380: i/o timeout)2016-12-30 12:07:20.781345 D | rafthttp: failed to dial 45b967575ff25cb2 on stream Message (dial tcp 192.168.10.150:2380: i/o timeout)2016-12-30 12:07:21.216792 D | rafthttp: failed to dial 45b967575ff25cb2 on stream MsgApp v2 (dial tcp 192.168.10.150:2380: i/o timeout)2016-12-30 12:07:21.885187 D | rafthttp: failed to dial 45b967575ff25cb2 on stream Message (dial tcp 192.168.10.150:2380: getsockopt: no route to host)2016-12-30 12:07:22.518689 D | rafthttp: failed to dial 45b967575ff25cb2 on stream MsgApp v2 (dial tcp 192.168.10.150:2380: i/o timeout)2016-12-30 12:07:22.620358 D | rafthttp: failed to dial 45b967575ff25cb2 on stream MsgApp v2 (dial tcp 192.168.10.150:2380: getsockopt: no route to host)2016-12-30 12:07:23.187832 D | rafthttp: failed to dial 45b967575ff25cb2 on stream Message (dial tcp 192.168.10.150:2380: i/o timeout)2016-12-30 12:07:23.925031 D | rafthttp: failed to dial 45b967575ff25cb2 on stream MsgApp v2 (dial tcp 192.168.10.150:2380: i/o timeout)2016-12-30 12:07:24.490547 D | rafthttp: failed to dial 45b967575ff25cb2 on stream Message (dial tcp 192.168.10.150:2380: i/o timeout)2016-12-30 12:07:24.592188 D | rafthttp: failed to dial 45b967575ff25cb2 on stream Message (dial tcp 192.168.10.150:2380: getsockopt: no route to host)2016-12-30 12:07:25.226673 D | rafthttp: failed to dial 45b967575ff25cb2 on stream MsgApp v2 (dial tcp 192.168.10.150:2380: i/o timeout)2016-12-30 12:07:25.696521 D | rafthttp: failed to dial 45b967575ff25cb2 on stream Message (dial tcp 192.168.10.150:2380: getsockopt: no route to host)2016-12-30 12:07:26.528616 D | rafthttp: failed to dial 45b967575ff25cb2 on stream MsgApp v2 (dial tcp 192.168.10.150:2380: i/o timeout)2016-12-30 12:07:26.630548 D | rafthttp: failed to dial 45b967575ff25cb2 on stream MsgApp v2 (dial tcp 192.168.10.150:2380: getsockopt: no route to host)2016-12-30 12:07:27.000087 D | rafthttp: failed to dial 45b967575ff25cb2 on stream Message (dial tcp 192.168.10.150:2380: i/o timeout)2016-12-30 12:07:27.932728 D | rafthttp: failed to dial 45b967575ff25cb2 on stream MsgApp v2 (dial tcp 192.168.10.150:2380: i/o timeout)2016-12-30 12:07:28.302774 D | rafthttp: failed to dial 45b967575ff25cb2 on stream Message (dial tcp 192.168.10.150:2380: i/o timeout)2016-12-30 12:07:28.404591 D | rafthttp: failed to dial 45b967575ff25cb2 on stream Message (dial tcp 192.168.10.150:2380: getsockopt: no r 如下调试信息是在leader 上的。 由于iptables防火墙的原因导致。 2016-12-30 14:03:10.838829 D | rafthttp: failed to dial 45b967575ff25cb2 on stream Message (dial tcp 192.168.10.150:2380: i/o timeout)2016-12-30 14:03:11.353601 W | etcdserver: failed to reach the peerURL(http://192.168.10.150:2380) of member 45b967575ff25cb2 (Get http://192.168.10.150:2380/version: dial tcp 192.168.10.150:2380: getsockopt: no route to host)2016-12-30 14:03:11.353640 W | etcdserver: cannot get the version of member 45b967575ff25cb2 (Get http://192.168.10.150:2380/version: dial tcp 192.168.10.150:2380: getsockopt: no route to host)2016-12-30 14:03:11.672262 D | rafthttp: failed to dial 45b967575ff25cb2 on stream MsgApp v2 (dial tcp 192.168.10.150:2380: i/o timeout)2016-12-30 14:03:12.140697 D | rafthttp: failed to dial 45b967575ff25cb2 on stream Message (dial tcp 192.168.10.150:2380: i/o timeout)2016-12-30 14:03:12.974912 D | rafthttp: failed to dial 45b967575ff25cb2 on stream MsgApp v2 (dial tcp 192.168.10.150:2380: i/o timeout)2016-12-30 14:03:13.445167 D | rafthttp: failed to dial 45b967575ff25cb2 on stream Message (dial tcp 192.168.10.150:2380: i/o timeout)2016-12-30 14:03:13.547340 D | rafthttp: failed to dial 45b967575ff25cb2 on stream Message (dial tcp 192.168.10.150:2380: getsockopt: no route to host)2016-12-30 14:03:14.278497 D | rafthttp: failed to dial 45b967575ff25cb2 on stream MsgApp v2 (dial tcp 192.168.10.150:2380: i/o timeout)2016-12-30 14:03:14.380259 D | rafthttp: failed to dial 45b967575ff25cb2 on stream MsgApp v2 (dial tcp 192.168.10.150:2380: getsockopt: no route to host)2016-12-30 14:03:14.850132 D | rafthttp: failed to dial 45b967575ff25cb2 on stream Message (dial tcp 192.168.10.150:2380: i/o timeout)2016-12-30 14:03:15.358565 W | etcdserver: failed to reach the peerURL(http://192.168.10.150:2380) of member 45b967575ff25cb2 (Get http://192.168.10.150:2380/version: dial tcp 192.168.10.150:2380: getsockopt: no route to host)2016-12-30 14:03:15.358613 W | etcdserver: cannot get the version of member 45b967575ff25cb2 (Get http://192.168.10.150:2380/version: dial tcp 192.168.10.150:2380: getsockopt: no route to host) 如下调试信息，是由于 192.168.10.231:2380 无法访问，&nbsp;b26f1b9a6c735437 为member id。 2016-12-30 21:38:43.130791 D | rafthttp: failed to dial b26f1b9a6c735437 on stream Message (dial tcp 192.168.10.231:2380: getsockopt: connection refused)2016-12-30 21:38:43.191227 D | rafthttp: failed to dial b26f1b9a6c735437 on stream MsgApp v2 (dial tcp 192.168.10.231:2380: getsockopt: connection refused)2016-12-30 21:38:43.232280 D | rafthttp: failed to dial b26f1b9a6c735437 on stream Message (dial tcp 192.168.10.231:2380: getsockopt: connection refused)2016-12-30 21:38:43.292289 D | rafthttp: failed to dial b26f1b9a6c735437 on stream MsgApp v2 (dial tcp 192.168.10.231:2380: getsockopt: connection refused)2016-12-30 21:38:43.334129 D | rafthttp: failed to dial b26f1b9a6c735437 on stream Message (dial tcp 192.168.10.231:2380: getsockopt: connection refused)2016-12-30 21:38:43.393576 D | rafthttp: failed to dial b26f1b9a6c735437 on stream MsgApp v2 (dial tcp 192.168.10.231:2380: getsockopt: connection refused) &nbsp; 2016-12-30 14:18:36.328628 W | rafthttp: the clock difference against peer 3c828782a67e0043 is too high [2.00405135s &gt; 1s]2016-12-30 14:19:06.329559 W | rafthttp: the clock difference against peer 3c828782a67e0043 is too high [2.003973758s &gt; 1s]2016-12-30 14:19:36.331189 W | rafthttp: the clock difference against peer 3c828782a67e0043 is too high [2.004098356s &gt; 1s] &nbsp; 2016-12-30 21:38:22.857546 W | rafthttp: the clock difference against peer 3c828782a67e0043 is too high [7h59m58.924003117s &gt; 1s] 2016-12-30 21:38:22.892541 W | rafthttp: health check for peer b26f1b9a6c735437 failed2016-12-30 21:38:22.892848 W | rafthttp: the clock difference against peer b26f1b9a6c735437 is too high [7h59m56.920465483s &gt; 1s] &nbsp; 转自: https://www.cnblogs.com/doscho/p/6227351.html]]></content>
      <categories>
        <category>NP</category>
      </categories>
      <tags>
        <tag>Etcd</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[gevent介绍]]></title>
    <url>%2Fblog%2F2019%2F07%2F09%2Fgevent_intro%2F</url>
    <content type="text"><![CDATA[. . . gevent的协程是基于greenlet的，并基于libev实现快速事件循环（Linux上是epoll，FreeBSD上是kqueue，Mac OS X上是select）。有了gevent，协程的使用将无比简单，你根本无须像greenlet一样显式的切换，每当一个协程阻塞时，程序将自动调度，gevent处理了所有的底层细节。让我们看个例子来感受下吧。 &lt;div id=&quot;crayon-5d24ce1c01df9805388809&quot; class=&quot;crayon-syntax crayon-theme-eclipse crayon-font-monaco crayon-os-pc print-yes notranslate&quot; data-settings=&quot; minimize scroll-mouseover&quot; style=&quot; margin-top: 12px; margin-bottom: 12px; font-size: 12px !important; line-height: 15px !important;&quot;&gt; &lt;div class=&quot;crayon-plain-wrap&quot;&gt;&lt;/div&gt; &lt;div class=&quot;crayon-main&quot; style=&quot;&quot;&gt; &lt;table class=&quot;crayon-table&quot;&gt; &lt;tr class=&quot;crayon-row&quot;&gt; &lt;td class=&quot;crayon-nums &quot; data-settings=&quot;show&quot;&gt; &lt;div class=&quot;crayon-nums-content&quot; style=&quot;font-size: 12px !important; line-height: 15px !important;&quot;&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01df9805388809-1&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01df9805388809-2&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01df9805388809-3&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01df9805388809-4&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01df9805388809-5&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01df9805388809-6&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01df9805388809-7&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01df9805388809-8&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01df9805388809-9&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01df9805388809-10&quot;&gt;10&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01df9805388809-11&quot;&gt;11&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01df9805388809-12&quot;&gt;12&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01df9805388809-13&quot;&gt;13&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01df9805388809-14&quot;&gt;14&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01df9805388809-15&quot;&gt;15&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01df9805388809-16&quot;&gt;16&lt;/div&gt;&lt;/div&gt; &lt;/td&gt; &lt;td class=&quot;crayon-code&quot;&gt;&lt;div class=&quot;crayon-pre&quot; style=&quot;font-size: 12px !important; line-height: 15px !important; -moz-tab-size:4; -o-tab-size:4; -webkit-tab-size:4; tab-size:4;&quot;&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01df9805388809-1&quot;&gt;&lt;span class=&quot;crayon-r&quot;&gt;import&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;gevent&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01df9805388809-2&quot;&gt;&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01df9805388809-3&quot;&gt;&lt;span class=&quot;crayon-r&quot;&gt;def&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;test1&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;crayon-o&quot;&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01df9805388809-4&quot;&gt;&lt;span class=&quot;crayon-h&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;crayon-k &quot;&gt;print&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-cn&quot;&gt;12&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01df9805388809-5&quot;&gt;&lt;span class=&quot;crayon-h&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;gevent&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;sleep&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-cn&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01df9805388809-6&quot;&gt;&lt;span class=&quot;crayon-h&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;crayon-k &quot;&gt;print&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-cn&quot;&gt;34&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01df9805388809-7&quot;&gt;&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01df9805388809-8&quot;&gt;&lt;span class=&quot;crayon-r&quot;&gt;def&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;test2&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;crayon-o&quot;&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01df9805388809-9&quot;&gt;&lt;span class=&quot;crayon-h&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;crayon-k &quot;&gt;print&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-cn&quot;&gt;56&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01df9805388809-10&quot;&gt;&lt;span class=&quot;crayon-h&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;gevent&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;sleep&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-cn&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01df9805388809-11&quot;&gt;&lt;span class=&quot;crayon-h&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;crayon-k &quot;&gt;print&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-cn&quot;&gt;78&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01df9805388809-12&quot;&gt;&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01df9805388809-13&quot;&gt;&lt;span class=&quot;crayon-v&quot;&gt;gevent&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;joinall&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;[&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01df9805388809-14&quot;&gt;&lt;span class=&quot;crayon-h&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;gevent&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;spawn&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;test1&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;,&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01df9805388809-15&quot;&gt;&lt;span class=&quot;crayon-h&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;gevent&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;spawn&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;test2&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;,&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01df9805388809-16&quot;&gt;&lt;span class=&quot;crayon-sy&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/div&gt; &lt;/div&gt; 解释下，&#8221;gevent.spawn()&#8221;方法会创建一个新的greenlet协程对象，并运行它。&#8221;gevent.joinall()&#8221;方法会等待所有传入的greenlet协程运行结束后再退出，这个方法可以接受一个&#8221;timeout&#8221;参数来设置超时时间，单位是秒。运行上面的程序，执行顺序如下： 先进入协程test1，打印12 遇到&#8221;gevent.sleep(0)&#8221;时，test1被阻塞，自动切换到协程test2，打印56 之后test2被阻塞，这时test1阻塞已结束，自动切换回test1，打印34 当test1运行完毕返回后，此时test2阻塞已结束，再自动切换回test2，打印78 所有协程执行完毕，程序退出 所以，程序运行下来的输出就是： 12 56 34 78 注意，这里与上一篇greenlet中第一个例子运行的结果不一样，greenlet一个协程运行完后，必须显式切换，不然会返回其父协程。而在gevent中，一个协程运行完后，它会自动调度那些未完成的协程。 我们换一个更有意义的例子： &lt;div id=&quot;crayon-5d24ce1c01e05798046415&quot; class=&quot;crayon-syntax crayon-theme-eclipse crayon-font-monaco crayon-os-pc print-yes notranslate&quot; data-settings=&quot; minimize scroll-mouseover&quot; style=&quot; margin-top: 12px; margin-bottom: 12px; font-size: 12px !important; line-height: 15px !important;&quot;&gt; &lt;div class=&quot;crayon-plain-wrap&quot;&gt;&lt;/div&gt; &lt;div class=&quot;crayon-main&quot; style=&quot;&quot;&gt; &lt;table class=&quot;crayon-table&quot;&gt; &lt;tr class=&quot;crayon-row&quot;&gt; &lt;td class=&quot;crayon-nums &quot; data-settings=&quot;show&quot;&gt; &lt;div class=&quot;crayon-nums-content&quot; style=&quot;font-size: 12px !important; line-height: 15px !important;&quot;&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e05798046415-1&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e05798046415-2&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e05798046415-3&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e05798046415-4&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e05798046415-5&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e05798046415-6&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e05798046415-7&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e05798046415-8&quot;&gt;8&lt;/div&gt;&lt;/div&gt; &lt;/td&gt; &lt;td class=&quot;crayon-code&quot;&gt;&lt;div class=&quot;crayon-pre&quot; style=&quot;font-size: 12px !important; line-height: 15px !important; -moz-tab-size:4; -o-tab-size:4; -webkit-tab-size:4; tab-size:4;&quot;&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e05798046415-1&quot;&gt;&lt;span class=&quot;crayon-r&quot;&gt;import&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;gevent&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e05798046415-2&quot;&gt;&lt;span class=&quot;crayon-r&quot;&gt;import&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-k &quot;&gt;socket&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e05798046415-3&quot;&gt;&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e05798046415-4&quot;&gt;&lt;span class=&quot;crayon-v&quot;&gt;urls&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;crayon-s&quot;&gt;&apos;www.baidu.com&apos;&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-s&quot;&gt;&apos;www.gevent.org&apos;&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-s&quot;&gt;&apos;www.python.org&apos;&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;]&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e05798046415-5&quot;&gt;&lt;span class=&quot;crayon-v&quot;&gt;jobs&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;gevent&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;spawn&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-k &quot;&gt;socket&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;gethostbyname&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-st&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;url &lt;/span&gt;&lt;span class=&quot;crayon-st&quot;&gt;in&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;urls&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;]&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e05798046415-6&quot;&gt;&lt;span class=&quot;crayon-v&quot;&gt;gevent&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;joinall&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;jobs&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;timeout&lt;/span&gt;&lt;span class=&quot;crayon-o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;crayon-cn&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e05798046415-7&quot;&gt;&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e05798046415-8&quot;&gt;&lt;span class=&quot;crayon-k &quot;&gt;print&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;job&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;value &lt;/span&gt;&lt;span class=&quot;crayon-st&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;job &lt;/span&gt;&lt;span class=&quot;crayon-st&quot;&gt;in&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;jobs&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;]&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/div&gt; &lt;/div&gt; 我们通过协程分别获取三个网站的IP地址，由于打开远程地址会引起IO阻塞，所以gevent会自动调度不同的协程。另外，我们可以通过协程对象的&#8221;value&#8221;属性，来获取协程函数的返回值。 猴子补丁 Monkey patching 细心的朋友们在运行上面例子时会发现，其实程序运行的时间同不用协程是一样的，是三个网站打开时间的总和。可是理论上协程是非阻塞的，那运行时间应该等于最长的那个网站打开时间呀？其实这是因为Python标准库里的socket是阻塞式的，DNS解析无法并发，包括像urllib库也一样，所以这种情况下用协程完全没意义。那怎么办？ 一种方法是使用gevent下的socket模块，我们可以通过&#8221;from gevent import socket&#8221;来导入。不过更常用的方法是使用猴子布丁（Monkey patching）: &lt;div id=&quot;crayon-5d24ce1c01e07499796964&quot; class=&quot;crayon-syntax crayon-theme-eclipse crayon-font-monaco crayon-os-pc print-yes notranslate&quot; data-settings=&quot; minimize scroll-mouseover&quot; style=&quot; margin-top: 12px; margin-bottom: 12px; font-size: 12px !important; line-height: 15px !important;&quot;&gt; &lt;div class=&quot;crayon-plain-wrap&quot;&gt;&lt;/div&gt; &lt;div class=&quot;crayon-main&quot; style=&quot;&quot;&gt; &lt;table class=&quot;crayon-table&quot;&gt; &lt;tr class=&quot;crayon-row&quot;&gt; &lt;td class=&quot;crayon-nums &quot; data-settings=&quot;show&quot;&gt; &lt;div class=&quot;crayon-nums-content&quot; style=&quot;font-size: 12px !important; line-height: 15px !important;&quot;&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e07499796964-1&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e07499796964-2&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e07499796964-3&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e07499796964-4&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e07499796964-5&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e07499796964-6&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e07499796964-7&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e07499796964-8&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e07499796964-9&quot;&gt;9&lt;/div&gt;&lt;/div&gt; &lt;/td&gt; &lt;td class=&quot;crayon-code&quot;&gt;&lt;div class=&quot;crayon-pre&quot; style=&quot;font-size: 12px !important; line-height: 15px !important; -moz-tab-size:4; -o-tab-size:4; -webkit-tab-size:4; tab-size:4;&quot;&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e07499796964-1&quot;&gt;&lt;span class=&quot;crayon-st&quot;&gt;from&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;gevent &lt;/span&gt;&lt;span class=&quot;crayon-r&quot;&gt;import&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;monkey&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;monkey&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;patch_socket&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e07499796964-2&quot;&gt;&lt;span class=&quot;crayon-r&quot;&gt;import&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;gevent&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e07499796964-3&quot;&gt;&lt;span class=&quot;crayon-r&quot;&gt;import&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-k &quot;&gt;socket&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e07499796964-4&quot;&gt;&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e07499796964-5&quot;&gt;&lt;span class=&quot;crayon-v&quot;&gt;urls&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;crayon-s&quot;&gt;&apos;www.baidu.com&apos;&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-s&quot;&gt;&apos;www.gevent.org&apos;&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-s&quot;&gt;&apos;www.python.org&apos;&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;]&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e07499796964-6&quot;&gt;&lt;span class=&quot;crayon-v&quot;&gt;jobs&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;gevent&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;spawn&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-k &quot;&gt;socket&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;gethostbyname&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-st&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;url &lt;/span&gt;&lt;span class=&quot;crayon-st&quot;&gt;in&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;urls&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;]&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e07499796964-7&quot;&gt;&lt;span class=&quot;crayon-v&quot;&gt;gevent&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;joinall&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;jobs&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;timeout&lt;/span&gt;&lt;span class=&quot;crayon-o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;crayon-cn&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e07499796964-8&quot;&gt;&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e07499796964-9&quot;&gt;&lt;span class=&quot;crayon-k &quot;&gt;print&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;job&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;value &lt;/span&gt;&lt;span class=&quot;crayon-st&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;job &lt;/span&gt;&lt;span class=&quot;crayon-st&quot;&gt;in&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;jobs&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;]&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/div&gt; &lt;/div&gt; 上述代码的第一行就是对socket标准库打上猴子补丁，此后socket标准库中的类和方法都会被替换成非阻塞式的，所有其他的代码都不用修改，这样协程的效率就真正体现出来了。Python中其它标准库也存在阻塞的情况，gevent提供了&#8221;monkey.patch_all()&#8221;方法将所有标准库都替换。 &lt;div id=&quot;crayon-5d24ce1c01e08835494690&quot; class=&quot;crayon-syntax crayon-theme-eclipse crayon-font-monaco crayon-os-pc print-yes notranslate&quot; data-settings=&quot; minimize scroll-mouseover&quot; style=&quot; margin-top: 12px; margin-bottom: 12px; font-size: 12px !important; line-height: 15px !important;&quot;&gt; &lt;div class=&quot;crayon-plain-wrap&quot;&gt;&lt;/div&gt; &lt;div class=&quot;crayon-main&quot; style=&quot;&quot;&gt; &lt;table class=&quot;crayon-table&quot;&gt; &lt;tr class=&quot;crayon-row&quot;&gt; &lt;td class=&quot;crayon-nums &quot; data-settings=&quot;show&quot;&gt; &lt;div class=&quot;crayon-nums-content&quot; style=&quot;font-size: 12px !important; line-height: 15px !important;&quot;&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e08835494690-1&quot;&gt;1&lt;/div&gt;&lt;/div&gt; &lt;/td&gt; &lt;td class=&quot;crayon-code&quot;&gt;&lt;div class=&quot;crayon-pre&quot; style=&quot;font-size: 12px !important; line-height: 15px !important; -moz-tab-size:4; -o-tab-size:4; -webkit-tab-size:4; tab-size:4;&quot;&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e08835494690-1&quot;&gt;&lt;span class=&quot;crayon-st&quot;&gt;from&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;gevent &lt;/span&gt;&lt;span class=&quot;crayon-r&quot;&gt;import&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;monkey&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;monkey&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;patch_all&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/div&gt; &lt;/div&gt; 使用猴子补丁褒贬不一，但是官网上还是建议使用&#8221;patch_all()&#8221;，而且在程序的第一行就执行。 获取协程状态 协程状态有已启动和已停止，分别可以用协程对象的&#8221;started&#8221;属性和&#8221;ready()&#8221;方法来判断。对于已停止的协程，可以用&#8221;successful()&#8221;方法来判断其是否成功运行且没抛异常。如果协程执行完有返回值，可以通过&#8221;value&#8221;属性来获取。另外，greenlet协程运行过程中发生的异常是不会被抛出到协程外的，因此需要用协程对象的&#8221;exception&#8221;属性来获取协程中的异常。下面的例子很好的演示了各种方法和属性的使用。 &lt;div id=&quot;crayon-5d24ce1c01e09760840873&quot; class=&quot;crayon-syntax crayon-theme-eclipse crayon-font-monaco crayon-os-pc print-yes notranslate&quot; data-settings=&quot; minimize scroll-mouseover&quot; style=&quot; margin-top: 12px; margin-bottom: 12px; font-size: 12px !important; line-height: 15px !important;&quot;&gt; &lt;div class=&quot;crayon-plain-wrap&quot;&gt;&lt;/div&gt; &lt;div class=&quot;crayon-main&quot; style=&quot;&quot;&gt; &lt;table class=&quot;crayon-table&quot;&gt; &lt;tr class=&quot;crayon-row&quot;&gt; &lt;td class=&quot;crayon-nums &quot; data-settings=&quot;show&quot;&gt; &lt;div class=&quot;crayon-nums-content&quot; style=&quot;font-size: 12px !important; line-height: 15px !important;&quot;&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e09760840873-1&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e09760840873-2&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e09760840873-3&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e09760840873-4&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e09760840873-5&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e09760840873-6&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e09760840873-7&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e09760840873-8&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e09760840873-9&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e09760840873-10&quot;&gt;10&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e09760840873-11&quot;&gt;11&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e09760840873-12&quot;&gt;12&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e09760840873-13&quot;&gt;13&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e09760840873-14&quot;&gt;14&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e09760840873-15&quot;&gt;15&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e09760840873-16&quot;&gt;16&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e09760840873-17&quot;&gt;17&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e09760840873-18&quot;&gt;18&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e09760840873-19&quot;&gt;19&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e09760840873-20&quot;&gt;20&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e09760840873-21&quot;&gt;21&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e09760840873-22&quot;&gt;22&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e09760840873-23&quot;&gt;23&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e09760840873-24&quot;&gt;24&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e09760840873-25&quot;&gt;25&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e09760840873-26&quot;&gt;26&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e09760840873-27&quot;&gt;27&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e09760840873-28&quot;&gt;28&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e09760840873-29&quot;&gt;29&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e09760840873-30&quot;&gt;30&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e09760840873-31&quot;&gt;31&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e09760840873-32&quot;&gt;32&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e09760840873-33&quot;&gt;33&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e09760840873-34&quot;&gt;34&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e09760840873-35&quot;&gt;35&lt;/div&gt;&lt;/div&gt; &lt;/td&gt; &lt;td class=&quot;crayon-code&quot;&gt;&lt;div class=&quot;crayon-pre&quot; style=&quot;font-size: 12px !important; line-height: 15px !important; -moz-tab-size:4; -o-tab-size:4; -webkit-tab-size:4; tab-size:4;&quot;&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e09760840873-1&quot;&gt;&lt;span class=&quot;crayon-c&quot;&gt;#coding:utf8&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e09760840873-2&quot;&gt;&lt;span class=&quot;crayon-r&quot;&gt;import&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;gevent&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e09760840873-3&quot;&gt;&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e09760840873-4&quot;&gt;&lt;span class=&quot;crayon-r&quot;&gt;def&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;win&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;crayon-o&quot;&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e09760840873-5&quot;&gt;&lt;span class=&quot;crayon-h&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;crayon-st&quot;&gt;return&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-s&quot;&gt;&apos;You win!&apos;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e09760840873-6&quot;&gt;&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e09760840873-7&quot;&gt;&lt;span class=&quot;crayon-r&quot;&gt;def&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;fail&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;crayon-o&quot;&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e09760840873-8&quot;&gt;&lt;span class=&quot;crayon-h&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;crayon-st&quot;&gt;raise&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-k &quot;&gt;Exception&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-s&quot;&gt;&apos;You failed!&apos;&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e09760840873-9&quot;&gt;&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e09760840873-10&quot;&gt;&lt;span class=&quot;crayon-v&quot;&gt;winner&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;gevent&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;spawn&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;win&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e09760840873-11&quot;&gt;&lt;span class=&quot;crayon-v&quot;&gt;loser&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;gevent&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;spawn&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;fail&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e09760840873-12&quot;&gt;&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e09760840873-13&quot;&gt;&lt;span class=&quot;crayon-k &quot;&gt;print&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;winner&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;started&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-c&quot;&gt;# True&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e09760840873-14&quot;&gt;&lt;span class=&quot;crayon-k &quot;&gt;print&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;loser&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;started&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;crayon-c&quot;&gt;# True&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e09760840873-15&quot;&gt;&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e09760840873-16&quot;&gt;&lt;span class=&quot;crayon-c&quot;&gt;# 在Greenlet中发生的异常，不会被抛到Greenlet外面。&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e09760840873-17&quot;&gt;&lt;span class=&quot;crayon-c&quot;&gt;# 控制台会打出Stacktrace，但程序不会停止&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e09760840873-18&quot;&gt;&lt;span class=&quot;crayon-st&quot;&gt;try&lt;/span&gt;&lt;span class=&quot;crayon-o&quot;&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e09760840873-19&quot;&gt;&lt;span class=&quot;crayon-h&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;gevent&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;joinall&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;winner&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;loser&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e09760840873-20&quot;&gt;&lt;span class=&quot;crayon-st&quot;&gt;except&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-k &quot;&gt;Exception&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-st&quot;&gt;as&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;crayon-o&quot;&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e09760840873-21&quot;&gt;&lt;span class=&quot;crayon-h&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;crayon-c&quot;&gt;# 这段永远不会被执行&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e09760840873-22&quot;&gt;&lt;span class=&quot;crayon-h&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;crayon-k &quot;&gt;print&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-s&quot;&gt;&apos;This will never be reached&apos;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e09760840873-23&quot;&gt;&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e09760840873-24&quot;&gt;&lt;span class=&quot;crayon-k &quot;&gt;print&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;winner&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;ready&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-c&quot;&gt;# True&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e09760840873-25&quot;&gt;&lt;span class=&quot;crayon-k &quot;&gt;print&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;loser&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;ready&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;crayon-c&quot;&gt;# True&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e09760840873-26&quot;&gt;&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e09760840873-27&quot;&gt;&lt;span class=&quot;crayon-k &quot;&gt;print&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;winner&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-c&quot;&gt;# &apos;You win!&apos;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e09760840873-28&quot;&gt;&lt;span class=&quot;crayon-k &quot;&gt;print&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;loser&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;crayon-c&quot;&gt;# None&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e09760840873-29&quot;&gt;&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e09760840873-30&quot;&gt;&lt;span class=&quot;crayon-k &quot;&gt;print&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;winner&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;successful&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-c&quot;&gt;# True&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e09760840873-31&quot;&gt;&lt;span class=&quot;crayon-k &quot;&gt;print&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;loser&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;successful&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;crayon-c&quot;&gt;# False&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e09760840873-32&quot;&gt;&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e09760840873-33&quot;&gt;&lt;span class=&quot;crayon-c&quot;&gt;# 这里可以通过raise loser.exception 或 loser.get()&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e09760840873-34&quot;&gt;&lt;span class=&quot;crayon-c&quot;&gt;# 来将协程中的异常抛出&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e09760840873-35&quot;&gt;&lt;span class=&quot;crayon-k &quot;&gt;print&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;loser&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;crayon-k &quot;&gt;exception&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/div&gt; &lt;/div&gt; 协程运行超时 之前我们讲过在&#8221;gevent.joinall()&#8221;方法中可以传入timeout参数来设置超时，我们也可以在全局范围内设置超时时间： &lt;div id=&quot;crayon-5d24ce1c01e0a365079033&quot; class=&quot;crayon-syntax crayon-theme-eclipse crayon-font-monaco crayon-os-pc print-yes notranslate&quot; data-settings=&quot; minimize scroll-mouseover&quot; style=&quot; margin-top: 12px; margin-bottom: 12px; font-size: 12px !important; line-height: 15px !important;&quot;&gt; &lt;div class=&quot;crayon-plain-wrap&quot;&gt;&lt;/div&gt; &lt;div class=&quot;crayon-main&quot; style=&quot;&quot;&gt; &lt;table class=&quot;crayon-table&quot;&gt; &lt;tr class=&quot;crayon-row&quot;&gt; &lt;td class=&quot;crayon-nums &quot; data-settings=&quot;show&quot;&gt; &lt;div class=&quot;crayon-nums-content&quot; style=&quot;font-size: 12px !important; line-height: 15px !important;&quot;&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e0a365079033-1&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e0a365079033-2&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e0a365079033-3&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e0a365079033-4&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e0a365079033-5&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e0a365079033-6&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e0a365079033-7&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e0a365079033-8&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e0a365079033-9&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e0a365079033-10&quot;&gt;10&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e0a365079033-11&quot;&gt;11&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e0a365079033-12&quot;&gt;12&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e0a365079033-13&quot;&gt;13&lt;/div&gt;&lt;/div&gt; &lt;/td&gt; &lt;td class=&quot;crayon-code&quot;&gt;&lt;div class=&quot;crayon-pre&quot; style=&quot;font-size: 12px !important; line-height: 15px !important; -moz-tab-size:4; -o-tab-size:4; -webkit-tab-size:4; tab-size:4;&quot;&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e0a365079033-1&quot;&gt;&lt;span class=&quot;crayon-r&quot;&gt;import&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;gevent&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e0a365079033-2&quot;&gt;&lt;span class=&quot;crayon-st&quot;&gt;from&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;gevent &lt;/span&gt;&lt;span class=&quot;crayon-r&quot;&gt;import&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;Timeout&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e0a365079033-3&quot;&gt;&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e0a365079033-4&quot;&gt;&lt;span class=&quot;crayon-v&quot;&gt;timeout&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;Timeout&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-cn&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;crayon-c&quot;&gt;# 2 seconds&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e0a365079033-5&quot;&gt;&lt;span class=&quot;crayon-v&quot;&gt;timeout&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e0a365079033-6&quot;&gt;&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e0a365079033-7&quot;&gt;&lt;span class=&quot;crayon-r&quot;&gt;def&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;wait&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;crayon-o&quot;&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e0a365079033-8&quot;&gt;&lt;span class=&quot;crayon-h&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;gevent&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;sleep&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-cn&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e0a365079033-9&quot;&gt;&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e0a365079033-10&quot;&gt;&lt;span class=&quot;crayon-st&quot;&gt;try&lt;/span&gt;&lt;span class=&quot;crayon-o&quot;&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e0a365079033-11&quot;&gt;&lt;span class=&quot;crayon-h&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;gevent&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;spawn&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;wait&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e0a365079033-12&quot;&gt;&lt;span class=&quot;crayon-st&quot;&gt;except&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;Timeout&lt;/span&gt;&lt;span class=&quot;crayon-o&quot;&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e0a365079033-13&quot;&gt;&lt;span class=&quot;crayon-h&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;crayon-k &quot;&gt;print&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-s&quot;&gt;&apos;Could not complete&apos;&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/div&gt; &lt;/div&gt; 上例中，我们将超时设为2秒，此后所有协程的运行，如果超过两秒就会抛出&#8221;Timeout&#8221;异常。我们也可以将超时设置在with语句内，这样该设置只在with语句块中有效： &lt;div id=&quot;crayon-5d24ce1c01e0c355039458&quot; class=&quot;crayon-syntax crayon-theme-eclipse crayon-font-monaco crayon-os-pc print-yes notranslate&quot; data-settings=&quot; minimize scroll-mouseover&quot; style=&quot; margin-top: 12px; margin-bottom: 12px; font-size: 12px !important; line-height: 15px !important;&quot;&gt; &lt;div class=&quot;crayon-plain-wrap&quot;&gt;&lt;/div&gt; &lt;div class=&quot;crayon-main&quot; style=&quot;&quot;&gt; &lt;table class=&quot;crayon-table&quot;&gt; &lt;tr class=&quot;crayon-row&quot;&gt; &lt;td class=&quot;crayon-nums &quot; data-settings=&quot;show&quot;&gt; &lt;div class=&quot;crayon-nums-content&quot; style=&quot;font-size: 12px !important; line-height: 15px !important;&quot;&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e0c355039458-1&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e0c355039458-2&quot;&gt;2&lt;/div&gt;&lt;/div&gt; &lt;/td&gt; &lt;td class=&quot;crayon-code&quot;&gt;&lt;div class=&quot;crayon-pre&quot; style=&quot;font-size: 12px !important; line-height: 15px !important; -moz-tab-size:4; -o-tab-size:4; -webkit-tab-size:4; tab-size:4;&quot;&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e0c355039458-1&quot;&gt;&lt;span class=&quot;crayon-st&quot;&gt;with&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;Timeout&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-cn&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;crayon-o&quot;&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e0c355039458-2&quot;&gt;&lt;span class=&quot;crayon-h&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;gevent&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;sleep&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-cn&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/div&gt; &lt;/div&gt; 此外，我们可以指定超时所抛出的异常，来替换默认的&#8221;Timeout&#8221;异常。比如下例中超时就会抛出我们自定义的&#8221;TooLong&#8221;异常。 &lt;div id=&quot;crayon-5d24ce1c01e0d244222216&quot; class=&quot;crayon-syntax crayon-theme-eclipse crayon-font-monaco crayon-os-pc print-yes notranslate&quot; data-settings=&quot; minimize scroll-mouseover&quot; style=&quot; margin-top: 12px; margin-bottom: 12px; font-size: 12px !important; line-height: 15px !important;&quot;&gt; &lt;div class=&quot;crayon-plain-wrap&quot;&gt;&lt;/div&gt; &lt;div class=&quot;crayon-main&quot; style=&quot;&quot;&gt; &lt;table class=&quot;crayon-table&quot;&gt; &lt;tr class=&quot;crayon-row&quot;&gt; &lt;td class=&quot;crayon-nums &quot; data-settings=&quot;show&quot;&gt; &lt;div class=&quot;crayon-nums-content&quot; style=&quot;font-size: 12px !important; line-height: 15px !important;&quot;&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e0d244222216-1&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e0d244222216-2&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e0d244222216-3&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e0d244222216-4&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e0d244222216-5&quot;&gt;5&lt;/div&gt;&lt;/div&gt; &lt;/td&gt; &lt;td class=&quot;crayon-code&quot;&gt;&lt;div class=&quot;crayon-pre&quot; style=&quot;font-size: 12px !important; line-height: 15px !important; -moz-tab-size:4; -o-tab-size:4; -webkit-tab-size:4; tab-size:4;&quot;&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e0d244222216-1&quot;&gt;&lt;span class=&quot;crayon-t&quot;&gt;class&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;TooLong&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-k &quot;&gt;Exception&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;crayon-o&quot;&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e0d244222216-2&quot;&gt;&lt;span class=&quot;crayon-h&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;crayon-r&quot;&gt;pass&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e0d244222216-3&quot;&gt;&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e0d244222216-4&quot;&gt;&lt;span class=&quot;crayon-st&quot;&gt;with&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;Timeout&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-cn&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;TooLong&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;crayon-o&quot;&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e0d244222216-5&quot;&gt;&lt;span class=&quot;crayon-h&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;gevent&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;sleep&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-cn&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/div&gt; &lt;/div&gt; 协程间通讯 greenlet协程间的异步通讯可以使用事件（Event）对象。该对象的&#8221;wait()&#8221;方法可以阻塞当前协程，而&#8221;set()&#8221;方法可以唤醒之前阻塞的协程。在下面的例子中，5个waiter协程都会等待事件evt，当setter协程在3秒后设置evt事件，所有的waiter协程即被唤醒。 &lt;div id=&quot;crayon-5d24ce1c01e0e766453087&quot; class=&quot;crayon-syntax crayon-theme-eclipse crayon-font-monaco crayon-os-pc print-yes notranslate&quot; data-settings=&quot; minimize scroll-mouseover&quot; style=&quot; margin-top: 12px; margin-bottom: 12px; font-size: 12px !important; line-height: 15px !important;&quot;&gt; &lt;div class=&quot;crayon-plain-wrap&quot;&gt;&lt;/div&gt; &lt;div class=&quot;crayon-main&quot; style=&quot;&quot;&gt; &lt;table class=&quot;crayon-table&quot;&gt; &lt;tr class=&quot;crayon-row&quot;&gt; &lt;td class=&quot;crayon-nums &quot; data-settings=&quot;show&quot;&gt; &lt;div class=&quot;crayon-nums-content&quot; style=&quot;font-size: 12px !important; line-height: 15px !important;&quot;&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e0e766453087-1&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e0e766453087-2&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e0e766453087-3&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e0e766453087-4&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e0e766453087-5&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e0e766453087-6&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e0e766453087-7&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e0e766453087-8&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e0e766453087-9&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e0e766453087-10&quot;&gt;10&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e0e766453087-11&quot;&gt;11&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e0e766453087-12&quot;&gt;12&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e0e766453087-13&quot;&gt;13&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e0e766453087-14&quot;&gt;14&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e0e766453087-15&quot;&gt;15&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e0e766453087-16&quot;&gt;16&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e0e766453087-17&quot;&gt;17&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e0e766453087-18&quot;&gt;18&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e0e766453087-19&quot;&gt;19&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e0e766453087-20&quot;&gt;20&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e0e766453087-21&quot;&gt;21&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e0e766453087-22&quot;&gt;22&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e0e766453087-23&quot;&gt;23&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e0e766453087-24&quot;&gt;24&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e0e766453087-25&quot;&gt;25&lt;/div&gt;&lt;/div&gt; &lt;/td&gt; &lt;td class=&quot;crayon-code&quot;&gt;&lt;div class=&quot;crayon-pre&quot; style=&quot;font-size: 12px !important; line-height: 15px !important; -moz-tab-size:4; -o-tab-size:4; -webkit-tab-size:4; tab-size:4;&quot;&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e0e766453087-1&quot;&gt;&lt;span class=&quot;crayon-c&quot;&gt;#coding:utf8&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e0e766453087-2&quot;&gt;&lt;span class=&quot;crayon-r&quot;&gt;import&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;gevent&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e0e766453087-3&quot;&gt;&lt;span class=&quot;crayon-st&quot;&gt;from&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;gevent&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;event &lt;/span&gt;&lt;span class=&quot;crayon-r&quot;&gt;import&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;Event&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e0e766453087-4&quot;&gt;&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e0e766453087-5&quot;&gt;&lt;span class=&quot;crayon-v&quot;&gt;evt&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;Event&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e0e766453087-6&quot;&gt;&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e0e766453087-7&quot;&gt;&lt;span class=&quot;crayon-r&quot;&gt;def&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;setter&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;crayon-o&quot;&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e0e766453087-8&quot;&gt;&lt;span class=&quot;crayon-h&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;crayon-k &quot;&gt;print&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-s&quot;&gt;&apos;Wait for me&apos;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e0e766453087-9&quot;&gt;&lt;span class=&quot;crayon-h&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;gevent&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;sleep&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-cn&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;crayon-c&quot;&gt;# 3秒后唤醒所有在evt上等待的协程&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e0e766453087-10&quot;&gt;&lt;span class=&quot;crayon-h&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;crayon-k &quot;&gt;print&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-s&quot;&gt;&quot;Ok, I&apos;m done&quot;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e0e766453087-11&quot;&gt;&lt;span class=&quot;crayon-h&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;evt&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;crayon-k &quot;&gt;set&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;crayon-c&quot;&gt;# 唤醒&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e0e766453087-12&quot;&gt;&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e0e766453087-13&quot;&gt;&lt;span class=&quot;crayon-r&quot;&gt;def&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;waiter&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;crayon-o&quot;&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e0e766453087-14&quot;&gt;&lt;span class=&quot;crayon-h&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;crayon-k &quot;&gt;print&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-s&quot;&gt;&quot;I&apos;ll wait for you&quot;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e0e766453087-15&quot;&gt;&lt;span class=&quot;crayon-h&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;evt&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;wait&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;crayon-c&quot;&gt;# 等待&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e0e766453087-16&quot;&gt;&lt;span class=&quot;crayon-h&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;crayon-k &quot;&gt;print&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-s&quot;&gt;&apos;Finish waiting&apos;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e0e766453087-17&quot;&gt;&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e0e766453087-18&quot;&gt;&lt;span class=&quot;crayon-v&quot;&gt;gevent&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;joinall&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;[&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e0e766453087-19&quot;&gt;&lt;span class=&quot;crayon-h&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;gevent&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;spawn&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;setter&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;,&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e0e766453087-20&quot;&gt;&lt;span class=&quot;crayon-h&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;gevent&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;spawn&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;waiter&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;,&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e0e766453087-21&quot;&gt;&lt;span class=&quot;crayon-h&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;gevent&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;spawn&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;waiter&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;,&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e0e766453087-22&quot;&gt;&lt;span class=&quot;crayon-h&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;gevent&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;spawn&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;waiter&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;,&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e0e766453087-23&quot;&gt;&lt;span class=&quot;crayon-h&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;gevent&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;spawn&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;waiter&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;,&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e0e766453087-24&quot;&gt;&lt;span class=&quot;crayon-h&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;gevent&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;spawn&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;waiter&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e0e766453087-25&quot;&gt;&lt;span class=&quot;crayon-sy&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/div&gt; &lt;/div&gt; 除了Event事件外，gevent还提供了AsyncResult事件，它可以在唤醒时传递消息。让我们将上例中的setter和waiter作如下改动: &lt;div id=&quot;crayon-5d24ce1c01e0f710879315&quot; class=&quot;crayon-syntax crayon-theme-eclipse crayon-font-monaco crayon-os-pc print-yes notranslate&quot; data-settings=&quot; minimize scroll-mouseover&quot; style=&quot; margin-top: 12px; margin-bottom: 12px; font-size: 12px !important; line-height: 15px !important;&quot;&gt; &lt;div class=&quot;crayon-plain-wrap&quot;&gt;&lt;/div&gt; &lt;div class=&quot;crayon-main&quot; style=&quot;&quot;&gt; &lt;table class=&quot;crayon-table&quot;&gt; &lt;tr class=&quot;crayon-row&quot;&gt; &lt;td class=&quot;crayon-nums &quot; data-settings=&quot;show&quot;&gt; &lt;div class=&quot;crayon-nums-content&quot; style=&quot;font-size: 12px !important; line-height: 15px !important;&quot;&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e0f710879315-1&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e0f710879315-2&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e0f710879315-3&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e0f710879315-4&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e0f710879315-5&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e0f710879315-6&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e0f710879315-7&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e0f710879315-8&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e0f710879315-9&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e0f710879315-10&quot;&gt;10&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e0f710879315-11&quot;&gt;11&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e0f710879315-12&quot;&gt;12&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e0f710879315-13&quot;&gt;13&lt;/div&gt;&lt;/div&gt; &lt;/td&gt; &lt;td class=&quot;crayon-code&quot;&gt;&lt;div class=&quot;crayon-pre&quot; style=&quot;font-size: 12px !important; line-height: 15px !important; -moz-tab-size:4; -o-tab-size:4; -webkit-tab-size:4; tab-size:4;&quot;&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e0f710879315-1&quot;&gt;&lt;span class=&quot;crayon-st&quot;&gt;from&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;gevent&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;event &lt;/span&gt;&lt;span class=&quot;crayon-r&quot;&gt;import&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;AsyncResult&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e0f710879315-2&quot;&gt;&lt;span class=&quot;crayon-v&quot;&gt;aevt&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;AsyncResult&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e0f710879315-3&quot;&gt;&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e0f710879315-4&quot;&gt;&lt;span class=&quot;crayon-r&quot;&gt;def&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;setter&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;crayon-o&quot;&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e0f710879315-5&quot;&gt;&lt;span class=&quot;crayon-h&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;crayon-k &quot;&gt;print&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-s&quot;&gt;&apos;Wait for me&apos;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e0f710879315-6&quot;&gt;&lt;span class=&quot;crayon-h&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;gevent&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;sleep&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-cn&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;crayon-c&quot;&gt;# 3秒后唤醒所有在evt上等待的协程&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e0f710879315-7&quot;&gt;&lt;span class=&quot;crayon-h&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;crayon-k &quot;&gt;print&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-s&quot;&gt;&quot;Ok, I&apos;m done&quot;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e0f710879315-8&quot;&gt;&lt;span class=&quot;crayon-h&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;aevt&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;crayon-k &quot;&gt;set&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-s&quot;&gt;&apos;Hello!&apos;&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;crayon-c&quot;&gt;# 唤醒，并传递消息&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e0f710879315-9&quot;&gt;&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e0f710879315-10&quot;&gt;&lt;span class=&quot;crayon-r&quot;&gt;def&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;waiter&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;crayon-o&quot;&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e0f710879315-11&quot;&gt;&lt;span class=&quot;crayon-h&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;crayon-k &quot;&gt;print&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-s&quot;&gt;&quot;I&apos;ll wait for you&quot;&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e0f710879315-12&quot;&gt;&lt;span class=&quot;crayon-h&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;message&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;aevt&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;crayon-c&quot;&gt;# 等待，并在唤醒时获取消息&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e0f710879315-13&quot;&gt;&lt;span class=&quot;crayon-h&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;crayon-k &quot;&gt;print&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-s&quot;&gt;&apos;Got wake up message: %s&apos;&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;message&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/div&gt; &lt;/div&gt; 队列 Queue 队列Queue的概念相信大家都知道，我们可以用它的put和get方法来存取队列中的元素。gevent的队列对象可以让greenlet协程之间安全的访问。运行下面的程序，你会看到3个消费者会分别消费队列中的产品，且消费过的产品不会被另一个消费者再取到： &lt;div id=&quot;crayon-5d24ce1c01e10976740165&quot; class=&quot;crayon-syntax crayon-theme-eclipse crayon-font-monaco crayon-os-pc print-yes notranslate&quot; data-settings=&quot; minimize scroll-mouseover&quot; style=&quot; margin-top: 12px; margin-bottom: 12px; font-size: 12px !important; line-height: 15px !important;&quot;&gt; &lt;div class=&quot;crayon-plain-wrap&quot;&gt;&lt;/div&gt; &lt;div class=&quot;crayon-main&quot; style=&quot;&quot;&gt; &lt;table class=&quot;crayon-table&quot;&gt; &lt;tr class=&quot;crayon-row&quot;&gt; &lt;td class=&quot;crayon-nums &quot; data-settings=&quot;show&quot;&gt; &lt;div class=&quot;crayon-nums-content&quot; style=&quot;font-size: 12px !important; line-height: 15px !important;&quot;&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e10976740165-1&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e10976740165-2&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e10976740165-3&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e10976740165-4&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e10976740165-5&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e10976740165-6&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e10976740165-7&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e10976740165-8&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e10976740165-9&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e10976740165-10&quot;&gt;10&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e10976740165-11&quot;&gt;11&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e10976740165-12&quot;&gt;12&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e10976740165-13&quot;&gt;13&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e10976740165-14&quot;&gt;14&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e10976740165-15&quot;&gt;15&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e10976740165-16&quot;&gt;16&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e10976740165-17&quot;&gt;17&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e10976740165-18&quot;&gt;18&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e10976740165-19&quot;&gt;19&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e10976740165-20&quot;&gt;20&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e10976740165-21&quot;&gt;21&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e10976740165-22&quot;&gt;22&lt;/div&gt;&lt;/div&gt; &lt;/td&gt; &lt;td class=&quot;crayon-code&quot;&gt;&lt;div class=&quot;crayon-pre&quot; style=&quot;font-size: 12px !important; line-height: 15px !important; -moz-tab-size:4; -o-tab-size:4; -webkit-tab-size:4; tab-size:4;&quot;&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e10976740165-1&quot;&gt;&lt;span class=&quot;crayon-r&quot;&gt;import&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;gevent&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e10976740165-2&quot;&gt;&lt;span class=&quot;crayon-st&quot;&gt;from&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;gevent&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;crayon-k &quot;&gt;queue&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-r&quot;&gt;import&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-k &quot;&gt;Queue&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e10976740165-3&quot;&gt;&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e10976740165-4&quot;&gt;&lt;span class=&quot;crayon-v&quot;&gt;products&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-k &quot;&gt;Queue&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e10976740165-5&quot;&gt;&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e10976740165-6&quot;&gt;&lt;span class=&quot;crayon-r&quot;&gt;def&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;consumer&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;crayon-o&quot;&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e10976740165-7&quot;&gt;&lt;span class=&quot;crayon-h&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;crayon-st&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-st&quot;&gt;not&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;products&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;empty&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;crayon-o&quot;&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e10976740165-8&quot;&gt;&lt;span class=&quot;crayon-h&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;crayon-k &quot;&gt;print&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-s&quot;&gt;&apos;%s got product %s&apos;&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;products&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e10976740165-9&quot;&gt;&lt;span class=&quot;crayon-h&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;gevent&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;sleep&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-cn&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e10976740165-10&quot;&gt;&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e10976740165-11&quot;&gt;&lt;span class=&quot;crayon-h&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;crayon-k &quot;&gt;print&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-s&quot;&gt;&apos;%s Quit&apos;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e10976740165-12&quot;&gt;&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e10976740165-13&quot;&gt;&lt;span class=&quot;crayon-r&quot;&gt;def&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;producer&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;crayon-o&quot;&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e10976740165-14&quot;&gt;&lt;span class=&quot;crayon-h&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;crayon-st&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-i&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-st&quot;&gt;in&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-k &quot;&gt;xrange&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-cn&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-cn&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;crayon-o&quot;&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e10976740165-15&quot;&gt;&lt;span class=&quot;crayon-h&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;products&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;put&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e10976740165-16&quot;&gt;&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e10976740165-17&quot;&gt;&lt;span class=&quot;crayon-v&quot;&gt;gevent&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;joinall&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;[&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e10976740165-18&quot;&gt;&lt;span class=&quot;crayon-h&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;gevent&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;spawn&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;producer&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;,&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e10976740165-19&quot;&gt;&lt;span class=&quot;crayon-h&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;gevent&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;spawn&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;consumer&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-s&quot;&gt;&apos;steve&apos;&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;,&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e10976740165-20&quot;&gt;&lt;span class=&quot;crayon-h&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;gevent&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;spawn&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;consumer&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-s&quot;&gt;&apos;john&apos;&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;,&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e10976740165-21&quot;&gt;&lt;span class=&quot;crayon-h&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;gevent&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;spawn&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;consumer&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-s&quot;&gt;&apos;nancy&apos;&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;,&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e10976740165-22&quot;&gt;&lt;span class=&quot;crayon-sy&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/div&gt; &lt;/div&gt; put和get方法都是阻塞式的，它们都有非阻塞的版本：put_nowait和get_nowait。如果调用get方法时队列为空，则抛出&#8221;gevent.queue.Empty&#8221;异常。 信号量 信号量可以用来限制协程并发的个数。它有两个方法，acquire和release。顾名思义，acquire就是获取信号量，而release就是释放。当所有信号量都已被获取，那剩余的协程就只能等待任一协程释放信号量后才能得以运行： &lt;div id=&quot;crayon-5d24ce1c01e11837954961&quot; class=&quot;crayon-syntax crayon-theme-eclipse crayon-font-monaco crayon-os-pc print-yes notranslate&quot; data-settings=&quot; minimize scroll-mouseover&quot; style=&quot; margin-top: 12px; margin-bottom: 12px; font-size: 12px !important; line-height: 15px !important;&quot;&gt; &lt;div class=&quot;crayon-plain-wrap&quot;&gt;&lt;/div&gt; &lt;div class=&quot;crayon-main&quot; style=&quot;&quot;&gt; &lt;table class=&quot;crayon-table&quot;&gt; &lt;tr class=&quot;crayon-row&quot;&gt; &lt;td class=&quot;crayon-nums &quot; data-settings=&quot;show&quot;&gt; &lt;div class=&quot;crayon-nums-content&quot; style=&quot;font-size: 12px !important; line-height: 15px !important;&quot;&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e11837954961-1&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e11837954961-2&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e11837954961-3&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e11837954961-4&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e11837954961-5&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e11837954961-6&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e11837954961-7&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e11837954961-8&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e11837954961-9&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e11837954961-10&quot;&gt;10&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e11837954961-11&quot;&gt;11&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e11837954961-12&quot;&gt;12&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e11837954961-13&quot;&gt;13&lt;/div&gt;&lt;/div&gt; &lt;/td&gt; &lt;td class=&quot;crayon-code&quot;&gt;&lt;div class=&quot;crayon-pre&quot; style=&quot;font-size: 12px !important; line-height: 15px !important; -moz-tab-size:4; -o-tab-size:4; -webkit-tab-size:4; tab-size:4;&quot;&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e11837954961-1&quot;&gt;&lt;span class=&quot;crayon-r&quot;&gt;import&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;gevent&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e11837954961-2&quot;&gt;&lt;span class=&quot;crayon-st&quot;&gt;from&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;gevent&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;coros &lt;/span&gt;&lt;span class=&quot;crayon-r&quot;&gt;import&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;BoundedSemaphore&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e11837954961-3&quot;&gt;&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e11837954961-4&quot;&gt;&lt;span class=&quot;crayon-v&quot;&gt;sem&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;BoundedSemaphore&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-cn&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e11837954961-5&quot;&gt;&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e11837954961-6&quot;&gt;&lt;span class=&quot;crayon-r&quot;&gt;def&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;worker&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;crayon-o&quot;&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e11837954961-7&quot;&gt;&lt;span class=&quot;crayon-h&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;sem&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;acquire&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e11837954961-8&quot;&gt;&lt;span class=&quot;crayon-h&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;crayon-k &quot;&gt;print&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-s&quot;&gt;&apos;Worker %i acquired semaphore&apos;&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e11837954961-9&quot;&gt;&lt;span class=&quot;crayon-h&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;gevent&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;sleep&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-cn&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e11837954961-10&quot;&gt;&lt;span class=&quot;crayon-h&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;sem&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;release&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e11837954961-11&quot;&gt;&lt;span class=&quot;crayon-h&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;crayon-k &quot;&gt;print&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-s&quot;&gt;&apos;Worker %i released semaphore&apos;&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e11837954961-12&quot;&gt;&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e11837954961-13&quot;&gt;&lt;span class=&quot;crayon-v&quot;&gt;gevent&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;joinall&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;gevent&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;spawn&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;worker&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-st&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-i&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-st&quot;&gt;in&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-k &quot;&gt;xrange&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-cn&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-cn&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/div&gt; &lt;/div&gt; 上面的例子中，我们初始化了&#8221;BoundedSemaphore&#8221;信号量，并将其个数定为2。所以同一个时间，只能有两个worker协程被调度。程序运行后的结果如下： Worker 0 acquired semaphore Worker 1 acquired semaphore Worker 0 released semaphore Worker 1 released semaphore Worker 2 acquired semaphore Worker 3 acquired semaphore Worker 2 released semaphore Worker 3 released semaphore Worker 4 acquired semaphore Worker 4 released semaphore Worker 5 acquired semaphore Worker 5 released semaphore 如果信号量个数为1，那就等同于同步锁。 协程本地变量 同线程类似，协程也有本地变量，也就是只在当前协程内可被访问的变量： &lt;div id=&quot;crayon-5d24ce1c01e12559646902&quot; class=&quot;crayon-syntax crayon-theme-eclipse crayon-font-monaco crayon-os-pc print-yes notranslate&quot; data-settings=&quot; minimize scroll-mouseover&quot; style=&quot; margin-top: 12px; margin-bottom: 12px; font-size: 12px !important; line-height: 15px !important;&quot;&gt; &lt;div class=&quot;crayon-plain-wrap&quot;&gt;&lt;/div&gt; &lt;div class=&quot;crayon-main&quot; style=&quot;&quot;&gt; &lt;table class=&quot;crayon-table&quot;&gt; &lt;tr class=&quot;crayon-row&quot;&gt; &lt;td class=&quot;crayon-nums &quot; data-settings=&quot;show&quot;&gt; &lt;div class=&quot;crayon-nums-content&quot; style=&quot;font-size: 12px !important; line-height: 15px !important;&quot;&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e12559646902-1&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e12559646902-2&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e12559646902-3&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e12559646902-4&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e12559646902-5&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e12559646902-6&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e12559646902-7&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e12559646902-8&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e12559646902-9&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e12559646902-10&quot;&gt;10&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e12559646902-11&quot;&gt;11&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e12559646902-12&quot;&gt;12&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e12559646902-13&quot;&gt;13&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e12559646902-14&quot;&gt;14&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e12559646902-15&quot;&gt;15&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e12559646902-16&quot;&gt;16&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e12559646902-17&quot;&gt;17&lt;/div&gt;&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-5d24ce1c01e12559646902-18&quot;&gt;18&lt;/div&gt;&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-5d24ce1c01e12559646902-19&quot;&gt;19&lt;/div&gt;&lt;/div&gt; &lt;/td&gt; &lt;td class=&quot;crayon-code&quot;&gt;&lt;div class=&quot;crayon-pre&quot; style=&quot;font-size: 12px !important; line-height: 15px !important; -moz-tab-size:4; -o-tab-size:4; -webkit-tab-size:4; tab-size:4;&quot;&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e12559646902-1&quot;&gt;&lt;span class=&quot;crayon-r&quot;&gt;import&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;gevent&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e12559646902-2&quot;&gt;&lt;span class=&quot;crayon-st&quot;&gt;from&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;gevent&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;local &lt;/span&gt;&lt;span class=&quot;crayon-r&quot;&gt;import&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;local&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e12559646902-3&quot;&gt;&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e12559646902-4&quot;&gt;&lt;span class=&quot;crayon-v&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;local&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e12559646902-5&quot;&gt;&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e12559646902-6&quot;&gt;&lt;span class=&quot;crayon-r&quot;&gt;def&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;f1&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;crayon-o&quot;&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e12559646902-7&quot;&gt;&lt;span class=&quot;crayon-h&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-cn&quot;&gt;1&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e12559646902-8&quot;&gt;&lt;span class=&quot;crayon-h&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;crayon-k &quot;&gt;print&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;crayon-i&quot;&gt;x&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e12559646902-9&quot;&gt;&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e12559646902-10&quot;&gt;&lt;span class=&quot;crayon-r&quot;&gt;def&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;f2&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;crayon-o&quot;&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e12559646902-11&quot;&gt;&lt;span class=&quot;crayon-h&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;crayon-st&quot;&gt;try&lt;/span&gt;&lt;span class=&quot;crayon-o&quot;&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e12559646902-12&quot;&gt;&lt;span class=&quot;crayon-h&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;crayon-k &quot;&gt;print&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;crayon-i&quot;&gt;x&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e12559646902-13&quot;&gt;&lt;span class=&quot;crayon-h&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;crayon-st&quot;&gt;except&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-k &quot;&gt;AttributeError&lt;/span&gt;&lt;span class=&quot;crayon-o&quot;&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e12559646902-14&quot;&gt;&lt;span class=&quot;crayon-h&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;crayon-k &quot;&gt;print&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;crayon-s&quot;&gt;&apos;x is not visible&apos;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e12559646902-15&quot;&gt;&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e12559646902-16&quot;&gt;&lt;span class=&quot;crayon-v&quot;&gt;gevent&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;joinall&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;[&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e12559646902-17&quot;&gt;&lt;span class=&quot;crayon-h&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;gevent&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;spawn&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;f1&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;,&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5d24ce1c01e12559646902-18&quot;&gt;&lt;span class=&quot;crayon-h&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;gevent&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;spawn&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;f2&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5d24ce1c01e12559646902-19&quot;&gt;&lt;span class=&quot;crayon-sy&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/div&gt; &lt;/div&gt; 通过将变量存放在local对象中，即可将其的作用域限制在当前协程内，当其他协程要访问该变量时，就会抛出异常。不同协程间可以有重名的本地变量，而且互相不影响。因为协程本地变量的实现，就是将其存放在以的&#8221;greenlet.getcurrent()&#8221;的返回为键值的私有的命名空间内。 更多参考资料 gevent的官方文档 gevent社区提供的教程 转自: http://www.bjhee.com/gevent.html]]></content>
      <categories>
        <category>Script</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>GEvent</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python的reload对于func_closure的处理]]></title>
    <url>%2Fblog%2F2019%2F07%2F08%2Fpython_reload_func_closure%2F</url>
    <content type="text"><![CDATA[带着问题学习动力是较强的, 直接上例子 . . . 引子代码如下： 1234567891011121314# coding:utf-8def make_actions(): acts = [] xxd = &#123;1: '1', 2: '2', 3: '3'&#125; for k, v in xxd.iteritems(): acts.append(lambda x: (k + int(v)) ** x) return actsfoo = make_actions()print(foo[0](2))print(foo[1](2))print(foo[2](2)) 众所周知, 结果肯定为: 123363636 因为这个闭包引用了外部的k, v嘛, 那是存在哪里呢? debug一波, 此时发现func_closure是这样存的 把 make_actions 改成 123456def make_actions(): acts = [] xxd = &#123;1: '1', 2: '2', 3: '3'&#125; for k, v in xxd.iteritems(): acts.append(lambda x, key=k, val=v, : (key + int(val)) ** x) return acts 再debug断点查看一波就完事了, 会发现现在 func_defaults 里现在有东西了 基于函数替换型reload中的应用实现python热更逻辑的时候要记得处理func_closure 如类似下方的这段代码要怎么reload呢? 12345678910111213def gg_make_actions(): acts = [] xxd_dict = &#123;1: '1', 2: '2', 3: '3'&#125; for k, v in xxd_dict.iteritems(): lb = lambda l, val=int(v): val + l acts.append(lambda x: lb(x)) return actsbar = gg_make_actions()print(bar[0](2))print(bar[1](2))print(bar[2](2)) debug断点查看一波, 发现func_closure里还有 function object 那reload需要对含有闭包的情况进行一些简单处理: 123456789101112def replace_func(new_func, old_func, is_closure = False): # 简单的closure的处理 re_attrs = ('func_doc', 'func_code', 'func_dict', 'func_defaults') for attr_name in re_attrs: setattr(old_func, attr_name, getattr(new_func, attr_name, None)) if not is_closure: old_cell_nums = len(old_func.func_closure) if old_func.func_closure else 0 new_cell_nums = len(new_func.func_closure) if new_func.func_closure else 0 if new_cell_nums and new_cell_nums == old_cell_nums: for idx, cell in enumerate(old_func.func_closure): if inspect.isfunction(cell.cell_contents): do_replace_func(new_func.func_closure[idx].cell_contents, cell.cell_contents, True)]]></content>
      <categories>
        <category>Script</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python的with与__enter__以及__exit__关系]]></title>
    <url>%2Fblog%2F2019%2F06%2F28%2Fpython_with%2F</url>
    <content type="text"><![CDATA[有一些任务，可能事先需要设置，事后做清理工作。对于这种场景，Python的with语句提供了一种非常方便的处理方式。一个很好的例子是文件处理，你需要获取一个文件句柄，从文件中读取数据，然后关闭文件句柄。 . . . 引子如果不用with语句，代码如下： 123file = open("/tmp/foo.txt")data = file.read()file.close() 这里有两个问题。一是可能忘记关闭文件句柄；二是文件读取数据发生异常，没有进行任何处理。下面是处理异常的加强版本： 12345file = open("/tmp/foo.txt")try: data = file.read()finally: file.close() 虽然这段代码运行良好，但是太冗长了。这时候就是with一展身手的时候了。除了有更优雅的语法，with还可以很好的处理上下文环境产生的异常。下面是with版本的代码： 12with open("/tmp /foo.txt") as file: data = file.read() with如何工作这看起来充满魔法，但不仅仅是魔法，Python对with的处理还很聪明。基本思想是with所求值的对象必须有一个__enter__()方法，一个__exit__()方法。 紧跟with后面的语句被求值后，返回对象的__enter__()方法被调用，这个方法的返回值将被赋值给as后面的变量。当with后面的代码块全部被执行完之后，将调用前面返回对象的__exit__()方法。 下面例子可以具体说明with如何工作： 1234567891011121314151617# -*- coding: UTF-8 -*-class Sample: def __enter__(self): print "In __enter__()" return "Foo" def __exit__(self, type, value, trace): print "In __exit__()"def get_sample(): return Sample()with get_sample() as sample: print "sample:", sample 输出如下 1234bash-3.2$ ./with_example01.pyIn __enter__()sample: FooIn __exit__() 正如你看到的， __enter__()方法被执行__enter__()方法返回的值 - 这个例子中是”Foo”，赋值给变量’sample’执行代码块，打印变量”sample”的值为 “Foo”__exit__()方法被调用 with还可以处理异常with真正强大之处是它可以处理异常。可能你已经注意到Sample类的__exit__方法有三个参数val, type 和 trace。 这些参数在异常处理中相当有用。我们来改一下代码，看看具体如何工作的。 1234567891011121314151617# -*- coding: UTF-8 -*-class Sample: def __enter__(self): return self def __exit__(self, type, value, trace): print "type:", type print "value:", value print "trace:", trace def do_something(self): bar = 1/0 return bar + 10with Sample() as sample: sample.do_something() 这个例子中，with后面的get_sample()变成了Sample()。这没有任何关系，只要紧跟with后面的语句所返回的对象有 __enter__()和__exit__()方法即可。此例中，Sample()的__enter__()方法返回新创建的Sample对象，并赋值给变量sample。 代码执行后： 12345678910bash-3.2$ ./with_example02.pytype: &lt;type &apos;exceptions.ZeroDivisionError&apos;&gt;value: integer division or modulo by zerotrace: &lt;traceback object at 0x1004a8128&gt;Traceback (most recent call last): File &quot;./with_example02.py&quot;, line 19, in &lt;module&gt; sample.do_somet hing() File &quot;./with_example02.py&quot;, line 15, in do_something bar = 1/0ZeroDivisionError: integer division or modulo by zero 实际上，在with后面的代码块抛出任何异常时，__exit__()方法被执行。正如例子所示，异常抛出时，与之关联的type，value和stack trace传给__exit__()方法，因此抛出的ZeroDivisionError异常被打印出来了。开发库时，清理资源，关闭文件等等操作，都可以放在__exit__方法当中。 因此，Python的with语句是提供一个有效的机制，让代码更简练，同时在异常产生时，清理工作更简单。]]></content>
      <categories>
        <category>Script</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[go安装备忘]]></title>
    <url>%2Fblog%2F2019%2F06%2F27%2Finstall_go%2F</url>
    <content type="text"><![CDATA[装go因为 ubuntu 默认装 go 是1.6 的, 不想装 1.6, 准备装 1.8, . . . step1wget https://storage.googleapis.com/golang/go1.8.3.linux-amd64.tar.gz step2sudo tar -xvf go1.8.3.linux-amd64.tar.gzsudo mv go /usr/local step3vi ~/.bashrcexport GOROOT=/usr/local/goexport GOPATH=/home/zhangxiao/goworkspace/mygoexport PATH=$GOPATH/bin:$GOROOT/bin:$PATHsource ~/.bashrc step4go versiongo version go1.8.3 linux/amd64 参考：https://tecadmin.net/install-go-on-ubuntu/]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>Go</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Etcd安装备忘]]></title>
    <url>%2Fblog%2F2019%2F06%2F27%2Fhello_etcd%2F</url>
    <content type="text"><![CDATA[参考https://skyao.gitbooks.io/learning-etcd3/content/installation/linux_single.html 下载/配置简而言之就是 : 先去 etcd 的 github找到他的release, 然后复制链接, 下载然后配置 注： 以 etcd-v3.2.1 为例，后续更新版本时可能细节有所不同。 . . . 下载执行下面的命令，下载(大概10M)解压即可，无需安装： 12345678910curl -L https://github.com/coreos/etcd/releases/download/v3.2.1/etcd-v3.2.1-linux-amd64.tar.gz -o etcd-v3.2.1-linux-amd64.tar.gztar xzvf etcd-v3.2.1-linux-amd64.tar.gzmv etcd-v3.2.1-linux-amd64 etcdcd etcd./etcd --versionetcd Version: 3.2.1Git SHA: 61fc123Go Version: go1.8.3Go OS/Arch: linux/amd64 安装目录文件列表如下： 123$ lsdefault.etcd etcd README-etcdctl.md READMEv2-etcdctl.mdDocumentation etcdctl README.md 运行直接运行命令 ./etcd 就可以启动了，非常简单。 默认使用2379端口为客户端提供通讯， 并使用端口2380来进行服务器间通讯。 配置为了方便使用，将 etcd 加入 PATH，另外设置 ETCDCTL_API 为3(后面解释)。 在 /etc/profile 中加入以下内容： 123# etcdexport PATH=/home/sky/work/soft/etcd:$PATHexport ETCDCTL_API=3 然后执行 source /etc/profile 重新加载。 客户端访问配置etcdctletcdctl 是 etcd 的客户端命令行。 特别提醒：使用 etcdctl 前，*务必设置环境变量 ETCDCTL_API=3 *! 注意：如果不设置 ETCDCTL_API=3，则默认是的API版本是2： 123$ etcdctl versionetcdctl version: 3.2.1API version: 2 正确设置后，API版本变成3： 123$ etcdctl versionetcdctl version: 3.2.1API version: 3.2 使用etcdctl通过下面的put和get命令来验证连接并操作etcd： 12345$ ./etcdctl put aaa 1OK$ ./etcdctl get aaaaaa1 总结上面操作完成之后，就有一个可运行的简单 etcd 服务器和一个可用的 etcdctl 客户端。]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>Etcd</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pybind11 intro]]></title>
    <url>%2Fblog%2F2019%2F05%2F19%2Fpybind11_intro%2F</url>
    <content type="text"><![CDATA[boost.python 迟暮, 久违 pybind11 , 来玩玩 官方介绍pybind11 is a lightweight header-only library that exposes C++ types in Python and vice versa, mainly to create Python bindings of existing C++ code. Its goals and syntax are similar to the excellent Boost.Python library by David Abrahams: to minimize boilerplate code in traditional extension modules by inferring type information using compile-time introspection. The main issue with Boost.Python—and the reason for creating such a similar project—is Boost. Boost is an enormously large and complex suite of utility libraries that works with almost every C++ compiler in existence. This compatibility has its cost: arcane template tricks and workarounds are necessary to support the oldest and buggiest of compiler specimens. Now that C++11-compatible compilers are widely available, this heavy machinery has become an excessively large and unnecessary dependency. 编写供 python 调用的 C++ 模块下载好 pybind11 之后，我们就可以开始对着官方的 pybind11 Tutorial 进行学习了，详细的入门教程及语法请参考官方文档，这里，我们简单演示下如何编写供 python 调用的 C++ 模块. . . . 首先，我们编写一个 C++ 源文件，命名为hello_pybind11.cpp 12345678910111213141516171819202122232425262728293031323334353637383940414243444546#include &lt;pybind11/pybind11.h&gt;int add(int i, int j) &#123; return i + j;&#125;namespace py = pybind11;PYBIND11_MODULE(hello_pybind11, m) &#123; m.doc() = R"pbdoc( Pybind11 hello_pybind11 plugin ----------------------- .. currentmodule:: hello_pybind11 .. autosummary:: :toctree: _generate add subtract )pbdoc"; m.def("add", &amp;add, R"pbdoc( Add two numbers Some other explanation about the add function. )pbdoc"); m.def("subtract", [](int i, int j) &#123; return i - j; &#125;, R"pbdoc( Subtract two numbers Some other explanation about the subtract function. )pbdoc"); // exporting variables m.attr("the_answer") = 42; py::object world = py::cast("World"); m.attr("what") = world;#ifdef VERSION_INFO m.attr("__version__") = VERSION_INFO;#else m.attr("__version__") = "dev";#endif&#125; CMake 的编译方法我们使用 CMake 进行编译。如果 hello_pybind11.cpp 放在和 pybind11 同一级的目录下,首先像这样写一个 CMakeLists.txt 12345cmake_minimum_required(VERSION 2.8.12)project(hello_pybind11)add_subdirectory(pybind11)pybind11_add_module(hello_pybind11 hello_pybind11.cpp) 然后 CMake，便会生成一个 vs 2015 的工程文件，用 vs 打开工程文件进行 build，就可以生成hello_pybind11.pyd了。 加入py测试脚本12345678910import syssys.path.append("C:/Users/b/Documents/practice/hello_pybind11/vs2017/proj/Debug") # hello_pybind11 在这个路径import hello_pybind11print hello_pybind11.add(1, 2)print hello_pybind11.subtract(12, 22)print hello_pybind11.the_answerprint hello_pybind11.what 参考文献 pybind11 github pybind11 Tutorial python 调用 C++ 之 pybind11 入门]]></content>
      <categories>
        <category>Script</category>
      </categories>
      <tags>
        <tag>CPP</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[asio none boost intro]]></title>
    <url>%2Fblog%2F2019%2F04%2F01%2Fasio_non_boost_intro%2F</url>
    <content type="text"><![CDATA[一晃2年过去了, 记得曾经看过 boost.asio, 现在 asio 已经可以完全脱离 boost 了,不过它项目里的一些例子还是依赖 boost 的, 比如他 src 文件夹里的 tests 里的 除了 unit , 其他的大多数还是老的例子,都是直接包含boost的一些头文件, 也就是依赖boost的 编译注意事项官网说支持c++11的编译器会自动检测, 然后走asio的standalone模式, 测试了一下, 显然不会.所以 ASIO_STANDALONE 这个宏是必须得自己加上的,define ASIO_STANDALONE on your Preprocessor Settings (如: g++ -DASIO_STANDALONE) or as part of the project options. . . . 包含 asio 的目录 (如: g++ -I) In C/C++ Preprocessor Settings, defined:1234567891011121314ASIO_STANDALONEASIO_HAS_STD_ADDRESSOFASIO_HAS_STD_ARRAYASIO_HAS_CSTDINTASIO_HAS_STD_SHARED_PTRASIO_HAS_STD_TYPE_TRAITSASIO_HAS_VARIADIC_TEMPLATESASIO_HAS_STD_FUNCTIONASIO_HAS_STD_CHRONOBOOST_ALL_NO_LIB_WIN32_WINNT=0x0501_WINSOCK_DEPRECATED_NO_WARNINGS 还可以参考: https://nnarain.github.io/2015/11/03/Building-ASIO-Standalone-with-Visual-Studio-2015.html https://segmentfault.com/a/1190000013031005 如何fix例子里的boost依赖或已过时的代码12asio::placeholders::error, asio::placeholders::bytes_transferred 上面代码里的可替换为 std::placeholders::_1 和 std::placeholders::_2 123#include &lt;boost/array.hpp&gt;#include &lt;boost/bind.hpp&gt;#include &lt;boost/shared_ptr.hpp&gt; 这种就可以替换为 12#include &lt;array&gt;#include &lt;memory&gt;]]></content>
      <categories>
        <category>NP</category>
      </categories>
      <tags>
        <tag>Boost</tag>
        <tag>CPP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SSR]]></title>
    <url>%2Fblog%2F2019%2F02%2F14%2Fssr%2F</url>
    <content type="text"><![CDATA[更多好用的一键脚本请转 https://github.com/ToyoDAdoubi/doubi ssr.sh 脚本说明: ShadowsocksR 一键安装管理脚本，支持单端口/多端口切换和管理 系统支持: CentOS6+ / Debian6+ / Ubuntu14+ 使用方法: https://doub.io/ss-jc42/ 项目地址: https://github.com/ToyoDAdoubiBackup/shadowsocksr . . . 脚本特点:目前网上的各个ShadowsocksR脚本基本都是只有 安装/启动/重启 等基础功能，对于小白来说还是不够简单方便。既然是一键脚本，那么就要尽可能地简单，小白更容易接受使用！ 支持 限制 用户速度 支持 限制 端口设备数 支持 显示 当前连接IP 支持 显示 SS/SSR连接+二维码 支持 切换管理 单/多端口 支持 一键安装 锐速 支持 一键安装 BBR 支持 一键封禁 垃圾邮件(SMAP)/BT/PT 下载安装:wget -N --no-check-certificate https://raw.githubusercontent.com/ToyoDAdoubi/doubi/master/ssr.sh &amp;&amp; chmod +x ssr.sh &amp;&amp; bash ssr.sh 安装ssr软件 &nbsp; 复制上面的代码到VPS服务器里，安装脚本后，以后只需要运行bash ssr.sh这个快捷命令就可以出现下图的界面进行设置管理了 &nbsp; 如上图出现管理界面后，输入数字1来安装SSR服务端。如果输入1后不能进入下一步，那么请重新连接VPS服务器，然后输入快捷管理命令 bash ssr.sh 再尝试 选择端口直接回车选择默认的就好了。当然了，你也可以设置其他的端口，这个根据个人需求 &nbsp; 然后设置密码，选择加密方式 &nbsp; 接下来选择协议插件 &nbsp; 再然后根据自己需求选择是否兼容原版ss客户端 &nbsp; 之后选择混淆插件 &nbsp; 进行混淆插件的设置后，会依次提示你对设备数、单线程限速和端口总限速进行设置，默认值是不进行限制，个人使用的话，选择默认即可，一路敲回车键。 &nbsp; 耐心等待一会，出现下面的界面即部署完成： &nbsp; 输入快捷管理命令：bash ssr.sh 进入管理界面 &nbsp; 根据上图就可以看到自己设置的ssr账号信息，包括IP、端口、密码、加密方式、协议插件、混淆插件等等，如果之后想修改账号信息，选择相应的数字来进行一键修改 加速VPS服务器 (谷歌BBR加速) wget --no-check-certificate https://github.com/teddysun/across/raw/master/bbr.sh &nbsp; &nbsp; &nbsp; chmod +x bbr.sh &nbsp; &nbsp; &nbsp;./bbr.sh 执行上面的代码，然后耐心等待，安装成功后重启VPS服务器即可 &nbsp; &nbsp; &nbsp; 最后重启服务器 ssr客户端下载 Windows SSR客户端 点击下载地址 MacOS SSR客户端 点击下载地址 Linux SSR客户端 点击下载地址 安卓 SSR客户端 点击下载地址 苹果手机SSR客户端：Potatso Lite、Potatso、shadowrocket都可以作为SSR客户端，但这些软件目前已经在国内的app商店下架，可以用美区的appid账号来下载。但是，如果你配置的SSR账号兼容SS客户端，或者协议选择origin且混淆选择plain，那么你可以选择苹果SS客户端软件(即协议和混淆可以不填)，APP商店里面有很多，比如：openwingy、superwingy、bestwingy、wingy+、greatwingy等。 有了账号后，打开SSR客户端，填上信息，这里以Windows版的SSR客户端为例子： &nbsp; 在对应的位置，填上服务器IP、服务器端口、密码、加密方式、协议和混淆，最后点击确认 常见问题解决方法 1、用了一段时间发现SSR账号用不了了? 多半是被墙了，即ip失效。首先ping一下自己的ip，看看能不能ping的通，ping不通那么就是ip被墙了，遇到这种情况重新部署一个新的服务器，新的服务器就是新的ip。关于怎么ping ip的方法，可以自行网上搜索，很简单。vultr服务商是折算成小时计费，且开通和删除服务器非常方便(新服务器即新ip。大多数vps服务商都没有这样的服务，一般的vps服务商可能会提供更换1次ip的服务，如果你买的是别家的vps，一定要了解是否能够更换ip，假如不能，那么万一你的ip不幸被墙，钱就打水漂了) 2、刚搭建好的ssr账号，ip能ping通，但是还是用不了? 首选排除杀毒软件的干扰，尤其是国产杀毒软件，比如360安全卫生、360杀毒软件、腾讯管家、金山卫士等。这些东西很容易干扰翻墙上网，如果你的电脑安装了这样的东西，建议至少翻墙时别用。其次，检查下SSR信息是否填写正确。浏览器的代理方式是否是ssr代理，即127.0.0.1 1080端口。如果以上条件都排除，还是用不了，那么可以更换端口、加密方式、协议、混淆，或者更换服务器位置 3、vultr服务商提供的 VPS 服务器是单向流量计算，有的vps服务商是双向流量计算，单向流量计算对于用户来说更实惠。因为我们是在 VPS 服务器上部署SSR服务端后，再用SSR客户端翻墙，所以SSR服务端就相当于中转，比如我们看一个视频，必然会产生流量，假如消耗流量80M，那么VPS服务器会产生上传80M和下载80M流量，vultr服务商只计算单向的80M流量。如果是双向计算流量，那么会计算为160M流量 4、如果你想把搭建的账号给多人使用，不用额外设置端口，因为一个账号就可以多人使用 &nbsp;]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>VPS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一个轻量级的kcp会话实现]]></title>
    <url>%2Fblog%2F2018%2F08%2F09%2Fkcpp_intro%2F</url>
    <content type="text"><![CDATA[QQ群因为 KCP 官方群已经满了, 可以加群 496687140 轻量级的kcp会话实现-kcppkcpp真正实现了只需要包含一个头文件再随意写几行代码就可以用上kcp, 而无需烦心如何组织代码来适配kcp 只需包含 kcpp.h 这一个头文件即可 只需调用 KcpSession::Send 和 KcpSession::Recv 和 KcpSession::Update 即可完成UDP的链接状态管理、会话控制、 RUDP协议调度 Features single-header-only session implementation dynamic redundancy two-channel reliable unreliable kcpp Examples realtime-server : A realtime dedicated game server ( FPS / MOBA ). 一个实时的专用游戏服务器. realtime-server-ue4-demo : A UE4 State Synchronization demo for realtime-server. 为realtime-server而写的一个UE4状态同步demo, Video Preview 视频演示 TestKcppServer.cpp TestKcppClient.cpp kcpp Usagethe main loop was supposed as: 12345678910111213141516171819202122232425Game.Init()// kcpp initkcpp::KcpSession myKcpSess( KcpSession::RoleTypeE, std::bind(udp_output, _1, _2), std::bind(udp_input), std::bind(timer));while (!isGameOver) myKcpSess.Update() while (myKcpSess.Recv(data, len)) if (len &gt; 0) Game.HandleRecvData(data, len) else if (len &lt; 0) Game.HandleRecvError(len); if (myKcpSess.CheckCanSend()) myKcpSess.Send(data, len) else Game.HandleCanNotSendForNow() Game.Logic() Game.Render() The Recv/Send/Update functions of kcpp are guaranteed to be non-blocking.Please read TestKcppClient.cpp and TestKcppServer.cpp for some basic usage. kcp源码注释本项目还附了一个注释版的kcp源码 ikcp.h 和 ikcp.c， 算是另一种的 kcp详解, 方便自己学习也为大家更快的上手, 原始代码来自： https://github.com/skywind3000/kcp , 感谢 skywind3000 带来 . . . 这么短小精悍的好项目 注 : 项目中使用 tab 缩进且设置了tab = 2 space 几乎每个段落都有注释, 且关键数据结构还带有图解, 比如 : 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950...//// kcp发送的数据包设计了自己的包结构，包头一共24bytes，包含了一些必要的信息，具体内容和大小如下：// // |&lt;------------ 4 bytes ------------&gt;|// +--------+--------+--------+--------+// | conv | conv：Conversation, 会话序号，用于标识收发数据包是否一致// +--------+--------+--------+--------+ cmd: Command, 指令类型，代表这个Segment的类型// | cmd | frg | wnd | frg: Fragment, 分段序号，分段从大到小，0代表数据包接收完毕// +--------+--------+--------+--------+ wnd: Window, 窗口大小// | ts | ts: Timestamp, 发送的时间戳// +--------+--------+--------+--------+// | sn | sn: Sequence Number, Segment序号// +--------+--------+--------+--------+// | una | una: Unacknowledged, 当前未收到的序号，// +--------+--------+--------+--------+ 即代表这个序号之前的包均收到// | len | len: Length, 后续数据的长度// +--------+--------+--------+--------+//...//---------------------------------------------------------------------// ...// rcv_queue 接收消息的队列, rcv_queue的数据是连续的，rcv_buf可能是间隔的// nrcv_que // 接收队列rcv_queue中的Segment数量, 需要小于 rcv_wnd// rcv_queue 如下图所示// +---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+// ... | 2 | 3 | 4 | ............................................... // +---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+// ^ ^ ^ // | | | // rcv_nxt rcv_nxt + nrcv_que rcv_nxt + rcv_wnd //// snd_buf 发送消息的缓存// snd_buf 如下图所示// +---+---+---+---+---+---+---+---+---+---+---+---+---+// ... | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | ...........// +---+---+---+---+---+---+---+---+---+---+---+---+---+// ^ ^ ^// | | |// snd_una snd_nxt snd_una + snd_wnd ////// rcv_buf 接收消息的缓存// rcv_buf 如下图所示, rcv_queue的数据是连续的，rcv_buf可能是间隔的// +---+---+---+---+---+---+---+---+---+---+---+---+---+// ... | 2 | 4 | 6 | 7 | 8 | 9 | ...........// +---+---+---+---+---+---+---+---+---+---+---+---+---+ //... 在注释的过程中， 除了少量空格和换行以及一处有无符号比较的调整(为保证高警告级别可编译过)外 :if ((IUINT32)count &gt;= IKCP_WND_RCV) return -2;没有对原始代码进行任何其他改动， 最大程度地保证了代码的“原汁原味”。]]></content>
      <categories>
        <category>NP</category>
      </categories>
      <tags>
        <tag>KCP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在材质传值给GBuffer]]></title>
    <url>%2Fblog%2F2018%2F08%2F05%2Fzen_%E5%9C%A8%E6%9D%90%E8%B4%A8%E9%87%8C%E9%9D%A2%E7%A9%BF%E5%80%BC%E7%BB%99GBuffer%2F</url>
    <content type="text"><![CDATA[. . . 在材质里面传值给GBuffer在材质里面设置一个值如果需要在材质里面设置一个值，然后把这个值传给GBuffer，然后可以通过设置这个值来动态修改效果，就像材质实例一般。首先在材质里面传值，首先可以自定义一个 shadermodel，然后把这个 shadermodel 的customdata1 的pin 打开。当然也可以不通过 customdata1 来传值。也可以通过在材质细节里面，找到Num Customized UVs 改成1，然后通过这里 pin 来传值。当然也可以通过 Base Color, Metallic 这些，这一部其实只是为了传值进去而已。并无所谓是在哪里传值。而选择customdata1 的意义只是因为这个节点一般不会被使用到，传值到这里去并不会影响到其它。只要确定传进去的值不会被其它地方用到。怎么传其实都无所谓的。 把值设置给GBuffer打开引擎文件中的 Shaders/Private/ShadingModelsMaterial.ush，这个文件就是引擎由ShderModel 设置GBuffer 的地方。进到这个文件后，找到自定义的shader model，或者直接修改引擎自带的shader model。然后写下如下代码： 12float TmpValue = saturate( GetMaterialCustomData1(MaterialParameters) );GBuffer.CustomData.y = TmpValue; 这样之后，GBuffer 中的 CustomData.y 就和材质里面你设置的值绑定了。然后你就可以在需要传值的地方传 GBuffer.CustomData.y 就可以了。 注意点这个做法首先要确保绑定的GBuffer 的值没有和其它效果关联。不然的话，修改值的同时也会影响到其它效果。]]></content>
      <categories>
        <category>UE4</category>
      </categories>
      <tags>
        <tag>UE4</tag>
        <tag>Zen</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[地图加载完毕后的回调]]></title>
    <url>%2Fblog%2F2018%2F07%2F20%2Fzen_map_load_finished_callback%2F</url>
    <content type="text"><![CDATA[在地图加载完后的回调首先自定义一个 UObject，UObject 可以跨地图存在。 . . . Object 如果切换地图的时候如果没有对它有引用的话，会自动释放掉，如果不让他释放的话，可以在GameInstance 里面用一个指针指向这个Obj。或者是Obj 生成的时候，在构造函数加上 1this-&gt;AddToRoot() 地图加载完成后的回调： 123// ExcuteFun 函数是回调会执行的函数FCoreUObjectDelegates::PostLoadMap.AddUObject(this, &amp;UMyObject::ExcuteFun);FCoreUObjectDelegates::PostLoadMapWithWorld.AddUObject(this, &amp;UMyObject::ExcuteFun); 上面的是老版本的用法，如 void UMyObject::ExcuteFun()){}下面的是新版本的用法，需要在绑定函数的参数添加上 UWorld* 的参数，如 1void UMyObject::ExcuteFun(UWorld* world))&#123;&#125; 添加了AddToRoot函数，再使用完成后就必须要记得 1this-&gt;RemoveFromRoot()]]></content>
      <categories>
        <category>UE4</category>
      </categories>
      <tags>
        <tag>UE4</tag>
        <tag>Zen</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[不受灯光颜色影响]]></title>
    <url>%2Fblog%2F2018%2F07%2F20%2Fzen_not_affected_by_the_color_of_the_light%2F</url>
    <content type="text"><![CDATA[. . . 让物体不受光照的颜色影响 让物体不受光照颜色影响，总的来说，我的实现方式就是找到光照颜色的地方，然后对颜色去饱和度。去饱和度的代码大概如下： 123// 计算亮度，这个各个颜色分量可以自己感觉调整，我用的是虚幻默认的数值,alpha 可以调整去多少饱和度，用 0 ~ 1 表示，1表示完全去除float lum = Color.r * 0.3f + Color.g * 0.59f + Color.b * 0.11f;Color = lerp(Color, float4(lum, lum, lum, 0), alpha); 自定义一个shader model网上已经有比较完整的流程了，可以参考： 自定义shadingmodel 虚幻4渲染编程(材质编辑器篇)【第二卷：自定义光照模型】 最好结合着看，第一个的有点老，第二个的流程顺序有的不对。 首先修改普通光源如果已经按照上面的方法自定义shader model 的话，就可以在 DeferredLightingCommon.ush 这个文件，在shader model 的文章中的他们自定义的那个float3 AttenuationColor 变量，把他的式子删了，然后把他改成 123// 把灯光的颜色去饱和度float Lum = LightColor.r * 0.3f + LightColor.g * 0.59f + LightColor.b * 0.11f;AttenuationColor = lerp(LightColor, float4(Lum, Lum, Lum, 0), 1.0f) * (NoL * SurfaceAttenuation) ; 简单的说就是去饱和度。然后在ShadingModels.ush 文件里面的SurfaceShading 函数中，自定义的函数放在 12345678910111213141516171819202122232425float3 SurfaceShading( FGBufferData GBuffer, float3 LobeRoughness, float3 LobeEnergy, float3 L, float3 V, half3 N, uint2 Random )&#123; switch( GBuffer.ShadingModelID ) &#123; case SHADINGMODELID_UNLIT: case SHADINGMODELID_DEFAULT_LIT: case SHADINGMODELID_SUBSURFACE: case SHADINGMODELID_PREINTEGRATED_SKIN: case SHADINGMODELID_SUBSURFACE_PROFILE: case SHADINGMODELID_TWOSIDED_FOLIAGE: // 就是把自己添加的模块加进来,上面自定义模块教程里面的自己写的函数就可以不要了 case SHADINGMODELID_MyTestModel: return StandardShading( GBuffer.DiffuseColor, GBuffer.SpecularColor, LobeRoughness, LobeEnergy, L, V, N ); case SHADINGMODELID_CLEAR_COAT: return ClearCoatShading( GBuffer, LobeRoughness, LobeEnergy, L, V, N ); case SHADINGMODELID_CLOTH: return ClothShading( GBuffer, LobeRoughness, LobeEnergy, L, V, N ); case SHADINGMODELID_EYE: return EyeShading( GBuffer, LobeRoughness, LobeEnergy, L, V, N ); default: return 0; &#125;&#125; 这样的话，除了天光以外的光源颜色就不会对用了这个shader model 的材质造成影响。 处理天光天光分两步，一个是动态一个是非动态。动态天光可以在 SkyLighting.usf 文件的 1Lighting += DiffuseIrradiance * GBuffer.DiffuseColor * (GBuffer.GBufferAO * ScreenSpaceData.AmbientOcclusion); 为动态天光去饱和度这行代码下面为动态天光去饱和度 1234567Lighting += DiffuseIrradiance * GBuffer.DiffuseColor * (GBuffer.GBufferAO * ScreenSpaceData.AmbientOcclusion);if (ShadingModelId == SHADINGMODELID_MyTestModel)&#123; float Lum = Lighting.r * 0.3f + Lighting.g * 0.59f + Lighting.b * 0.11f; Lighting = lerp(Lighting, float4(Lum, Lum, Lum, 0), 1.0f) ;&#125; 为非动态天光去饱和度非动态天光就在 BasePassPixelShader.usf 文件的 GetPrecomputedIndirectLightingAndSkyLight(MaterialParameters, Interpolants, BasePassInterpolants, DiffuseDir, VolumetricLightmapBrickTextureUVs, DiffuseIndirectLighting, SubsurfaceIndirectLighting, IndirectIrradiance);这行代码下面添加判断让颜色去饱和度 123456if (GBuffer.ShadingModelID == SHADINGMODELID_MyTestModel)&#123; half DiffLum = DiffuseIndirectLighting.r * 0.3f + DiffuseIndirectLighting.g * 0.59f + DiffuseIndirectLighting.b * 0.11f; DiffuseIndirectLighting = lerp(DiffuseIndirectLighting, half4(DiffLum, DiffLum, DiffLum, 0), 0.8f);&#125;LightAccumulator_Add(LightAccumulator, Color, 0, 1.0f, false); 这样就可以让这个材质不会受到光照颜色的影响，但是会有明暗对比。 这样做之后，只要在材质里面使用这个自定义 shader model 的物体，就不会受到灯光颜色的影响，但是会有明暗对比。再次说明一次，lerp函数的第3个参数就是一个alpha 值，可以通过调整这个值来改变受光颜色的比例。我设置成 1， 所以不会受到灯光颜色的影响 出现的问题目前遇到的问题有，在受到的天光，纵使天光的颜色是白色的，也会和原来的颜色不会完全一样。总的来说，就是相对来说比原来偏白了一点。补上漫反射颜色后就和原来一样，但是这样就会受到漫反射颜色，导致了物体会受光的颜色，所以暂时没找到解决方法。]]></content>
      <categories>
        <category>UE4</category>
      </categories>
      <tags>
        <tag>UE4</tag>
        <tag>Zen</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sol2实现Cpp和Lua绑定]]></title>
    <url>%2Fblog%2F2018%2F06%2F20%2Flua_cpp_sol2%2F</url>
    <content type="text"><![CDATA[Sol2简介Sol是一个用于C++绑定Lua脚本的库，仅由头文件组成，方便集成，并提供了大量易用的API接口，可以便利地将Lua脚本与C++代码绑定起来，而不必去关心如何使用那些晦涩的Lua C API。正如其作者所言，Sol的目的就是提供极其简洁的API，并能高效到与C语言媲美，极大地来方便人们使用。 编译条件Sol支持Lua的绝大多数版本，包括 5.1、5.2、5.3和LuaJit等，但由于代码中用到了许多C++11/14特性，因此编译时需要编译器支持C++14标准甚至C++17标准, 本人测试g++4.8.2无法编译过Sol2.20+的版本, 用g++6.2方能编过. 安装方法去 https://github.com/ThePhD/sol2 下载一个sol.hpp , 然后放到 /usr/local/include 里即可 为什么强大只需要包含一个sol.hpp头文件即可,需要任何其他的东西, 没有什么静态库/动态库之类的东西, 也不需要像tolua++一样那么麻烦每个类都要写pkg文件. 只需要稍微学习一下Sol2的导出API即可. . . . 基础使用从Sol的Github仓库clone下代码后，我们发现其目录下很多test开头的cpp/hpp文件，这些文件里面有着大量的Sol的使用示例以及各种特性的展示，而在example目录下的cpp文件都仅仅是一些最基础的使用示例。为了方便测试和体验Sol，你也可以自己建立一些自己的test.cpp文件，首先你要在源文件中include引用sol.hpp头文件，这样才能使用Sol提供的接口。而在使用gcc编译的时候，需要指定关联头文件的路径，可以使用类似于如下命令： g++ test.cpp -Isolpath/single/sol -llua -std=c++1z 其中solpath是你Sol2的具体路径，在Sol2的项目目录下，有一个single/sol/sol.hpp头文件，这个头文件集成了所有的相关代码到一起，所以编译时 -I 后仅指定这一个路径就可以了，同时要保证你的gcc编译器支持C++14或17标准。 一个简单例子例子目录结构如下 : ├─test_sol2.cpp ├─assert.hpp ├─test_sol2.lua ├─sol.hpp编译命令 : g++ *.cpp -llua -std=c++1z test_sol2.cpp123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657#define SOL_CHECK_ARGUMENTS 1#include "sol.hpp"#include &lt;iostream&gt;#include "assert.hpp"int main()&#123; std::cout &lt;&lt; "=== namespacing ===" &lt;&lt; std::endl; struct my_class &#123; int b = 24; int f() const &#123; return 24; &#125; void g() &#123; ++b; &#125; &#125;; sol::state lua; lua.open_libraries(); // "bark" namespacing in Lua // namespacing is just putting things in a table sol::table bark = lua.create_named_table("bark"); bark.new_usertype&lt;my_class&gt;("my_class", "f", &amp;my_class::f, "g", &amp;my_class::g); // the usual // can add functions, as well (just like the global table) bark.set_function("print_my_class", [](my_class &amp;self) &#123; std::cout &lt;&lt; "my_class &#123; b: " &lt;&lt; self.b &lt;&lt; " &#125;" &lt;&lt; std::endl; &#125;); // // this works // lua.script("obj = bark.my_class.new()"); // lua.script("obj:g()"); // // calling this function also works // lua.script("bark.print_my_class(obj)"); // load and execute from file lua.script_file("test_sol2.lua"); my_class &amp;obj = lua["obj"]; c_assert(obj.b == 25); std::cout &lt;&lt; (obj.b == 25 ? "assert success" : "assert fail") &lt;&lt; std::endl; return 0;&#125; assert.hpp123456789101112131415161718192021222324252627282930313233343536373839404142#ifndef EXAMPLES_ASSERT_HPP#define EXAMPLES_ASSERT_HPP#ifdef SOL2_CIstruct pre_main &#123; pre_main() &#123; #ifdef _MSC_VER _set_abort_behavior(0, _WRITE_ABORT_MSG); #endif &#125;&#125; pm;#endif // Prevent lockup when doing Continuous Integration#ifndef NDEBUG#include &lt;exception&gt;#include &lt;iostream&gt;#include &lt;cstdlib&gt;# define m_assert(condition, message) \ do &#123; \ if (! (condition)) &#123; \ std::cerr &lt;&lt; "Assertion `" #condition "` failed in " &lt;&lt; __FILE__ \ &lt;&lt; " line " &lt;&lt; __LINE__ &lt;&lt; ": " &lt;&lt; message &lt;&lt; std::endl; \ std::terminate(); \ &#125; \ &#125; while (false)# define c_assert(condition) \ do &#123; \ if (! (condition)) &#123; \ std::cerr &lt;&lt; "Assertion `" #condition "` failed in " &lt;&lt; __FILE__ \ &lt;&lt; " line " &lt;&lt; __LINE__ &lt;&lt; std::endl; \ std::terminate(); \ &#125; \ &#125; while (false)#else# define m_assert(condition, message) do &#123; if (false) &#123; (void)(condition); \ (void)sizeof(message); &#125; &#125; while (false)# define c_assert(condition) do &#123; if (false) &#123; (void)(condition); &#125; &#125; while (false)#endif#endif // EXAMPLES_ASSERT_HPP test_sol2.lua123obj = bark.my_class.new()obj:g()bark.print_my_class(obj) 打印结果=== namespacing === my_class { b: 25 } assert success]]></content>
      <categories>
        <category>CPP</category>
      </categories>
      <tags>
        <tag>CPP</tag>
        <tag>Lua</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++11的Raw String Literals]]></title>
    <url>%2Fblog%2F2018%2F06%2F17%2Fcpp11_raw_string%2F</url>
    <content type="text"><![CDATA[如何绕过 g++ 4.8.1 那个不能在宏里面使用 R”(…)” 的 bug？ 看到形如：R”” 这样的写法，相信学过 Python 的童鞋会感到似曾相识。Python 支持所谓的 “raw string”。Python 文档这样介绍 raw string： Both string and bytes literals may optionally be prefixed with a letter ‘r’ or ‘R’; such strings are called raw strings and treat backslashes as literal characters. As a result, in string literals, ‘\U’ and ‘\u’ escapes in raw strings are not treated specially. Given that Python 2.x’s raw unicode literals behave differently than Python 3.x’s the ‘ur’ syntax is not supported.从这段文字中我们可以看出，raw string 最大的特点就是：它不会对反斜杠’&#39;进行特殊的转义处理。那么，它的这一特性有什么好处呢？不用正则，不知 raw string 大法好！我们知道，正则表达式里，有很多元字符，当没有 raw string 时，我们需要在书写正则表达式的时候使用’\‘来表示元字符里的’&#39;，这样将导致正则表达式变得冗长，而且可读性也会降低。 . . . C++ 11 中的 raw string，简化了我们在使用 regex 库时正则表达式的书写。下面是我找到的一些资料： C++11 raw strings literals tutorialWikipedia: C++ 11 # New String Literals 示例代码123456789101112131415161718192021222324252627282930313233343536373839#include &lt;iostream&gt;#include &lt;string&gt;int main()&#123; // 一个普通的字符串，'\n'被当作是转义字符，表示一个换行符。 std::string normal_str = "First line.\nSecond line.\nEnd of message.\n"; // 一个raw string，'\'不会被转义处理。因此，"\n"表示两个字符：字符反斜杠 和 字母n。 // 注意其语法格式，稍后会介绍C++ 11中为什么会采用这种语法格式来表达一个raw string。 std::string raw_str = R"(First line.\nSecond line.\nEnd of message.\n)"; std::cout &lt;&lt; normal_str &lt;&lt; std::endl; std::cout &lt;&lt; raw_str &lt;&lt; std::endl; std::cout &lt;&lt; R"foo(Hello, world!)foo" &lt;&lt; std::endl; // raw string可以跨越多行，其中的空白和换行符都属于字符串的一部分。 std::cout &lt;&lt;R"( Hello, world! )" &lt;&lt; std::endl; // 下面两行代码意图说明C++ 11采用一对圆括号以及自定义分割字符串来表示raw string的原因。 // 1. // 如果没有一对圆括号及空的分割字符串做定界处理，R"""将会出现语法错误。Python中，r"""也不会是一个合法的 // raw string literal。 std::cout &lt;&lt; R"(")" &lt;&lt; std::endl; // 输出一个双引号：" // 2. // 自定义分割字符串为：delimiter。分割字符串的长度以及其中包含的字符集，都有明文规定。维基百科： // The string delimiter can be any string up to 16 characters in length, including the empty string. // This string cannot contain spaces, control characters, '(', ')', or the '\' character. // // 如果不使用自定义分割字符串，这里：R"()")"编译器无法识别raw string在何处结束。自定义分割字符串的用途 // 维基百科中也有介绍： // The use of this delimiter string allows the user to have ")" characters within raw string literals. std::cout &lt;&lt; R"delimiter()")delimiter" &lt;&lt; std::endl; // 输出：)" return 0;&#125; 打印结果First line. Second line. End of message. First line.\nSecond line.\nEnd of message.\n Hello, world! Hello, world! &quot; )&quot;分析上面这段代码及其中注释大致讲解了 C++ 11 中的 raw string 的特点。但是为什么我们要在字符串中使用一对小括号呢？我找到了如下资料： What is the rationale for parenthesis in C++11’s raw string literals R“(…)”? C++11 FAQ 中文版：原生字符串标识 所以，小伙伴们以后在 C++ 11 中书写正则表达式的时候，记得用 raw string literals 啊。]]></content>
      <categories>
        <category>CPP</category>
      </categories>
      <tags>
        <tag>CPP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一个实时的专用游戏服务器]]></title>
    <url>%2Fblog%2F2018%2F05%2F02%2Fa_real_time_game_server_and_a_ue4_demo_for_it%2F</url>
    <content type="text"><![CDATA[A realtime dedicated game server. 一个实时的专用游戏服务器. GitHubrealtime-server Video Preview 视频演示 About This Linux/Win Multi-Thread RUDP Bit Stream Add KCP support for muduo C++11 Example Server side 服务端 : realtime-server example Client side 客户端 : UE4 demo State Sync 状态同步 No replication component from UE4, just socket 没有用UE4的网络同步组件, 唯socket而已 UE4: 4.16 - 4.19]]></content>
      <categories>
        <category>GitHub</category>
      </categories>
      <tags>
        <tag>GitHub</tag>
        <tag>UE4</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[跨平台开发之CMake笔记]]></title>
    <url>%2Fblog%2F2018%2F04%2F21%2Fcmake_tutorial%2F</url>
    <content type="text"><![CDATA[因为近来需要把一些 Linux 项目转到 Windows 上来开发, 所以有了一些跨平台开发的笔记, 此篇讲CMake, 供以后查阅. CMake介绍你或许听过好几种 Make 工具，例如 GNU Make ，QT 的 qmake ，微软的 MS nmake，BSD Make（pmake），Makepp，等等。这些 Make 工具遵循着不同的规范和标准，所执行的 Makefile 格式也千差万别。这样就带来了一个严峻的问题：如果软件想跨平台，必须要保证能够在不同平台编译。而如果使用上面的 Make 工具，就得为每一种标准写一次 Makefile ，这将是一件让人抓狂的工作。 就是针对上面问题所设计的工具：它首先允许开发者编写一种平台无关的 CMakeList.txt 文件来定制整个编译流程，然后再根据目标用户的平台进一步生成所需的本地化 Makefile 和工程文件，如 Unix 的 Makefile 或 Windows 的 Visual Studio 工程。从而做到“Write once, run everywhere”。 显然， . . . CMake 是一个比上述几种 make 更高级的编译配置工具。一些使用 CMake 作为项目架构系统的知名开源项目有 VTK、ITK、KDE、OpenCV、OSG 等。 CMake一些有用的网站 CMake官网 CMake入门实战 进阶 将build和项目源文件分离的方法假设项目A的根目录下有一个 CMakeLists, 在项目的根目录新建一个叫 build 的文件夹, 然后进入 build 文件夹内, 执行命令 cmake .. 即可. CMakeLists实例讲解比如有一个目录结构如下的项目 : ├─RealTimeServer ├─RealTimeServer │ ├─CMakeFiles.txt │ ├─a.h │ ├─a.cpp │ ├─b.h │ ├─b.cpp │ ├─TestFolder │ │ ├─a.h │ │ ├─a.cpp │ │ ├─b.h │ │ ├─b.cpp │ └─... ├─Tool这是一个比较通用的CMakeLists.txt : 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120# CMake 最低版本号要求cmake_minimum_required (VERSION 2.8)# support C++11add_definitions(-std=c++11)# 项目信息set (PROJ_NAME RealTimeServer)set (BIN_NAME rts)project ($&#123;PROJ_NAME&#125;)# 设置执行文件输出目录# SET(EXECUTABLE_OUTPUT_PATH $&#123;PROJECT_SOURCE_DIR&#125;/bin)# 设置库输出路径# SET(LIBRARY_OUTPUT_PATH $&#123;PROJECT_SOURCE_DIR&#125;/lib)# 查找当前目录下的所有源文件# 并将名称保存到 DIR_SRCS 变量# aux_source_directory(./$&#123;PROJ_NAME&#125; DIR_SRCS)# 查找当前目录以及子目录下的所有头文件# 并将名称保存到 CURRENT_HEADERS 变量# file(GLOB_RECURSE CURRENT_HEADERS *.h *.hpp)# 此命令可以用来收集源文件 CURRENT_HEADERS 作为变量保存收集的结果。 # 后面为文件过滤器，其中 PROJ_NAME 为起始搜索的文件夹，即在 RealTimeServer 目录下，# 开始收集，而且会遍历子目录# file(# GLOB_RECURSE CURRENT_HEADERS # LIST_DIRECTORIES false# "$&#123;PROJ_NAME&#125;/*.h*"# )# 生成一个名为Include的VS筛选器# source_group("Include" FILES $&#123;CURRENT_HEADERS&#125;) IF(WIN32) # Check if we are on Windows file(GLOB_RECURSE project_headers *.h) file(GLOB_RECURSE project_cpps *.c*) set(all_files $&#123;project_headers&#125; $&#123;project_cpps&#125;) macro(create_filters source_files) if(MSVC) # 获取当前目录 set(current_dir $&#123;CMAKE_CURRENT_SOURCE_DIR&#125;) foreach(src_file $&#123;$&#123;source_files&#125;&#125;) # 求出相对路径 string(REPLACE $&#123;current_dir&#125;/ "" rel_path_name $&#123;src_file&#125;) # 删除相对路径中的文件名部分 string(REGEX REPLACE "(.*)/.*" \\1 rel_path $&#123;rel_path_name&#125;) # 比较是否是当前路径下的文件 string(COMPARE EQUAL $&#123;rel_path_name&#125; $&#123;rel_path&#125; is_same_path) # 替换成Windows平台的路径分隔符 string(REPLACE "/" "\\" rel_path $&#123;rel_path&#125;) if(is_same_path) set(rel_path "\\") endif(is_same_path) # CMake 命令 source_group($&#123;rel_path&#125; FILES $&#123;src_file&#125;) endforeach(src_file) endif(MSVC) endmacro(create_filters) create_filters(all_files) # add_executable(app $&#123;all_files&#125;) # 指定生成目标 add_executable($&#123;PROJ_NAME&#125; $&#123;all_files&#125;) if(MSVC) # Check if we are using the Visual Studio compiler # set_target_properties($&#123;PROJ_NAME&#125; PROPERTIES LINK_FLAGS "/SUBSYSTEM:WINDOWS") # works for all build modes set_target_properties($&#123;PROJ_NAME&#125; PROPERTIES LINK_FLAGS "/SUBSYSTEM:CONSOLE") # works for all build modes target_link_libraries($&#123;PROJ_NAME&#125; wsock32 ws2_32) set_target_properties($&#123;PROJ_NAME&#125; PROPERTIES COMPILE_FLAGS /wd"4819" ) elseif(CMAKE_COMPILER_IS_GNUCXX) # SET(CMAKE_CXX_FLAGS "$&#123;CMAKE_CXX_FLAGS&#125; -mwindows") # Not tested else() message(SEND_ERROR "You are using an unsupported Windows compiler! (Not MSVC or GCC)") endif()elseif(UNIX) file(GLOB_RECURSE all_files "*.*") include_directories($&#123;PROJECT_SOURCE_DIR&#125;) # add_subdirectory(muduo/base) # add_subdirectory(muduo/net) # 指定生成目标 add_executable($&#123;BIN_NAME&#125; $&#123;all_files&#125;) # 添加链接库 target_link_libraries($&#123;BIN_NAME&#125; pthread rt) # Inhibit all warning messages. if(CMAKE_COMPILER_IS_GNUCC OR CMAKE_COMPILER_IS_GNUCXX) # set(CMAKE_CXX_FLAGS "$&#123;CMAKE_CXX_FLAGS&#125; -Wall -Wno-long-long -pedantic") set(CMAKE_CXX_FLAGS "$&#123;CMAKE_CXX_FLAGS&#125; -w") endif() # For gdb set(CMAKE_BUILD_TYPE "Debug") set(CMAKE_CXX_FLAGS_DEBUG "$ENV&#123;CXXFLAGS&#125; -O0 -Wall -g -ggdb") set(CMAKE_CXX_FLAGS_RELEASE "$ENV&#123;CXXFLAGS&#125; -O3 -Wall")else() message(SEND_ERROR "You are on an unsupported platform! (Not Win32 or Unix)")ENDIF() 对于像上面这样一个CMake的CMakeLists来说, 需要着重解释的有以下几点 : add_definitions(-std=c++11) 这句是为了解决 linux 默认不支持 c++11 的问题 set_target_properties(${PROJ_NAME} PROPERTIES LINK_FLAGS &quot;/SUBSYSTEM:WINDOWS&quot;) # works for all build modes 这句是为了解决WinMain的问题, 否则在vs平台会报main非法引用的问题. 这句会影响到vs的 “项目属性-链接器-系统-子系统” target_link_libraries(${PROJ_NAME} wsock32 ws2_32) 这句是为了在vs下链接ws2_32库, windows需要链接这个库才能用socket. 这句会影响到vs的 “项目属性-链接器-输入-附加依赖项” file(GLOB_RECURSE CURRENT_HEADERS *.h *.hpp) source_group(&quot;Include&quot; FILES ${CURRENT_HEADERS}) 这句是为了解决在vs下不显示头文件的问题 set(CMAKE_CXX_FLAGS &quot;${CMAKE_CXX_FLAGS} -w&quot;)-w的意思是关闭编译时的警告，也就是编译后不显示任何warning，因为有时在编译之后编译器会显示一些例如数据转换之类的警告，这些警告是我们平时可以忽略的。-Wall选项意思是编译后显示所有警告。-W选项类似-Wall，会显示警告，但是只显示编译器认为会出现错误的警告。在编译一些项目的时候可以-W和-Wall选项一起使用。这里可以查看gcc的各种警告级别.]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>CrossPlatform</tag>
        <tag>Compile</tag>
        <tag>Make</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git设置代理]]></title>
    <url>%2Fblog%2F2018%2F03%2F11%2Fgit_shadow_socks_proxy%2F</url>
    <content type="text"><![CDATA[介绍目前，网络上可选择的Git远程仓库比较多，其中用的较多的可能就是github和bitbucket（当然，你也可以使用自己搭建的远程仓库）。github和bitbucket的主要区别在于：bitbucket创建私人库是免费的。如果你不介意自己的代码公开，那你就可以使用github。如果，你有些私人的代码的话，又需要版本控制，这时候bitbucket就满足需要了。 但是，在国内的主要问题是：网络不稳定。这样，就会经常发生git不能push的情况。所以，这时候如果你有个代理服务器，就可以通过设置使git通过代理访问远程仓库，达到家和公司代码同步的目的。 Git允许使用三种协议来连接远程仓库：ssh、http、git。所以，如果你要设置代理， . . . 必须首先明确本地git使用何种协议连接远程仓库，然后根据不同协议设置代理。 本文前提socks5代理服务器，默认端口1080 设置SSH协议的代理如果你的远程仓库拥有如下的格式： git@github.com:archerie/learngit.git 那么，你使用的是SSH协议连接的远程仓库。因为git依赖ssh去连接，所以， 我们需要配置ssh的socks5代理实现git的代理。在ssh的配置文件~/.ssh/config（没有则新建）使用ProxyCommand配置： 123456789101112131415161718192021222324252627# Linux 环境Host bitbucket.orgHostname bitbucket.orgUser gitPort 22ProxyCommand nc -x 127.0.0.1:1080 %h %portability# Linux 环境Host github github.comHostname github.comUser gitPort 22ProxyCommand nc -x 127.0.0.1:1080 %h %portability# windows 环境Host bitbucket.orgUser gitPort 22Hostname bitbucket.orgProxyCommand connect -S 127.0.0.1:1080 %h %portability# windows 环境Host github github.comHostname github.comPort 22User gitProxyCommand connect -S 127.0.0.1:1080 %h %p 设置http/https协议代理如果你的远程仓库链接拥有如下格式： 12http://github.com/archerie/learngit.githttps://github.com/archerie/learngit.git 说明你使用的是http/https协议，所以可以使用git配套的CMSSW支持的代理协议：SOCKS4、SOCKS5和HTTPS/HTTPS。可通过配置http.proxy配置, 有两种方式分别为： 直接编辑.gitconfig文件的方式可以直接编辑 git 的设置文件，该文件通常位于用户目录下，名为 .gitconfig，如果看不到，需要显示隐藏文件。在 .gitconfig 文件的末尾加上 1234[https] proxy = https://127.0.0.1:1080[http] proxy = http://127.0.0.1:1080 敲命令的方式当然，你也可以敲命令, 命令如下： 12git config --global http.proxy http://127.0.0.1:1080git config --global https.proxy https://127.0.0.1:1080 经过测试，不需要设置sock5。取消的命令如下： 12git config --global --unset http.proxygit config --global --unset https.proxy 还可以设置只本次用一下代理 : 12# 本次设置git clone https://github.com/example/example.git --config &quot;http.proxy=127.0.0.1:1080&quot; 设置Git协议的代理Git协议是Git提供的一个守护进程，它监听专门的端口（9418），然后提供类似于ssh协议一样的服务，只是它不需要验证。所以，然后用户通过网络都可以使用git协议连接提供git连接的仓库。如果远程仓库的链接是如下形式： git://github.com/archerie/learngit.git 那么，该仓库使用git协议连接。所以，需要使用CMSSW提供的简单脚本去通过socks5代理访问：git-proxy。配置如下： 12git config --global core.gitproxy &quot;git-proxy&quot;git config --global socks.proxy &quot;localhost:1080&quot; 还想了解更多，使用git-proxy –help。]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS备忘]]></title>
    <url>%2Fblog%2F2018%2F02%2F01%2Fcentos_notes%2F</url>
    <content type="text"><![CDATA[查询系统相关信息$ cat /etc/centos-release CentOS release 6.7 (Final) [root@host ~]# uname -i i386安装gcc4.8centos 6 的gcc版本太低才4.4.7, 要安装高版本的4.8才完全支持c++11 . . . 卸载gcc先卸载当前gcc sudo yum remove --skip-broken gcc安装步骤安装 devtoolset-2 工具链 wget http://people.centos.org/tru/devtools-2/devtools-2.repo -O /etc/yum.repos.d/devtools-2.repo yum install devtoolset-2-gcc devtoolset-2-binutils devtoolset-2-gcc-c++ sudo yum install devtoolset-2启用 devtoolset-2 bash 环境 scl enable devtoolset-2 bash检查gcc版本看是否安装成功 gcc --version为了配合cmake, 需要创建相关的软链, cmake才找得到相关编译器 ln -s $(which gcc) /usr/bin/cc ln -s $(which g++) /usr/bin/c++以后每次想启用 devtoolset-2 bash 环境来使用gcc都需要命令 : scl enable devtoolset-2 bash当然你也可以把下面这条语句加入到你的 .bashrc 里来让 devtoolset-2 bash 环境一直保持开启 : source /opt/rh/devtoolset-2/enable]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>CentOS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[3分钟理解一致性hash]]></title>
    <url>%2Fblog%2F2018%2F01%2F27%2F3%E5%88%86%E9%92%9F%E7%90%86%E8%A7%A3%E4%B8%80%E8%87%B4%E6%80%A7hash%2F</url>
    <content type="text"><![CDATA[在了解一致性哈希算法之前，最好先了解一下缓存中的一个应用场景，了解了这个应用场景之后，再来理解一致性哈希算法，就容易多了，也更能体现出一致性哈希算法的优点，那么，我们先来描述一下这个经典的分布式缓存的应用场景。 场景描述假设，我们有三台缓存服务器，用于缓存图片，我们为这三台缓存服务器编号为0号、1号、2号，现在，有3万张图片需要缓存，我们希望这些图片被均匀的缓存到这3台服务器上，以便它们能够分摊缓存的压力。也就是说，我们希望每台服务器能够缓存1万张左右的图片，那么，我们应该怎样做呢？如果我们没有任何规律的将3万张图片平均的缓存在3台服务器上，可以满足我们的要求吗？可以！但是如果这样做，当我们需要访问某个缓存项时，则需要遍历3台缓存服务器，从3万个缓存项中找到我们需要访问的缓存，遍历的过程效率太低，时间太长，当我们找到需要访问的缓存项时，时长可能是不能被接收的，也就失去了缓存的意义，缓存的目的就是提高速度，改善用户体验，减轻后端服务器压力，如果每次访问一个缓存项都需要遍历所有缓存服务器的所有缓存项，想想就觉得很累，那么，我们该怎么办呢？原始的做法是对缓存项的键进行哈希，将hash后的结果对缓存服务器的数量进行取模操作，通过取模后的结果，决定缓存项将会缓存在哪一台服务器上，这样说可能不太容易理解，我们举例说明，仍然以刚才描述的场景为例，假设我们使用图片名称作为访问图片的key，假设图片名称是不重复的，那么，我们可以使用如下公式，计算出图片应该存放在哪台服务器上。 hash（图片名称）% N . . . 因为图片的名称是不重复的，所以，当我们对同一个图片名称做相同的哈希计算时，得出的结果应该是不变的，如果我们有3台服务器，使用哈希后的结果对3求余，那么余数一定是0、1或者2，没错，正好与我们之前的服务器编号相同，如果求余的结果为0， 我们就把当前图片名称对应的图片缓存在0号服务器上，如果余数为1，就把当前图片名对应的图片缓存在1号服务器上，如果余数为2，同理，那么，当我们访问任意一个图片的时候，只要再次对图片名称进行上述运算，即可得出对应的图片应该存放在哪一台缓存服务器上，我们只要在这一台服务器上查找图片即可，如果图片在对应的服务器上不存在，则证明对应的图片没有被缓存，也不用再去遍历其他缓存服务器了，通过这样的方法，即可将3万张图片随机的分布到3台缓存服务器上了，而且下次访问某张图片时，直接能够判断出该图片应该存在于哪台缓存服务器上，这样就能满足我们的需求了，我们暂时称上述算法为HASH算法或者取模算法，取模算法的过程可以用下图表示。 但是，使用上述HASH算法进行缓存时，会出现一些缺陷，试想一下，如果3台缓存服务器已经不能满足我们的缓存需求，那么我们应该怎么做呢？没错，很简单，多增加两台缓存服务器不就行了，假设，我们增加了一台缓存服务器，那么缓存服务器的数量就由3台变成了4台，此时，如果仍然使用上述方法对同一张图片进行缓存，那么这张图片所在的服务器编号必定与原来3台服务器时所在的服务器编号不同，因为除数由3变为了4，被除数不变的情况下，余数肯定不同，这种情况带来的结果就是当服务器数量变动时，所有缓存的位置都要发生改变，换句话说，当服务器数量发生改变时，所有缓存在一定时间内是失效的，当应用无法从缓存中获取数据时，则会向后端服务器请求数据，同理，假设3台缓存中突然有一台缓存服务器出现了故障，无法进行缓存，那么我们则需要将故障机器移除，但是如果移除了一台缓存服务器，那么缓存服务器数量从3台变为2台，如果想要访问一张图片，这张图片的缓存位置必定会发生改变，以前缓存的图片也会失去缓存的作用与意义，由于大量缓存在同一时间失效，造成了缓存的雪崩，此时前端缓存已经无法起到承担部分压力的作用，后端服务器将会承受巨大的压力，整个系统很有可能被压垮，所以，我们应该想办法不让这种情况发生，但是由于上述HASH算法本身的缘故，使用取模法进行缓存时，这种情况是无法避免的，为了解决这些问题，一致性哈希算法诞生了。 我们来回顾一下使用上述算法会出现的问题。 问题1：当缓存服务器数量发生变化时，会引起缓存的雪崩，可能会引起整体系统压力过大而崩溃（大量缓存同一时间失效）。 问题2：当缓存服务器数量发生变化时，几乎所有缓存的位置都会发生改变，怎样才能尽量减少受影响的缓存呢？ 其实，上面两个问题是一个问题，那么，一致性哈希算法能够解决上述问题吗？ 我们现在就来了解一下一致性哈希算法。 一致性哈希算法的基本概念其实，一致性哈希算法也是使用取模的方法，只是，刚才描述的取模法是对服务器的数量进行取模，而一致性哈希算法是对2^32取模，什么意思呢？我们慢慢聊。 首先，我们把二的三十二次方想象成一个圆，就像钟表一样，钟表的圆可以理解成由60个点组成的圆，而此处我们把这个圆想象成由2^32个点组成的圆，示意图如下： 圆环的正上方的点代表0，0点右侧的第一个点代表1，以此类推，2、3、4、5、6……直到2^32-1,也就是说0点左侧的第一个点代表2^32-1 我们把这个由2的32次方个点组成的圆环称为hash环。 那么，一致性哈希算法与上图中的圆环有什么关系呢？我们继续聊，仍然以之前描述的场景为例，假设我们有3台缓存服务器，服务器A、服务器B、服务器C，那么，在生产环境中，这三台服务器肯定有自己的IP地址，我们使用它们各自的IP地址进行哈希计算，使用哈希后的结果对2^32取模，可以使用如下公式示意。 hash（服务器A的IP地址） % 2^32 通过上述公式算出的结果一定是一个0到2^32-1之间的一个整数，我们就用算出的这个整数，代表服务器A，既然这个整数肯定处于0到2^32-1之间，那么，上图中的hash环上必定有一个点与这个整数对应，而我们刚才已经说明，使用这个整数代表服务器A，那么，服务器A就可以映射到这个环上，用下图示意 同理，服务器B与服务器C也可以通过相同的方法映射到上图中的hash环中 hash（服务器B的IP地址） % 2^32 hash（服务器C的IP地址） % 2^32 通过上述方法，可以将服务器B与服务器C映射到上图中的hash环上，示意图如下 假设3台服务器映射到hash环上以后如上图所示（当然，这是理想的情况，我们慢慢聊）。 好了，到目前为止，我们已经把缓存服务器与hash环联系在了一起，我们通过上述方法，把缓存服务器映射到了hash环上，那么使用同样的方法，我们也可以将需要缓存的对象映射到hash环上。 假设，我们需要使用缓存服务器缓存图片，而且我们仍然使用图片的名称作为找到图片的key，那么我们使用如下公式可以将图片映射到上图中的hash环上。 hash（图片名称） % 2^32 映射后的示意图如下，下图中的橘黄色圆形表示图片 好了，现在服务器与图片都被映射到了hash环上，那么上图中的这个图片到底应该被缓存到哪一台服务器上呢？上图中的图片将会被缓存到服务器A上，为什么呢？因为从图片的位置开始，沿顺时针方向遇到的第一个服务器就是A服务器，所以，上图中的图片将会被缓存到服务器A上，如下图所示。 没错，一致性哈希算法就是通过这种方法，判断一个对象应该被缓存到哪台服务器上的，将缓存服务器与被缓存对象都映射到hash环上以后，从被缓存对象的位置出发，沿顺时针方向遇到的第一个服务器，就是当前对象将要缓存于的服务器，由于被缓存对象与服务器hash后的值是固定的，所以，在服务器不变的情况下，一张图片必定会被缓存到固定的服务器上，那么，当下次想要访问这张图片时，只要再次使用相同的算法进行计算，即可算出这个图片被缓存在哪个服务器上，直接去对应的服务器查找对应的图片即可。 刚才的示例只使用了一张图片进行演示，假设有四张图片需要缓存，示意图如下 1号、2号图片将会被缓存到服务器A上，3号图片将会被缓存到服务器B上，4号图片将会被缓存到服务器C上。 一致性哈希算法的优点经过上述描述，我想兄弟你应该已经明白了一致性哈希算法的原理了，但是话说回来，一致性哈希算法能够解决之前出现的问题吗，我们说过，如果简单的对服务器数量进行取模，那么当服务器数量发生变化时，会产生缓存的雪崩，从而很有可能导致系统崩溃，那么使用一致性哈希算法，能够避免这个问题吗？我们来模拟一遍，即可得到答案。 假设，服务器B出现了故障，我们现在需要将服务器B移除，那么，我们将上图中的服务器B从hash环上移除即可，移除服务器B以后示意图如下。 在服务器B未移除时，图片3应该被缓存到服务器B中，可是当服务器B移除以后，按照之前描述的一致性哈希算法的规则，图片3应该被缓存到服务器C中，因为从图片3的位置出发，沿顺时针方向遇到的第一个缓存服务器节点就是服务器C，也就是说，如果服务器B出现故障被移除时，图片3的缓存位置会发生改变 但是，图片4仍然会被缓存到服务器C中，图片1与图片2仍然会被缓存到服务器A中，这与服务器B移除之前并没有任何区别，这就是一致性哈希算法的优点，如果使用之前的hash算法，服务器数量发生改变时，所有服务器的所有缓存在同一时间失效了，而使用一致性哈希算法时，服务器的数量如果发生改变，并不是所有缓存都会失效，而是只有部分缓存会失效，前端的缓存仍然能分担整个系统的压力，而不至于所有压力都在同一时间集中到后端服务器上。 这就是一致性哈希算法所体现出的优点。 hash环的偏斜在介绍一致性哈希的概念时，我们理想化的将3台服务器均匀的映射到了hash环上，如下图所示 但是，理想很丰满，现实很骨感，我们想象的与实际情况往往不一样。 在实际的映射中，服务器可能会被映射成如下模样。 聪明如你一定想到了，如果服务器被映射成上图中的模样，那么被缓存的对象很有可能大部分集中缓存在某一台服务器上，如下图所示。 上图中，1号、2号、3号、4号、6号图片均被缓存在了服务器A上，只有5号图片被缓存在了服务器B上，服务器C上甚至没有缓存任何图片，如果出现上图中的情况，A、B、C三台服务器并没有被合理的平均的充分利用，缓存分布的极度不均匀，而且，如果此时服务器A出现故障，那么失效缓存的数量也将达到最大值，在极端情况下，仍然有可能引起系统的崩溃，上图中的情况则被称之为hash环的偏斜，那么，我们应该怎样防止hash环的偏斜呢？一致性hash算法中使用”虚拟节点”解决了这个问题，我们继续聊。 虚拟节点话接上文，由于我们只有3台服务器，当我们把服务器映射到hash环上的时候，很有可能出现hash环偏斜的情况，当hash环偏斜以后，缓存往往会极度不均衡的分布在各服务器上，聪明如你一定已经想到了，如果想要均衡的将缓存分布到3台服务器上，最好能让这3台服务器尽量多的、均匀的出现在hash环上，但是，真实的服务器资源只有3台，我们怎样凭空的让它们多起来呢，没错，就是凭空的让服务器节点多起来，既然没有多余的真正的物理服务器节点，我们就只能将现有的物理节点通过虚拟的方法复制出来，这些由实际节点虚拟复制而来的节点被称为”虚拟节点”。加入虚拟节点以后的hash环如下。 “虚拟节点”是”实际节点”（实际的物理服务器）在hash环上的复制品,一个实际节点可以对应多个虚拟节点。 从上图可以看出，A、B、C三台服务器分别虚拟出了一个虚拟节点，当然，如果你需要，也可以虚拟出更多的虚拟节点。引入虚拟节点的概念后，缓存的分布就均衡多了，上图中，1号、3号图片被缓存在服务器A中，5号、4号图片被缓存在服务器B中，6号、2号图片被缓存在服务器C中，如果你还不放心，可以虚拟出更多的虚拟节点，以便减小hash环偏斜所带来的影响，虚拟节点越多，hash环上的节点就越多，缓存被均匀分布的概率就越大。]]></content>
      <categories>
        <category>NP</category>
      </categories>
      <tags>
        <tag>Distributed</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[两个例子对比理解Actor模型]]></title>
    <url>%2Fblog%2F2018%2F01%2F17%2Ftwo_example_for_understanding_actor_model%2F</url>
    <content type="text"><![CDATA[Actor模型介绍Actor模式是一种并发模型，与另一种模型共享内存完全相反，Actor模型share nothing。所有的线程(或进程)通过消息传递的方式进行合作，这些线程(或进程)称为Actor。共享内存更适合单机多核的并发编程，而且共享带来的问题很多，编程也困难。 随着多核时代和分布式系统的到来，共享模型已经不太适合并发编程，因此几十年前就已经出现的Actor模型又重新受到了人们的重视。MapReduce就是一种典型的Actor模式，而在语言级对Actor支持的编程语言Erlang又重新火了起来，Scala也提供了Actor，但是并不是在语言层面支持，Java也有第三方的Actor包，Go语言channel机制也是一种类Actor模型。 Actor的基础就是消息传递, Actor由状态(state)、行为(Behavior)和邮箱(mailBox)三部分组成 : 状态(state)：Actor中的状态指的是Actor对象的变量信息，状态由Actor自己管理，避免了并发环境下的锁和内存原子性等问题 行为(Behavior)：行为指定的是Actor中计算逻辑，通过Actor接收到消息来改变Actor的状态 邮箱(mailBox)：邮箱是Actor和Actor之间的通信桥梁，邮箱内部通过FIFO消息队列来存储发送方Actor消息，接受方Actor从邮箱队列中获取消息 . . . 使用Actor模型的好处 事件模型驱动–Actor之间的通信是异步的，即使Actor在发送消息后也无需阻塞或者等待就能够处理其他事情 强隔离性–Actor中的方法不能由外部直接调用，所有的一切都通过消息传递进行的，从而避免了Actor之间的数据共享，想要 观察到另一个Actor的状态变化只能通过消息传递进行询问 位置透明–无论Actor地址是在本地还是在远程机上对于代码来说都是一样的 轻量性–Actor是非常轻量的计算单机，单个Actor仅占400多字节，只需少量内存就能达到高并发 单线程编程 单核单机时代一般都是单线程编程，如果把程序比作一个工厂，那么只有一个工人，这个工人负责所有的事情，所有的原料，工具产品等都放到一个地方，因为只有一个人，因此使用一套工具就行，取原料也不用排队等。 多线程编程-共享内存 到了多核时代，有多个工人，这些工人共同使用一个仓库和车间，干什么都要排队。比如我要从一块钢料切出一块来用，我得等别人先用完。有个扳手，另一个人在用，我得等他用完。两个人都要用一个切割机从一块钢材切一块钢铁下来用，但是一个人拿到了钢材，一个人拿到了切割机，他们互相都不退让，结果谁都干不了活。假如现在有一个任务，找100000以内的素数的个数，最多使用是个线程，如果用共享内存的方法，可以用下面的代码实现。可以看到，这些线程共享了currentNum和totalPrimeCount，对它们做操作时必须上锁。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566public class PrimeCount implements Runnable &#123; private int currentNum = 2; //从2开始找 private int totalPrimeCount = 0; //当前已经找到的 //取一个数，不能重复，最大到100000 private int incrCurrentNum() &#123; synchronized (this) &#123; //如果不用锁，必然会出错。 if(currentNum &gt; 100000) &#123; return -1; &#125; else &#123; int result = currentNum; currentNum++; return result; &#125; &#125; &#125; //把某个线程找到的素数个数加上 private void accPrimeCount(int count) &#123; synchronized (this) &#123; totalPrimeCount += count; &#125; &#125; @Override //一直取数并判断是否为素数，取不到了就把找到的个数累加 public void run() &#123; int primeCount = 0; int num; while((num=incrCurrentNum()) != -1) &#123; if(isPrime(num)) &#123; primeCount++; &#125; &#125; accPrimeCount(primeCount); &#125; private boolean isPrime(int num) &#123; for(int i = 2; i &lt; num; i++) &#123; if(num % i == 0) &#123; return false; &#125; &#125; return true; &#125; @SuppressWarnings("static-access") public static void main(String[] args)&#123; PrimeCount pc = new PrimeCount(); for(int i = 0; i &lt; 10; i++) &#123; new Thread(pc).start(); &#125; try &#123; Thread.currentThread().sleep(5000); &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; System.out.println(pc.getTotalPrimeCount()); &#125; public int getTotalPrimeCount() &#123; return totalPrimeCount; &#125; &#125; 多线程/分布式编程-Actor模型 到了分布式系统时代，工厂已经用流水线了，每个人都有明确分工，这就是Actor模式。每个线程都是一个Actor，这些Actor不共享任何内存，所有的数据都是通过消息传递的方式进行的。如果用Actor模型实现统计素数个数，那么我们需要1个actor做原料的分发，就是提供要处理的整数，然后10个actor加工，每次从分发actor那里拿一个整数进行加工，最终把加工出来的半成品发给组装actor，组装actor把10个加工actor的结果汇总输出。用scala实现，下面是工程的结构：这是它们传递的消息，有一些指令，剩下的都是Int数据:一个Actor的代码结构一般是下面这种结构，不停的接受消息并处理，没有消息就等待：组装者代码：分发者代码：加工者代码：主线程代码：工程代码可以在附件中下载。这个代码实现的效果与前面用Java实现的是一样的，但是各个线程没有共享内存，也没有锁，这样开发起来容易，而且更适合分布式编程，因为分布式编程本身就不适合共享内存。Scala的Actor不能原生的支持分布式，但是Erlang可以，使用Erlang的Actor，分布式编程就和本地编程基本一样。但是Erlang的语法难懂，而且没有变量，几乎所有需要使用循环的地方都得用递归。 参考 : 十分钟理解Actor模式 Actor模型原理]]></content>
      <categories>
        <category>NP</category>
      </categories>
      <tags>
        <tag>Actor</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VirtualBox安装Ubuntu教程]]></title>
    <url>%2Fblog%2F2018%2F01%2F01%2Fvbox_install_ubuntu_tutorial%2F</url>
    <content type="text"><![CDATA[最近因某些原因重装了Win10, 虚拟机也需要重装, 记录一下过程, 供以后查阅, 以免走更多弯路 需准备的工具和材料 虚拟机软件 Ubuntu : 我用的是16.04版本的ubuntu的server版本(不是desktop桌面版) 建议给虚拟机分配20G硬盘空间 建议给虚拟机分配4G内存 . . . 网络设置在安装ubuntu-server之前需知 : 你的那个虚拟机里的网络设置里只留两张网卡, 注意这两张网卡的顺序最好别颠倒, 免得麻烦 网卡1是 : “网络地址转换（NAT）”， 不是那个“NAT网络”噢， 这张网卡是用来访问宿主机和外网的网卡2是 : “仅主机（Host-Olny）网络”， 这张网卡是用来让宿主机是访问你的这个虚拟机的， 这样当虚拟机装了openssh-server (sudo apt-get install openssh-server) 之后就能用ssh工具从宿主机连到你的这个虚拟机了 安装ubuntu的时候有一步是需要你选择主网卡, 此时得记住两张网卡的名字, 比如我的是 enp0s3对应网卡1 和 enp0s8对应网卡2, 那主网卡应该选“网络地址转换（NAT）”的那张, 即网卡1, 因为要上网. 安装完毕之后 : 使用ifconfig命令查看会发现只有一个网卡工作，因为第二块网卡还没有进行配置。 使用vim编辑/etc/network/interfaces，添加第二块网卡的网络配置，宿主机需要长期连接虚拟机，需要为Host-Only网络配置静态IP，IP需要和宿主机的Host-Only网段一致, 到windows的控制面板的网络适配器页面查看VirtualBox Host-Only这个网络适配器的网段, 比如我的是192.168.80.1 ,则 : auto enp0s8 iface enp0s8 inet static address 192.168.80.8重启网络(service networking restart)或者系统。 更换源见原网站 Ubuntu 的软件源配置文件是 /etc/apt/sources.list。将系统自带的该文件做个备份，将该文件替换为下面内容，即可使用 TUNA 的软件源镜像。替换之后记得 sudo apt-get update # 默认注释了源码镜像以提高 apt update 速度，如有需要可自行取消注释 deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-updates main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-updates main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-backports main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-backports main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-security main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-security main restricted universe multiverse # 预发布软件源，不建议启用 # deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-proposed main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-proposed main restricted universe multiversebash增强可参考 Bash定制 必装软件 sudo apt-get install openssh-server sudo apt-get install g++ sudo apt-get install cmake sudo apt-get install gdb 装完gdb之后添加一个pstack脚本方便查看运行时的程序堆栈(用法 : pstack pid) : sudo vi /usr/bin/pstacksudo chmod +x /usr/bin/pstack pstack脚本的内容如下 : 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748#!/bin/shif test $# -ne 1; then echo &quot;Usage: `basename $0 .sh` &lt;process-id&gt;&quot; 1&gt;&amp;2 exit 1fiif test ! -r /proc/$1; then echo &quot;Process $1 not found.&quot; 1&gt;&amp;2 exit 1fi# GDB doesn&apos;t allow &quot;thread apply all bt&quot; when the process isn&apos;t# threaded; need to peek at the process to determine if that or the# simpler &quot;bt&quot; should be used.backtrace=&quot;bt&quot;if test -d /proc/$1/task ; then # Newer kernel; has a task/ directory. if test `/bin/ls /proc/$1/task | /usr/bin/wc -l` -gt 1 2&gt;/dev/null ; then backtrace=&quot;thread apply all bt&quot; fielif test -f /proc/$1/maps ; then # Older kernel; go by it loading libpthread. if /bin/grep -e libpthread /proc/$1/maps &gt; /dev/null 2&gt;&amp;1 ; then backtrace=&quot;thread apply all bt&quot; fifiGDB=$&#123;GDB:-/usr/bin/gdb&#125;if $GDB -nx --quiet --batch --readnever &gt; /dev/null 2&gt;&amp;1; then readnever=--readneverelse readnever=fi# Run GDB, strip out unwanted noise.$GDB --quiet $readnever -nx /proc/$1/exe $1 &lt;&lt;EOF 2&gt;&amp;1 |set width 0set height 0set pagination no$backtraceEOF/bin/sed -n \ -e &apos;s/^\((gdb) \)*//&apos; \ -e &apos;/^#/p&apos; \ -e &apos;/^Thread/p&apos; 若是ubuntu桌面版的话可以安装一下增强功能 桌面版ubuntu安装增强功能 注 : 如果在侧边找到如下图加载的虚拟光驱，就需要先右击，点击弹出，然后才可正常安装增强功能 点击安装增强功能 点击“运行” 输入登录系统的密码，点击授权，就开始自动安装了 如图，为安装界面，安装完成后按下回车键，就按照成功了。 安装好后关闭ubuntu再次启动ubuntu的时候，虚拟机就可以在无缝模式和自动显示尺寸下运行了。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>VBox</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Win10原版重装]]></title>
    <url>%2Fblog%2F2018%2F01%2F01%2Foriginal_win10_reinstall%2F</url>
    <content type="text"><![CDATA[最近因某些原因重装了Win10, 记录一下过程, 供以后查阅, 以免走更多弯路, 搞得新装的Win10都一堆广告… 目的 可以格式化系统盘 原版Win10, 正版激活(某宝买一个非一次性的可重装的激活码, 用这种激活码激活过的机器, 重装后会自动激活的) 装完没有广告和预装的垃圾软件 : 不要用大白菜和老毛桃之类的, 否则装完就会有一堆垃圾软件和改过的主页, 真是无利不起早. . . . 需提前准备的工具 UltraISO(软碟通) : UltraISO(软碟通)下载网站, 下载一个试用版即可 U盘 : 6G以上, 4G可能不够 下载原版Win10的ISO镜像文件到微软官网, 点击”立即下载工具”, 用此工具一步一步的把win10的ISO镜像文件下载下来. 主要注意下面两步要按照下图中的选对 : 用UltraISO把Win10的ISO镜像文件写入U盘 2.1、打开工具“UltraISO”（工具自行搜索名称找到下载），选择“文件~打开”2.2、在路径中选择下载到的Win10 ISO镜像文件2.3、打开后点击“启动~写入硬盘映像”2.4、在弹出的窗口中选择需写入的U盘，U盘需提前插入电脑USB口中，同时不要插入多个U盘先。其它设置保持默认可以，然后点“写入”2.5、弹出的窗口点击“是”（注：过程会清空U盘数据，请注意提前备份U盘数据）2.6、写入中，请耐心等待完成即可2.7、刻录成功后就可以关闭工具2.8、同时检查U盘也会发现里面带了Win10镜像的相关文件了，接下来可以拿它去装逼（机）了 安装Win10 2.9、接着用这个制作好的U盘安装原版Win10，把U盘插入想安装的电脑，按笔记本的品牌（台式机按主板厂商）选择启动热键，热键对应如下表：2.10、制作好的U盘需先插进电脑2.11、接着是一系列的过程截图，有需要注意的地方已注明注意：没有产品密钥请跳过输入密钥，待安装完系统再想办法激活注意：全新不保留原系统的内容可格式化一下系统盘再安装请耐心等待即可，待上一步完成后会自动重启]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>Win10</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[UML类图与类的关系详解]]></title>
    <url>%2Fblog%2F2017%2F11%2F22%2FUML%E7%B1%BB%E5%9B%BE%E4%B8%8E%E7%B1%BB%E7%9A%84%E5%85%B3%E7%B3%BB%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[原文出处 在画类图的时候，理清类和类之间的关系是重点。类的关系有泛化(Generalization)、实现（Realization）、依赖(Dependency)和关联(Association)。其中关联又分为一般关联关系和聚合关系(Aggregation)，合成关系(Composition)。下面我们结合实例理解这些关系。 基本概念类图（Class Diagram）: 类图是面向对象系统建模中最常用和最重要的图，是定义其它图的基础。类图主要是用来显示系统中的类、接口以及它们之间的静态结构和关系的一种静态模型。类图的3个基本组件： 类名 属性 方法。 . . . 泛化泛化(generalization)：表示is-a的关系，是对象之间耦合度最大的一种关系，子类继承父类的所有细节。直接使用语言中的继承表达。在类图中使用带三角箭头的实线表示，箭头从子类指向父类。 实现实现（Realization）:在类图中就是接口和实现的关系。这个没什么好讲的。在类图中使用带三角箭头的虚线表示，箭头从实现类指向接口。 依赖依赖(Dependency)：对象之间最弱的一种关联方式，是临时性的关联。代码中一般指由局部变量、函数参数、返回值建立的对于其他对象的调用关系。一个类调用被依赖类中的某些方法而得以完成这个类的一些职责。在类图使用带箭头的虚线表示，箭头从使用类指向被依赖的类。 关联关联(Association) : 对象之间一种引用关系，比如客户类与订单类之间的关系。这种关系通常使用类的属性表达。关联又分为一般关联、聚合关联与组合关联。后两种在后面分析。在类图使用带箭头的实线表示，箭头从使用类指向被关联的类。可以是单向和双向。 聚合聚合(Aggregation) : 表示has-a的关系，是一种不稳定的包含关系。较强于一般关联,有整体与局部的关系,并且没有了整体,局部也可单独存在。如公司和员工的关系，公司包含员工，但如果公司倒闭，员工依然可以换公司。在类图使用空心的菱形表示，菱形从局部指向整体。 组合组合(Composition) : 表示contains-a的关系，是一种强烈的包含关系。组合类负责被组合类的生命周期。是一种更强的聚合关系。部分不能脱离整体存在。如公司和部门的关系，没有了公司，部门也不能存在了；调查问卷中问题和选项的关系；订单和订单选项的关系。在类图使用实心的菱形表示，菱形从局部指向整体。 多重性多重性(Multiplicity) : 通常在关联、聚合、组合中使用。就是代表有多少个关联对象存在。使用数字..星号（数字）表示。如下图，一个割接通知可以关联0个到N个故障单。 聚合和组合的区别 这两个比较难理解，重点说一下。聚合和组合的区别在于：聚合关系是“has-a”关系，组合关系是“contains-a”关系；聚合关系表示整体与部分的关系比较弱，而组合比较强；聚合关系中代表部分事物的对象与代表聚合事物的对象的生存期无关，一旦删除了聚合对象不一定就删除了代表部分事物的对象。组合中一旦删除了组合对象，同时也就删除了代表部分事物的对象。 实例分析 联通客户响应OSS。系统有故障单、业务开通、资源核查、割接、业务重保、网络品质性能等功能模块。现在我们抽出部分需求做为例子讲解。 大家可以参照着类图，好好理解。 通知分为一般通知、割接通知、重保通知。这个是继承关系。 NoticeService和实现类NoticeServiceImpl是实现关系。 NoticeServiceImpl通过save方法的参数引用Notice,是依赖关系。同时调用了BaseDao完成功能，也是依赖关系。 割接通知和故障单之间通过中间类(通知电路)关联，是一般关联。 重保通知和预案库间是聚合关系。因为预案库可以事先录入，和重保通知没有必然联系，可以独立存在。在系统中是手工从列表中选择。删除重保通知，不影响预案。 割接通知和需求单之间是聚合关系。同理，需求单可以独立于割接通知存在。也就是说删除割接通知，不影响需求单。 通知和回复是组合关系。因为回复不能独立于通知存在。也就是说删除通知，该条通知对应的回复也要级联删除。 经过以上的分析，相信大家对类的关系已经有比较好的理解了。大家有什么其它想法或好的见解，欢迎拍砖。PS：还是那句话：以上类图用Enterprise Architect 7.5所画，在此推荐一下EA,非常不错。可以替代Visio和Rose了。Visio功能不够强大，Rose太重。唯有EA比较合适。]]></content>
      <categories>
        <category>Misc</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[next的文章使用自定义的css]]></title>
    <url>%2Fblog%2F2017%2F11%2F14%2Fnext%E7%9A%84%E6%96%87%E7%AB%A0%E4%BD%BF%E7%94%A8%E8%87%AA%E5%AE%9A%E4%B9%89%E7%9A%84css%2F</url>
    <content type="text"><![CDATA[今天，创建自己的about页面的时候，像使用自定义的css样式，就像这是不是NexT可以使用自定义的CSS样式，片尝试了一下，还是可以的，因为markdown支持html标签，使用自定义的CSS样式还是不错的。下面总结一下具体的使用过程： 添加样式支持 为了不吧原先的像是文件搞得太乱，这里，添加子集的样式文件。 首先，在样式文件的source文件夹下找到css文件夹，打开main.styl文件，在最后添加： 123//My Layer//--------------------------------------------------@import &quot;_my/mycss&quot;; 新建自定义样式找到样式文件夹css 新建_my文件夹，在其中新建mycss.styl文件，之后就可以按照stylus的格式自定义样式了。 例子例如：我想在文章中添加个自定义样式的按钮，怎么做呢？？？ 打开新建的mycss.styl文件，在其中添加样式： 123456789101112131415161718192021.myButton &#123; background-color:#0f94bd; -moz-border-radius:15px; -webkit-border-radius:15px; border-radius:15px; display:inline-block; cursor:pointer; color:#ffffff; font-family:Arial; font-size:17px; padding:11px 27px; text-decoration:none; text-shadow:0px 1px 0px #2f6627;&#125;.myButton:hover &#123; background-color:#5cbf2a;&#125;.myButton:active &#123; position:relative; top:1px;&#125; (ps:这里直接使用的css的格式写的，因为css的代码在网上很好找到，而stylus预处理器的就不那么容易找到了，stylus一样支持css格式，所以在这里直接使用了css文件，没有写成stylus语法。其实让我写我也不会，哈哈！) 引用：在想要引用的时候添加1&lt;a href=&quot;#&quot; class=&quot;myButton&quot;&gt;MyButton&lt;/a&gt;]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>Next</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[修复next搜索框弹不出手机键盘bug和更简洁的搜索框]]></title>
    <url>%2Fblog%2F2017%2F11%2F07%2F%E4%BF%AE%E5%A4%8Dnext%E6%90%9C%E7%B4%A2%E6%A1%86%E5%BC%B9%E4%B8%8D%E5%87%BA%E6%89%8B%E6%9C%BA%E9%94%AE%E7%9B%98bug%E5%92%8C%E6%9B%B4%E7%AE%80%E6%B4%81%E7%9A%84%E6%90%9C%E7%B4%A2%E6%A1%86%2F</url>
    <content type="text"><![CDATA[动机 next自带的搜索框有手机键盘不能自动弹出来的bug, 需要点击两次才可以弹出来 渴望拥有更简洁的搜索框 展示图如我博客右上角所示 GitHub我的博客源码, 欢迎Fork+Star. 如果你也喜欢这样的搜索框, 参考这个commit便可改成跟我一样.]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>Next</tag>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何同时把项目放在coding和github之ssh的config篇]]></title>
    <url>%2Fblog%2F2017%2F10%2F20%2F%E5%A6%82%E4%BD%95%E5%90%8C%E6%97%B6%E6%8A%8A%E9%A1%B9%E7%9B%AE%E6%94%BE%E5%9C%A8coding%E5%92%8Cgithub%E4%B9%8Bssh%E7%9A%84config%E7%AF%87%2F</url>
    <content type="text"><![CDATA[无法使用 22 端口的 SSH 服务怎么办？遇到了以下这两种错误怎么办? connect to host git.coding.net port 22: Connection timed out connect to host github.com port 22: Connection timed out SSH 的默认端口是 22，有时您或您的公司的防火墙会完全屏蔽掉这个端口。如果此时您不方便通过 HTTPS 方式进行 Git 操作，您可以使用 Coding.net和GitHub 提供的 443 端口的 SSH 服务. ssh的config配置在home目录下的.ssh文件夹里新建一个config文件, 添加如下代码即可 12345678910111213Host github.comUser &quot;xxxxx@email.com&quot;Hostname ssh.github.comPreferredAuthentications publickeyIdentityFile ~/.ssh/id_rsaPort 443Host git.coding.netUser &quot;xxxxx@email.com&quot;Hostname git-ssh.coding.netPreferredAuthentications publickeyIdentityFile ~/.ssh/id_rsaPort 443 测试命令您需要确保 SSH 已配置成功，然后执行以下命令测试： ssh -T git@git.coding.net ssh -T git@github.com]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何展开next的所有标题]]></title>
    <url>%2Fblog%2F2017%2F10%2F19%2F%E5%A6%82%E4%BD%95%E5%B1%95%E5%BC%80next%E7%9A%84%E6%89%80%E6%9C%89%E6%A0%87%E9%A2%98%2F</url>
    <content type="text"><![CDATA[根目录下的\themes\next\source\css_custom\custom.styl 的最上方加入一行加入一行 .post-toc .nav .nav-child { display: block; }即可]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>Next</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vscode配置与技巧]]></title>
    <url>%2Fblog%2F2017%2F10%2F16%2Fvscode%E9%85%8D%E7%BD%AE%E4%B8%8E%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[序听说网易云音乐可以一玩就是一个下午, 但有些编辑器怕是一玩就是一辈子… 转vscode原因因为工作关系, sublime对于gb2312编码的问题无法良好解决.即使装了convert2utf还是无法解决搜索中文字符串的问题, 因为搜索的时候sublime是没有转码的所以无法搜到想要的中文.所以转战vscode, 谨以这篇笔记来记录vscode心得. . . . 编码问题摘自 比如解决我上述的gb2312乱码的问题的步骤如下 : 在你项目的根目录新建一个名为 .vscode的文件夹(其实这个文件夹就是专门针对这个项目的配置文件了) 在文件夹里新建一个名为 settings.json 的文件 上述文件中加入一对花括号 花括号中添加下面内容 12345678&quot;files.encoding&quot;: &quot;shiftjis&quot;,&quot;files.encoding&quot;: &quot;eucjp&quot;,&quot;files.encoding&quot;: &quot;big5hkscs&quot;,&quot;files.encoding&quot;: &quot;Big5&quot;,&quot;files.encoding&quot;: &quot;GB18030&quot;,&quot;files.encoding&quot;: &quot;GBK&quot;,&quot;files.encoding&quot;: &quot;utf8&quot;,&quot;files.encoding&quot;: &quot;GB2312&quot;, 这样支持这样几种编码方式，最下面的就是默认的， 这个默认支持GB2312,还需要其他编码可以自己按需要添加 设置搜索文件排除文件夹例如在此工作区设置内设置不搜索Map文件里的文件和扩展名为xml的文件, 1234&quot;search.exclude&quot;: &#123; &quot;**/Map&quot;: true, &quot;*.xml&quot;: true,&#125;, vscode常用快捷键以及改键 ctrl+p 搜文件 ctrl+shift+o 搜当前文件的符号 f12 转到定义 f12+ctrl 转到声明 alt+-&gt; 导航前进 alt+&lt;- 导航后退 ctrl+shift+\ 跳到对应的括号 ctrl+shift+p 打开命令输入框 ctrl+shift+a 切换块注释 ctrl+/ 切换行注释 多行编辑 : alt+shift+拖动鼠标左键或者alt+左键多处选择 不同机器间同步vscode设置的插件(必装) Shan khan的Settings Sync插件, 可以同步你的快键键/用户设置/icon/snippet/主题等等, 这样你另一台电脑上就可以同步你在公司所用的设置了. 另一个机器也获得上传settings权限的方法 这个是插件官方说明页面没有写的, 官方页面只教了你如何在另一台机器上下载之前机器的settings 到另外一个机器也装上这个插件, 然后ctrl+shift+p调出控制命令板然后输出sync选中Sync : Advanced Options, 然后选中Edit Extension local settings, 就会打开一个配置文件, 然后填入你的token即可 Vim插件1:amVim作者为auiWorks的名为amVim的插件, 装上即可使用, 无须配置, 跟vscode默认的多行编辑也不冲突 切记在键盘快捷方式中把amVim的ctrl+c快捷键改成其他键或者直接删除, 否则vscode内的东西无法复制到vscode外 MetaGo(一个类似EasyMotion的插件)目前类似EasyMotion的插件只有这个兼容amVim, 改键改成ctrl+shift之后非常好用. Vim插件2:vim(不推荐)作者为vscodevim的名为vim的插件, 不推荐使用这个插件, 下面说一下他的不足之处 vscode的欢迎页就有推荐vim的插件, 几乎所有键盘映射都移植过来了,还加了一些特性,比如多行编辑. 如果要用此插件建议改键 alt+shift+f 从在文件夹中查找改为当前文件中查找, 因为ctrl+f跟vim的冲突 alt+shift+q 改为格式化选中代码快捷键 ctrl+d改为ctrl+q, 因为ctrl+d跟vim的冲突 难用之处不过这个插件自带的多行编辑极为难用.而且他的多行编辑还处于beta阶段, 所以我们还是使用vscode自带的多行编辑, 解决方案首先因为vscode多行编辑的ctrl+d快捷键被插件占用了, 所以我们要把vscode原来的ctrl+d改为ctrl+q 对于多个不止一个单字符多处编辑的情况 : 123456当处于normal mode的时候, 按下v键, 选中想要多处编辑的字符串, 然后按ctrl+q,此时vscode编辑器左下角会提示你目前处于visual mode multi cursor状态,我们得按下esc退到normal mode multi cursor状态,然后按下i进入insert mode multi mode状态, 此时就可以多处编辑了,按两次esc就可以退回到normal mode了 对于多个单字符多处编辑的情况 : 因为vim的v模式选中一个字符, vscode的ctrl+q选中了多个单字符还是无法同时编辑, 所以我们用vim的宏, 例子如下 : 1234567891011121314151617比如：aaa,bbb,ccc,dddeee,fff,ggg,hhhiii,jjj,kkk,lll怎样把b, f, j后面的逗号改成引号？ 最快速方便的方法是使用宏命令模式下按qa进入录制状态，按照以下顺序操作就可以了，“#”字符之后为注释，宏将保存在寄存器a中0 #定位到行首2f, #定位到第二个,字符r&apos; #将光标下的字符替换为&apos;j #进入下一行q #退出宏录制状态针对剩余的行调用宏就可以了，比如在命令行模式下键入“100@a”，就是重复执行100次或者键入&quot;@@&quot;一个一个的执行, &quot;@@&quot;的意思是执行最近录入的一个宏 对于某一纵列多行编辑的情况 : 12345vim进了多行编辑模式：&lt;ESC&gt;之后按CTRL+V进入visual block模式（列编辑）。光标移到某行行首，上下键选择行，按I（i的大写字母），输入##，然后按&lt;ESC&gt;键，这样就在多行行首添加##了。也可以在多行的固定位置添加固定字符。切记一定要按了I之后再按键盘上的home或者end键光标才能百分之百到行首或者行尾如果要删除这些##，进入visual block模式，选中这些##，按d即可。 还可以使用vscode自带的多行编辑快捷键, alt+shift+拖动鼠标左键或者alt+左键多处选择 Lua插件lua插件三件套(推荐) keyring 的 Lua插件来完成提示 xxxg0001的lua-for-vscode来完成跳转 trixnz的vscode-lua来完成代码linting和列出属性以及方法 luaide国人的插件(不推荐)参考LuaIde文档导航页 到这个页面下载他的免费版本安装即可. luaide免费版本的已知问题 对于包含lua文件很多的文件夹来说:他要扫描非常久, 而且每次都要扫描, 解决方案就是把要用的小文件夹单独开一个vscode窗口来工作 有时候不解析, 无法跳转或者无法列出当前文件的方法和属性:随便编辑一下这个lua文件, 再ctrl+z, 这个文件就被解析了 Python插件直接安装vscode商城中推荐的作者为Don Jayamanne的python插件 把terminal换成GitBash如果你的PATH里有 git bash 的话, 按照直接 Shift-Ctrl-p 然后 输入 Select Default Shell即可选择.]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>VSCode</tag>
        <tag>Vim</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Base64编码原理与应用]]></title>
    <url>%2Fblog%2F2017%2F10%2F13%2FBase64%E7%BC%96%E7%A0%81%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8%2F</url>
    <content type="text"><![CDATA[简单解释作者：郭无心链接：https://www.zhihu.com/question/36306744/answer/71626823来源：知乎著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 我们知道在计算机中任何数据都是按ascii码存储的，而ascii码的128～255之间的值是不可见字符。而在网络上交换数据时，比如说从A地传到B地，往往要经过多个路由设备，由于不同的设备对字符的处理方式有一些不同，这样那些不可见字符就有可能被处理错误，这是不利于传输的。 所以就先把数据先做一个Base64编码，统统变成可见字符，这样出错的可能性就大降低了。 对证书来说，特别是根证书，一般都是作Base64编码的，因为它要在网上被许多人下载。 电子邮件的附件一般也作Base64编码的，因为一个附件数据往往是有不可见字符的。 那么Base64到底是怎样编码的呢？简单来说，任何一个数据无非可以看作一个比特流，如01000100010011101100111010111100011001010……那么我们取6个比特为一组，计算它的ascii值，得到一个字符，这个字符肯定是可见字符，好，把它对应的字符写出来，再取6个比特，计算…，如此下去，直到最后，就完成了编码。 1.标准base64只有64个字符（英文大小写、数字和+、/）以及用作后缀等号；2.base64是把3个字节变成4个可打印字符，所以base64编码后的字符串一定能被4整除（不算用作后缀的等号）；3.等号一定用作后缀，且数目一定是0个、1个或2个。 这是因为如果原文长度不能被3整除，base64要在后面添加\0凑齐3n位。 为了正确还原，添加了几个\0就加上几个等号。 显然添加等号的数目只能是0、1或2；4.严格来说base64不能算是一种加密，只能说是编码转换。 使用base64的初衷。 是为了方便把含有不可见字符串的信息用可见字符串表示出来，以便复制粘贴 . . . 详细解释转自Base64编码原理与应用 Base64编码原理 Base64编码之所以称为Base64，是因为其使用64个字符来对任意数据进行编码，同理有Base32、Base16编码。标准Base64编码使用的64个字符为： 这64个字符是各种字符编码（比如ASCII编码）所使用字符的子集，基本，并且可打印。唯一有点特殊的是最后两个字符，因对最后两个字符的选择不同，Base64编码又有很多变种，比如Base64 URL编码。 Base64编码本质上是一种将二进制数据转成文本数据的方案。对于非二进制数据，是先将其转换成二进制形式，然后每连续6比特（2的6次方=64）计算其十进制值，根据该值在上面的索引表中找到对应的字符，最终得到一个文本字符串。 假设我们要对 Hello! 进行Base64编码，按照ASCII表，其转换过程如下图所示： 可知 Hello! 的Base64编码结果为 SGVsbG8h ，原始字符串长度为6个字符，编码后长度为8个字符，每3个原始字符经Base64编码成4个字符，编码前后长度比4/3，这个长度比很重要 - 比原始字符串长度短，则需要使用更大的编码字符集，这并不我们想要的；长度比越大，则需要传输越多的字符，传输时间越长。Base64应用广泛的原因是在字符集大小与长度比之间取得一个较好的平衡，适用于各种场景。 是不是觉得Base64编码原理很简单？ 但这里需要注意一个点：Base64编码是每3个原始字符编码成4个字符，如果原始字符串长度不能被3整除，那怎么办？使用0值来补充原始字符串。 以 Hello!! 为例，其转换过程为： 注：图表中蓝色背景的二进制0值是额外补充的。 Hello!! Base64编码的结果为 SGVsbG8hIQAA 。最后2个零值只是为了Base64编码而补充的，在原始字符中并没有对应的字符，那么Base64编码结果中的最后两个字符 AA 实际不带有效信息，所以需要特殊处理，以免解码错误。 标准Base64编码通常用 = 字符来替换最后的 A，即编码结果为 SGVsbG8hIQ==。因为 = 字符并不在Base64编码索引表中，其意义在于结束符号，在Base64解码时遇到 = 时即可知道一个Base64编码字符串结束。 如果Base64编码字符串不会相互拼接再传输，那么最后的 = 也可以省略，解码时如果发现Base64编码字符串长度不能被4整除，则先补充 = 字符，再解码即可。 解码是对编码的逆向操作，但注意一点：对于最后的两个 = 字符，转换成两个 A 字符，再转成对应的两个6比特二进制0值，接着转成原始字符之前，需要将最后的两个6比特二进制0值丢弃，因为它们实际上不携带有效信息。 为了理解Base64编码解码过程，个人实现了一个非常简陋的Base64编码解码程序，见：youngsterxyf/xiaBase64。 由于Base64应用广泛，所以很多编程语言的标准库都内置Base64编码解码包，如： PHP：base64_encode、base64_decode Python：base64包 Go：encoding/base64 ... Base64编码应用 本文开始提到的青云应用例子只是Base64编码的应用场景之一。由于Base64编码在字符集大小与编码后数据长度之间做了较好的平衡，以及Base64编码变种形式的多样，使得Base64编码的应用场景非常广泛。下面举2个常用常见的例子。 HTML内嵌Base64编码图片 前端在实现页面时，对于一些简单图片，通常会选择将图片内容直接内嵌在页面中，避免不必要的外部资源加载，增大页面加载时间，但是图片数据是二进制数据，该怎么嵌入呢？绝大多数现代浏览器都支持一种名为 Data URLs 的特性，允许使用Base64对图片或其他文件的二进制数据进行编码，将其作为文本字符串嵌入网页中。以百度搜索首页为例，其中语音搜索的图标是个背景图片，其内容以 Data URLs 形式直接写在css中，这个css内容又直接嵌在HTML页面中，如下图所示： Data URLs 格式为：url(data:文件类型;编码方式,编码后的文件内容)。 当然，也可以直接基于image标签嵌入图片，如下所示： &lt;img alt=&quot;Embedded Image&quot; src=&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADIA...&quot; /&gt; 但请注意：如果图片较大，图片的色彩层次比较丰富，则不适合使用这种方式，因为其Base64编码后的字符串非常大，会明显增大HTML页面，影响加载速度。 MIME（多用途互联网邮件扩展） 我们的电子邮件系统，一般是使用SMTP（简单邮件传输协议）将邮件从客户端发往服务器端，邮件客户端使用POP3（邮局协议，第3版本）或IMAP（交互邮件访问协议）从服务器端获取邮件。 SMTP协议一开始是基于纯ASCII文本的，对于二进制文件（比如邮件附件中的图像、声音等）的处理并不好，所以后来新增MIME标准来编码二进制文件，使其能够通过SMTP协议传输。 举例来说，我给自己发封邮件，正文为空，带一个名为hello.txt的附件，内容为 您好！世界！。导出邮件源码，其关键部分如下图所示： MIME-Version: 1.0：表示当前使用MIME标准1.0版本。 Content-Type: text/plain; name="hello.txt"：表示附件文件名为 hello.txt ，格式为纯文本。 Content-Transfer-Encoding: base64：表示附件文件内容使用base64编码后传输。 5oKo5aW977yM5LiW55WM77yB：则是文件内容 您好，世界！ Base64编码后的结果。 不过，MIME使用的不是标准Base64编码。 切忌误用 可能会有人在不理解Base64编码的情况下，将其误用于数据加密或数据校验。 Base64是一种数据编码方式，目的是让数据符合传输协议的要求。标准Base64编码解码无需额外信息即完全可逆，即使你自己自定义字符集设计一种类Base64的编码方式用于数据加密，在多数场景下也较容易破解。 对于数据加密应该使用专门的目前还没有有效方式快速破解的加密算法。比如：对称加密算法AES-128-CBC，对称加密需要密钥，只要密钥没有泄露，通常难以破解；也可以使用非对称加密算法，如 RSA，利用极大整数因数分解的计算量极大这一特点，使得使用公钥加密的数据，只有使用私钥才能快速解密。 对于数据校验，也应该使用专门的消息认证码生成算法，如 HMAC - 一种使用单向散列函数构造消息认证码的方法，其过程是不可逆的、唯一确定的，并且使用密钥来生成认证码，其目的是防止数据在传输过程中被篡改或伪造。将原始数据与认证码一起传输，数据接收端将原始数据使用相同密钥和相同算法再次生成认证码，与原有认证码进行比对，校验数据的合法性。 那么针对各大网站被脱库的问题，请问应该怎么存储用户的登录密码？ 答案是：在注册时，根据用户设置的登录密码，生成其消息认证码，然后存储用户名和消息认证码，不存储原始密码。每次用户登录时，根据登录密码，生成消息认证码，与数据库中存储的消息认证码进行比对，以确认是否为有效用户，这样即使网站被脱库，用户的原始密码也不会泄露，不会为用户使用的其他网站带来账号风险。 当然，使用的消息认证码算法其哈希碰撞的概率应该极低才行，目前一般在HMAC算法中使用SHA256。对于这种方式需要注意一点：防止用户使用弱密码，否则也可能会被暴力破解。现在的网站一般要求用户密码6个字符以上，并且同时有数字和大小写字母，甚至要求有特殊字符。 另外，也可以使用加入随机salt的哈希算法来存储校验用户密码。这里暂不细述。 总结 Base64兼顾字符集大小和编码后数据长度，并且可以灵活替换字符集的最后两个字符，以应对多样的需求，使其适用场景非常广泛。 当然，很多场景下有多种编码方式可选择，并非Base64编码不可，视需求，权衡利弊而定。]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>Base64</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多人快节奏游戏五之演示Demo]]></title>
    <url>%2Fblog%2F2017%2F10%2F10%2F%E5%A4%9A%E4%BA%BA%E5%BF%AB%E8%8A%82%E5%A5%8F%E6%B8%B8%E6%88%8F%E4%BA%94%E4%B9%8B%E6%BC%94%E7%A4%BADemo%2F</url>
    <content type="text"><![CDATA[在浏览器中玩 移动蓝球 ：受 Player1 控制, 用左右箭头键 移动红球 ：受 Player2 控制, 用A和D键 canvas { border: dotted 1px; padding: 0; background: lightgray; } This is a sample implementation of a client-server architecture demonstrating the main concepts explained in my Fast-Paced Multiplayer(原文出处) series of articles. It won’t make much sense unless you’ve read the articles first. The code is pure JavaScript and it’s fully contained in this page. It’s less than 500 lines of code, including a lot of comments, showing that once you really understand the concepts, implementing them is relatively straightforward. Although it’s not production-quality code, you may use this code in your own applications. Credit is appreciated although not required. Player 1 view - move with LEFT and RIGHT arrow keys Lag = ms · Prediction · Reconciliation · Interpolation Waiting for connection… Server view · Update times per second Player 2 view - move with A and D keys Lag = ms · Prediction · Reconciliation · Interpolation Waiting for connection… Guided Tour Move the blue ball. There’s considerable delay between pressing the arrow keys and the blue ball actually moving. Without client-side prediction, the client only renders the new position of the ball only after a round-trip to the server. Because of the 250ms lag, this takes a while. Set the player 1 Lag to 0ms, and try again. Now the client and the server move in sync because there’s no delay between them, but the movement isn’t smooth, because the server only updates its internal state 3 times per second. If you increase the update rate of the server to 60, we get smooth movement. But this is not a very realistic scenario. Set the player 1 lag back to 250ms, and the server update rate back to 3. This is closer to the awful conditions where a real game still needs to work. Client-side prediction and server reconciliation to the rescue! Enable both of them for Player 1 and move the blue ball. Now the movement is very smooth, and there’s no perceptible delay between pressing the arrow keys and moving the ball. This still works if you make the conditions even worse - try setting the player 1 lag to 500ms and the server update rate to 1. Now things look fantastic for player 1’s own entity, the blue ball. However, player 2’s view of this same entity looks terrible. Because the low update rate of the server, player 2 only gets a new position for player 1’s entity once per second, so the movement is very jumpy. Enabling client-side prediction and server reconciliation for player 2 do nothing to smooth the movement of the blue ball, because these techniques only affect how a player renders its own entity. It does make a difference if you move the red ball, but now we have the same jumpiness in player 1’s view. To solve this, we use entity interpolation. Enable entity interpolation for player 2 and move the blue ball. Now it moves smoothly, but is always rendered “in the past” compared to player 1 and to the server. You may notice the speed of the interpolated entities may vary. This is an artifact of the interpolation, caused by setting the server update rate too low in relationship with the speeds. This effect should disappear almost entirely if you set the server update rate to 10, which is still pretty low. Summary Client-Side Prediction and Server Reconciliation are very powerful techniques to make multiplayer games feel responsive even under extremely bad network conditions. Therefore, they are a fundamental part of almost any client/server multiplayer network architecture. // ============================================================================= // An Entity in the world. // ============================================================================= var Entity = function() { this.x = 0; this.speed = 2; // units/s this.position_buffer = []; } // Apply user's input to this entity. Entity.prototype.applyInput = function(input) { this.x += input.press_time*this.speed; } // ============================================================================= // A message queue with simulated network lag. // ============================================================================= var LagNetwork = function() { this.messages = []; } // "Send" a message. Store each message with the timestamp when it should be // received, to simulate lag. LagNetwork.prototype.send = function(lag_ms, message) { this.messages.push({recv_ts: +new Date() + lag_ms, payload: message}); } // Returns a "received" message, or undefined if there are no messages available // yet. LagNetwork.prototype.receive = function() { var now = +new Date(); for (var i = 0; i < this.messages.length; i++) { var message = this.messages[i]; if (message.recv_ts]]></content>
      <categories>
        <category>GS</category>
      </categories>
      <tags>
        <tag>GabrielGambetta</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[白话TCP快速重传]]></title>
    <url>%2Fblog%2F2017%2F09%2F23%2F%E7%99%BD%E8%AF%9Dtcp%E5%BF%AB%E9%80%9F%E9%87%8D%E4%BC%A0%2F</url>
    <content type="text"><![CDATA[快速重传算法概绍在收到一个失序的报文段时， TCP立即需要产生一个ACK(一个重复的ACK)。这个重复的ACK不应该被迟延。该重复的ACK的曰的在于让对方知道收到一个失序的报文段，并告诉对方自己希望收到的序号。 由于我们不知道一个重复的ACK足由一个丢失的报文段引起的，还是由于仅仅出现了几个报文段的重新排序，因此我们等待少量重复的ACK到来。假如这只是一些报文段的重新排序，则在重新排序的报文段被处理并产生一个新的ACK之前，只可能产生1～2个重复的ACK。如果一连串收到3个或3个以上的重复ACK，就非常可能足一个报文段丢失了。于足我们就重传丢失的数据报文段，而无需等待超时定时器溢出。这就是快速重传算法。接下来执行的不是慢启动算法而是拥塞避免算法。这就是快速恢复算法。 在这种情况下没有执行慢启动的原因是由于收到重复的ACK不仅仅告诉我们一个分组丢失了。由于接收方只有在收到另一个报文段耐才会产生重复的ACK，而该报文段已经离开了网络并进入了接收方的缓存。也就是说，在收发两端之问仍然有流动的数据，而我们不想执行慢启动来突然减少数据流。 . . . 为什么是3个重复ACK之后就要重传?两次duplicated ACK肯定是乱序造成的！丢包肯定会造成三次duplicated ACK!假定通信双方如下，A发送4个TCP Segment 给B，编号如下，N-1成功到达，因为A收到B的ACK(N)，其它按照到达顺序，分别收到ACK(N)的数目： A ———&gt; BA方发送顺序N-1，N，N+1，N+2B方到达顺序N-1，N，N+1，N+2 A收到1个ACK (N)N-1，N，N+2，N+1 A收到1个ACK (N)N-1，N+1，N，N+2 A收到2个ACK (N)N-1，N+1，N+2，N A收到3个ACK (N)N-1，N+2，N，N+1 A收到2个ACK (N)N-1，N+2，N+1，N A收到3个ACK (N)如果N丢了，没有到达BN-1，N+1，N+2 A收到3个ACK (N)N-1，N+2，N+1 A收到3个ACK (N)TCP segment 乱序 有2/5 = 40% 的概率会造成A收到三次 duplicated ACK(N);而如果N丢了，则会100%概率A会收到三次duplicated ACK(N);基于以上的统计，当A接收到三次 duplicated ACK(N)启动 Fast Retransmit 算法是合理的，即立马retransmit N，可以起到Fast Recovery的功效，快速修复一个丢包对TCP管道的恶劣影响。而如果A接收到二次 duplicated ACK(N)，则一定说明是乱序造成的，即然是乱序，说明 数据都到达了B，B的TCP负责重新排序而已，没有必要A再来启动Fast Retransmit算法。补充阅读——————————————–TCP segment 乱序的由来TCP segment 封装在IP包里，如果IP包乱序，则相应TCP也会乱序，乱序的原因一般如下：1）ECMP 负载均衡多路径的负载均衡，基于per-packet load balance，比如 packet 1，3，5…走路径1，packet 2，4，6…走路径2，很难保证packet 1 在 packet 2 之前到达目的地。Per-session load balance 会基于TCP五元组来负载均衡，同一个TCP会话会走同一条路径，克服多路径造成的乱序。2）路由器内部流量调度有些路由器采用多个流量处理单元，比如packet 1，3，5…由处理单元1来处理，packet 2，4，6…由处理单元2来处理，也很难保证packet 1 在 packet 2 之前到达目的地。TCP接收到乱序的segment，会放在自己的接收缓冲区，等所有乱序的segment 都顺利到达，TCP重新排序，并将数据提交给 application。乱序的segment 会占用接收缓冲区，直接造成B advertised window size 变小，造成对方A发送window 一直在变小，影响A发送效率。即使A不快速重传，最后也会由retransmit timer timeout 超时重传，但这个时候A的发送window 非常小，发送速率也从天上掉到了地下。———-///——–在没有fast retransmit / recovery 算法之前，重传依靠发送方的retransmit timeout，就是在timeout内如果没有接收到对方的ACK，默认包丢了，发送方就重传，包的丢失原因 1）包checksum 出错 2）网络拥塞 3）网络断，包括路由重收敛，但是发送方无法判断是哪一种情况，于是采用最笨的办法，就是将自己的发送速率减半，即CWND 减为1/2，这样的方法对2是有效的，可以缓解网络拥塞，3则无所谓，反正网络断了，无论发快发慢都会被丢；但对于1来说，丢包是因为偶尔的出错引起，一丢包就对半减速不合理。于是有了fast retransmit 算法，基于在反向还可以接收到ACK，可以认为网络并没有断，否则也接收不到ACK，如果在timeout 时间内没有接收到&gt; 2 的duplicated ACK，则概率大事件为乱序，乱序无需重传，接收方会进行排序工作；而如果接收到三个或三个以上的duplicated ACK，则大概率是丢包，可以逻辑推理，发送方可以接收ACK，则网络是通的，可能是1、2造成的，先不降速，重传一次，如果接收到正确的ACK，则一切OK，流速依然（包出错被丢）。而如果依然接收到duplicated ACK，则认为是网络拥塞造成的，此时降速则比较合理。]]></content>
      <categories>
        <category>NP</category>
      </categories>
      <tags>
        <tag>TCP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于hexo的next个性化配置小技巧]]></title>
    <url>%2Fblog%2F2017%2F09%2F21%2F%E5%9F%BA%E4%BA%8Ehexo%E7%9A%84next%E4%B8%AA%E6%80%A7%E5%8C%96%E9%85%8D%E7%BD%AE%E5%B0%8F%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[原文出处 1. 在右上角或者左上角实现fork me on github实现效果图具体实现方法点击这里挑选自己喜欢的样式，并复制代码。 例如，我是复制如下代码：然后粘贴刚才复制的代码到themes/next/layout/_layout.swig文件中(放在&lt;div class=&quot;headband&quot;&gt;&lt;/div&gt;的下面)，并把href改为你的github地址2.添加RSS实现效果图具体实现方法切换到你的blog（我是取名blog，具体的看你们的取名是什么）的路径，例如我是在/Users/chenzekun/Code/Hexo/blog这个路径上，也就是在你的根目录下然后安装 Hexo 插件：(这个插件会放在node_modules这个文件夹里)1$ npm install &minus;&minus;save hexo-generator-feed接下来打开画红线的文件，如下图：在里面的末尾添加：(请注意在冒号后面要加一个空格，不然会发生错误！)123# Extensions## Plugins: http://hexo.io/plugins/plugins: hexo-generate-feed然后打开next主题文件夹里面的_config.yml,在里面配置为如下样子：(就是在rss:的后面加上/atom.xml,注意在冒号后面要加一个空格)1234# Set rss to false to disable feed link.# Leave rss as empty to use site's feed link.# Set rss to specific value if you have burned your feed already.rss: /atom.xml配置完之后运行：1$ hexo g重新生成一次，你会在./public 文件夹中看到 atom.xml 文件。然后启动服务器查看是否有效，之后再部署到 Github 中。3. 添加动态背景实现效果图具体实现方法这个我之前有一篇文章有讲过了，详情点击我的博客4. 实现点击出现桃心效果实现效果图具体实现方法在网址输入如下1http://7u2ss1.com1.z0.glb.clouddn.com/love.js然后将里面的代码copy一下，新建love.js文件并且将代码复制进去，然后保存。将love.js文件放到路径/themes/next/source/js/src里面，然后打开\themes\next\layout\_layout.swig文件,在末尾（在前面引用会出现找不到的bug）添加以下代码：12&lt;!-- 页面点击小红心 --&gt;&lt;script type="text/javascript" src="/js/src/love.js"&gt;&lt;/script&gt;5. 修改文章内链接文本样式实现效果图具体实现方法修改文件 themes\next\source\css\_common\components\post\post.styl，在末尾添加如下css样式，：1234567891011// 文章内链接文本样式.post-body p a&#123; color: #0593d3; border-bottom: none; border-bottom: 1px solid #0593d3; &amp;:hover &#123; color: #fc6423; border-bottom: none; border-bottom: 1px solid #fc6423; &#125;&#125;其中选择.post-body 是为了不影响标题，选择 p 是为了不影响首页“阅读全文”的显示样式,颜色可以自己定义。6. 修改文章底部的那个带#号的标签实现效果图具体实现方法修改模板/themes/next/layout/_macro/post.swig，搜索 rel=&quot;tag&quot;&gt;#，将 # 换成&lt;i class=&quot;fa fa-tag&quot;&gt;&lt;/i&gt;7. 在每篇文章末尾统一添加“本文结束”标记实现效果图具体实现方法在路径 \themes\next\layout\_macro 中新建 passage-end-tag.swig 文件,并添加以下内容：12345&lt;div&gt; &#123;% if not is_index %&#125; &lt;div style="text-align:center;color: #ccc;font-size:14px;"&gt;-------------本文结束&lt;i class="fa fa-paw"&gt;&lt;/i&gt;感谢您的阅读-------------&lt;/div&gt; &#123;% endif %&#125;&lt;/div&gt;接着打开\themes\next\layout\_macro\post.swig文件，在post-body 之后， post-footer 之前添加如下画红色部分代码（post-footer之前两个DIV）：代码如下：12345&lt;div&gt; &#123;% if not is_index %&#125; &#123;% include 'passage-end-tag.swig' %&#125; &#123;% endif %&#125;&lt;/div&gt;然后打开主题配置文件（_config.yml),在末尾添加：123# 文章末尾添加“本文结束”标记passage_end_tag: enabled: true完成以上设置之后，在每篇文章之后都会添加如上效果图的样子。8. 修改作者头像并旋转实现效果图具体实现方法打开\themes\next\source\css\_common\components\sidebar\sidebar-author.styl，在里面添加如下代码：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960.site-author-image &#123; display: block; margin: 0 auto; padding: $site-author-image-padding; max-width: $site-author-image-width; height: $site-author-image-height; border: $site-author-image-border-width solid $site-author-image-border-color; /* 头像圆形 */ border-radius: 80px; -webkit-border-radius: 80px; -moz-border-radius: 80px; box-shadow: inset 0 -1px 0 #333sf; /* 设置循环动画 [animation: (play)动画名称 (2s)动画播放时长单位秒或微秒 (ase-out)动画播放的速度曲线为以低速结束 (1s)等待1秒然后开始动画 (1)动画播放次数(infinite为循环播放) ]*/ /* 鼠标经过头像旋转360度 */ -webkit-transition: -webkit-transform 1.0s ease-out; -moz-transition: -moz-transform 1.0s ease-out; transition: transform 1.0s ease-out;&#125;img:hover &#123; /* 鼠标经过停止头像旋转 -webkit-animation-play-state:paused; animation-play-state:paused;*/ /* 鼠标经过头像旋转360度 */ -webkit-transform: rotateZ(360deg); -moz-transform: rotateZ(360deg); transform: rotateZ(360deg);&#125;/* Z 轴旋转动画 */@-webkit-keyframes play &#123; 0% &#123; -webkit-transform: rotateZ(0deg); &#125; 100% &#123; -webkit-transform: rotateZ(-360deg); &#125;&#125;@-moz-keyframes play &#123; 0% &#123; -moz-transform: rotateZ(0deg); &#125; 100% &#123; -moz-transform: rotateZ(-360deg); &#125;&#125;@keyframes play &#123; 0% &#123; transform: rotateZ(0deg); &#125; 100% &#123; transform: rotateZ(-360deg); &#125;&#125;9. 博文压缩在站点的根目录下执行以下命令：12$ npm install gulp -g$ npm install gulp-minify-css gulp-uglify gulp-htmlmin gulp-htmlclean gulp &minus;&minus;save在如下图所示，新建 gulpfile.js ，并填入以下内容：123456789101112131415161718192021222324252627282930313233var gulp = require('gulp');var minifycss = require('gulp-minify-css');var uglify = require('gulp-uglify');var htmlmin = require('gulp-htmlmin');var htmlclean = require('gulp-htmlclean');// 压缩 public 目录 cssgulp.task('minify-css', function() &#123; return gulp.src('./public/**/*.css') .pipe(minifycss()) .pipe(gulp.dest('./public'));&#125;);// 压缩 public 目录 htmlgulp.task('minify-html', function() &#123; return gulp.src('./public/**/*.html') .pipe(htmlclean()) .pipe(htmlmin(&#123; removeComments: true, minifyJS: true, minifyCSS: true, minifyURLs: true, &#125;)) .pipe(gulp.dest('./public'))&#125;);// 压缩 public/js 目录 jsgulp.task('minify-js', function() &#123; return gulp.src('./public/**/*.js') .pipe(uglify()) .pipe(gulp.dest('./public'));&#125;);// 执行 gulp 命令时执行的任务gulp.task('default', [ 'minify-html','minify-css','minify-js']);生成博文是执行 hexo g &amp;&amp; gulp 就会根据 gulpfile.js 中的配置，对 public 目录中的静态资源文件进行压缩。 10. 修改代码块自定义样式实现效果图具体实现方法打开\themes\next\source\css\_custom\custom.styl,向里面加入：(颜色可以自己定义)123456789101112131415// Custom styles.code &#123; color: #ff7600; background: #fbf7f8; margin: 2px;&#125;// 大代码块的自定义样式.highlight, pre &#123; margin: 5px 0; padding: 5px; border-radius: 3px;&#125;.highlight, code, pre &#123; border: 1px solid #d6d6d6;&#125;11. 侧边栏社交小图标设置实现效果图具体实现方法打开主题配置文件（_config.yml），搜索social_icons:,在图标库找自己喜欢的小图标，并将名字复制在如下位置，保存即可12. 主页文章添加阴影效果实现效果图具体实现方法打开\themes\next\source\css\_custom\custom.styl,向里面加入：12345678// 主页文章添加阴影效果 .post &#123; margin-top: 60px; margin-bottom: 60px; padding: 25px; -webkit-box-shadow: 0 0 5px rgba(202, 203, 203, .5); -moz-box-shadow: 0 0 5px rgba(202, 203, 204, .5); &#125;13. 在网站底部加上访问量实现效果图具体实现方法打开\themes\next\layout_partials\footer.swig文件,在copyright前加上画红线这句话：代码如下：1&lt;script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"&gt;&lt;/script&gt;然后再合适的位置添加显示统计的代码，如图：代码如下：12345&lt;div class="powered-by"&gt;&lt;i class="fa fa-user-md"&gt;&lt;/i&gt;&lt;span id="busuanzi_container_site_uv"&gt; 本站访客数:&lt;span id="busuanzi_value_site_uv"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;在这里有两中不同计算方式的统计代码：pv的方式，单个用户连续点击n篇文章，记录n次访问量123&lt;span id="busuanzi_container_site_pv"&gt; 本站总访问量&lt;span id="busuanzi_value_site_pv"&gt;&lt;/span&gt;次&lt;/span&gt;uv的方式，单个用户连续点击n篇文章，只记录1次访客数123&lt;span id="busuanzi_container_site_uv"&gt; 本站总访问量&lt;span id="busuanzi_value_site_uv"&gt;&lt;/span&gt;次&lt;/span&gt;添加之后再执行hexo d -g，然后再刷新页面就能看到效果14. 添加热度实现效果图具体实现方法next主题集成leanCloud，打开/themes/next/layout/_macro/post.swig,在画红线的区域添加℃：然后打开，/themes/next/languages/zh-Hans.yml,将画红框的改为热度就可以了15. 网站底部字数统计实现效果图具体方法实现切换到根目录下，然后运行如下代码1$ npm install hexo-wordcount &minus;&minus;save然后在/themes/next/layout/_partials/footer.swig文件尾部加上：1234&lt;div class="theme-info"&gt; &lt;div class="powered-by"&gt;&lt;/div&gt; &lt;span class="post-count"&gt;博客全站共&#123;&#123; totalcount(site) &#125;&#125;字&lt;/span&gt;&lt;/div&gt;16. 添加 README.md 文件每个项目下一般都有一个 README.md 文件，但是使用 hexo 部署到仓库后，项目下是没有 README.md 文件的。在 Hexo 目录下的 source 根目录下添加一个 README.md 文件，修改站点配置文件 _config.yml，将 skip_render 参数的值设置为1skip_render: README.md保存退出即可。再次使用 hexo d 命令部署博客的时候就不会在渲染 README.md 这个文件了。17. 设置网站的图标Favicon实现效果图具体方法实现在EasyIcon中找一张（32*32）的ico图标,或者去别的网站下载或者制作，并将图标名称改为favicon.ico，然后把图标放在/themes/next/source/images里，并且修改主题配置文件：12# Put your favicon.ico into `hexo-site/source/` directory.favicon: /favicon.ico18. 实现统计功能实现效果图具体实现方法在根目录下安装 hexo-wordcount,运行：1$ npm install hexo-wordcount &minus;&minus;save然后在主题的配置文件中，配置如下：123456# Post wordcount display settings# Dependencies: https://github.com/willin/hexo-wordcountpost_wordcount: item_text: true wordcount: true min2read: true19. 添加顶部加载条实现效果图具体实现方法打开/themes/next/layout/_partials/head.swig文件，添加红框上的代码代码如下：12&lt;script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"&gt;&lt;/script&gt;&lt;link href="//cdn.bootcss.com/pace/1.0.2/themes/pink/pace-theme-flash.css" rel="stylesheet"&gt;但是，默认的是粉色的，要改变颜色可以在/themes/next/layout/_partials/head.swig文件中添加如下代码（接在刚才link的后面）12345678910111213&lt;style&gt; .pace .pace-progress &#123; background: #1E92FB; /*进度条颜色*/ height: 3px; &#125; .pace .pace-progress-inner &#123; box-shadow: 0 0 10px #1E92FB, 0 0 5px #1E92FB; /*阴影颜色*/ &#125; .pace .pace-activity &#123; border-top-color: #1E92FB; /*上边框颜色*/ border-left-color: #1E92FB; /*左边框颜色*/ &#125;&lt;/style&gt;目前，博主的增加顶部加载条的pull request 已被Merge😀===&gt;详情现在升级最新版的next主题，升级后只需修改主题配置文件(_config.yml)将pace: false改为pace: true就行了，你还可以换不同样式的加载条，如下图：20. 在文章底部增加版权信息实现效果图在目录 next/layout/_macro/下添加 my-copyright.swig：1234567891011121314151617181920212223242526272829303132&#123;% if page.copyright %&#125;&lt;div class="my_post_copyright"&gt; &lt;script src="//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js"&gt;&lt;/script&gt; &lt;!-- JS库 sweetalert 可修改路径 --&gt; &lt;script type="text/javascript" src="http://jslibs.wuxubj.cn/sweetalert_mini/jquery-1.7.1.min.js"&gt;&lt;/script&gt; &lt;script src="http://jslibs.wuxubj.cn/sweetalert_mini/sweetalert.min.js"&gt;&lt;/script&gt; &lt;link rel="stylesheet" type="text/css" href="http://jslibs.wuxubj.cn/sweetalert_mini/sweetalert.mini.css"&gt; &lt;p&gt;&lt;span&gt;本文标题:&lt;/span&gt;&lt;a href="&#123;&#123; url_for(page.path) &#125;&#125;"&gt;&#123;&#123; page.title &#125;&#125;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;span&gt;文章作者:&lt;/span&gt;&lt;a href="/" title="访问 &#123;&#123; theme.author &#125;&#125; 的个人博客"&gt;&#123;&#123; theme.author &#125;&#125;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;span&gt;发布时间:&lt;/span&gt;&#123;&#123; page.date.format("YYYY年MM月DD日 - HH:MM") &#125;&#125;&lt;/p&gt; &lt;p&gt;&lt;span&gt;最后更新:&lt;/span&gt;&#123;&#123; page.updated.format("YYYY年MM月DD日 - HH:MM") &#125;&#125;&lt;/p&gt; &lt;p&gt;&lt;span&gt;原始链接:&lt;/span&gt;&lt;a href="&#123;&#123; url_for(page.path) &#125;&#125;" title="&#123;&#123; page.title &#125;&#125;"&gt;&#123;&#123; page.permalink &#125;&#125;&lt;/a&gt; &lt;span class="copy-path" title="点击复制文章链接"&gt;&lt;i class="fa fa-clipboard" data-clipboard-text="&#123;&#123; page.permalink &#125;&#125;" aria-label="复制成功！"&gt;&lt;/i&gt;&lt;/span&gt; &lt;/p&gt; &lt;p&gt;&lt;span&gt;许可协议:&lt;/span&gt;&lt;i class="fa fa-creative-commons"&gt;&lt;/i&gt; &lt;a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" title="Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)"&gt;署名-非商业性使用-禁止演绎 4.0 国际&lt;/a&gt; 转载请保留原文链接及作者。&lt;/p&gt; &lt;/div&gt;&lt;script&gt; var clipboard = new Clipboard('.fa-clipboard'); clipboard.on('success', $(function()&#123; $(".fa-clipboard").click(function()&#123; swal(&#123; title: "", text: '复制成功', html: false, timer: 500, showConfirmButton: false &#125;); &#125;); &#125;)); &lt;/script&gt;&#123;% endif %&#125;在目录next/source/css/_common/components/post/下添加my-post-copyright.styl：123456789101112131415161718192021222324252627282930313233343536373839404142434445.my_post_copyright &#123; width: 85%; max-width: 45em; margin: 2.8em auto 0; padding: 0.5em 1.0em; border: 1px solid #d3d3d3; font-size: 0.93rem; line-height: 1.6em; word-break: break-all; background: rgba(255,255,255,0.4);&#125;.my_post_copyright p&#123;margin:0;&#125;.my_post_copyright span &#123; display: inline-block; width: 5.2em; color: #b5b5b5; font-weight: bold;&#125;.my_post_copyright .raw &#123; margin-left: 1em; width: 5em;&#125;.my_post_copyright a &#123; color: #808080; border-bottom:0;&#125;.my_post_copyright a:hover &#123; color: #a3d2a3; text-decoration: underline;&#125;.my_post_copyright:hover .fa-clipboard &#123; color: #000;&#125;.my_post_copyright .post-url:hover &#123; font-weight: normal;&#125;.my_post_copyright .copy-path &#123; margin-left: 1em; width: 1em; +mobile()&#123;display:none;&#125;&#125;.my_post_copyright .copy-path:hover &#123; color: #808080; cursor: pointer;&#125;修改next/layout/_macro/post.swig，在代码12345&lt;div&gt; &#123;% if not is_index %&#125; &#123;% include 'wechat-subscriber.swig' %&#125; &#123;% endif %&#125;&lt;/div&gt;之前添加增加如下代码：12345&lt;div&gt; &#123;% if not is_index %&#125; &#123;% include 'my-copyright.swig' %&#125; &#123;% endif %&#125;&lt;/div&gt;如下：修改next/source/css/_common/components/post/post.styl文件，在最后一行增加代码：1@import "my-post-copyright"保存重新生成即可。如果要在该博文下面增加版权信息的显示，需要在 Markdown 中增加copyright: true的设置，类似：小技巧：如果你觉得每次都要输入copyright: true很麻烦的话,那么在/scaffolds/post.md文件中添加：这样每次hexo new &quot;你的内容&quot;之后，生成的md文件会自动把copyright:加到里面去(注意：如果解析出来之后，你的原始链接有问题：如：http://yoursite.com/前端小项目：使用canvas绘画哆啦A梦.html,那么在根目录下_config.yml中写成类似这样：）就行了。21. 添加网易云跟帖(跟帖关闭，已失效，改为来必力)实现效果图具体方法实现有两种实现方法：①更新next主题，因为最新版本的主题已经支持这种评论。直接在主题配置文件_config.yml 文件中添加如下配置:1gentie_productKey: #your-gentie-product-key②如果你不想更新的话，那么按下面步骤进行：首先，还是在主题配置文件_config.yml 文件中添加如下配置:1gentie_productKey: #your-gentie-product-key你的productKey就是下面画红线部分然后在在layout/_scripts/third-party/comments/ 目录中添加 gentie.swig，文件内容如下：1234567891011121314&#123;% if not (theme.duoshuo and theme.duoshuo.shortname) and not theme.duoshuo_shortname and not theme.disqus_shortname and not theme.hypercomments_id %&#125; &#123;% if theme.gentie_productKey %&#125; &#123;% set gentie_productKey = theme.gentie_productKey %&#125; &lt;script&gt; var cloudTieConfig = &#123; url: document.location.href, sourceId: "", productKey: "&#123;&#123;gentie_productKey&#125;&#125;", target: "cloud-tie-wrapper" &#125;; &lt;/script&gt; &lt;script src="https://img1.ws.126.net/f2e/tie/yun/sdk/loader.js"&gt;&lt;/script&gt; &#123;% endif %&#125;&#123;% endif %&#125;然后在layout/_scripts/third-party/comments.swig文件中追加：1&#123;% include './comments/gentie.swig' %&#125;最后，在 layout/_partials/comments.swig 文件中条件最后追加网易云跟帖插件引用的判断逻辑：123&#123;% elseif theme.gentie_productKey %&#125; &lt;div id="cloud-tie-wrapper" class="cloud-tie-wrapper"&gt; &lt;/div&gt;具体位置如下：可能你hexo s时可能看不到，直接hexo d就可以看到了近日，我朋友发来消息，说网易云跟帖要关了，我网上查了一下，果然如此😭都是泪,上次用了多说，结果多说关了，接着是网易云跟帖😷，这次直接用国外的来必力，应该不会这么容易关吧😏方法其实还是跟上面差不多的首先在 _config.yml 文件中添加如下配置：(注意！如果主题是最新版的，直接写你的liver_uid就行了)123# Support for LiveRe comments system.# You can get your uid from https://livere.com/insight/myCode (General web site)livere_uid: your uid其中，livere_uid就是画红线的部分然后在 layout/_scripts/third-party/comments/ 目录中添加 livere.swig，文件内容如下：1234567891011121314&#123;% if not (theme.duoshuo and theme.duoshuo.shortname) and not theme.duoshuo_shortname and not theme.disqus_shortname and not theme.hypercomments_id and not theme.gentie_productKey %&#125; &#123;% if theme.livere_uid %&#125; &lt;script type="text/javascript"&gt; (function(d, s) &#123; var j, e = d.getElementsByTagName(s)[0]; if (typeof LivereTower === 'function') &#123; return; &#125; j = d.createElement(s); j.src = 'https://cdn-city.livere.com/js/embed.dist.js'; j.async = true; e.parentNode.insertBefore(j, e); &#125;)(document, 'script'); &lt;/script&gt; &#123;% endif %&#125;&#123;% endif %&#125;然后在 layout/_scripts/third-party/comments.swig文件中追加：1&#123;% include './comments/livere.swig' %&#125;最后，在 layout/_partials/comments.swig 文件中条件最后追加 LiveRe 插件是否引用的判断逻辑：123&#123;% elseif theme.livere_uid %&#125; &lt;div id="lv-container" data-id="city" data-uid="&#123;&#123; theme.livere_uid &#125;&#125;"&gt;&lt;/div&gt;&#123;% endif %&#125;完22. 隐藏网页底部powered By Hexo / 强力驱动打开themes/next/layout/_partials/footer.swig,使用””隐藏之间的代码即可，或者直接删除。位置如图：23. 修改网页底部的桃心还是打开themes/next/layout/_partials/footer.swig，找到：，然后还是在图标库中找到你自己喜欢的图标，然后修改画红线的部分就可以了。24. 文章加密访问实现效果图具体实现方法打开themes-&gt;next-&gt;layout-&gt;_partials-&gt;head.swig文件,在以下位置插入这样一段代码：代码如下：12345678910&lt;script&gt; (function()&#123; if('&#123;&#123; page.password &#125;&#125;')&#123; if (prompt('请输入文章密码') !== '&#123;&#123; page.password &#125;&#125;')&#123; alert('密码错误！'); history.back(); &#125; &#125; &#125;)();&lt;/script&gt;然后在文章上写成类似这样：25. 添加jiathis分享在主题配置文件中，jiathis为true，就行了，如下图默认是这样子的：如果你想自定义话，打开themes/next/layout/_partials/share/jiathis.swig修改画红线部分就可以了26. 博文置顶修改 hero-generator-index 插件，把文件：node_modules/hexo-generator-index/lib/generator.js 内的代码替换为：12345678910111213141516171819202122232425262728'use strict';var pagination = require('hexo-pagination');module.exports = function(locals)&#123; var config = this.config; var posts = locals.posts; posts.data = posts.data.sort(function(a, b) &#123; if(a.top &amp;&amp; b.top) &#123; // 两篇文章top都有定义 if(a.top == b.top) return b.date - a.date; // 若top值一样则按照文章日期降序排 else return b.top - a.top; // 否则按照top值降序排 &#125; else if(a.top &amp;&amp; !b.top) &#123; // 以下是只有一篇文章top有定义，那么将有top的排在前面（这里用异或操作居然不行233） return -1; &#125; else if(!a.top &amp;&amp; b.top) &#123; return 1; &#125; else return b.date - a.date; // 都没定义按照文章日期降序排 &#125;); var paginationDir = config.pagination_dir || 'page'; return pagination('', posts, &#123; perPage: config.index_generator.per_page, layout: ['index', 'archive'], format: paginationDir + '/%d/', data: &#123; __index: true &#125; &#125;);&#125;;在文章中添加 top 值，数值越大文章越靠前，如12345678---title: 解决Charles乱码问题date: 2017-05-22 22:45:48tags: 技巧categories: 技巧copyright: truetop: 100---27. 修改字体大小打开\themes\next\source\css\ _variables\base.styl文件，将$font-size-base改成16px，如下所示：1$font-size-base =16px28. 修改打赏字体不闪动修改文件next/source/css/_common/components/post/post-reward.styl，然后注释其中的函数wechat:hover和alipay:hover，如下：123456789101112/* 注释文字闪动函数 #wechat:hover p&#123; animation: roll 0.1s infinite linear; -webkit-animation: roll 0.1s infinite linear; -moz-animation: roll 0.1s infinite linear;&#125; #alipay:hover p&#123; animation: roll 0.1s infinite linear; -webkit-animation: roll 0.1s infinite linear; -moz-animation: roll 0.1s infinite linear;&#125;*/29. 侧边栏推荐阅读今天有位网友问推荐阅读是怎么弄，其实挺简单的，打开主题配置文件修改成这样就行了(links里面写你想要的链接):1234567891011# Blogrollslinks_title: 推荐阅读#links_layout: blocklinks_layout: inlinelinks: 优设: http://www.uisdc.com/ 张鑫旭: http://www.zhangxinxu.com/ Web前端导航: http://www.alloyteam.com/nav/ 前端书籍资料: http://www.36zhen.com/t?id=3448 百度前端技术学院: http://ife.baidu.com/ google前端开发基础: http://wf.uisdc.com/cn/30. 自定义鼠标样式打开themes/next/source/css/_custom/custom.styl,在里面写下如下代码1234567// 鼠标样式 * &#123; cursor: url("http://om8u46rmb.bkt.clouddn.com/sword2.ico"),auto!important &#125; :active &#123; cursor: url("http://om8u46rmb.bkt.clouddn.com/sword1.ico"),auto!important &#125;其中 url 里面必须是 ico 图片，ico 图片可以上传到网上（我是使用七牛云图床），然后获取外链，复制到 url 里就行了31.为博客加上萌萌的宠物实现效果图具体实现方法在终端切换到你的博客的路径里，然后输入如下代码：1npm install --save hexo-helper-live2d然后打开Hexo/blog/themes/next/layout的_layout.swig,将下面代码放到&lt;/body&gt;之前：1&#123;&#123; live2d() &#125;&#125;然后在在 hexo 的 _config.yml中添加参数：123live2d: model: wanko bottom: -30然后hexo clean ，hexo g ，hexo d 就可以看到了。下面是一些model，可以换不同的宠物model 模型名称 默认值: z16Gantzert_FelixanderEpsilon2.1harumikuni-jniconitonipsilonnietzscheshizukutsumikiwankoz16hibikikoharuharutoUnitychantororohijikiwidth 宽度 默认值: 150height 高度 默认值： 300className &lt;canvas&gt;元素的类名 默认值： live2did &lt;canvas&gt; 元素的id 默认值： live2dcanvasbottom &lt;canvas&gt; 元素的底部偏移 默认值： -20 如果嫌模型位置不正确 可以调整这个参数用这个有缺点，如果是在手机上看的话，感觉不是很好，宠物一直挡着文字😂😂，还有就是加载有点慢注意！如果你在 hexo d 的时候出现我下面这个问题你可以这样，首先删除hexo 下面的.deploy_git文件夹，然后运行1git config --global core.autocrlf false重新 hexo clean,hexo g,hexo d就行了致谢感谢大神们的文章，真的学到了许多，有些忘了记录下来，在这里由衷的感谢。虽然比较折腾，但是确实满满的成就感，Road endless its long and far, I will seek up and down！欢迎访问我的博客参考的文章：http://blog.csdn.net/MasterAnt_D/article/details/56839222http://zidingyi4qh.com/2017/04/27/NexT%E5%BA%95%E9%83%A8logo%E6%B7%BB%E5%8A%A0%E8%AE%BF%E9%97%AE%E9%87%8F/https://fuyis.me/2017/01/25/Hexo-theme-next-and-optimized-configuration/http://www.vitah.net/posts/20f300cc/http://thief.one/2017/03/03/Hexo%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E6%95%99%E7%A8%8B/]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>Next</tag>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kbe服务端笔记(二)]]></title>
    <url>%2Fblog%2F2017%2F07%2F29%2Fkbe_note_two%2F</url>
    <content type="text"><![CDATA[FixedMessages：FixedMessages存储所有固定消息（有显示制定id的消息，当然，这并不表示非固定消息就没有id，也是有的，只是不是显示制定的）。 它的构造地方如下（lib/network/message_handler.cpp）： 123456789101112MessageHandlers::MessageHandlers():msgHandlers_(),msgID_(1),exposedMessages_()&#123; g_fm = Network::FixedMessages::getSingletonPtr(); if(g_fm == NULL) g_fm = newNetwork::FixedMessages; Network::FixedMessages::getSingleton().loadConfig(&quot;server/messages_fixed.xml&quot;); messageHandlers().push_back(this);&#125; 意即MessageHandlers构造的时候，如果它还没构造，那就构造。它的初始化（配置）是由loadConfig接口来完成的，代码见上。 . . . loginapp Loginapp组件主要用来处理账户登录/注册的业务 消息与handler映射的建立：两次包含xxx_interface.h，实现声明和定义：每个app组件的接口定义都在xxxapp_interface.cpp中开始，代码如下： 123456789#include&quot;loginapp_interface.h&quot;#defineDEFINE_IN_INTERFACE#defineLOGINAPP#include&quot;loginapp_interface.h&quot;namespaceKBEngine&#123;namespaceLoginappInterface&#123;&#125;&#125; 所有的戏法都是通过包含loginapp_interface.h前后定义了DEFINE_IN_INTERFACE和LOGINAPP来完成的。第一次的包含就是各种变量，类的声明（当然也有一些类是声明类时使用类inline函数定义完成了，比如MESSAGE_ARGS0/1/2……）。我们看看loginapp_interface.h中的代码： 消息与handlers的存储首先是这一句：NETWORK_INTERFACE_DECLARE_BEGIN(LoginappInterface)此句展开的话声明和定义了Network::MessageHandlers messageHandlers（记住它们都在LoginappInterface命名空间内），展开宏之后的代码看起来像这样（是的，你的眼睛是好的，没有}闭合）：声明： 12345namespaceLoginappInterface &#123;extern Network::MessageHandlers messageHandlers;定义：namespaceLoginappInterface &#123; Network::MessageHandlers messageHandlers; 消息与handle建立映射然后是这一句：LOGINAPP_MESSAGE_DECLARE_ARGS0(importClientMessages, NETWORK_FIXED_MESSAGE)此句展开的话分明声明和定义了一个importClientMessagesLoginappMessagehandler0的类，这个类继承自Network::MessageHandler，这里就是实现了handle的虚函数接口；声明和定义了importClientMessagesLoginappMessagehandler0的一个名为importClientMessages的全局变量；声明和定义了importClientMessagesArgs0的类，这个类继承自Network::MessageArgs。我们一个个地分析一下：首先展开下面的宏： 1LOGINAPP_MESSAGE_DECLARE_ARGS0(importClientMessages, NETWORK_FIXED_MESSAGE) 之后是这样： 12345678910111213141516#defineLOGINAPP_MESSAGE_DECLARE_ARGS0(NAME, MSG_LENGTH) \LOGINAPP_MESSAGE_HANDLER_ARGS0(NAME) \NETWORK_MESSAGE_DECLARE_ARGS0(Loginapp, NAME, \ NAME#LoginappMessagehandler0, MSG_LENGTH)展开LOGINAPP_MESSAGE_HANDLER_ARGS0(NAME)之后分别得到importClientMessagesLoginappMessagehandler0的声明和定义：声明：classimportClientMessagesLoginappMessagehandler0 : public Network::MessageHandler&#123;public:virtualvoidhandle(Network::Channel* pChannel, KBEngine::MemoryStream&amp;s);&#125;;定义：voidimportClientMessagesLoginappMessagehandler0::handle(Network::Channel* pChannel, KBEngine::MemoryStream&amp;s)&#123; KBEngine::Loginapp::getSingleton().importClientMessages(pChannel);&#125; （handle/handler，傻傻分不清楚。。。这里的handle是xxxApp中真正用来处理这个消息的接口，而这里的handler提供一个中间层的作用，集中处理一些通用的工作，可以将耦合减少一点）上面完成了相当于是importClientMessages消息的handler的声明和定义，下面则将这个类实例化之后添加到messageHandlers： 12345#defineNETWORK_MESSAGE_DECLARE_ARGS0(DOMAIN, NAME, MSGHANDLER, \ MSG_LENGTH) \ NETWORK_MESSAGE_HANDLER(DOMAIN, NAME, MSGHANDLER, MSG_LENGTH, 0)\ MESSAGE_ARGS0(NAME) \ 展开NETWORK_MESSAGE_HANDLER(DOMAIN, NAME, MSGHANDLER, MSG_LENGTH, 0)之后得到importClientMessages的handler类（importClientMessagesLoginappMessagehandler0）的名为importClientMessages的全局变量（不过欣慰的是他们都在各自的XXXInterface命名空间内）。声明：externconstimportClientMessagesLoginappMessagehandler0&amp;importClientMessages; 定义： 12importClientMessagesLoginappMessagehandler0* pimportClientMessages = static_cast&lt;importClientMessagesLoginappMessagehandler0*&gt;(messageHandlers.add(&quot;Loginapp::importClientMessages&quot;,new importClientMessagesArgs0, NETWORK_FIXED_MESSAGE, newimportClientMessagesLoginappMessagehandler0);constimportClientMessagesLoginappMessagehandler0&amp;importClientMessages = *pimportClientMessages; 下面的MESSAGE_ARGS0(NAME)展开后对importClientMessagesArgs0进行了声明和定义（其他它声明的时候就已经完成了全部的定义），声明的时候就是个空语句：声明兼定义： 1234567891011121314151617181920212223classimportClientMessagesArgs0 : public Network::MessageArgs&#123;public:importClientMessagesArgs0() :Network::MessageArgs() &#123;&#125; ~importClientMessagesArgs0() &#123;&#125;staticvoidstaticAddToBundle(Network::Bundle&amp;s) &#123; &#125;staticvoidstaticAddToStream(MemoryStream&amp;s) &#123; &#125;virtual int32 dataSize(void) &#123;return 0; &#125;virtualvoidaddToStream(MemoryStream&amp;s) &#123; &#125;virtualvoidcreateFromStream(MemoryStream&amp;s) &#123; &#125;&#125;; 唯一需要小注意一下的就是importClientMessagesArgs0的声明（兼定义）是和importClientMessagesLoginappMessagehandler0的实例的声明和定义是错开的，因为后者实例化添加到messageHandlers的时候需要new一个importClientMessagesArgs0的实例。 流程的伪代码稍微整理一下之后，使用LOGINAPP_MESSAGE_HANDLER_ARGSn建立一个消息到handler的映射的代码很像是这样： 声明：（第一次包含loginapp_interface.h产生的代码）12345678910111213141516171819202122232425262728293031classimportClientMessagesLoginappMessagehandler0 : public Network::MessageHandler&#123;public:virtualvoidhandle(Network::Channel* pChannel, KBEngine::MemoryStream&amp;s);&#125;;externconstimportClientMessagesLoginappMessagehandler0&amp;importClientMessages;classimportClientMessagesArgs0 : public Network::MessageArgs&#123;public:importClientMessagesArgs0() :Network::MessageArgs() &#123;&#125; ~importClientMessagesArgs0() &#123;&#125;staticvoidstaticAddToBundle(Network::Bundle&amp;s) &#123; &#125;staticvoidstaticAddToStream(MemoryStream&amp;s) &#123; &#125;virtual int32 dataSize(void) &#123;return 0; &#125;virtualvoidaddToStream(MemoryStream&amp;s) &#123; &#125;virtualvoidcreateFromStream(MemoryStream&amp;s) &#123; &#125;&#125;; 定义：（定义DEFINE_IN_INTERFACE和LOGINAPP之后第二次包含loginapp_interface.h产生的代码）1234567891011voidimportClientMessagesLoginappMessagehandler0::handle(Network::Channel* pChannel, KBEngine::MemoryStream&amp;s)&#123; KBEngine::Loginapp::getSingleton().importClientMessages(pChannel);&#125;importClientMessagesLoginappMessagehandler0* pimportClientMessages = static_cast&lt;importClientMessagesLoginappMessagehandler0*&gt;(messageHandlers.add(&quot;Loginapp::importClientMessages&quot;,newimportClientMessagesArgs0, NETWORK_FIXED_MESSAGE,newimportClientMessagesLoginappMessagehandler0);constimportClientMessagesLoginappMessagehandler0&amp;importClientMessages = *pimportClientMessages; 消息id：固定消息与非固定消息要接着v0.0.3的分析继续写，回过头来要看之前写的东西说实话自己都有点难以理解。。。不过出于幸运或者努力，总算是看懂了;-(，读源代码（感觉特别是C++）本来就不是件容易的事，所以读源代码一定要做好长期战斗的准备。上面我们分析到了，其实一个消息，就是由这样一个宏来和它的handle建立链接的：LOGINAPP_MESSAGE_DECLARE_ARGS0(importClientMessages, NETWORK_FIXED_MESSAGE)通过上面的分析，我们得知，实际上建立消息和handle映射，起到核心作用的接口是messageHandlers.add(xxx, xxxx)，所以我们跟进去看看（lib/network/message_handler.cpp）： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869MessageHandler* MessageHandlers::add(std::stringihName, MessageArgs* args, int32msgLen, MessageHandler* msgHandler)&#123; if(msgID_ == 1) &#123; //printf(&quot;\n------------------------------------------------------------------\n&quot;); //printf(&quot;KBEMessage_handlers begin:\n&quot;); &#125; //bool isfixedMsg = false; FixedMessages::MSGInfo* msgInfo = FixedMessages::getSingleton().isFixed(ihName.c_str()); if(msgInfo == NULL) &#123; while(true) &#123; if(FixedMessages::getSingleton().isFixed(msgID_)) &#123; msgID_++; //isfixedMsg = true; &#125; else &#123; break; &#125; &#125;; msgHandler-&gt;msgID = msgID_++; &#125; else &#123; msgHandler-&gt;msgID = msgInfo-&gt;msgid; &#125; msgHandler-&gt;name = ihName; msgHandler-&gt;pArgs = args; msgHandler-&gt;msgLen = msgLen; msgHandler-&gt;exposed = false; msgHandler-&gt;pMessageHandlers = this; msgHandler-&gt;onInstall(); msgHandlers_[msgHandler-&gt;msgID] = msgHandler; if(msgLen == NETWORK_VARIABLE_MESSAGE) &#123; //printf(&quot;\tMessageHandlers::add(%d): name=%s, msgID=%d, size=Variable.\n&quot;, // (int32)msgHandlers_.size(), ihName.c_str(), msgHandler-&gt;msgID); &#125; else &#123; if(msgLen == 0) &#123; msgHandler-&gt;msgLen = args-&gt;dataSize(); if(msgHandler-&gt;type() == NETWORK_MESSAGE_TYPE_ENTITY) &#123; msgHandler-&gt;msgLen += sizeof(ENTITY_ID); &#125; &#125; //printf(&quot;\tMessageHandlers::add(%d): name=%s, msgID=%d, size=Fixed(%d).\n&quot;, // (int32)msgHandlers_.size(), ihName.c_str(), msgHandler-&gt;msgID, msgHandler-&gt;msgLen); &#125; //if(isfixedMsg) // printf(&quot;\t\t!!!message is fixed.!!!\n&quot;); returnmsgHandler;&#125; 大意可以理解为，首先看看消息名称是不是一个固定消息，我们跟进去看看（lib/network/fixed_messages.cpp）： 123456789101112131415161718192021222324252627FixedMessages::MSGInfo* FixedMessages::isFixed(constchar* msgName)&#123; MSGINFO_MAP::iteratoriter = _infomap.find(msgName); if(iter != _infomap.end()) &#123; MSGInfo* infos = &amp;iter-&gt;second; returninfos; &#125; returnNULL;&#125;//-------------------------------------------------------------------------------------boolFixedMessages::isFixed(MessageIDmsgid)&#123; MSGINFO_MAP::iteratoriter = _infomap.begin(); while (iter != _infomap.end()) &#123; FixedMessages::MSGInfo&amp;infos = iter-&gt;second; if(infos.msgid == msgid) returntrue; ++iter; &#125; returnfalse;&#125; 固定消息通过通读FixedMessages（fixed_message.h/.cpp）可以看到这个_infomap是在loadConfig中建立的，这个_infomap就是所谓的固定消息（fixed message）与其id的映射表。loadConfig就是检视server/messages_fixed.xml，将其中的消息名称与其id关联建立这个映射表。我们继续接着看MessageHandlers::add接口。 非固定消息对于isFixed为假的消息（非固定消息），则为其生成一个id（随着调用add的次序依次递增），这个id是在MessageHandlers类中唯一的，而每个组件的MessageHandlers又是处于自己的命名空间内，所以当出现某个组件的非固定消息时，则会为其生成单一组件内唯一的id（但这个id并不是所有组件内唯一的）。于是可能出现这种情况，Loginapp::xxxx与Dbmgr::yyyy都是非固定消息，但他们却有着同样的消息id，此时若有其他组件发送其中任一消息给其他组件，接受消息的组件将无法识别到底是Loginapp::xxxx或者是Dbmgr::yyyy。当然，只要我们将非固定消息发送给所属的组件，则不会有问题（上例中任何组件将Loginapp::xxxx发送给loginapp都是不会出乱子的）。 dbmgr dbmgr组件主要负责数据库相关的事务，比如：账户登录/注册事务；账户充值]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>KBE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[重读UNP（UNIX网络编程）13章到31章笔记整理（结合TLPI和APUE两书的笔记整理）(二)]]></title>
    <url>%2Fblog%2F2017%2F07%2F29%2Funp_chapter_thirdteen_to_thirty_one_note_second_part%2F</url>
    <content type="text"><![CDATA[16章 16.3节 ： 非阻塞connect 有三个用途： 我们想在connect的时候处理其他事情 可以同时建立多个连接 可以通过select设置一个更短一点的超时时间 实现步骤： a. 用fcntl把套接字设置为非阻塞 b. 处理客户端和服务器都在同一主机上的情况 c. 使用select设置超时，并处理超时情况 d. 处理当连接建立的时候，描述符变为可写；以及当连接建立遇到错误的时候， 描述符变为可写并可读的情况 . . . 16.6节 ： 非阻塞accept， 用于解决下面问题：用select检测socket状态，如果有连接就调用accept，这样如果在select检测到有连接请求，但在调用accept之前，这个请求断开了，然后调用accept的时候就会阻塞在哪里，除非这时有另外一个连接请求，如果没有，则一直被阻塞在accept调用上, 无法处理任何其他已就绪的描述符。 解决方案：使用select在一个监听套接字准备好要被accept时总是把套接字设置为非阻塞 26章和30章这两章介绍了线程和并发/并行的服务器设计范式. 关于线程可参考 : 阅读开源服务器源码基础 关于服务器设计范式可参考 : 目前Linux比较通用的基于epoll的服务器设计范式大体如epoll扼要总结]]></content>
      <categories>
        <category>NP</category>
      </categories>
      <tags>
        <tag>UNP</tag>
        <tag>TLPI</tag>
        <tag>APUE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[重读UNP（UNIX网络编程）13章到31章笔记整理（结合TLPI和APUE两书的笔记整理）(一)]]></title>
    <url>%2Fblog%2F2017%2F07%2F28%2Funp_chapter_thirdteen_to_thirty_one_note_first_part%2F</url>
    <content type="text"><![CDATA[因为UNP第三部分（第三版13-31章）的内容结合APUE（UNIX环境高级编程）和TLPI（The Linux Programming Interface）来看才能比较清晰，所以笔记整理会穿插很多这两本书的内容 引申知识，作业控制以及相关命令 . . . 注: 但是如上方到后台执行的进程，其父进程还是当前终端shell的进程，而一旦父进程退出，则会发送hangup信号给所有子进程，子进程收到hangup以后也会退出。 13.4节本节主要讲守护进程的相关知识 创建守护进程的步骤自定义一个daemon_init函数，涉及到知识点为“如何创建一个daemon（守护进程）”，实现步骤如下(两fork一set, u工文dev)： (1)首先要做的是调用umask将文件模式创建屏蔽字设置为一个已知值(通常是0)。由继承得来的文件模式创建屏蔽字可能会被设置为拒绝某些权限。如果守护进程要创建文件，那么它可能要设置特定的权限。例如，若守护进程要创建组可读、组可写的文件，继承的文件模式创建屏蔽字可能会屏蔽上述两种权限中的一种，而使其无法发挥作用。另一方面，如果守护进程调用的库函数创建了文件，那么将文件模式创建屏蔽字设置为一个限制性更强的值（如007）可能会更明智，因为库函数可能不允许调用者通过一个显式的函数参数来设置权限。 (2)调用fork，然后使父进程exit。这样做实现了下面几点。第一，如果该守护进程是作为一条简单的shell命令启动的，那么父进程终止会让shell认为这条命令已经执行完毕。第二，虽然子进程继承了父进程的进程组ID，但获得了一个新的进程ID，这就保证了子进程不是一个进程组的组长进程。这是下面将要进行的setsid调用的先决条件。 (3)调用setsid创建一个新会话。使调用进程：(a)成为新会话的首进程，(b)成为一个新进程组的组长进程．(c)没有控制终端。也可概括为 : 开启一个新会话并释放它与控制终端之间的所有关联关系 (4)再次fork并杀掉首进程.这样就确保了子进程不是一个会话首进程， 根据linux中获取终端的规则（只有会话首进程才能请求一个控制终端）， 这样进程永远不会重新请求一个控制终端 (5)将当前工作目录更改为根目录。从父进程处继承过来的当前工作目录可能在一个挂载的文件系统中。因为守护进程通常在系统再引导之前是一直存在的，所以如果守护进程的当前工作目录在一个挂载文件系统中，那么该文件系统就不能被卸载。或者，某些守护进程还可能会把、与前工作目录更改到某个指定位置，并在此位置进行它们的全部工作。例如，行式打印机假脱机守护进程就可能将其工作目录更改到它们的spool目录上。 (6)关闭不再需要的文件描述符。这使守护进程不再持有从其父进程继承来的任何文件描述符（父进程可能是shell进程，或某个其他进程）。可以使用open_max函数（见2.17节）或getrlimit函数（见7.11节）来判定最高文件描述符值，并关闭直到该值的所有描述符。 (7)某些守护进程打开/dev/null使其具有文件描述符0、l和2．这样，任何一个试图读标准输入、写标准输出或标准错误的库例程都不会产生任何效果。因为守护进程并不与终端设备相关联，所以其输出无处显示，也无处从交互式用户那里接收输入。即使守护进程是从交互式会话启动的，但是守护进程是在后台运行的，所以登录会话的终止并不影响守护进程。如果其他用户在同一终端设备上登录，我们不希望在该终端上见到守护进程的输出，用户也不期望他们在终端上的输入被守护进程读取。( 在关闭了文件描述符0、1和2之后，daemon通常会打开/dev/null并使用dup2()（或类似的函数）使所有这些描述符指向这个设备。之所以要这样做是因为下面两个原因 : 它确保了当daemon调用了在这些描述符上执行I/O的库函数时不会出乎意料地 失败。 它防止了daemon后面使用描述符1或2打开…个文件的情况，因为库函数会将这些 描述符当做标准输出和标准错误来写入数据（进而破坏了原有的数据）。 ) dup介绍函数原型 : int dup( int oldfd ) dup() 调用复制一个打开的文件描述符 oldfd , 并返回一个新描述符, 二者都指向同一打开的文件句柄. 系统会保证新描述符一定是编号值最低的未用文件描述符. 假设发起如下调用 : newfd = dup(1); 再假定在正常情况下, shell已经代表程序打开了文件描述符0, 1和2, 且没有其他描述符在用, dup()调用会创建文件描述符1的副本, 返回的文件描述符编号值为3. 如果希望返回的文件描述符为2, 可以使用如下技术 : 12close(2);newfd = dup(1); 只有当描述符 0 已经打开时, 这段代码方可工作. dup2介绍如果想进一步简化上述代码, 同时总是能获得所期望的文件描述符, 可以调用dup2(). 函数原型 : int dup2( int oldfd, int newfd ) dup2函数跟dup函数相似，但dup2函数允许调用者规定一个有效描述符和目标描述符的id 。dup2函数成功返回时，目标描述符（dup2函数的第二个参数）将变成源描述符（dup2函数的 第一个参数）的复制品，换句话说，两个文件描述符现在都指向同一个文件，并且是函数第一 个参数指向的文件。 下面我们用一段代码加以说明： 1234int oldfd; oldfd = open("app_log", (O_RDWR | O_CREATE), 0644 ); dup2( oldfd, 1 ); close( oldfd ); 本例中，我们打开了一个新文件，称为“app_log”，并收到一个文件描述符，该描述符 叫做fd1。我们调用dup2函数，参数为oldfd和1，这会导致用我们新打开的文件描述符替换掉 由1代表的文件描述符（即stdout，因为标准输出文件的id为1）。任何写到stdout的东西，现 在都将改为写入名为“app_log”的文件中。 umask介绍当我们登录系统之后创建一个文件总是有一个默认权限的，那么这个权限是怎么来的呢？这就是umask干的事情。umask设置了用户创建文件的默认 权限，它与chmod的效果刚好相反，umask设置的是权限“补码”，而chmod设置的是文件权限码。 如何计算umaskumask 命令允许你设定文件创建时的缺省模式，对应每一类用户(文件属主、同组用户、其他用户)存在一个相应的umask值中的数字。对于文件来说，这一数字的最 大值分别是6。系统不允许你在创建一个文本文件时就赋予它执行权限，必须在创建后用chmod命令增加这一权限。目录则允许设置执行权限，这样针对目录来 说，umask中各个数字最大可以到7。 例如，对于umask值0 0 2，相应的文件和目录缺省创建权限是什么呢？ 第一步，我们首先写下目录具有全部权限的模式，即777 (所有用户都具有读、写和执行权限)。第二步，在下面一行按照umask值写下相应的位，在本例中是0 0 2。第三步，在接下来的一行中记下上面两行中没有匹配的位。这就是目录的缺省创建权限。稍加练习就能够记住这种方法。第四步，对于文件来说，在创建时不能具有执行权限，只要拿掉相应的执行权限比特即可。这就是上面的例子， 其中umask值为0 0 2： 1) 文件的最大权限 rwx rwx rwx (777)2) umask值为0 0 2 — — -w-3) 目录权限 rwx rwx r-x (775) 这就是目录创建缺省权限4) 文件权限 rw- rw- r– (664) 这就是文件创建缺省权限 下面是另外一个例子，假设这次u m a s k值为0 2 2： 1) 文件的最大权限 rwx rwx rwx (777)2 ) u m a s k值为0 2 2 — -w- -w-3) 目录权限 rwx r-x r-x (755) 这就是目录创建缺省权限4) 文件权限 rw- r– r– (644) 这就是文件创建缺省权限 创建守护进程的例子程序下面是becomeDaemon()函数的实现，becomeDaeomon()函数接收一个位掩码参数flags，它允许调用者有选择地执行其中的步骤，具体可参考注释。 become_daemon.h 1234567891011121314151617#ifndef BECOME_DAEMON_H /* Prevent double inclusion */#define BECOME_DAEMON_H/* Bit-mask values for 'flags' argument of becomeDaemon() */#define BD_NO_CHDIR 01 /* Don't chdir("/") */#define BD_NO_CLOSE_FILES 02 /* Don't close all open files */#define BD_NO_REOPEN_STD_FDS 04 /* Don't reopen stdin, stdout, and stderr to /dev/null */#define BD_NO_UMASK0 010 /* Don't do a umask(0) */#define BD_MAX_CLOSE 8192 /* Maximum file descriptors to close if sysconf(_SC_OPEN_MAX) is indeterminate */int becomeDaemon(int flags);#endif become_daemon.c 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061#include &lt;sys/stat.h&gt;#include &lt;fcntl.h&gt;#include "become_daemon.h"#include "tlpi_hdr.h"int /* Returns 0 on success, -1 on error */becomeDaemon(int flags)&#123; int maxfd, fd; switch (fork()) &#123; /* Become background process */ case -1: return -1; case 0: break; /* Child falls through... */ default: _exit(EXIT_SUCCESS); /* while parent terminates */ &#125; if (setsid() == -1) /* Become leader of new session */ return -1; switch (fork()) &#123; /* Ensure we are not session leader */ case -1: return -1; case 0: break; default: _exit(EXIT_SUCCESS); &#125; if (!(flags &amp; BD_NO_UMASK0)) umask(0); /* Clear file mode creation mask */ if (!(flags &amp; BD_NO_CHDIR)) chdir("/"); /* Change to root directory */ if (!(flags &amp; BD_NO_CLOSE_FILES)) &#123; /* Close all open files */ maxfd = sysconf(_SC_OPEN_MAX); if (maxfd == -1) /* Limit is indeterminate... */ maxfd = BD_MAX_CLOSE; /* so take a guess */ for (fd = 0; fd &lt; maxfd; fd++) close(fd); &#125; if (!(flags &amp; BD_NO_REOPEN_STD_FDS)) &#123; // /* Standard file descriptors. */ // #define STDIN_FILENO 0 /* Standard input. */ // #define STDOUT_FILENO 1 /* Standard output. */ // #define STDERR_FILENO 2 /* Standard error output. */ close(STDIN_FILENO); /* Reopen standard fd's to /dev/null */ fd = open("/dev/null", O_RDWR); // open 返回的文件描述符一定是最小的未被使用的描述符。 if (fd != STDIN_FILENO) /* 'fd' should be 0 */ return -1; if (dup2(STDIN_FILENO, STDOUT_FILENO) != STDOUT_FILENO) return -1; if (dup2(STDIN_FILENO, STDERR_FILENO) != STDERR_FILENO) return -1; &#125; return 0;&#125; nohup和setsid用法如果我们要在退出shell的时候继续运行进程，则需要使用nohup忽略hangup信号，或者setsid将父进程设为init进程； 1234567891011121314151617181920212223242526272829303132333435363738394041b@b-VirtualBox:~/my_temp_test$ nohup ./o_multi_thread_process &amp;[1] 3487b@b-VirtualBox:~/my_temp_test$ nohup: ignoring input and appending output to ‘nohup.out’^Cb@b-VirtualBox:~/my_temp_test$ jobs[1]+ Running nohup ./o_multi_thread_process &amp;b@b-VirtualBox:~/my_temp_test$ ps -ef | grep multib 3487 3004 0 20:05 pts/3 00:00:00 ./o_multi_thread_processb 3488 3487 0 20:05 pts/3 00:00:00 ./o_multi_thread_processb 3491 3004 0 20:05 pts/3 00:00:00 grep --color=auto multib@b-VirtualBox:~/my_temp_test$ bg %1bash: bg: job 1 already in backgroundb@b-VirtualBox:~/my_temp_test$ fg %1nohup ./o_multi_thread_process^Z[1]+ Stopped nohup ./o_multi_thread_processb@b-VirtualBox:~/my_temp_test$ bg %1[1]+ nohup ./o_multi_thread_process &amp;b@b-VirtualBox:~/my_temp_test$ jobs -l[1]+ 3487 Running nohup ./o_multi_thread_process &amp;b@b-VirtualBox:~/my_temp_test$ fg %1nohup ./o_multi_thread_process^Cb@b-VirtualBox:~/my_temp_test$ jobsb@b-VirtualBox:~/my_temp_test$ ps -ef | grep multib 3499 3004 0 20:11 pts/3 00:00:00 grep --color=auto multib@b-VirtualBox:~/my_temp_test$ setsid ./o_multi_thread_process &amp;[1] 3502b@b-VirtualBox:~/my_temp_test$ ProcessA: 3503 step1ProcessA: 3503 thread 139947724490496 step2ProcessA: 3503 thread 139947724490496 step3ProcessB: 3504 step1ProcessB: 3504 step2ProcessB: 3504 step3^C[1]+ Done setsid ./o_multi_thread_processb@b-VirtualBox:~/my_temp_test$ ps -ef | grep multib 3503 1256 0 20:12 ? 00:00:00 ./o_multi_thread_processb 3504 3503 0 20:12 ? 00:00:00 ./o_multi_thread_processb 3507 3004 0 20:12 pts/3 00:00:00 grep --color=auto multib@b-VirtualBox:~/my_temp_test$ jobs disown用法那么对于已经在后台运行的进程，如果我们要在退出shell的时候继续运行进程, 该怎么办呢？可以使用disown命令 1234567891011121314b@b-VirtualBox:~/my_temp_test$ ./o_multi_thread_process &amp;[1] 3523b@b-VirtualBox:~/my_temp_test$ ProcessA: 3523 step1ProcessA: 3523 thread 140501901821696 step2ProcessA: 3523 thread 140501901821696 step3ProcessB: 3524 step1ProcessB: 3524 step2ProcessB: 3524 step3^Cb@b-VirtualBox:~/my_temp_test$ jobs -l[1]+ 3523 Running ./o_multi_thread_process &amp;b@b-VirtualBox:~/my_temp_test$ disown -h %1b@b-VirtualBox:~/my_temp_test$ jobs[1]+ Running ./o_multi_thread_process &amp;]]></content>
      <categories>
        <category>NP</category>
      </categories>
      <tags>
        <tag>UNP</tag>
        <tag>TLPI</tag>
        <tag>APUE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[crontab笔记整理]]></title>
    <url>%2Fblog%2F2017%2F07%2F09%2Fcrontab%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86%2F</url>
    <content type="text"><![CDATA[crontab命令被用来提交和管理用户的需要周期性执行的任务，与windows下的计划任务类似，当安装完成操作系统后，默认会安装此服务工具，并且会自动启动crond进程，crond进程每分钟会定期检查是否有要执行的任务，如果有要执行的任务，则自动执行该任务。 -e：编辑该用户的计时器设置； -l：列出该用户的计时器设置； -r：删除该用户的计时器设置； -u&lt;用户名称&gt;：指定要设定计时器的用户名称 . . . m h dom mon dow command分 时 日 月 周 命令 星号（*）：代表所有可能的值，例如month字段如果是星号，则表示在满足其它字段的制约条件后每月都执行该命令操作 逗号（,）：可以用逗号隔开的值指定一个列表范围，例如，“1,2,5,7,8,9” 中杠（-）：可以用整数之间的中杠表示一个整数范围，例如“2-6”表示“2,3,4,5,6” 正斜线（/）：可以用正斜线指定时间的间隔频率，例如“0-23/2”表示每两小时执行一次。同时正斜线可以和星号一起使用，例如*/10，如果用在minute字段，表示每十分钟执行一次。 例子 */1 * * * * sed -i &#39;$a\nani&#39; /home/b/my_temp_test/practice.cpp每隔一分钟就在practice.cpp文件的最后一行插入字符串“nani” 3,15 8-11/2 * 12 0 sed -i &#39;$a\nani&#39; /home/b/my_temp_test/practice.cpp12月的周日的8-11时的时间段每隔两个小时就在第3分钟和第15分钟的时候，在practice.cpp文件的最后一行插入字符串“nani”]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Command</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[重读UNIX网络编程第三章到第十一章笔记整理(二)]]></title>
    <url>%2Fblog%2F2017%2F07%2F03%2Funp_chapter_three_to_eleven_note_second_part%2F</url>
    <content type="text"><![CDATA[第七章 7.5节 ： 通用套接字选项， 常用的有 SO_KEEPALIVE SO_REVBUF SO_SNDBUF SO_REUSEADDR 7.9节 ： tcp套接字选项， 常用的有 TCP_NODELAY TCP_MAXSEG 7.11节 ：fcntl函数，常用的用法是使用F_SETFL命令设置O_NOBLOCK文件状态标志， 我们可以把一个套接字设置为非阻塞型。 . . . 第八章基本UDP套接字编程 8.11节 ： UDP的connect函数，可以获得性能提升，因为未连接的udp每次sendto发送数据报的时候都要连接然后发送然后断开， 之后第二个数据报又要重复上述步骤，而连接后的udp套接字只需要连接然后发送第一个数据报然后发送第二个、第三个就行了]]></content>
      <categories>
        <category>NP</category>
      </categories>
      <tags>
        <tag>UNP</tag>
        <tag>TLPI</tag>
        <tag>APUE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[重读UNIX网络编程第三章到第十一章笔记整理(一)]]></title>
    <url>%2Fblog%2F2017%2F07%2F02%2Funp_chapter_three_to_eleven_note_first_part%2F</url>
    <content type="text"><![CDATA[因为第二章之后基本都是纯Socket API的内容， 第三章到第十一章的笔记整理合并到一起。 第三章 3.4 ：字节排序函数，涉及到大小端，处理网络字节序和主机字节序的转换 如何判别是大端(Big-Endian)还是小端(Little-Endian): 1234567891011121314151617181920212223242526union TestBigOrLittle&#123; short var_short; char array_char[2];&#125;;int main()&#123; TestBigOrLittle unTestUnion; unTestUnion.var_short = 0x1234; if (sizeof(short) == 2) &#123; if (unTestUnion.array_char[0] == 0x12) printf("BigEndian\n"); else if(unTestUnion.array_char[0] == 0x34) printf("LittleEndian\n"); else printf("unkonw endian\n"); &#125; else &#123; printf("sizeof(short) : %d \n", sizeof(short)); &#125; return 0; &#125; 网际协议使用大端字节序来传送这些多字节整数, 也就是说网络字节序就是大端字节序. 由图中我们可以知道, htons和ntohs是用于端口的字节序转换的, 而htonl和ntohl是用于32位IP地址的, 下图就是一个例子: 3.6 ： 地址转换函数，它们在ASCII字符串(这是人们偏爱使用的格式)与网络字节序的二进制值(这是存放在套接字地址结构中的值)之间转换网际地址 第四章基本TCP套接字编程 listen函数 int listen(int sockfd, int backlog); 当来自客户的SYN到达时，TCP在未完成连接队列中创建个新项，然后响应以三路握手 的第—个分节服务器的SYN响应，其中捎带对客户SYN的ACK（2.6节）。这一项．直保留在 未完成连接队列中，直到三路握手的第二个分节（客户对服务器SYN的ACK）到达或者该项超 时为止。（源白Berkeley的实现为这些末完成连接的项设置的超时值为75s。）如果3路握手正常 完成，该项就从未完成连接队列移到已完成连接队列的队尾。当进程调用accept时（该函数在 下一节讲解），己完成连接队列巾的队头项将返回给给进程，或者如果该队列为空，那么进程将被 投入睡眠，直到TCP在该队列中放入一项才唤醒它。 4.6节: accept函数 accept函数用于从已完成连接队列对头返回下一个已完成连接 int accept(int sockfd, struct sockaddr *cliaddr, socklent_t *addrlen); 4.7节: fork函数 fork函数的内存语义: 共享代码段, 子指向父 : 父子进程共享同一代码段, 子进程的页表项指向父进程相同的物理内存页(即数据段/堆段/栈段的各页) 写时复制(copy-on-write) : 内核会捕获所有父进程或子进程针对这些页面(即数据段/堆段/栈段的各页)的修改企图, 并为将要修改的页面创建拷贝, 将新的页面拷贝分配给遭内核捕获的进程, 从此父/子进程可以分别修改各自的页拷贝, 不再相互影响. 4.9节： close函数， 涉及到描述符引用计数，所以多进程并发服务器才可以共享已连接套接字，因为父进程调用close函数知识把该套接字标记成已关闭并导致该套接字描述符减1。只要引用计数的值仍大于0，就不会引发tcp的四分组连接终止序列 第五章 5.9节： 处理SIGCHLD信号， 涉及到僵死进程（子进程终止时给父进程发送了一个SIGCHLD信号，若父进程未加处理，则子进程进入僵死状态），所以要建立该信号处理函数，并在函数中调用waitpid来处理 5.10节 ： 使用wait或者waitpid来处理已终止的子进程，通常是使用waitpid并指定WNOHANG选项，来告知waitpid在有尚未终止的子进程在运行时不要阻塞。 . . . 第六章 同步I/O操作：导致请求进程阻塞，知道I/O操作完成 异步I/O操作：不导致请求进程阻塞 6.6节 ： shutdown函数，shutdown可以不用管引用计数就激发tcp的正常连接终止序列。当关闭连接的写这一半，对于tcp连接， 这称为半关闭（half-close）]]></content>
      <categories>
        <category>NP</category>
      </categories>
      <tags>
        <tag>UNP</tag>
        <tag>TLPI</tag>
        <tag>APUE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[重读UNIX网络编程第二章笔记修正（结合TLPI和APUE两书的笔记整理）]]></title>
    <url>%2Fblog%2F2017%2F06%2F05%2Funp_chapter_two_note%2F</url>
    <content type="text"><![CDATA[为加深理解, 故本章老笔记内容大幅删减重写.第二章重点如下 : TCP (Transmission Control Protocol)传输控制协议.特性如下 : TCP头为20字节 面向连接 全双工 可靠, 关心确认/超时/重传等, 保证顺序 流量控制 字节流, 没有任何记录边界 UDP (User Datagram Protocol)用户数据报协议.特性如下 : UDP头为8字节 无连接 不可靠, 不保证顺序/是否到达/是否重复 每个数据报都有一个长度 TCP三路握手(three-way handshake) TCP选项 : MSS选项 发送SYN的TCP一端使用本选项通告对端他的最大分节大小(maximum segment size) 窗口规模选项 时间戳选项, 对于高速网络连接是必要的. . . . TCP连接终止 : TCP状态转换图 TCP连接的分组交换 TIME_WAIT状态存在的理由 : 可靠地实现TCP全双工连接的终止 :我们假设客户端不维护一个TIME_WAIT状态的情况 : 如果服务器没有收到客户端最后一个ACK, 服务器就会重传它最终的那个FIN, 此时客户端就会将会响应一个RST, 该分节将使服务器解释为一个错误. 允许老的重复分节在网络中消逝(分节是TCP传递给IP的数据单元) :我们假设客户端不维护一个TIME_WAIT状态的情况 : 我们先以12.106.32.254的1500端口和206.168.112.219的21端口之间有一个tcp连接A.我们关闭这个连接, 过一段时间后再用相同的IP和端口之间建立另一个连接B, 连接B将有可能收到连接A的老的重复分节. Linux下面一共有65535个端口 其中1–1023是系统保留的， 1024–65535是供用户使用的。 0到1024是众所周知的端口（知名端口，常用于系统服务等，例如http服务的端口号是80)。个人写的应用程序，尽量不要使用0到1024之间的端口号。 套接字对是一个定义该连接的两个端点的四元组: 本地IP地址 本地TCP端口号 外地IP地址 外地TCP端口号 缓冲区大小及限制 IPv4数据报的最大大小为65535字节, 因为其总长度字段占据16位 以太网的MTU是1500字节, IPv4要求的最小链路MTU是68字节, 这允许最大的IPv4首部(包括20字节的固定长度部分和最多40字节的选项部分)拼接最小的片段 在两个主机之间的路径中最小的MTU成为路径MTU 当一个IP数据报将从某个接口送出时, 如果他的大小超过相应链路的MTU, IPv4和IPv6都将执行分片 IPv4首部的”不分片(don’t fragment)”位(即DF位)若被设置, 那么不管是发送这些数据报的主机还是转发他们的路由器, 都不允许对他们分片 IPv4和IPv6都定义了最小重组缓冲区大小(minimum reassembly buffersize), 它是IPv4或IPv6的任何事先都必须保证支持的最小数据报大小. 对于IPv4是576字节, 对于IPv6是1500字节. 例如, 就IPv4而言, 我们不能主观地认为某个给定目的地一定能接受577字节的数据报(因为我们只能保证它一定能接受576字节的数据报). 所以很多使用UDP的IPv4应用（如DNS）都避免产生大于这个大小的数据报 MSS(maximum segment size) : TCP最大分节大小，用于向对端TCP通告对端在每个分节中能发送的最大TCP数据量. 在以太网中使用IPv4的MSS值为1460（以太网的MTU - IPv4首部 - TCP首部 = 1500 - 20 - 20） TCP输出示意图 : UDP输出示意图(因为UDP是不可靠的, 他不必保存应用进程数据的一个副本, 因此无需一个真正的发送缓冲区, 所以为虚线框): 常见因特网应用所使用的协议 ping ： ICMP DNS ： UDP、TCP DHCP : UDP SSH : TCP FTP : TCP HTTP : TCP]]></content>
      <categories>
        <category>NP</category>
      </categories>
      <tags>
        <tag>UNP</tag>
        <tag>TLPI</tag>
        <tag>APUE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[重读UNIX网络编程第一章笔记修正]]></title>
    <url>%2Fblog%2F2017%2F06%2F02%2F%E9%87%8D%E8%AF%BBUNIX%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E7%AC%AC%E4%B8%80%E7%AB%A0%E7%AC%94%E8%AE%B0%E4%BF%AE%E6%AD%A3%2F</url>
    <content type="text"><![CDATA[又准备从头看一遍unp, 把一些老笔记放到博客里来就当网盘吧, 顺便修正以及删减一些之前不够精炼的老笔记内容. 第一章重点如下 : OSI (open systems interconnection), 即计算机通信开放系统互联模型 OSI分为七层, 从上到下依次为 应用层 表现层 会话层 传输层 网络层 数据链路层 物理层 对于网际网协议族, OSI顶上三层合并为一层, 称为应用层. 传输层对应着tcp/udp等, 网络层对应着IPv4/IPv6, OSI的数据链路层和物理层是随系统提供的设备驱动程序和网络硬件 . . . 套接字编程接口是从OSI顶上三层(网际协议的应用层)进入传输层的接口. 为何套接字要设计为顶上三层进入传输层的接口??因为OSI顶上三层处理具体网络应用的所有细节却对通信细节了解很少;底下四层对具体网络应用了解不多, 却处理所有的通信细节.]]></content>
      <categories>
        <category>NP</category>
      </categories>
      <tags>
        <tag>UNP</tag>
        <tag>TLPI</tag>
        <tag>APUE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[UE4中如何解决Failed To Launch Editor]]></title>
    <url>%2Fblog%2F2017%2F05%2F02%2Fue4_fix_failed_to_launch_editor%2F</url>
    <content type="text"><![CDATA[当你点击 .uproject 文件却打不开项目, 弹出一个窗口写着 “Failed TO Launch Editor”的时候,大概率是因为你对 UE4Editor.exe 设置为了以管理员身份打开,所以解决方法就是 : 只要对 UE4Editor.exe 右键-属性-兼容性, 去掉”以管理员身份运行此程序”的勾 以及去掉”更改所有用户的设置”中的以管理员身份运行此程序的√]]></content>
      <categories>
        <category>UE4</category>
      </categories>
      <tags>
        <tag>UE4</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[UE4打印大全]]></title>
    <url>%2Fblog%2F2017%2F05%2F02%2Fue4_print_tricks%2F</url>
    <content type="text"><![CDATA[介绍都是自描述性的且项目无关的, 可以直接放心使用. . . . 使用例子覆盖了以下几种场合 : Example usage: A_LOG(); Example usage: A_LOG_1( “Action!” ); Example usage: A_LOG_2(“Action!”, “Cut!”); Example usage: A_LOG_N(“Action!”, 88.f); Example usage: A_LOG_M(“Action! %f, %d”, 88.f, 88); 示例代码PrintHelper.h12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273// Fill out your copyright notice in the Description page of Project Settings.#pragma once#define ACTION_SHOW_DEBUG_SCREEN_MSG false#define ACTION_SHOW_DEBUG_OUTPUT_LOG false#define ACTION_SHOW_DEBUG_OUTPUT_LOG_EXTRA false//Current Class Name + Function Name where this is called!#define STR_CUR_CLASS_FUNC (FString(__FUNCTION__))//Current Class where this is called!#define STR_CUR_CLASS (FString(__FUNCTION__).Left(FString(__FUNCTION__).Find(TEXT(":"))) )//Current Function Name where this is called!#define STR_CUR_FUNC (FString(__FUNCTION__).Right(FString(__FUNCTION__).Len() - FString(__FUNCTION__).Find(TEXT("::")) - 2 ))//Current Line Number in the code where this is called!#define STR_CUR_LINE (FString::FromInt(__LINE__))//Current Class and Line Number where this is called!#define STR_CUR_CLASS_LINE (STR_CUR_CLASS + "(" + STR_CUR_LINE + ")")//Current Class and Line Number where this is called!#define STR_CUR_CLASS_FUNC_LINE (STR_CUR_CLASS_FUNC + "(" + STR_CUR_LINE + ")")//Current Function Signature where this is called!#define STR_CUR_FUNCSIG (FString(__FUNCSIG__))////////////// Screen Message// Gives you the Class name and exact line number where you print a message to yourself!#define A_MSG(TimeToDisplay ) if (ACTION_SHOW_DEBUG_SCREEN_MSG) (GEngine-&gt;AddOnScreenDebugMessage(-1, (float)TimeToDisplay, FColor::Red, *(STR_CUR_CLASS_FUNC_LINE )) )#define A_MSG_1(TimeToDisplay, StringParam1) if (ACTION_SHOW_DEBUG_SCREEN_MSG) (GEngine-&gt;AddOnScreenDebugMessage(-1, (float)TimeToDisplay, FColor::Red, *(STR_CUR_CLASS_FUNC_LINE + " : " + StringParam1)) )#define A_MSG_2(TimeToDisplay, StringParam1, StringParam2) if (ACTION_SHOW_DEBUG_SCREEN_MSG) (GEngine-&gt;AddOnScreenDebugMessage(-1, (float)TimeToDisplay, FColor::Red, *(STR_CUR_CLASS_FUNC_LINE + " : " + StringParam1 + " " + StringParam2)) )//#define A_SCREENMSG_F(StringParam1, NumericalParam2) if (ACTION_SHOW_DEBUG_SCREEN_MSG) (GEngine-&gt;AddOnScreenDebugMessage(-1, (float)TimeToDisplay, FColor::Red, *(STR_CUR_CLASS_FUNC_LINE + " : " + StringParam1 + " " + FString::SanitizeFloat(NumericalParam2))) )#define A_MSG_N(TimeToDisplay, StringParam1, NumericalParam2) if (ACTION_SHOW_DEBUG_SCREEN_MSG) (GEngine-&gt;AddOnScreenDebugMessage(-1, (float)TimeToDisplay, FColor::Red, FString::Printf( TEXT("%s : %s %f"), *STR_CUR_CLASS_FUNC_LINE, *FString(StringParam1), float(NumericalParam2) ) ) )#define A_MSG_M(TimeToDisplay, FormatString, ...) if (ACTION_SHOW_DEBUG_SCREEN_MSG) (GEngine-&gt;AddOnScreenDebugMessage(-1, (float)TimeToDisplay, FColor::Red, FString::Printf( TEXT("%s : %s"), *STR_CUR_CLASS_FUNC_LINE, *FString::Printf(TEXT(FormatString), ##__VA_ARGS__ ) ) ) )///////// UE LOG!// Example usage: A_LOG();#define A_LOG() if (ACTION_SHOW_DEBUG_OUTPUT_LOG) UE_LOG(LogTemp, Warning, TEXT("%s"), *STR_CUR_CLASS_FUNC_LINE )// Example usage: A_LOG_1( "Action!" );#define A_LOG_1(StringParam1) if (ACTION_SHOW_DEBUG_OUTPUT_LOG) UE_LOG(LogTemp, Warning, TEXT("%s : %s"), *STR_CUR_CLASS_FUNC_LINE, *FString(StringParam1))// Example usage: A_LOG_2("Action!", "Cut!");#define A_LOG_2(StringParam1, StringParam2) if (ACTION_SHOW_DEBUG_OUTPUT_LOG) UE_LOG(LogTemp, Warning, TEXT("%s : %s %s"), *STR_CUR_CLASS_FUNC_LINE, *FString(StringParam1), *FString(StringParam2))// Example usage: A_LOG_N("Action!", 88.f);#define A_LOG_N(StringParam1, NumericalParam2) if (ACTION_SHOW_DEBUG_OUTPUT_LOG) UE_LOG(LogTemp, Warning, TEXT("%s : %s %f"), *STR_CUR_CLASS_FUNC_LINE, *FString(StringParam1), float(NumericalParam2) )// #define A_LOG_M(FormatString, ...) if (ACTION_SHOW_DEBUG_OUTPUT_LOG) UE_LOG(LogTemp, Warning, TEXT("%s : %s"), *STR_CUR_CLASS_FUNC_LINE, *FString::Printf(TEXT(FormatString), ##__VA_ARGS__ ) )class PrintHelper&#123;public: static void ScreenMsg( const FString&amp; Msg ); static void ScreenMsg( const FString&amp; Msg, const FString&amp; Msg2 ); static void ScreenMsg( const FString&amp; Msg, const float FloatValue );&#125;; PrintHelper.cpp123456789101112131415161718192021222324// Fill out your copyright notice in the Description page of Project Settings.#include "PrintHelper.h"//ScreenMsgvoid PrintHelper::ScreenMsg( const FString&amp; Msg )&#123; if (!ACTION_SHOW_DEBUG_SCREEN_MSG) return; GEngine-&gt;AddOnScreenDebugMessage( -1, 55.f, FColor::Red, *Msg );&#125;void PrintHelper::ScreenMsg( const FString&amp; Msg, const FString&amp; Msg2 )&#123; if (!ACTION_SHOW_DEBUG_SCREEN_MSG) return; GEngine-&gt;AddOnScreenDebugMessage( -1, 55.f, FColor::Red, FString::Printf( TEXT( "%s %s" ), *Msg, *Msg2 ) );&#125;void PrintHelper::ScreenMsg( const FString&amp; Msg, const float Value )&#123; if (!ACTION_SHOW_DEBUG_SCREEN_MSG) return; GEngine-&gt;AddOnScreenDebugMessage( -1, 55.f, FColor::Red, FString::Printf( TEXT( "%s %f" ), *Msg, Value ) );&#125;]]></content>
      <categories>
        <category>UE4</category>
      </categories>
      <tags>
        <tag>UE4</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[UE4中如何不继承UObject就能spawn一个actor]]></title>
    <url>%2Fblog%2F2017%2F04%2F22%2Fue4_how_to_spawn_actor_but_not_use_uobj%2F</url>
    <content type="text"><![CDATA[介绍其实不用继承 UObject 也可以生成一个 actor, 关键的点就是拿到 UWorld ,所以只要从一个拥有 UWorld 的虚幻相关实例中传递它的 UWorld 给一个原生 C++ 类也可以. 示例代码. . . RealTimeSrvEntityFactory.h1234567891011121314151617181920212223242526272829303132333435363738394041// Fill out your copyright notice in the Description page of Project Settings.#pragma once#include &lt;memory&gt;#include "RealTimeSrvEntity.h"#include "RealTimeSrvPawn.h"/** * */typedef RealTimeSrvEntityPtr( *GameObjectCreationFunc )( );class RealTimeSrvEntityFactory&#123;public: static void StaticInit(UWorld * inWorld); UWorld* GetWorld() const; RealTimeSrvEntityPtr CreateGameObject( uint32_t inFourCCName ); void SetDefaultPawnClass( TSubclassOf&lt;class ARealTimeSrvPawn&gt; inDefaultCharacterClasses ) &#123; DefaultCharacterClasses = inDefaultCharacterClasses; &#125;public: static std::unique_ptr&lt;RealTimeSrvEntityFactory&gt; sInstance;private: RealTimeSrvEntityFactory(); RealTimeSrvEntityPtr CreateActionPawn();private: TSubclassOf&lt;class ARealTimeSrvPawn&gt; DefaultCharacterClasses; UWorld* mWorld;&#125;; RealTimeSrvEntityFactory.cpp12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667// Fill out your copyright notice in the Description page of Project Settings.#include "RealTimeSrvEntityFactory.h"#include "RealTimeSrvWorld.h"std::unique_ptr&lt;RealTimeSrvEntityFactory&gt; RealTimeSrvEntityFactory::sInstance;RealTimeSrvEntityFactory::RealTimeSrvEntityFactory()&#123;&#125;void RealTimeSrvEntityFactory::StaticInit(UWorld * inWorld)&#123; sInstance.reset( new RealTimeSrvEntityFactory() ); check( sInstance ); if (sInstance) &#123; sInstance-&gt;mWorld = inWorld; &#125;&#125;UWorld* RealTimeSrvEntityFactory::GetWorld() const&#123; return mWorld;&#125;RealTimeSrvEntityPtr RealTimeSrvEntityFactory::CreateGameObject( uint32_t inFourCCName )&#123; switch ( inFourCCName ) &#123; case 'CHRT': return CreateActionPawn(); default: break; &#125; return RealTimeSrvEntityPtr();&#125;RealTimeSrvEntityPtr RealTimeSrvEntityFactory::CreateActionPawn()&#123; check( GetWorld() ); UWorld* const world = GetWorld(); if ( world ) &#123; FActorSpawnParameters SpawnParams; SpawnParams.SpawnCollisionHandlingOverride = ESpawnActorCollisionHandlingMethod::AlwaysSpawn; ARealTimeSrvPawn* const newActionPawn = world-&gt;SpawnActor&lt;ARealTimeSrvPawn&gt;( DefaultCharacterClasses, FTransform::Identity, SpawnParams ); if ( newActionPawn ) &#123; RealTimeSrvWorld::sInstance-&gt;AddGameObject( newActionPawn ); &#125; return RealTimeSrvEntityPtr( newActionPawn ); &#125; return RealTimeSrvEntityPtr();&#125;]]></content>
      <categories>
        <category>UE4</category>
      </categories>
      <tags>
        <tag>UE4</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[构建游戏网络协议六之客户端与服务器的连接]]></title>
    <url>%2Fblog%2F2017%2F04%2F09%2Fclient_server_connection%2F</url>
    <content type="text"><![CDATA[本篇自我总结这篇文章网上找不到翻译, 我也没时间详细翻译, 大概总结一下吧.本篇主要讲了把本系列之前五篇文章的技术应用到实战中处理客户端与服务器的的连接.请看总结, 不明之处再看文中具体讲解. 简单的连接协议First up we have the client state machine.The client is in one of three states: Disconnected Connecting Connected The goal is to create an abstraction on top of a UDP socket where our server presents a number of virtual slots for clients to connect to. When a client requests a connection, it gets assigned to one of these slots. If a client requests connection, but no slots are available, the server is full and the connection request is denied. On the server, we have the following data structure: 123456789const int MaxClients = 64;class Server&#123; int m_maxClients; int m_numConnectedClients; bool m_clientConnected[MaxClients]; Address m_clientAddress[MaxClients];&#125;; 这个简单协议存在的问题 易被攻击者利用我们的服务器当做DDos放大攻击的工具 攻击者可以很轻松的占满我们的client slots Traffic between the client and server can be read and modified in transit by a third party. 一旦被攻击者知道了客户端或者服务器的地址, 他就可以伪装服务器或客户端来欺骗对方获取利益 没有一个明确的断开连接的方式, 只能等time out 这些问题需要用授权系统和加密系统来解决. 如何改进这个连接协议 we no longer accept client connections immediately on connection request, instead we send back a challenge packet, and only complete connection when a client replies with information that can only be obtained by receiving the challenge packet. 为了防止攻击者利用我们的服务器当做DDos放大攻击的工具, 我们让客户端发的包比服务器发的包要大些. We’ll add some unique random identifiers, or ‘salts’, to make each client connection unique from previous ones coming from the same IP address and port. 一旦彼此连接上之后, 就用 client salt 和 server salt 的异或值来标识彼此. 以上的防御措施让我们的服务器做到了 no longer able to be used as port of DDoS amplification attacks, and with a trivial xor based authentication, 但对于一个经验丰富的会抓包分析的攻击者来说, 还存在以下问题 : This attacker can read and modify packets in flight. This breaks the trivial identification based around salt values… giving an attacker the power to disconnect any client at will. To solve this, we need to get serious with cryptography to encrypt and sign packets so they can’t be read or modified by a third party. 原文原文出处 原文标题 : Client Server Connection (How to create a client/server connection over UDP) Introduction Hi, I’m Glenn Fiedler and welcome to Building a Game Network Protocol. So far in this article series we&rsquo;ve discussed how games read and write packets, how to unify packet read and write into a single function, how to fragment and re-assemble packets, and how to send large blocks of data over UDP. Now in this article we&rsquo;re going to bring everything together and build a client/server connection on top of UDP. Background Developers from a web background often wonder why games go to such effort to build a client/server connection on top of UDP, when for many applications, TCP is good enough. These days even web servers are transitioning to UDP via Google&rsquo;s QUIC. If you still think TCP is good enough for time critical data in 2016, I encourage you to put that in your pipe and smoke it :) The reason is that games send time critical data. Why don&rsquo;t games use TCP for time critical data? The answer is that TCP delivers data reliably and in-order, and to do this on top of IP (which is unreliable, unordered) it holds more recent packets hostage in a queue while older packets are resent over the network. This is known as head of line blocking and it&rsquo;s a huuuuuge problem for games. To understand why, consider a game server broadcasting the state of the world to clients 10 times per-second. Each client advances time forward and wants to display the most recent state it receives from the server. But if the packet containing state for time t = 10.0 is lost, under TCP we must wait for it to be resent before we can access t = 10.1 and 10.2, even though those packets have already arrived and contain the state the client wants to display. Worse still, by the time the resent packet arrives, it&rsquo;s far too late for the client to actually do anything useful with it. The client has already advanced past 10.0 and wants to display something around 10.3 or 10.4! So why resend dropped packets at all? BINGO! What we&rsquo;d really like is an option to tell TCP: &ldquo;Hey, I don&rsquo;t care about old packets being resent, by they time they arrive I can&rsquo;t use them anyway, so just let me skip over them and access the most recent data&rdquo;. Unfortunately, TCP simply does not give us this option :( All data must be delivered reliably and in-order. This creates terrible problems for time critical data where packet loss and latency exist. Situations like, you know, The Internet, where people play FPS games. Large hitches corresponding to multiples of round trip time are added to the stream of data as TCP waits for dropped packets to be resent, which means additional buffering to smooth out these hitches, or long pauses where the game freezes and is non-responsive. Neither option is acceptable for first person shooters, which is why virtually all first person shooters are networked using UDP. UDP doesn&rsquo;t provide any reliability or ordering, so protocols built on top it can access the most recent data without waiting for lost packets to be resent, implementing whatever reliability they need in radically different ways to TCP. But, using UDP comes at a cost: UDP doesn&rsquo;t provide any concept of connection. We have to build that ourselves. This is a lot of work! So strap in, get ready, because we&rsquo;re going to build it all up from scratch using the same basic techniques first person shooters use when creating their protocols over UDP. You can use this client/server protocol for games or non-gaming applications and, provided the data you send is time critical, I promise you, it&rsquo;s well worth the effort. Client/Server Abstraction The goal is to create an abstraction on top of a UDP socket where our server presents a number of virtual slots for clients to connect to: When a client requests a connection, it gets assigned to one of these slots: If a client requests connection, but no slots are available, the server is full and the connection request is denied: Once a client is connected, packets are exchanged in both directions. These packets form the basis for the custom protocol between the client and server which is game specific. In a first person shooter, packets are sent continuously in both directions. Clients send input to the server as quickly as possible, often 30 or 60 times per-second, and the server broadcasts the state of the world to clients 10, 20 or even 60 times per-second. Because of this steady flow of packets in both directions there is no need for keep-alive packets. If at any point packets stop being received from the other side, the connection simply times out. No packets for 5 seconds is a good timeout value in my opinion, but you can be more aggressive if you want. When a client slot times out on the server, it becomes available for other clients to connect. When the client times out, it transitions to an error state. Simple Connection Protocol Let&rsquo;s get started with the implementation of a simple protocol. It&rsquo;s a bit basic and more than a bit naive, but it&rsquo;s a good starting point and we&rsquo;ll build on it during the rest of this article, and the next few articles in this series. First up we have the client state machine. The client is in one of three states: Disconnected Connecting Connected Initially the client starts in disconnected. When a client connects to a server, it transitions to the connecting state and sends connection request packets to the server: The CRC32 and implicit protocol id in the packet header allow the server to trivially reject UDP packets not belonging to this protocol or from a different version of it. Since connection request packets are sent over UDP, they may be lost, received out of order or in duplicate. Because of this we do two things: 1) we keep sending packets for the client state until we get a response from the server or the client times out, and 2) on both client and server we ignore any packets that don&rsquo;t correspond to what we are expecting, since a lot of redundant packets are flying over the network. On the server, we have the following data structure: const int MaxClients = 64; class Server{ int m_maxClients; int m_numConnectedClients; bool m_clientConnected[MaxClients]; Address m_clientAddress[MaxClients];}; Which lets the server lookup a free slot for a client to join (if any are free): int Server::FindFreeClientIndex() const { for ( int i = 0; i &lt; m_maxClients; ++i ) { if ( !m_clientConnected[i] ) return i; } return -1; } Find the client index corresponding to an IP address and port: int Server::FindExistingClientIndex( const Address &amp; address ) const { for ( int i = 0; i &lt; m_maxClients; ++i ) { if ( m_clientConnected[i] &amp;&amp; m_clientAddress[i] == address ) return i; } return -1; } Check if a client is connected to a given slot: bool Server::IsClientConnected( int clientIndex ) const { return m_clientConnected[clientIndex]; } &hellip; and retrieve a client’s IP address and port by client index: const Address &amp; Server::GetClientAddress( int clientIndex ) const { return m_clientAddress[clientIndex]; } Using these queries we implement the following logic when the server processes a connection request packet: If the server is full, reply with connection denied. If the connection request is from a new client and we have a slot free, assign the client to a free slot and respond with connection accepted. If the sender corresponds to the address of a client that is already connected, also reply with connection accepted. This is necessary because the first response packet may not have gotten through due to packet loss. If we don&rsquo;t resend this response, the client gets stuck in the connecting state until it times out. The connection accepted packet tells the client which client index it was assigned, which the client needs to know which player it is in the game: Once the server sends a connection accepted packet, from its point of view it considers that client connected. As the server ticks forward, it watches connected client slots, and if no packets have been received from a client for 5 seconds, the slot times out and is reset, ready for another client to connect. Back to the client. While the client is in the connecting state the client listens for connection denied and connection accepted packets from the server. Any other packets are ignored. If the client receives connection accepted, it transitions to connected. If it receives connection denied, or after 5 seconds hasn&rsquo;t received any response from the server, it transitions to disconnected. Once the client hits connected it starts sending connection payload packets to the server. If no packets are received from the server in 5 seconds, the client times out and transitions to disconnected. Naive Protocol is Naive While this protocol is easy to implement, we can&rsquo;t use a protocol like this in production. It&rsquo;s way too naive. It simply has too many weaknesses to be taken seriously: Spoofed packet source addresses can be used to redirect connection accepted responses to a target (victim) address. If the connection accepted packet is larger than the connection request packet, attackers can use this protocol as part of a DDoS amplification attack. Spoofed packet source addresses can be used to trivially fill all client slots on a server by sending connection request packets from n different IP addresses, where n is the number of clients allowed per-server. This is a real problem for dedicated servers. Obviously you want to make sure that only real clients are filling slots on servers you are paying for. An attacker can trivially fill all slots on a server by varying the client UDP port number on each client connection. This is because clients are considered unique on an address + port basis. This isn&rsquo;t easy to fix because due to NAT (network address translation), different players behind the same router collapse to the same IP address with only the port being different, so we can&rsquo;t just consider clients to be unique at the IP address level sans port. Traffic between the client and server can be read and modified in transit by a third party. While the CRC32 protects against packet corruption, an attacker would simply recalculate the CRC32 to match the modified packet. If an attacker knows the client and server IP addresses and ports, they can impersonate the client or server. This gives an attacker the power to completely a hijack a client’s connection and perform actions on their behalf. Once a client is connected to a server there is no way for them to disconnect cleanly, they can only time out. This creates a delay before the server realizes a client has disconnected, or before a client realizes the server has shut down. It would be nice if both the client and server could indicate a clean disconnect, so the other side doesn’t need to wait for timeout in the common case. Clean disconnection is usually implemented with a disconnect packet, however because an attacker can impersonate the client and server with spoofed packets, doing so would give the attacker the ability to disconnect a client from the server whenever they like, provided they know the client and server IP addresses and the structure of the disconnect packet. If a client disconnects dirty and attempts to reconnect before their slot times out on the server, the server still thinks that client is connected and replies with connection accepted to handle packet loss. The client processes this response and thinks it&rsquo;s connected to the server, but it&rsquo;s actually in an undefined state. While some of these problems require authentication and encryption before they can be fully solved, we can make some small steps forward to improve the protocol before we get to that. These changes are instructive. Improving The Connection Protocol The first thing we want to do is only allow clients to connect if they can prove they are actually at the IP address and port they say they are. To do this, we no longer accept client connections immediately on connection request, instead we send back a challenge packet, and only complete connection when a client replies with information that can only be obtained by receiving the challenge packet. The sequence of operations in a typical connect now looks like this: To implement this we need an additional data structure on the server. Somewhere to store the challenge data for pending connections, so when a challenge response comes in from a client we can check against the corresponding entry in the data structure and make sure it&rsquo;s a valid response to the challenge sent to that address. While the pending connect data structure can be made larger than the maximum number of connected clients, it&rsquo;s still ultimately finite and is therefore subject to attack. We&rsquo;ll cover some defenses against this in the next article. But for the moment, be happy at least that attackers can&rsquo;t progress to the connected state with spoofed packet source addresses. Next, to guard against our protocol being used in a DDoS amplification attack, we&rsquo;ll inflate client to server packets so they&rsquo;re large relative to the response packet sent from the server. This means we add padding to both connection request and challenge response packets and enforce this padding on the server, ignoring any packets without it. Now our protocol effectively has DDoS minification for requests -&gt; responses, making it highly unattractive for anyone thinking of launching this kind of attack. Finally, we&rsquo;ll do one last small thing to improve the robustness and security of the protocol. It&rsquo;s not perfect, we need authentication and encryption for that, but it at least it ups the ante, requiring attackers to actually sniff traffic in order to impersonate the client or server. We&rsquo;ll add some unique random identifiers, or &lsquo;salts&rsquo;, to make each client connection unique from previous ones coming from the same IP address and port. The connection request packet now looks like this: The client salt in the packet is a random 64 bit integer rolled each time the client starts a new connect. Connection requests are now uniquely identified by the IP address and port combined with this client salt value. This distinguishes packets from the current connection from any packets belonging to a previous connection, which makes connection and reconnection to the server much more robust. Now when a connection request arrives and a pending connection entry can&rsquo;t be found in the data structure (according to IP, port and client salt) the server rolls a server salt and stores it with the rest of the data for the pending connection before sending a challange packet back to the client. If a pending connection is found, the salt value stored in the data structure is used for the challenge. This way there is always a consistent pair of client and server salt values corresponding to each client session. The client state machine has been expanded so connecting is replaced with two new states: sending connection request and sending challenge response, but it&rsquo;s the same idea as before. Client states repeatedly send the packet corresponding to that state to the server while listening for the response that moves it forward to the next state, or back to an error state. If no response is received, the client times out and transitions to disconnected. The challenge response sent from the client to the server looks like this: The utility of this being that once the client and server have established connection, we prefix all payload packets with the xor of the client and server salt values and discard any packets with the incorrect salt values. This neatly filters out packets from previous sessions and requires an attacker to sniff packets in order to impersonate a client or server. Now that we have at least a basic level of security, it&rsquo;s not much, but at least it&rsquo;s something, we can implement a disconnect packet: And when the client or server want to disconnect clean, they simply fire 10 of these over the network to the other side, in the hope that some of them get through, and the other side disconnects cleanly instead of waiting for timeout. Conclusion We now have a much more robust protocol. It&rsquo;s secure against spoofed IP packet headers. It&rsquo;s no longer able to be used as port of DDoS amplification attacks, and with a trivial xor based authentication, we are protected against casual attackers while client reconnects are much more robust. But it&rsquo;s still vulnerable to a sophisticated actors who can sniff packets: This attacker can read and modify packets in flight. This breaks the trivial identification based around salt values&hellip; &hellip; giving an attacker the power to disconnect any client at will. To solve this, we need to get serious with cryptography to encrypt and sign packets so they can&rsquo;t be read or modified by a third party.]]></content>
      <categories>
        <category>GS</category>
      </categories>
      <tags>
        <tag>GafferOnGames</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kbe服务端笔记(一)]]></title>
    <url>%2Fblog%2F2017%2F04%2F01%2Fkbe_note_one%2F</url>
    <content type="text"><![CDATA[main看起来似乎所有的组件都有一个这样的宏(KBENGINE_MAIN)来包裹main函数 123456intKBENGINE_MAIN(intargc, char* argv[])&#123; ENGINE_COMPONENT_INFO&amp;info = g_kbeSrvConfig.getXXX(); returnkbeMainT&lt;XXX&gt;(argc, argv, YYY, info.externalPorts_min, info.externalPorts_max, info.externalInterface, 0, info.internalInterface);&#125; . . . 这个宏展开是这样子： 123456789101112131415kbeMain(intargc, char* argv[]); \intmain(intargc, char* argv[]) \&#123; \ loadConfig(); \ g_componentID = genUUID64(); \ parseMainCommandArgs(argc, argv); \ char dumpname[MAX_BUF] = &#123;0&#125;; \ kbe_snprintf(dumpname, MAX_BUF, &quot;%&quot;PRAppID, g_componentID); \ KBEngine::exception::installCrashHandler(1, dumpname); \ intretcode = -1; \ THREAD_TRY_EXECUTION; \ retcode = kbeMain(argc, argv); \ THREAD_HANDLE_CRASH; \ returnretcode; \&#125; \ 稍微整理一下之后main函数看起来很像是这个样子： 1234567891011121314151617181920intkbeMain(intargc, char* argv[]);intmain(intargc, char* argv[])&#123; loadConfig(); g_componentID = genUUID64(); parseMainCommandArgs(argc, argv);chardumpname[MAX_BUF] = &#123;0&#125;; kbe_snprintf(dumpname, MAX_BUF, &quot;%&quot;PRAppID, g_componentID); KBEngine::exception::installCrashHandler(1, dumpname);intretcode = -1; THREAD_TRY_EXECUTION;retcode = kbeMain(argc, argv); THREAD_HANDLE_CRASH;return (retcode);&#125;intkbeMain(intargc, char* argv[])&#123; ENGINE_COMPONENT_INFO&amp;info = g_kbeSrvConfig.getXXX(); return kbeMainT&lt;XXX&gt;(argc, argv, YYY, info.externalPorts_min, info.externalPorts_max, info.externalInterface, 0, info.internalInterface);&#125; 基本可以理解为每个组件的main函数流程都是一样的，只是在特化kbeMainT时所给参数不一样。 ServerConfig：ServerConfig涉及到服务端每个组件的各种配置选项，比如数据库访问。它的构造在组件名.cpp中，比如loginapp就在loginapp.cpp，machine就在machine.cpp中，loginapp的如下（server/loginapp/loginapp.cpp）： 12ServerConfigg_serverConfig;KBE_SINGLETON_INIT(Loginapp); 它的初始化（配置）工作主要由loadConfig接口完成，如下（lib/server/kbemain.h）： 12345678910inlinevoidloadConfig()&#123; Resmgr::getSingleton().initialize(); // &quot;../../res/server/kbengine_defs.xml&quot; g_kbeSrvConfig.loadConfig(&quot;server/kbengine_defs.xml&quot;); // &quot;../../../assets/res/server/kbengine.xml&quot; g_kbeSrvConfig.loadConfig(&quot;server/kbengine.xml&quot;);&#125; Resmgr：Resmgr负责管理kbe的所有资源管理，比如资源路径，环境变量。Resmgr的构造地方如下（lib/network/fixed_messages.cpp）： 1234567FixedMessages::FixedMessages():_infomap(),_loaded(false)&#123; newResmgr(); Resmgr::getSingleton().initialize();&#125; 我们可以理解为FixedMessages构造的时候Resmgr就构造了。 Resmgr的初始化（配置）工作主要由initialize接口完成，代码如上。]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>KBE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[构建游戏网络协议五之可靠的有序消息]]></title>
    <url>%2Fblog%2F2017%2F03%2F29%2Freliable_ordered_messages%2F</url>
    <content type="text"><![CDATA[本篇自我总结本篇主要讲了数据包的分包和重组问题, 到底数据包多大才好呢?是不是越大越好呢?包太大了怎么办呢?请看总结, 不明之处再看文中具体讲解. 为什么需要做这个可靠UDP协议网络协议在动作游戏类型（FPS）中的典型特征就是一个持续发送的数据包，在两个方向上以稳定的速度如20或30包每秒发送。这些数据包都包含有不可靠的无序数据例如t时间内的世界状态；所以，当一个数据包丢失，重新发送它并不是特别有用。当重新发送的数据包到达时，时间t已经过去了。 所以这就是我们将要实现可靠性的现状。对于我们90%的数据包，仅丢弃并不再重新发送它会更好。对于10%或更少（误差允许范围内）的情况，我们确实需要可靠性，但这样的数据是非常罕见的，很少被发送而且比不可靠的数据的平均大小要小得多。这个使用案例适用于所有过去十五年来发布的AAA级的FPS游戏。 应答系统是实现可靠UDP的最重要的部分为实现数据包层级的应答，在每个包的前面添加如下的报头： 123456struct Header&#123; uint16_t sequence; uint16_t ack; uint32_t ack_bits;&#125;; 这些报头元素组合起来以创建应答系统： sequence 是一个数字，随每个数据包发送而增长（并且在达到65535后回往复）。 ack 是从另一方接收到的最新的数据包序列号。 ack_bits 是一个位字段，它编码与ack相关的收到的数据包组合：如果位n已经设置，即 ack– n 数据包被接收了。 ack_bits 不仅是一个节省带宽的巧妙的编码，它同样也增加了信息冗余来抵御包的丢失。每个应答码要被发送32次。如果有一个包丢失了，仍然有其他31个包有着相同的应答码。从统计上来说，应答码还是非常有可能送达的。 但突发的传送数据包的丢失还是有可能发生的，所以重要的是要注意： 如果你收到一个数据包n的应答码，那么这个包肯定已经收到了。 如果你没有收到应答码，那么这个包就很有可能 没有被收到。但是…它也许会是，仅是应答码没有送达。这种情况是极其罕见的。 以我的经验，没有必要设计完善的应答机制。在一个极少丢应答码的系统上构建一个可靠性系统并不会增加什么大问题。 发送方如何追踪数据包是否已经被应答为实现这个应答系统，我们在发送方还需要一个数据结构来追踪一个数据包是否已经被应答，这样我们就可以忽略冗余的应答（每个包会通过 ack_bits多次应答)。我们同样在接收方也还需要一个数据结构来追踪那些已经收到的包，这样我们就可以在数据包的报头填写ack_bits的值。 12345678910111213141516171819const int BufferSize = 1024; uint32_t sequence_buffer[BufferSize]; struct PacketData&#123; bool acked;&#125;; PacketData packet_data[BufferSize]; PacketData * GetPacketData( uint16_t sequence )&#123; const int index = sequence % BufferSize; if ( sequence_buffer[index] == sequence ) return &amp;packet_data[index]; else return NULL;&#125; 你在这可以看到的窍门是这个滚动的缓冲区是以序列号来作为索引的： const int index =sequence % BufferSize; 当条目被顺序添加，就像一个被发送的队列，对插入所需要做的就是把这个序列缓冲区的值更新为新的序列号并且在该索引处重写这个数据： 123456PacketData &amp; InsertPacketData( uint16_t sequence )&#123; const int index = sequence % BufferSize; sequence_buffer[index] = sequence; return packet_data[index];&#125; 原文原文出处 原文标题 : Reliable Ordered Messages (How to implement reliable-ordered messages on top of UDP) Introduction Hi, I’m Glenn Fiedler and welcome to Building a Game Network Protocol. Many people will tell you that implementing your own reliable message system on top of UDP is foolish. After all, why reimplement TCP? But why limit ourselves to how TCP works? But there are so many different ways to implement reliable-messages and most of them work nothing like TCP! So let&rsquo;s get creative and work out how we can implement a reliable message system that&rsquo;s better and more flexible than TCP for real-time games. Different Approaches A common approach to reliability in games is to have two packet types: reliable-ordered and unreliable. You&rsquo;ll see this approach in many network libraries. The basic idea is that the library resends reliable packets until they are received by the other side. This is the option that usually ends up looking a bit like TCP-lite for the reliable-packets. It&rsquo;s not that bad, but you can do much better. The way I prefer to think of it is that messages are smaller bitpacked elements that know how to serialize themselves. This makes the most sense when the overhead of length prefixing and padding bitpacked messages up to the next byte is undesirable (eg. lots of small messages included in each packet). Sent messages are placed in a queue and each time a packet is sent some of the messages in the send queue are included in the outgoing packet. This way there are no reliable packets that need to be resent. Reliable messages are simply included in outgoing packets until they are received. The easiest way to do this is to include all unacked messages in each packet sent. It goes something like this: each message sent has an id that increments each time a message is sent. Each outgoing packet includes the start message id followed by the data for n messages. The receiver continually sends back the most recent received message id to the sender as an ack and only messages newer than the most recent acked message id are included in packets. This is simple and easy to implement but if a large burst of packet loss occurs while you are sending messages you get a spike in packet size due to unacked messages. You can avoid this by extending the system to have an upper bound on the number of messages included per-packet n. But now if you have a high packet send rate (like 60 packets per-second) you are sending the same message multiple times until you get an ack for that message. If your round trip time is 100ms each message will be sent 6 times redundantly before being acked on average. Maybe you really need this amount of redundancy because your messages are extremely time critical, but in most cases, your bandwidth would be better spent on other things. The approach I prefer combines packet level acks with a prioritization system that picks the n most important messages to include in each packet. This combines time critical delivery and the ability to send only n messages per-packet, while distributing sends across all messages in the send queue. Packet Level Acks To implement packet level acks, we add the following packet header: struct Header { uint16_t sequence; uint16_t ack; uint32_t ack_bits; }; These header elements combine to create the ack system: sequence is a number that increases with each packet sent, ack is the most recent packet sequence number received, and ack_bits is a bitfield encoding the set of acked packets. If bit n is set in ack_bits, then ack - n is acked. Not only is ack_bits a smart encoding that saves bandwidth, it also adds redundancy to combat packet loss. Each ack is sent 32 times. If one packet is lost, there&rsquo;s 31 other packets with the same ack. Statistically speaking, acks are very likely to get through. But bursts of packet loss do happen, so it&rsquo;s important to note that: If you receive an ack for packet n then that packet was definitely received. If you don&rsquo;t receive an ack, the packet was most likely not received. But, it might have been, and the ack just didn&rsquo;t get through. This is extremely rare. In my experience it&rsquo;s not necessary to send perfect acks. Building a reliability system on top of a system that very rarely drops acks adds no significant problems. But it does create a challenge for testing this system works under all situations because of the edge cases when acks are dropped. So please if you do implement this system yourself, setup a soak test with terrible network conditions to make sure your ack system is working correctly. You&rsquo;ll find such a soak test in the example source code for this article, and the open source network libraries reliable.io and yojimbo which also implement this technique. Sequence Buffers To implement this ack system we need a data structure on the sender side to track whether a packet has been acked so we can ignore redundant acks (each packet is acked multiple times via ack_bits. We also need a data structure on the receiver side to keep track of which packets have been received so we can fill in the ack_bits value in the packet header. The data structure should have the following properties: Constant time insertion (inserts may be random, for example out of order packets&hellip;) Constant time query if an entry exists given a packet sequence number Constant time access for the data stored for a given packet sequence number Constant time removal of entries You might be thinking. Oh of course, hash table. But there&rsquo;s a much simpler way: const int BufferSize = 1024; uint32_t sequence_buffer[BufferSize]; struct PacketData{ bool acked;}; PacketData packet_data[BufferSize]; PacketData * GetPacketData( uint16_t sequence ){ const int index = sequence % BufferSize; if ( sequence_buffer[index] == sequence ) return &amp;packet_data[index]; else return NULL;} As you can see the trick here is a rolling buffer indexed by sequence number: const int index = sequence % BufferSize; This works because we don&rsquo;t care about being destructive to old entries. As the sequence number increases older entries are naturally overwritten as we insert new ones. The sequence_buffer[index] value is used to test if the entry at that index actually corresponds to the sequence number you&rsquo;re looking for. A sequence buffer value of 0xFFFFFFFF indicates an empty entry and naturally returns NULL for any sequence number query without an extra branch. When entries are added in order like a send queue, all that needs to be done on insert is to update the sequence buffer value to the new sequence number and overwrite the data at that index: PacketData &amp; InsertPacketData( uint16_t sequence ) { const int index = sequence % BufferSize; sequence_buffer[index] = sequence; return packet_data[index]; } Unfortunately, on the receive side packets arrive out of order and some are lost. Under ridiculously high packet loss (99%) I&rsquo;ve seen old sequence buffer entries stick around from before the previous sequence number wrap at 65535 and break my ack logic (leading to false acks and broken reliability where the sender thinks the other side has received something they haven&rsquo;t&hellip;). The solution to this problem is to walk between the previous highest insert sequence and the new insert sequence (if it is more recent) and clear those entries in the sequence buffer to 0xFFFFFFFF. Now in the common case, insert is very close to constant time, but worst case is linear where n is the number of sequence entries between the previous highest insert sequence and the current insert sequence. Before we move on I would like to note that you can do much more with this data structure than just acks. For example, you could extend the per-packet data to include time sent: struct PacketData { bool acked; double send_time; }; With this information you can create your own estimate of round trip time by comparing send time to current time when packets are acked and taking an exponentially smoothed moving average. You can even look at packets in the sent packet sequence buffer older than your RTT estimate (you should have received an ack for them by now&hellip;) to create your own packet loss estimate. Ack Algorithm Now that we have the data structures and packet header, here is the algorithm for implementing packet level acks: On packet send: Insert an entry for for the current send packet sequence number in the sent packet sequence buffer with data indicating that it hasn&rsquo;t been acked yet Generate ack and ack_bits from the contents of the local received packet sequence buffer and the most recent received packet sequence number Fill the packet header with sequence, ack and ack_bits Send the packet and increment the send packet sequence number On packet receive: Read in sequence from the packet header If sequence is more recent than the previous most recent received packet sequence number, update the most recent received packet sequence number Insert an entry for this packet in the received packet sequence buffer Decode the set of acked packet sequence numbers from ack and ack_bits in the packet header. Iterate across all acked packet sequence numbers and for any packet that is not already acked call OnPacketAcked( uint16_t sequence ) and mark that packet as acked in the sent packet sequence buffer. Importantly this algorithm is done on both sides so if you have a client and a server then each side of the connection runs the same logic, maintaining its own sequence number for sent packets, tracking most recent received packet sequence # from the other side and a sequence buffer of received packets from which it generates sequence, ack and ack_bits to send to the other side. And that&rsquo;s really all there is to it. Now you have a callback when a packet is received by the other side: OnPacketAcked. The main benefit of this ack system is now that you know which packets were received, you can build any reliability system you want on top. It&rsquo;s not limited to just reliable-ordered messages. For example, you could use it to implement delta encoding on a per-object basis. Message Objects Messages are small objects (smaller than packet size, so that many will fit in a typical packet) that know how to serialize themselves. In my system they perform serialization using a unified serialize functionunified serialize function. The serialize function is templated so you write it once and it handles read, write and measure. Yes. Measure. One of my favorite tricks is to have a dummy stream class called MeasureStream that doesn&rsquo;t do any actual serialization but just measures the number of bits that would be written if you called the serialize function. This is particularly useful for working out which messages are going to fit into your packet, especially when messages themselves can have arbitrarily complex serialize functions. struct TestMessage : public Message { uint32_t a,b,c; TestMessage() { a = 0; b = 0; c = 0; } template &amp;lt;typename Stream&amp;gt; bool Serialize( Stream &amp;amp; stream ) { serialize_bits( stream, a, 32 ); serialize_bits( stream, b, 32 ); serialize_bits( stream, c, 32 ); return true; } virtual SerializeInternal( WriteStream &amp;amp; stream ) { return Serialize( stream ); } virtual SerializeInternal( ReadStream &amp;amp; stream ) { return Serialize( stream ); } virtual SerializeInternal( MeasureStream &amp;amp; stream ) { return Serialize( stream ); }}; The trick here is to bridge the unified templated serialize function (so you only have to write it once) to virtual serialize methods by calling into it from virtual functions per-stream type. I usually wrap this boilerplate with a macro, but it&rsquo;s expanded in the code above so you can see what&rsquo;s going on. Now when you have a base message pointer you can do this and it just works: Message * message = CreateSomeMessage(); message-&gt;SerializeInternal( stream ); An alternative if you know the full set of messages at compile time is to implement a big switch statement on message type casting to the correct message type before calling into the serialize function for each type. I&rsquo;ve done this in the past on console platform implementations of this message system (eg. PS3 SPUs) but for applications today (2016) the overhead of virtual functions is neglible. Messages derive from a base class that provides a common interface such as serialization, querying the type of a message and reference counting. Reference counting is necessary because messages are passed around by pointer and stored not only in the message send queue until acked, but also in outgoing packets which are themselves C++ structs. This is a strategy to avoid copying data by passing both messages and packets around by pointer. Somewhere else (ideally on a separate thread) packets and the messages inside them are serialized to a buffer. Eventually, when no references to a message exist in the message send queue (the message is acked) and no packets including that message remain in the packet send queue, the message is destroyed. We also need a way to create messages. I do this with a message factory class with a virtual function overriden to create a message by type. It&rsquo;s good if the packet factory also knows the total number of message types, so we can serialize a message type over the network with tight bounds and discard malicious packets with message type values outside of the valid range: enum TestMessageTypes { TEST_MESSAGE_A, TEST_MESSAGE_B, TEST_MESSAGE_C, TEST_MESSAGE_NUM_TYPES }; // message definitions omitted class TestMessageFactory : public MessageFactory{public: Message * Create( int type ) { switch ( type ) { case TEST_MESSAGE_A: return new TestMessageA(); case TEST_MESSAGE_B: return new TestMessageB(); case TEST_MESSAGE_C: return new TestMessageC(); } } virtual int GetNumTypes() const { return TEST_MESSAGE_NUM_TYPES; }}; Again, this is boilerplate and is usually wrapped by macros, but underneath this is what&rsquo;s going on. Reliable Ordered Message Algorithm The algorithm for sending reliable-ordered messages is as follows: On message send: Measure how many bits the message serializes to using the measure stream Insert the message pointer and the # of bits it serializes to into a sequence buffer indexed by message id. Set the time that message has last been sent to -1 Increment the send message id On packet send: Walk across the set of messages in the send message sequence buffer between the oldest unacked message id and the most recent inserted message id from left -&gt; right (increasing message id order). Never send a message id that the receiver can&rsquo;t buffer or you&rsquo;ll break message acks (since that message won&rsquo;t be buffered, but the packet containing it will be acked, the sender thinks the message has been received, and will not resend it). This means you must never send a message id equal to or more recent than the oldest unacked message id plus the size of the message receive buffer. For any message that hasn&rsquo;t been sent in the last 0.1 seconds and fits in the available space we have left in the packet, add it to the list of messages to send. Messages on the left (older messages) naturally have priority due to the iteration order. Include the messages in the outgoing packet and add a reference to each message. Make sure the packet destructor decrements the ref count for each message. Store the number of messages in the packet n and the array of message ids included in the packet in a sequence buffer indexed by the outgoing packet sequence number so they can be used to map packet level acks to the set of messages included in that packet. Add the packet to the packet send queue. On packet receive: Walk across the set of messages included in the packet and insert them in the receive message sequence buffer indexed by their message id. The ack system automatically acks the packet sequence number we just received. On packet ack: Look up the set of messages ids included in the packet by sequence number. Remove those messages from the message send queue if they exist and decrease their ref count. Update the last unacked message id by walking forward from the previous unacked message id in the send message sequence buffer until a valid message entry is found, or you reach the current send message id. Whichever comes first. On message receive: Check the receive message sequence buffer to see if a message exists for the current receive message id. If the message exists, remove it from the receive message sequence buffer, increment the receive message id and return a pointer to the message. Otherwise, no message is available to receive. Return NULL. In short, messages keep getting included in packets until a packet containing that message is acked. We use a data structure on the sender side to map packet sequence numbers to the set of message ids to ack. Messages are removed from the send queue when they are acked. On the receive side, messages arriving out of order are stored in a sequence buffer indexed by message id, which lets us receive them in the order they were sent. The End Result This provides the user with an interface that looks something like this on send: TestMessage * message = (TestMessage*) factory.Create( TEST_MESSAGE ); if ( message ) { message-&gt;a = 1; message-&gt;b = 2; message-&gt;c = 3; connection.SendMessage( message ); } And on the receive side: while ( true ) { Message * message = connection.ReceiveMessage(); if ( !message ) break; if ( message-&amp;gt;GetType() == TEST_MESSAGE ) { TestMessage * testMessage = (TestMessage*) message; // process test message } factory.Release( message );} Which is flexible enough to implement whatever you like on top of it. 译文 译文出处 译者：翁僖骏（∈星际长途←） 审校：侯鹏（叶落&amp;无痕） 嗨，我是格伦费德勒，欢迎来到创建一个游戏网络协议第五篇文章。 从上一篇文章到现在已经有很长一段时间了，上次我已经率先而且实现了余下的这一系列文章所需的源码并创建了开源库libyojimbo，是本系列文章所要描述的网络协议的一个质量有保证的的和经过单元测试的版本。 如果你想要有一个开源库来为自己在UDP之上实现可靠消息或是为了其他更多，看看libyojimbo。但是，如果你像我这样是想理解它具体是怎么工作的并且可能自己去实现它，阅读下去，因为我们将要从头到脚地去建立一个在UDP之上用来发送可靠有序消息的完整的系统！ 说明 很多人也许会跟你说，要在UDP之上实现你自己的可靠消息系统是愚蠢的。为什么要撰写你特有的简化版本的TCP？这些人深信，任何可靠性的实现不可避免地 最终会成为一个（简化的）TCP的重实现。 但也有很多不同的方法来在UDP之上实现可靠消息，各有不同的优势和劣势。TCP的方法并不是唯一的选择。事实上，我所了解到的大多数可靠有序信息的选择的原理和TCP并不相同。所以让我们为我们的目标发挥创造力并弄懂我们该如何充分利用我们的现状来实现一个比TCP更好 的可靠性系统。 网络协议在动作游戏类型（FPS）中的典型特征就是一个持续发送的数据包，在两个方向上以稳定的速度如20或30包每秒发送。这些数据包都包含有不可靠的无序数据例如t时间内的世界状态；所以，当一个数据包丢失，重新发送它并不是特别有用。当重新发送的数据包到达时，时间t已经过去了。 所以这就是我们将要实现可靠性的现状。对于我们90%的数据包，仅丢弃并不再重新发送它会更好。对于10%或更少（误差允许范围内）的情况，我们确实需要可靠性，但这样的数据是非常罕见的，很少被发送而且比不可靠的数据的平均大小要小得多。这个使用案例适用于所有过去十五年来发布的AAA级的FPS游戏。 不同的方法 可靠性的一个常用的方法是使用两种包类型：可靠有序的和不可靠的。你在众多网络库中都会看到这个方法。它基本的想法是，这个库不断重新发送可靠的数据包直到它的另一方接收到为止。这是一个最终看起来会有一点像TCP方式传输的可靠包的选择。这并没有很糟糕，但你也可以做得更好。 我更愿意去考虑的方法就是消息其实是更小的位包装元素，它知道如何使它们自己序列化。这就显得非常有意义了，因为按位打包的消息中，用于描述下个字节的前缀或者后缀的字节开销在大部分的情况下是不必需的（例如每个包中包含的许多小的消息）。被发送的消息会被放在一个队列并且每次一个包被发送时，发送队列中的一些消息就会被包含在外发的包中。这样一来，就没有可靠的数据包需要被重新发送了。可靠消息也只会包含在数据包里直到它们被接收。 要做到这样最简单的方法就是，把所有未应答的消息包含到每个被发送的包中。它是这样的：每个被发送的消息都有一个随每当一个消息被发送时递增的id。每个输出数据包包含起始消息id ，紧跟着的是n 个消息的数据。接收方不断发回最新收到消息的id给发送方作为一个应答信号，并且消息要当且仅当比最新的应答消息id要更新，才会被包含在数据包中。 这很简单也易于实现，但当你正在发送消息时如果突发一个很大的包丢失情况，你会遇到一个数据包大小的峰值，因为有很多未应答的消息。。正如在数据包分割和重组中讨论的需要按照MTU分割包的方式来发送大的数据包会增加丢包的情况。在高丢包率下你最不想做的就是增大包的规格并引起更多的包的丢失。这是一个潜在的无底洞。 你可以通过扩展系统来给每个包的消息数量n设置一个上限，来避免这种情况。但现在如果你有一个高数据包发送率（如每秒60包）你就要多次发送同样的消息直到你得到该消息的应答信号。如果的往返时间是100ms，每条消息在被应答之前将要平均被多余发送六次。也许你真的需要这些多余的发送数量因为你的消息是对时间极其敏感的，但在大多数情况下，你应该给队列里的其他消息分配合理的带宽。 我比较喜欢的方法是用一个优先次序系统整合每包的应答信号，这个系统检出n条最重要的消息并包含在每个包中。在散布的消息穿过所有在发送队列中的消息发送时，这样就把对时间敏感的递送与每包仅发送n条消息的能力联合起来了。 数据包层级应答 让我们行动起来实现它。 这种可靠性系统的基础是每个包的应答。但为什么应答是在数据包层级而不是在消息层级呢？简要截说原因就是包的数量会远远少于消息的数量。假设每个包中有32或64条消息，显然让一个包含32或64条消息的包来应答会比让每个消息都分别应答要高效得多。 这样同样也增加了灵活性，因为你可以在数据包层级应答上构建其他可靠性系统，不仅仅是为了可靠有序的消息。例如，使用了数据包层级应答，你就知道哪一个时间先决的不可靠状态更新已结束，所以你可以轻易地构建一个系统，在一旦一个数据包所包含的最后一个状态更新已经应答时，停止发送不再改变的对象状态。 为实现数据包层级的应答，在每个包的前面添加如下的报头：?123456struct Header{ uint16_t sequence; uint16_t ack; uint32_t ack_bits;};这些报头元素组合起来以创建应答系统： sequence 是一个数字，随每个数据包发送而增长（并且在达到65535后回往复）。 ack 是从另一方接收到的最新的数据包序列号。 ack_bits 是一个位字段，它编码与ack相关的收到的数据包组合：如果位n已经设置，即 ack– n 数据包被接收了。 ack_bits 不仅是一个节省带宽的巧妙的编码，它同样也增加了信息冗余来抵御包的丢失。每个应答码要被发送32次。如果有一个包丢失了，仍然有其他31个包有着相同的应答码。从统计上来说，应答码还是非常有可能送达的。 但突发的传送数据包的丢失还是有可能发生的，所以重要的是要注意： 如果你收到一个数据包n的应答码，那么这个包肯定已经收到了。 如果你没有收到应答码，那么这个包就很有可能 没有被收到。但是…它也许会是，仅是应答码没有送达。这种情况是极其罕见的。 以我的经验，没有必要设计完善的应答机制。在一个极少丢应答码的系统上构建一个可靠性系统并不会增加什么大问题。但对于在所有情况下来测试这个系统的工作将会成为很大的挑战，因为还要考虑应答码丢失的边界情况。 所以如果你自己实现这个系统的话，请设置一个浸泡测试来覆盖糟糕的网络情况，用来确保你的应答系统是在正确的工作，相关地，你的消息系统的执行实际上是在这些网络情况下可靠地而且有序的交付可靠有序消息。以我之见（并且我已经写了许多这样的系统的变式至少有十次了），这是确保正确行为的一个必要步骤。 你在这篇文章的示例源代码中会找到这样一个浸泡测试，它对patreon支持是有效的，并且也在开源网络库libyojimbo中。 序列缓冲区 为实现这个应答系统，我们在发送方还需要一个数据结构来追踪一个数据包是否已经被应答，这样我们就可以忽略冗余的应答（每个包会通过 ack_bits多次应答）。我们同样在接收方也还需要一个数据结构来追踪那些已经收到的包，这样我们就可以在数据包的报头填写ack_bits的值。 这个数据结构应该具有以下属性： 常量时间内插入（插入可能会是随机的，例如乱序数据包…） 给定的数据包的序列号在常量时间内查询一个条目是否存在 对给定的数据包序列号，在常量时间内访问数据存储 常量时间内删除条目 你可能会想。哦，当然，哈希表。但还有一个更简单的方法：?12345678910111213141516171819const int BufferSize = 1024; uint32_t sequence_buffer[BufferSize]; struct PacketData{ bool acked;}; PacketData packet_data[BufferSize]; `PacketData * GetPacketData( uint16_t sequence )`{ const int index = sequence % BufferSize; if ( sequence_buffer[index] == sequence ) return &amp;packet_data[index]; else return NULL;}你在这可以看到的窍门是这个滚动的缓冲区是以序列号来作为索引的： const int index =sequence % BufferSize; 这是可行的，因为我们并不在意旧条目破坏。随着序列号的递增，旧的条目也自然而然地随着我们插入了新条目而被重写。sequence_buffer[index]的值是用来测试该索引的条目是否实际上与你所搜寻的序列号相符。一个缓冲序列的值是0xFFFFFFFF 就表示一个空的条目并自然地对任何序列号查询返回NULL，没有任何其他（代码）分支。 当条目被顺序添加，就像一个被发送的队列，对插入所需要做的就是把这个序列缓冲区的值更新为新的序列号并且在该索引处重写这个数据：?123456PacketData &amp; InsertPacketData( uint16_t sequence ){ const int index = sequence % BufferSize; sequence_buffer[index] = sequence; return packet_data[index];}但在接收端数据包以乱序到达并且有一部分丢失。在高得离谱的丢包率下（99%），我就会看到旧的序列缓冲区条目还存在，但是新条目的序列号已经超过了65535并且循环到达了旧条目之前，并且打破了我的应答逻辑（导致错误应答并打破了可靠性，这时发送方会真的认为对方已经接收到了一些东西但其实并不是…） 解决这个问题的办法是遍历上一个最高的插入序列与最新收到的插入序列之间的条目（如果它是更加新的话）并在缓冲区清除这些条目即都置为0xFFFFFFFF。现在，在一般情况下，插入操作是非常接近 时间常量的，但最糟的情况是，在先前最高的序列号和当前插入的序列号之间线性遍历的次数n等于缓冲区的长度。 在我们继续之前，我想指出，你可以用这个数据结构做更多事情而不仅是对于应答码。例如，你可以加入发送时间，来扩展每个包的数据：?12345struct PacketData{ bool acked; double send_time;};有了这些信息你可以对往返时间通过做指数级的平滑取平均数做修正，最终得到合理的预期往返时间。你甚至可以看到在发送数据包的序列缓存区的数据包会比你RTT预计的（你现在应该已经收到了它们的应答码…）要旧，通过这个往返时间对还没有应答的包做判断，来决定创建你的数据包丢失预计。 应答算法 现在我们来把注意力集中在数据包层级应答的实际算法上。该算法如下：在数据包发送端： 在数据包发送缓冲区插入一个为当前发送的数据包序列号的条目，并且带着表示它还没有被应答的字段 从本地接收到的数据包序列缓存和最新接收到的数据包序列号中生成 ack 和ack_bits 填写数据包报头的sequence, ack 和 ack_bits 值 发送数据包并递增发送数据包的序列号 在数据包接收端： 从数据包报头读取 sequence 如果 sequence 比之前的最新收到的数据包序列号要新，就更新最新的接收到的数据包序列号 在接收数据包序列缓冲区中为这个数据包插入一个条目 从数据包报头中的ack和ack_bits解码应答的数据包序列号组合 迭代应答的数据包序列号以及任何还没有被应答的数据包调用 OnPacketAcked( uint16_t sequence ) 在数据包发送缓冲区把这个数据包设置为‘已应答的’。 重要的一点是这个算法是在两端都可以执行的，所以如果你有一个客户端和一个服务端，然后每一方的连接运行着同样的逻辑，维护自己的序列号发送的数据包，跟踪最新从另一方收到的数据包序列#还有从一个序列缓冲区里接收到的数据包中生成sequence, ack 和ack_bits 来发送到另一方。 并且这真的就是和它有关的全部了。现在当一个数据包被另一方接收到时，你有一个回调：OnPacketAcked 。这个可靠性系统的关键就在于你得知道哪个数据包被接收，你可以在你想的媒介之上创建任何 可靠性系统。它不仅限于可靠有序的消息。例如，你可以用它确认哪个不可靠的状态更新已经完成了，用以实现基于每个物体的增量编码。 消息对象 消息是小型的对象（比数据包大小要小，所以很多消息装配在一个典型的数据包中）并且知道如何将它们自己序列化。在我的系统里，它们使用一个统一的序列化函数来执行序列化。 这个序列化的函数是模板化的，所以你只要写它一次它就会处理读、写以及测量 。 是的。测量。我喜欢的一个技巧就是有一个虚拟流类叫做MeasureStream，如果你调用了序列化函数，它不参与任何的序列化，而只是测量可能被写入的比特数。这对于解决哪个消息要装载到你的数据包里，特别是当消息可以有任意复杂的序列化函数的情况时是特别有用的。?12345678910111213141516171819202122232425262728293031323334struct TestMessage : public Message{ uint32_t a,b,c; TestMessage() { a = 0; b = 0; c = 0; } template &lt;typename stream&gt; bool Serialize( Stream &amp; stream ) { serialize_bits( stream, a, 32 ); serialize_bits( stream, b, 32 ); serialize_bits( stream, c, 32 ); return true; } virtual SerializeInternal( WriteStream &amp; stream ) { return Serialize( stream ); } virtual SerializeInternal( ReadStream &amp; stream ) { return Serialize( stream ); } virtual SerializeInternal( MeasureStream &amp; stream ) { return Serialize( stream ); }};这里的技巧是桥接统一模板的序列化函数（所以你只需要写一次）与虚拟序列化方法，这通过从虚函数每个流类型中调入它。我通常用一个宏来打包这个引用，但它在上文的代码中这个宏已经被展开，所以你可以看到发生了什么。 现在，假设你有一个基于消息的指针可以让你做到这样并且它只是通过重载来工作：?12Message * message = CreateSomeMessage();message-&gt;SerializeInternal( stream );另外一个就是如果你在编译时间知道了消息的完整组合，就可以为每个类型在被调入序列化函数之前实现一个关于消息类型转换为确切消息类型的大的switch语句。我在过去已经在控制台平台实现的这个消息系统这么做了（如PS3 SPUs），但对于现在（2016）的应用程序，虚函数的总开销是忽略不计的。 消息从一个基类派生，这个基类提供一个通用的接口例如序列化、消息的查询类型还有引用计数。引用计数是必要的，因为消息是通过指针传递的并且在应答之前不只是存储在消息发送队列，而且也存储在外发的数据包中，包本身是C++结构体。 这是一个策略，就是避免通过指针传递消息和数据包来复制数据。别的一些场景（理想的情况是在一个单独的线程）它们里面的数据包和消息会序列化到一个缓冲区。最终，当不再有对存在消息发送队列的消息的引用时（消息已经被应答）并且没有数据包包含保留在数据包发送队列里的消息，消息即是被销毁的。 我们也需要一种方式来创建消息。我用一个消息的工厂类来做这件事情，它有一个被复写的虚函数来根据类型创建一个消息。如果这个数据包工厂还知道消息类型的总数量就好了，那样我们就可以在网络上序列化一个消息类型，因为有严格的界限和在有效范围之外的消息类型值的包的恶意丢弃：?1234567891011121314151617181920212223242526272829enum TestMessageTypes{ TEST_MESSAGE_A, TEST_MESSAGE_B, TEST_MESSAGE_C, TEST_MESSAGE_NUM_TYPES}; // message definitions omitted class TestMessageFactory : public MessageFactory{ public: Message * Create( int type ) { switch ( type ) { case TEST_MESSAGE_A: return new TestMessageA(); case TEST_MESSAGE_B: return new TestMessageB(); case TEST_MESSAGE_C: return new TestMessageC(); } } virtual int GetNumTypes() const { return TEST_MESSAGE_NUM_TYPES; }};再次重申，这是一个引用并且通常是被包裹在宏里面的，但下面要说明的就是它具体是怎么回事了。 可靠的有序消息算法 现在让我们来着手于如何在应答系统中实现可靠有序消息的细节。 发送可靠有序消息的算法如下： 对于消息发送： 使用测量流测量消息序列化后的大小 插入消息指针和它序列化的位数到一个序列缓冲区，它以消息id为索引。设置消息最后被发送的时间为-1 递增发送的消息的id 对于数据包发送： 从左-&gt;右（递增的消息id顺序）遍历在最早的未应答消息id和最新插入的消息id之间的发送消息序列缓冲区的这组消息。 超级重要的： 不要发送一个接收方不能缓冲的消息id，不然你会破坏消息的应答（由于这个消息不能被缓冲，但包含它的数据包会被应答，发送方就会认为这个消息已经被接收了，就不再重新发送它了）。这意味着你必须不能发送一个消息id等于或比最早的未应答消息的id加上消息接收缓冲区大小要新。 对于那些在最后0.1秒没有被发送的消息并且适合我们留在数据包的有效空间，就把它追加到消息列表去发送。根据迭代顺序得到优先级。 包括在外发数据包中的消息，并且要为每个消息添加一个引用。确保每个数据包的析构函数中减了引用计数。 在数据包n存储消息的数量并且消息的标识数组包含在一个序列缓冲区的数据包中，以外发数据包的序列号为索引。 把数据包添加到数据包发送队列。 对于数据包接收： 遍历包含在数据包中的消息组合并且把它们插入到消息接收队列缓冲区，以它们的消息id为索引。 前面的应答系统自动地应答我们刚刚收到的数据包序列号。 对于数据包应答： 用序列号查找包含在数据包中消息组合的标识部分。 从消息发送队列中移除那些已经存在的消息，并减少它们的引用计数。 通过从发送消息队列缓冲区中之前未应答消息的id的转寄来更新最后一个未应答的消息的id，直到发现一个有效的消息条目，或者你会到达当前发送消息的id。以先到者为准。 对于消息接收： 检查接受消息缓冲区确保当前收到消息的id对应的消息是否存在。 如果消息存在，将它从消息队列缓冲区中移除，递增接收消息的id并给这个消息返回一个指针。 如果否，就是没有有效的消息可接收。返回NULL。 总之，消息要保持被包含在数据包中直到这个数据包包含的消息得到应答。我们在发送者方使用一个数据结构来给消息标识的组合映射数据包序列号以便应答。当消息被应答时，要从发送队列中移除。对于接收方，以乱序到达的消息会被存储在一个序列缓冲区，并以消息id为索引，这个id会让我们以它们被发送的顺序接收它们。 最终的结果 在发送方，这为用户提供了一个像这样的接口：?12345678TestMessage * message = (TestMessage*) factory.Create( TEST_MESSAGE );if ( message ){ message-&gt;a = 1; message-&gt;b = 2; message-&gt;c = 3; connection.SendMessage( message );}还有在接收方：?1234567891011121314while ( true ){ Message * message = connection.ReceiveMessage(); if ( !message ) break; if ( message-&gt;GetType() == TEST_MESSAGE ) { TestMessage * testMessage = (TestMessage*) message; // process test message } factory.Release( message );}正如你所看到的，它已经是简单得不能再简单了。 如果这几个接口有引起你的兴趣，请看看我的新开源库 libyojimbo。 我希望你到现在为止对这个系列的文章是享受的请在 patreon上支持我的写作，并且我将更快写新的文章，再者你会在加州大学伯克利分校软件的开源许可证下获得这篇文章的示例源代码。谢谢你的支持！ 即将到来：客户端与服务器的连接 在“创建一个游戏网络协议”的下一篇文章会展示你如何在UDP之上创建你自己的客户端/服务器连接层，它会实现挑战/响应，会在服务器上分配客户端插槽，当服务器爆满或检测超时就拒绝客户端的连接。 【版权声明】 原文作者未做权利声明，视为共享知识产权进入公共领域，自动获得授权；]]></content>
      <categories>
        <category>GS</category>
      </categories>
      <tags>
        <tag>GafferOnGames</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[KBE的UE4的demo大体解读]]></title>
    <url>%2Fblog%2F2017%2F03%2F11%2Fkbe_ue4_demo%2F</url>
    <content type="text"><![CDATA[写到一半发现论坛的热门帖子里官方写了个u3d的demo源码解析, 内容几乎重复, u3d跟ue4的demo框架流程几乎都是差不多的, 直接给出官方帖子的链接好了, 尴尬:http://bbs.kbengine.org/forum.php?mod=viewthread&amp;tid=166]]></content>
      <categories>
        <category>UE4</category>
      </categories>
      <tags>
        <tag>KBE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[构建游戏网络协议四之发送大块数据]]></title>
    <url>%2Fblog%2F2017%2F02%2F28%2Fsending_large_blocks_of_data%2F</url>
    <content type="text"><![CDATA[本篇自我总结有了本系列上篇文章中的分包和重组系统为何还要这个发送大块数据系统?是否是多余的?是雷同的吗?请看总结概要理清思路, 再细看文章. 为什么需要做这个发送大块数据系统第一眼看上去，这种替代性的技术似乎非常类似于数据包的分包和重组，但是它的实现是完全不同的。这种实现上的差异的目的是为了解决数据包分包和重组的一个关键弱点 : 一个片段的丢失就会导致整个数据包都要被丢弃掉然后重新分包重发。 你可能需要这样做的一些常见的例子包括：客户端在首次加入的时候，服务器需要下发一个大的数据块给客户端(可能是世界的初始状态)、一开始用来做增量编码的基线或者是在一个多人在线网络游戏里面客户端在加载界面所等待的大块数据。 在这些情况下非常重要的是不仅要优雅地处理数据包的丢失，还要尽可能的利用可用的带宽并尽可能快的发送大块数据。 这个发送大块数据系统大致可以理解为是一个在原来分包和重组系统的基础上增加了分包确认功能, 也就是说增加了可靠性的部分. 本篇基本术语In this new system blocks of data are called chunks. Chunks are split up into slices. This name change keeps the chunk system terminology (chunks/slices) distinct from packet fragmentation and reassembly (packets/fragments). 块 : 在这个新系统中，大块的数据被称为”块”(chunks) 片段 : 而块被分成的分包被称为”片段”(slices) 数据包的结构设计这个系统在网络上发送的数据包类型一共有两种类型： Slice packet片段数据包 : 这包括了一个块的片段，最多大小为1k。 1234567891011121314151617181920212223242526272829const int SliceSize = 1024;const int MaxSlicesPerChunk = 256;const int MaxChunkSize = SliceSize MaxSlicesPerChunk; struct SlicePacket : public protocol2::Packet&#123; uint16_t chunkId; int sliceId; int numSlices; int sliceBytes; uint8_t data[SliceSize]; template &lt;typename stream&gt; bool Serialize( Stream &amp; stream ) &#123; serialize_bits( stream, chunkId, 16 ); serialize_int( stream, sliceId, 0, MaxSlicesPerChunk - 1 ); serialize_int( stream, numSlices, 1, MaxSlicesPerChunk ); if ( sliceId == numSlices - 1 ) &#123; serialize_int( stream, sliceBytes, 1, SliceSize ); &#125; else if ( Stream::IsReading ) &#123; sliceBytes = SliceSize; &#125; serialize_bytes( stream, data, sliceBytes ); return true; &#125;&#125;; Ack packet确认数据包 : 一个位域bitfield指示哪些片段已经收到, we just send the entire state of all acked slices in each ack packet. When the ack packet is received (including the slice that was just received). 1234567891011121314struct AckPacket : public protocol2::Packet &#123; uint16_t chunkId; int numSlices; bool acked[MaxSlicesPerChunk]; bool Serialize( Stream &amp; stream ) &#123; serialize_bits( stream, chunkId, 16 ); serialize_int( stream, numSlices, 1, MaxSlicesPerChunk ); for ( int i = 0; i &lt; numSlices; ++i ) serialize_bool( stream, acked[i] ); return true; &#125; &#125;; &#125;&#125;; 发送方的实现与之前文章介绍的数据包的分包和重组系统不同，块系统在同一时间只能由一个块正在传输。发送方的策略是： 持续的发送片段数据包，直到所有的片段数据包都被确认。 不再对已经确认过的片段数据包进行发送。 对于发送方而言有一点比较微妙，实现一个片段数据包重新发送的最小延迟是一个很棒的主意，如果不这么做的话，就可能会出现这种一样情况，对于很小的块数据或者一个块的最后几个片段数据包，很容易不停的发送它们把整个网络都塞满。正是因为这一原因，我们使用了一个数组来记录每个片段数据包的上一次发送时间。重新发送延迟的一个选择是使用一个估计的网络往返时延，或者只有在超过上一次发送时间网络往返时延*1.25还没有收到确认数据包的情况才会重新发送。或者，你可以说“这根本就无所谓”，只要超过上一次发送时间100毫秒了就重新发送。我只是列举适合我自己的方案！ 我们使用以下的数据结构来描述发送方： 123456789101112class ChunkSender&#123; bool sending; uint16_t chunkId; int chunkSize; int numSlices; int numAckedSlices; int currentSliceId; bool acked[MaxSlicesPerChunk]; uint8_t chunkData[MaxChunkSize]; double timeLastSent[MaxSlicesPerChunk];&#125;; 接收方的实现思路首先，接收方的设置会从块0开始。当一个片段数据包从网络上传递过来，并且能够匹配这个块id的话，“receiving”状态会从false翻转为true，第一个片段数据包的数据会插入” chunkData“变量的合适位置，片段数据包的数量会根据第一个片段数据包里面的数据进行正确的设置，已经接收到的片段数据包的数量会加一，也就是从0到1，针对每个片段数据包的接收标记里面对应这个片段数据包的项会变为true。 随着这个块数据的其他片段数据包的到来，会对每一个片段数据包进行检测，判断它们的id是否与当前块的id相同，如果不相同的话就会被丢弃。如果这个片段数据包已经收到过的话，那么这个包也会被丢弃。否则，这个片段数据包的数据会插入” chunkData“变量的合适位置、已经接收到的片段数据包的数量会加一、针对每个片段数据包的接收标记里面对应这个片段数据包的项会变为true。 这一过程会持续进行，直到接收到所有的片段数据包。一旦接收到所有的片段数据包（也就是已经接收到的片段数据包的数量等于片段数据包的数量的时候），接收方会把“receiving “状态改为false，而把”readyToRead“状态改为true。当”readyToRead”状态为true的时候，所有收到的片段数据包都会被丢弃。在这一点上，这个处理过程通常非常的短，会在收到片段数据包的同一帧进行处理，调用者会检查”我有一块数据要读取么？“并处理块数据。然后会重置数据块接收器的所有数据为默认值，除了块数据的id从0增加到1，这样我们就准备好接收下一个块了。 1234567891011class ChunkReceiver&#123; bool receiving; bool readyToRead; uint16_t chunkId; int chunkSize; int numSlices; int numReceivedSlices; bool received[MaxSlicesPerChunk]; uint8_t chunkData[MaxChunkSize];&#125;; 防DDos如果你对每个收到的片段数据包都会回复一个确认数据包的话，那么发送方能够构造一个很小的片段数据包发送给你，而你会回复一个比发送给你的片段数据包还大的确认数据包，这样你的服务器就变成了一个可以被人利用来进行DDos放大攻击的工具。 永远不要设计一个包含对接收到的数据包进行一对一的映射响应的协议。让我们举个简单例子来说明一下这个问题。如果有人给你发送1000个片段数据包，永远不要给他回复1000个确认数据包。相反只发一个确认数据包，而且最多每50毫秒或者100毫秒才发送一个确认数据包。如果你是这样设计的话，那么DDos攻击完全不可能的。 原文原文出处 原文标题 : Sending Large Blocks of Data (How to send blocks quickly and reliably over UDP) Introduction Hi, I’m Glenn Fiedler and welcome to Building a Game Network Protocol. In the previous article we implemented packet fragmentation and reassembly so we can send packets larger than MTU. This approach works great when the data block you&rsquo;re sending is time critical and can be dropped, but in other cases you need to send large blocks of quickly and reliably over packet loss, and you need the data to get through. In this situation, a different technique gives much better results. Background It&rsquo;s common for servers to send large block of data to the client on connect, for example, the initial state of the game world for late join. Let&rsquo;s assume this data is 256k in size and the client needs to receive it before they can join the game. The client is stuck behind a load screen waiting for the data, so obviously we want it to be transmitted as quickly as possible. If we send the data with the technique from the previous article, we get packet loss amplification because a single dropped fragment results in the whole packet being lost. The effect of this is actually quite severe. Our example block split into 256 fragments and sent over 1% packet loss now has a whopping 92.4% chance of being dropped! Since we just need the data to get across, we have no choice but to keep sending it until it gets through. On average, we have to send the block 10 times before it&rsquo;s received. You may laugh but this actually happened on a AAA game I worked on! To fix this, I implemented a new system for sending large blocks, one that handles packet loss by resends fragments until they are acked. Then I took the problematic large blocks and piped them through this system, fixing a bunch of players stalling out on connect, while continuing to send time critical data (snapshots) via packet fragmentation and reassembly. Chunks and Slices In this new system blocks of data are called chunks. Chunks are split up into slices. This name change keeps the chunk system terminology (chunks/slices) distinct from packet fragmentation and reassembly (packets/fragments). The basic idea is that slices are sent over the network repeatedly until they all get through. Since we are implementing this over UDP, simple in concept becomes a little more complicated in implementation because have to build in our own basic reliability system so the sender knows which slices have been received. This reliability gets quite tricky if we have a bunch of different chunks in flight, so we&rsquo;re going to make a simplifying assumption up front: we&rsquo;re only going to send one chunk over the network at a time. This doesn&rsquo;t mean the sender can&rsquo;t have a local send queue for chunks, just that in terms of network traffic there&rsquo;s only ever one chunk in flight at any time. This makes intuitive sense because the whole point of the chunk system is to send chunks reliably and in-order. If you are for some reason sending chunk 0 and chunk 1 at the same time, what&rsquo;s the point? You can&rsquo;t process chunk 1 until chunk 0 comes through, because otherwise it wouldn&rsquo;t be reliable-ordered. That said, if you dig a bit deeper you&rsquo;ll see that sending one chunk at a time does introduce a small trade-off, and that is that it adds a delay of RTT between chunk n being received and the send starting for chunk n+1 from the receiver&rsquo;s point of view. This trade-off is totally acceptable for the occasional sending of large chunks like data sent once on client connect, but it&rsquo;s definitely not acceptable for data sent 10 or 20 times per-second like snapshots. So remember, this system is useful for large, infrequently sent blocks of data, not for time critical data. Packet Structure There are two sides to the chunk system, the sender and the receiver. The sender is the side that queues up the chunk and sends slices over the network. The receiver is what reads those slice packets and reassembles the chunk on the other side. The receiver is also responsible for communicating back to the sender which slices have been received via acks. The netcode I work on is usually client/server, and in this case I usually want to be able to send blocks of data from the server to the client and from the client to the server. In that case, there are two senders and two receivers, a sender on the client corresponding to a receiver on the server and vice-versa. Think of the sender and receiver as end points for this chunk transmission protocol that define the direction of flow. If you want to send chunks in a different direction, or even extend the chunk sender to support peer-to-peer, just add sender and receiver end points for each direction you need to send chunks. Traffic over the network for this system is sent via two packet types: Slice packet - contains a slice of a chunk up to 1k in size. Ack packet - a bitfield indicating which slices have been received so far. The slice packet is sent from the sender to the receiver. It is the payload packet that gets the chunk data across the network and is designed so each packet fits neatly under a conservative MTU of 1200 bytes. Each slice is a maximum of 1k and there is a maximum of 256 slices per-chunk, therefore the largest data you can send over the network with this system is 256k. const int SliceSize = 1024; const int MaxSlicesPerChunk = 256; const int MaxChunkSize = SliceSize * MaxSlicesPerChunk; struct SlicePacket : public protocol2::Packet{ uint16_t chunkId; int sliceId; int numSlices; int sliceBytes; uint8_t data[SliceSize]; template &amp;amp;lt;typename Stream&amp;amp;gt; bool Serialize( Stream &amp;amp;amp; stream ) { serialize_bits( stream, chunkId, 16 ); serialize_int( stream, sliceId, 0, MaxSlicesPerChunk - 1 ); serialize_int( stream, numSlices, 1, MaxSlicesPerChunk ); if ( sliceId == numSlices - 1 ) { serialize_int( stream, sliceBytes, 1, SliceSize ); } else if ( Stream::IsReading ) { sliceBytes = SliceSize; } serialize_bytes( stream, data, sliceBytes ); return true; }}; There are two points I&rsquo;d like to make about the slice packet. The first is that even though there is only ever one chunk in flight over the network, it&rsquo;s still necessary to include the chunk id (0,1,2,3, etc&hellip;) because packets sent over UDP can be received out of order. Second point. Due to the way chunks are sliced up we know that all slices except the last one must be SliceSize (1024 bytes). We take advantage of this to save a small bit of bandwidth sending the slice size only in the last slice, but there is a trade-off: the receiver doesn&rsquo;t know the exact size of a chunk until it receives the last slice. The other packet sent by this system is the ack packet. This packet is sent in the opposite direction, from the receiver back to the sender. This is the reliability part of the chunk network protocol. Its purpose is to lets the sender know which slices have been received. struct AckPacket : public protocol2::Packet { uint16_t chunkId; int numSlices; bool acked[MaxSlicesPerChunk]; bool Serialize( Stream &amp;amp;amp; stream ) { serialize_bits( stream, chunkId, 16 ); serialize_int( stream, numSlices, 1, MaxSlicesPerChunk ); for ( int i = 0; i &amp;amp;lt; numSlices; ++i ) { serialize_bool( stream, acked[i] ); return true; } }; } }}; Acks are short for &lsquo;acknowledgments&rsquo;. So an ack for slice 100 means the receiver is acknowledging that it has received slice 100. This is critical information for the sender because not only does it let the sender determine when all slices have been received so it knows when to stop, it also allows the sender to use bandwidth more efficiently by only sending slices that haven&rsquo;t been acked. Looking a bit deeper into the ack packet, at first glance it seems a bit redundant. Why are we sending acks for all slices in every packet? Well, ack packets are sent over UDP so there is no guarantee that all ack packets are going to get through. You certainly don&rsquo;t want a desync between the sender and the receiver regarding which slices are acked. So we need some reliability for acks, but we don&rsquo;t want to implement an ack system for acks because that would be a huge pain in the ass. Since the worst case ack bitfield is just 256 bits or 32 bytes, we just send the entire state of all acked slices in each ack packet. When the ack packet is received, we consider a slice to be acked the instant an ack packet comes in with that slice marked as acked and locally that slice is not seen as acked yet. This last step, biasing in the direction of non-acked to ack, like a fuse getting blown, means we can handle out of order delivery of ack packets. Sender Implementation Let&rsquo;s get started with the implementation of the sender. The strategy for the sender is: Keep sending slices until all slices are acked Don&rsquo;t resend slices that have already been acked We use the following data structure for the sender: class ChunkSender { bool sending; uint16_t chunkId; int chunkSize; int numSlices; int numAckedSlices; int currentSliceId; bool acked[MaxSlicesPerChunk]; uint8_t chunkData[MaxChunkSize]; double timeLastSent[MaxSlicesPerChunk]; }; As mentioned before, only one chunk is sent at a time, so there is a &lsquo;sending&rsquo; state which is true if we are currently sending a chunk, false if we are in an idle state ready for the user to send a chunk. In this implementation, you can&rsquo;t send another chunk while the current chunk is still being sent over the network. If you don&rsquo;t like this, stick a queue in front of the sender. Next, we have the id of the chunk we are currently sending, or, if we are not sending a chunk, the id of the next chunk to be sent, followed by the size of the chunk and the number of slices it has been split into. We also track, per-slice, whether that slice has been acked, which lets us count the number of slices that have been acked so far while ignoring redundant acks. A chunk is considered fully received from the sender&rsquo;s point of view when numAckedSlices == numSlices. We also keep track of the current slice id for the algorithm that determines which slices to send, which works like this. At the start of a chunk send, start at slice id 0 and work from left to right and wrap back around to 0 again when you go past the last slice. Eventually, you stop iterating across because you&rsquo;ve run out of bandwidth to send slices. At this point, remember our current slice index via current slice id so you can pick up from where you left off next time. This last part is important because it distributes sends across all slices, not just the first few. Now let&rsquo;s discuss bandwidth limiting. Obviously you don&rsquo;t just blast slices out continuously as you&rsquo;d flood the connection in no time, so how do we limit the sender bandwidth? My implementation works something like this: as you walk across slices and consider each slice you want to send, estimate roughly how many bytes the slice packet will take eg: roughly slice bytes + some overhead for your protocol and UDP/IP header. Then compare the amount of bytes required vs. the available bytes you have to send in your bandwidth budget. If you don&rsquo;t have enough bytes accumulated, stop. Otherwise, subtract the bytes required to send the slice and repeat the process for the next slice. Where does the available bytes in the send budget come from? Each frame before you update the chunk sender, take your target bandwidth (eg. 256kbps), convert it to bytes per-second, and add it multiplied by delta time (dt) to an accumulator. A conservative send rate of 256kbps means you can send 32000 bytes per-second, so add 32000 * dt to the accumulator. A middle ground of 512kbit/sec is 64000 bytes per-second. A more aggressive 1mbit is 125000 bytes per-second. This way each update you accumulate a number of bytes you are allowed to send, and when you&rsquo;ve sent all the slices you can given that budget, any bytes left over stick around for the next time you try to send a slice. One subtle point with the chunk sender and is that it&rsquo;s a good idea to implement some minimum resend delay per-slice, otherwise you get situations where for small chunks, or the last few slices of a chunk that the same few slices get spammed over the network. For this reason we maintain an array of last send time per-slice. One option for this resend delay is to maintain an estimate of RTT and to only resend a slice if it hasn&rsquo;t been acked within RTT * 1.25 of its last send time. Or, you could just resend the slice it if it hasn&rsquo;t been sent in the last 100ms. Works for me! Kicking it up a notch Do the math you&rsquo;ll notice it still takes a long time for a 256k chunk to get across: 1mbps = 2 seconds 512kbps = 4 seconds 256kbps = 8 seconds :( Which kinda sucks. The whole point here is quickly and reliably. Emphasis on quickly. Wouldn&rsquo;t it be nice to be able to get the chunk across faster? The typical use case of the chunk system supports this. For example, a large block of data sent down to the client immediately on connect or a block of data that has to get through before the client exits a load screen and starts to play. You want this to be over as quickly as possible and in both cases the user really doesn&rsquo;t have anything better to do with their bandwidth, so why not use as much of it as possible? One thing I&rsquo;ve tried in the past with excellent results is an initial burst. Assuming your chunk size isn&rsquo;t so large, and your chunk sends are infrequent, I can see no reason why you can&rsquo;t just fire across the entire chunk, all slices of it, in separate packets in one glorious burst of bandwidth, wait 100ms, and then resume the regular bandwidth limited slice sending strategy. Why does this work? In the case where the user has a good internet connection (some multiple of 10mbps or greater&hellip;), the slices get through very quickly indeed. In the situation where the connection is not so great, the burst gets buffered up and most slices will be delivered as quickly as possible limited only by the amount bandwidth available. After this point switching to the regular strategy at a lower rate picks up any slices that didn&rsquo;t get through the first time. This seems a bit risky so let me explain. In the case where the user can&rsquo;t quite support this bandwidth what you&rsquo;re relying on here is that routers on the Internet strongly prefer to buffer packets rather than discard them at almost any cost. It&rsquo;s a TCP thing. Normally, I hate this because it induces latency in packet delivery and messes up your game packets which you want delivered as quickly as possible, but in this case it&rsquo;s good behavior because the player really has nothing else to do but wait for your chunk to get through. Just don&rsquo;t go too overboard with the spam or the congestion will persist after your chunk send completes and it will affect your game for the first few seconds. Also, make sure you increase the size of your OS socket buffers on both ends so they are larger than your maximum chunk size (I recommend at least double), otherwise you&rsquo;ll be dropping slices packets before they even hit the wire. Finally, I want to be a responsible network citizen here so although I recommend sending all slices once in an initial burst, it&rsquo;s important for me to mention that I think this really is only appropriate, and only really borderline appropriate behavior for small chunks in the few 100s of k range in 2016, and only when your game isn&rsquo;t sending anything else that is time-critical. Please don&rsquo;t use this burst strategy if your chunk is really large, eg: megabytes of data, because that&rsquo;s way too big to be relying on the kindness of strangers, AKA. the buffers in the routers between you and your packet&rsquo;s destination. For this it&rsquo;s necessary to implement something much smarter. Something adaptive that tries to send data as quickly as it can, but backs off when it detects too much latency and/or packet loss as a result of flooding the connection. Such a system is outside of the scope of this article. Receiver Implementation Now that we have the sender all sorted out let&rsquo;s move on to the reciever. As mentioned previously, unlike the packet fragmentation and reassembly system from the previous article, the chunk system only ever has one chunk in flight. This makes the reciever side of the chunk system much simpler: class ChunkReceiver { bool receiving; bool readyToRead; uint16_t chunkId; int chunkSize; int numSlices; int numReceivedSlices; bool received[MaxSlicesPerChunk]; uint8_t chunkData[MaxChunkSize]; }; We have a state whether we are currently &lsquo;receiving&rsquo; a chunk over the network, plus a &rsquo;readyToRead&rsquo; state which indicates that a chunk has received all slices and is ready to be popped off by the user. This is effectively a minimal receive queue of length 1. If you don&rsquo;t like this, of course you are free to add a queue. In this data structure we also keep track of chunk size (although it is not known with complete accuracy until the last slice arrives), num slices and num received slices, as well as a received flag per-slice. This per-slice received flag lets us discard packets containing slices we have already received, and count the number of slices received so far (since we may receive the slice multiple times, we only increase this count the first time we receive a particular slice). It&rsquo;s also used when generating ack packets. The chunk receive is completed from the receiver&rsquo;s point of view when numReceivedSlices == numSlices. So what does it look like end-to-end receiving a chunk? First, the receiver sets up set to start at chunk 0. When the a slice packet comes in over the network matching the chunk id 0, &lsquo;receiving&rsquo; flips from false to true, data for that first slice is inserted into &lsquo;chunkData&rsquo; at the correct position, numSlices is set to the value in that packet, numReceivedSlices is incremented from 0 -&gt; 1, and the received flag in the array entry corresponding to that slice is set to true. As the remaining slice packets for the chunk come in, each of them are checked that they match the current chunk id and numSlices that are being received and are ignored if they don&rsquo;t match. Packets are also ignored if they contain a slice that has already been received. Otherwise, the slice data is copied into the correct place in the chunkData array, numReceivedSlices is incremented and received flag for that slice is set to true. This process continues until all slices of the chunk are received, at which point the receiver sets receiving to &lsquo;false&rsquo; and &lsquo;readyToRead&rsquo; to true. While &lsquo;readyToRead&rsquo; is true, incoming slice packets are discarded. At this point, the chunk receive packet processing is performed, typically on the same frame. The caller checks &lsquo;do I have a chunk to read?&rsquo; and processes the chunk data. All chunk receive data is cleared back to defaults, except chunk id which is incremented from 0 -&gt; 1, and we are ready to receive the next chunk. Conclusion The chunk system is simple in concept, but the implementation is certainly not. I encourage you to take a close look at the source code for this article for further details. 译文 译文出处 翻译：张华栋 (wcby) 审校：王磊(未来的未来) 大家好，我是格伦·菲德勒。欢迎大家阅读系列教程《构建游戏网络协议》的第四篇文章。在之前的文章中，我们讨论了如何在游戏协议这一层实现对数据包的分包和重组。现在在这篇文章里面，我们将继续通过探索在UDP协议上发送大块数据的替代方案来继续我们构建一个专业级别的游戏网络协议的征程。 第一眼看上去，这种替代性的技术似乎非常类似于数据包的分包和重组，但是它的实现是完全不同的。这种实现上的差异的目的是为了解决数据包分包和重组的一个关键弱点-一个片段的丢失就会导致整个数据包都要被丢弃掉。这种行为是非常不好的，因为它会随着分包数量的增加而放大数据包丢失的概率。当你遇到大块数据包的时候，这种放大是如此的明显，加入256 k大小的分包丢失率是1%的话，那么原始数据包就有92.4%的概率被丢弃。平均来说，你需要发送原始数据包10次，它才能顺利的到达网络的另外一端！ 如果你需要在一个可能会有数据包丢失的网络上比如说互联网，快速和可靠地发送大量的数据，很明显，这样的方法是完全不可接受的。你可能需要这样做的一些常见的例子包括：客户端在首次加入的时候，服务器需要下发一个大的数据块给客户端(可能是世界的初始状态)、一开始用来做增量编码的基线或者是在一个多人在线网络游戏里面客户端在加载界面所等待的大块数据。 在这些情况下非常重要的是不仅要优雅地处理数据包的丢失，还要尽可能的利用可用的带宽并尽可能快的发送大块数据。这正是我要在这篇文章里面告诉你该如何做的内容。块和片段 让我们开始使用基本术语。在这个新系统中，大块的数据被称为”块“，而它们被分成的分包被称为”片段”。 这个名字上的改变使的块系统的术语(块和片段)不同于数据包分包和重组的术语(数据包和分包)。这是我认为很重要的一个事情，因为这些系统是在解决不同的问题，没有理由你不能在相同的网络协议中同时这两个系统。事实上，我经常把这两个结合起来，在时间比较关键的增量数据包里面使用数据包的分包和重组，当客户端加入游戏的时候，使用块系统来下发整个游戏世界的初始状态下(非常大的数据包)。 块系统的基本思想，真是一点都不复杂，是把块分成很多片段，然后通过网络多次发送片段，直到他们都顺利的到达网络的另外一端。当然，因为我们正在UDP协议上实现这个功能，同时还有可能数据包会丢失、数据包乱序到达以及数据包重复到达的情况，简单的概念在实现中也会变得非常复杂，因为我们必须在UDP协议上建立我们自己的具有基本可靠性的系统，这样发送方才能知道这个片段已经被网络的另外一端成功收到。 如果我们有一组不同的块正在传输过程中(就像我们在数据包的分包和重组中所做的那样)，那么可靠性的问题就会变得非常棘手，所以我们要做一个简化的假设。我们一次只会通过网络发送一个块的数据。这并不意味着发送者不能在本地有一个块的发送队列，这只是意味着在实际的网络传输中只有一个块的数据会正在传递。 这么做之所以有意义，是因为有了这一点假设以后就能保证块系统能够可靠有序的发送块。如果你因为某些原因在同一时间发送块0和块1，这会发生什么？你不能在块0到来之前处理块1，否则这个传输过程就不是有序可靠了。也就是说，如果你挖得深一些的话，你会发现一次只能发送一个块确实引入了一个小的权衡，它给正在接收的块Ｎ增加了一个网络往返延迟，以及从接收方的角度看块Ｎ＋１的发送开始时间也被延迟了一个网络往返延迟。 这个代价是完全可以接受的，因为发送大块数据是一个非常偶然的事情（举些简单的例子来说，当客户端连接上来的时候会发送大块数据，当新的关卡需要进行加载的时候才会发送大块数据。。。），但是如果1秒钟内10次或者20次发送块数据的话这就是绝对不能被接受的了。所以我希望你能看到这个系统是专为什么目的设计的以及不是为什么目的设计的。 数据包的结构 在块系统中有两方会参与，分别是发送方和接收方。 发送方是负责将块压入队列并通过网络发送片段。接收方是负责在网络的另外一端读取这些片段并进行重组。接收方还负责通过发送“确认”数据包给发送方来与发送方交流表明这个片段已经收到。 我工作过的网络模式通常是客户端与服务端之间的通信，在这种情况下，我通常希望能够从服务器往客户端发送大块数据，以及从客户端到服务器发送大块数据。所以在这种情况下，有两个发送方和两个接收方，一个发送方在客户端对应着在服务器那边有一个接收方，反过来也是如此。可以把发送方和接收方认为是块传输协议的终点，这样也就定义了网络流的方向。如果你想在不同的方向发送块，甚至是扩展块的发送方来支持点对点的发送，只需要在你需要发送块的每个方向添加一个发送方和一个接收方作为终点。 这个系统在网络上发送的数据包类型一共有两种类型： 1）片段数据包-这包括了一个块的片段，最多大小为1k。 2）确认数据包-一个位域指示哪些片段已经收到。 片段数据包是从发送方发送到接收器的。这是通过网络对块数据进行传递的有效载荷数据包，在设计的时候每个片段数据包的大小都贴近一个保守的最大传输单元的大小，也就是 1200字节。每个片段数据包最大是1 k，每个块最多有256个片段数据包，所以通过这个系统可以通过网络发送的最大的数据是256k（如果你愿意的话，你可以增加这个片段的最大数目）。我建议保持片段的大小为1k，这主要是基于最大传输单元方面的考虑。 ?1234567891011121314151617181920212223242526272829const int SliceSize = 1024;const int MaxSlicesPerChunk = 256;const int MaxChunkSize = SliceSize * MaxSlicesPerChunk; struct SlicePacket : public protocol2::Packet{ uint16_t chunkId; int sliceId; int numSlices; int sliceBytes; uint8_t data[SliceSize]; template &lt;typename stream&gt; bool Serialize( Stream &amp; stream ) { serialize_bits( stream, chunkId, 16 ); serialize_int( stream, sliceId, 0, MaxSlicesPerChunk - 1 ); serialize_int( stream, numSlices, 1, MaxSlicesPerChunk ); if ( sliceId == numSlices - 1 ) { serialize_int( stream, sliceBytes, 1, SliceSize ); } else if ( Stream::IsReading ) { sliceBytes = SliceSize; } serialize_bytes( stream, data, sliceBytes ); return true; }};在这里我想对片段数据包进行两点说明。第一点是即使只有一个块在网络上进行传输，仍然是有必要在数据包里面包含一个块的id(比如说，0、1、2、3、等等等)，这是,因为通过UDP协议发送的数据包可以是乱序到达的。通过这种方式的话，如果一个片段数据包到达的时候对应着一个已经接受过的块，举个简单的例子来说明，你正在接受块2的数据，但是块1的一个片段数据包现在到达了，你可以直接拒绝这个数据包，而不是接受它的数据包并把它的数据插入到块2从而把块2的数据给弄混了。 第二点。由于我们知道块分成片段的方法会把所有的片段除了最后一个以外都弄成必须SliceSize的大小(也就是1024字节)。我们利用这一点来节省一点带宽，我们只在最后一个片段里面发送片段的大小，但这是一种权衡:接收方不知道块的确切大小到底是多少字节，直到它接收到最后一个片段才能知道。 可以让这个系统继续往后发送新的数据包的机制是确认数据包。这个数据包是沿着另外一个方向进行发送的，也就是从接收方发回给发送方，这也是块网络协议中负责可靠性的部分。它存在的目的是让发送方知道这个片段已经被发送方收到。?1234567891011121314struct AckPacket : public protocol2::Packet { uint16_t chunkId; int numSlices; bool acked[MaxSlicesPerChunk]; bool Serialize( Stream &amp; stream ) { serialize_bits( stream, chunkId, 16 ); serialize_int( stream, numSlices, 1, MaxSlicesPerChunk ); for ( int i = 0; i &lt; numSlices; ++i ) serialize_bool( stream, acked[i] ); return true; } }; }};ack是“确认”的缩写。所以一个对片段100的确认数据包意味着接收方确认它已经接收到了片段100。这对于发送方来说是一条关键信息，因为它不仅让发送方知道什么时候所有的片段都已经被成功接收，这样发送方就可以停止发送了，它还允许发送方只重发那些还没有被确认的片段，这样就能让发送方更有效率的利用带宽。 让我们对确认数据包再深入一点思考，似乎在一开始看上去对于每个数据包的所有分片都发送确认包似乎有点多余。我们为什么要这么做?是的，这是因为确认数据包是通过UDP协议发送的，所以没有办法保证所有的确认数据包都会成功的到达网络的另外一端，你当然不会希望发送方和接收方之间对于目前确认到那个片段的信息都是不同步的。 所以我们需要一些确认数据包传输的可靠性，但是我们不希望实现一个确认数据包的确认系统，因为这将会是一个非常痛苦和麻烦的过程。因为在最坏的情况下，确认数据包的大小是256位或32字节，最简单的方法是也是最好的。we just send the entire state of all acked slices in each ack packet. When the ack packet is received, we consider a slice to be acked the instant an ack packet comes in with that slice marked as acked and locally that slice is not seen as acked yet. 基本的发送方实现 现在我们已经了解了这个系统背后的基本概念，让我们从发送方的实现开始实现整个系统。 发送方的策略是： 1）持续的发送片段数据包，直到所有的片段数据包都被确认。 2）不再对已经确认过的片段数据包进行发送。 我们使用以下的数据结构来描述发送方：?123456789101112class ChunkSender{ bool sending; uint16_t chunkId; int chunkSize; int numSlices; int numAckedSlices; int currentSliceId; bool acked[MaxSlicesPerChunk]; uint8_t chunkData[MaxChunkSize]; double timeLastSent[MaxSlicesPerChunk];};正如之前提到的那样，一次只会发送一个块的数据，如果我们正在发送一个块的数据的时候，那么关于“发送”的状态是true，假如我们处于闲置状态、正在准备发送一个块的数据的时候，那么关于“发送”的状态是false。在这个实现中，如果当前有一个块的数据仍在通过网络进行发送的话，你不能发送另外一个块的数据。你必须等待当前块的数据发送完毕之后才可以发送另外一个块的数据。如果你不喜欢的话，在块的发送器的前端按照你的意愿可以放置一个发送队列。 接下来，我们有我们正在发送的块数据的id，或者如果我们没有在发送块数据的话，那么我们有要发送的下一个块数据的id、以及这个块所分成的片段数据包的数量。我们也会跟踪每个片段数据包，来记录这个片段数据包是否已经被确认，这可以让我们避免重发那些已经收到的片段数据包，并且我们还会记录迄今为止已经确认收到的片段数据包的数量，这个数量会去掉冗余的确认，也就是每个片段数据包的确认只算一次。从发送方的观点来看，只有当确认的片段数据包的数量等于这个块所分成的片段数据包的数量的时候才会这个块数据已经被完全收到了。 我们还需要为这个算法记录当前发送的片段数据包的id，因为这将决定了哪些片段数据包将被发送。它的工作机制大致是这样：一个块数据开始发送的时候，是从id为0的片段数据包开始发送的，然后依次从左到右开始发送直到经过最后一个片段数据包(也就是id为分包数量的大小-1)的时候会回头从id为0的片段数据包继续发送。最终，你会停止这个迭代因为发送的片段数据包已经耗尽了带宽。在这一点上，我们通过记录当前发送的片段数据包的id就能记住我们当前遍历的片段数据包的索引，这样在下一次开始遍历的时候你就可以继续从这个位置开始发送片段数据包。最后一部分是非常重要的，这是因为它可以把发送一个块数据所有的片段数据包这个事情是分散开，而不是在一起就全部发出去。 现在让我们讨论下带宽限制。显然你不能把所有的片段数据包一次全部发完，因为如果这么做的话，会把整个链接堵住，那么，我们该如何限制发送方所使用的带宽?我的实现机制大概是这样的：当你对全部的片段数据包进行遍历并且考虑你想要发送的每个片段数据包的时候，大概估计下这个片段数据包会需要占据多少字节，比如可以用这种估计算法：大概这个片段的字节数+一些协议的开销和UDP / IP的报头。然后用所需的字节数和你带宽预算里面可用来进行发送的字节数进行比较。如果带宽预算里面没有足够可用的字节数，那么就停止发送。否则的话，从带宽预算里面减去发送这个片段数据包所需的字节数，然后对于下个片段数据包重复整个过程。 带宽预算里面可用的字节发送预算是从哪里计算得来的？在每一帧更新块的发送方之前，把你的目标带宽（比如说每秒256KB）转换成每秒可以发送的字节数，然后用它乘以更新时间来把记过放到一个累加器里面。每秒256KB是一个比较保守的发送速率，这意味你可以每秒发送32000个字节，所以把32000 * dt这个值添加到累加器里面。每秒512KB是一个比较适中的估计，意味你可以每秒发送64000个字节。每秒1MB是一个比较激进的估计，意味你可以每秒发送125000个字节。通过这种方法，在每次更新的时候你就可以累加你被允许发送的字节数了，这样当你可以按照预算来发送最大数量的片段数据包，如果还有数据没有发完的话，会等到下一帧的时候再尝试发送。 对于发送方而言有一点比较微妙，实现一个片段数据包重新发送的最小延迟是一个很棒的主意，如果不这么做的话，就可能会出现这种一样情况，对于很小的块数据或者一个块的最后几个片段数据包，很容易不停的发送它们把整个网络都塞满。正是因为这一原因，我们使用了一个数组来记录每个片段数据包的上一次发送时间。重新发送延迟的一个选择是使用一个估计的网络往返时延，或者只有在超过上一次发送时间网络往返时延*1.25还没有收到确认数据包的情况才会重新发送。或者，你可以说“这根本就无所谓”，只要超过上一次发送时间100毫秒了就重新发送。我只是列举适合我自己的方案！ 把发送方实现的更完美一点 如果你仔细用数学计算一下的话，你会注意到对于一个256K 的数据块而言，它要在网络上发送完毕仍然需要发送很长的时间： 如果网络速率是每秒1M的话，就需要2秒钟的时间。 如果网络速率是每秒512KB的话，就需要4秒钟的时间。 如果网络速率是每秒256KB的话，就需要8秒钟的时间。 这可有点糟糕。我们实现系统的重点是快速和可靠性。再次强调下需要能够快速传递。如果块系统的传输不能做到快速的话，这是不是会不太好？块系统的一些典型用例会支持这一点。举个简单的例子来说明，当客户端第一次连接上服务器的时候，一大块数据需要立刻发送给客户端，或者在客户端退出加载界面开始游戏的时候需要能够大量数据快速下发给客户端。你想要尽快的传递完需要的数据，而且在这两种情况下，用户对于自己的带宽并没有什么太多其他的用途，那么为什么不使用尽可能多的带宽? 在过去我曾经尝试过一个方法，就是在一开始的时候尽量传递，这取得了很好的效果。假设你的块大小并不是那么大，而且你的块发送频率并不那么频繁，我没找到什么理由为什么你不能在一开始就把所有的片段数据包都发送出去，填充满贷款，然后等待100毫秒，在恢复成正常的带宽受限的片段数据包发送策略。 为什么这样会取得良好的效果？如果用户有一个良好的网络连接（可以每秒发送超过10MB的数据甚至更多。。。），事实上，片段数据包在网络上的传输非常的快速。如果是连接的情况并不是那么好的情况下，大部分的片段数据包会得到缓冲，大部分的片段数据包受限于带宽但是会尽可能快的发送出去。处理完这些数据包之后，就会切换到常规的策略，从那些第一次没有发送出去的片段数据包选择合适的进行发送。 这似乎有点冒险，所以让我来解释一下。如果出现大量数据需要传输但是已经超过带宽限制的情况，互联网上的路由器会倾向于缓冲这些数据包，而不是不惜代价的抛弃它们。这就是TCP协议会做的事情。通常情况下，我讨厌这个机制因为它会诱发延迟而且会弄乱那些你想要尽快交付的游戏数据包，但在这种情况下它是一个非常好的行为，这是因为玩家真的没有其他事情可以做，智能等待你的块数据赶紧传输完毕。只是在你的块数据传输完毕以后，会有一些垃圾数据或者交通拥堵，它会影响你的游戏开始的几秒钟。另外，请确保你增加了网络两端的加操作系统的套接字缓冲区的大小，这样它们才可以比你最大的块数据的大小要大(我建议至少增加一倍)，否则在超过网络带宽的限制之前你就会出现丢弃段数据包的情况。 最后，我想成为一个负责任的网络公民，虽然在这里我推荐在最开始连接的时候一次发送所有的片段数据包，所以对我来说介绍下我认为这真的是适当的是非常非常重要的，在2016年的网络环境下，发送几百个KB量级的数据包是没什么大不了的行为，而且只会发生在没有其他关键数据同时发送的情况下。让我们举个简单的例子来说明，如果用户正在玩你的游戏，那么当你发送大块数据的时候，使用保守的策略。如果不这么做的话，就会冒影响用户游戏体验的风险，这是因为你的发送行为可能会诱导额外的网络延迟或者出现数据包丢失的情况。 同样，如果你的块数据非常大的情况下，比如说是十几MB的情况，那么请不要使用这种野蛮发送的策略，这是因为这种方法太过于依赖陌生人的仁慈，也就是在你和你的数据包目的地之间的路由器缓冲区。如果要持续发送非常大的数据块保持一个高吞吐量有必要实施一些更聪明的方法。这是某种自适应的方法，它会试图尽快发送数据，但是一旦检测到因为连接上有太多的数据在传输导致太多的延迟或者数据包的丢失，就能切换回一个低速的方式。这样一个系统超出了本文的范围。 接收方的实现 现在我们已经解决了发送方实现的所有细节和小问题，那么让我们开始实现接收方。正如之前提到的那样，与之前文章介绍的数据包的分包和重组系统不同，块系统在同一时间只能由一个块正在传输。 这使得块系统的接收方可以实现的更加简单，你可以看下面的实现: ?1234567891011class ChunkReceiver{ bool receiving; bool readyToRead; uint16_t chunkId; int chunkSize; int numSlices; int numReceivedSlices; bool received[MaxSlicesPerChunk]; uint8_t chunkData[MaxChunkSize];}; 我们有一个状态来记录我们是否正在网络上“接收”一个块数据，加上“readyToRead’”状态来表明是否已经有一个块的所有片段数据包都已经收到、已经准备好被用户弹出进行读取处理了。接收队列的最小长度是1，这是非常有效的。如果你不喜欢这个的话，你当然可以立即从块数据接收器里面将这个数据弹出并把它插入实际的接收队列。 在这个数据结构中我们还记录了块数据的大小（尽管不是完全准确，直到收到最后一个片段数据包才能准确的计算块数据的大小）、片段数据包的数量、已经接收到的片段数据包的数量还有针对每个片段数据包的一个接收标记。针对每个片段数据包的接收标记可以让我们丢弃那些我们已经收到的片段数据包，并计算到目前为止我们已经收到的片段数据包的数量（因为我们可能会多次收到同一个片段数据包，但是我们只会在第一次收到这个片段数据包的才会增加计数器的值）。它也被用在生成确认数据包上。当已经接收到的片段数据包的数量等于片段数据包的数量的时候，从接收方的角度看这个块数据的接收才算完成。 首先，接收方的设置会从块0开始。当一个片段数据包从网络上传递过来，并且能够匹配这个块id的话，“receiving”状态会从false翻转为true，第一个片段数据包的数据会插入” chunkData“变量的合适位置，片段数据包的数量会根据第一个片段数据包里面的数据进行正确的设置，已经接收到的片段数据包的数量会加一，也就是从0到1，针对每个片段数据包的接收标记里面对应这个片段数据包的项会变为true。 随着这个块数据的其他片段数据包的到来，会对每一个片段数据包进行检测，判断它们的id是否与当前块的id相同，如果不相同的话就会被丢弃。如果这个片段数据包已经收到过的话，那么这个包也会被丢弃。否则，这个片段数据包的数据会插入” chunkData“变量的合适位置、已经接收到的片段数据包的数量会加一、针对每个片段数据包的接收标记里面对应这个片段数据包的项会变为true。 这一过程会持续进行，直到接收到所有的片段数据包。一旦接收到所有的片段数据包（也就是已经接收到的片段数据包的数量等于片段数据包的数量的时候），接收方会把“receiving “状态改为false，而把”readyToRead“状态改为true。当”readyToRead”状态为true的时候，所有收到的片段数据包都会被丢弃。在这一点上，这个处理过程通常非常的短，会在收到片段数据包的同一帧进行处理，调用者会检查”我有一块数据要读取么？“并处理块数据。然后会重置数据块接收器的所有数据为默认值，除了块数据的id从0增加到1，这样我们就准备好接收下一个块了。 浸泡测试的重要性和确认数据包 第一眼看上去，确认数据包这个系统似乎很简单： 1）记录已经接收到的片段数据包。 2）当一个片段数据包收到以后，回复一个包含所有确认收到的片段数据包信息的确认数据包。 这看上去实现起来似乎相当的简单，但是像大多数发生在UDP协议的事情一样，当涉及到数据包丢失的时候，就有一些微妙的点让它的处理有点棘手。 一个对于确认数据包比较天真的实现可能是这样子的。每次收到片段数据包，就回复一个包含所有确认收到的片段数据包信息的确认数据包（也会包括刚收到的片段数据包的信息）。这看上去非常符合逻辑，但是这使得块协议给恶意发送者一个漏洞使得它们可以块协议作为一个DDos的工具。如何作为一个DDos的工具?如果你对每个收到的片段数据包都会回复一个确认数据包的话，那么发送方能够构造一个很小的片段数据包发送给你，而你会回复一个比发送给你的片段数据包还大的确认数据包，这样你的服务器就变成了一个可以被人利用来进行DDos放大攻击的工具。 现在也许是因为我对DDos这个事情有一点偏执（我确实是有一点），但是一般来说你可以防止对DDos的放大，永远不要设计一个包含对接收到的数据包进行一对一的映射响应的协议。让我们举个简单例子来说明一下这个问题。如果有人给你发送1000个片段数据包，永远不要给他回复1000个确认数据包。相反只发一个确认数据包，而且最多每50毫秒或者100毫秒才发送一个确认数据包。如果你是这样设计的话，那么滥用你的UDP协议对DDos进行放大就是完全不可能的。 还有其他的方法让这个确认系统容易出错，而这些都往往表现为”发送挂起“。换句话说，接收方已经知道这个块已经发送完毕了，但是由于程序员的错误，发送方错过了一个确认数据包(可能是针对最后一个片段数据包的确认数据包)并且卡入到一个状态，会不停的反复重发这个片段数据包而没有得到一个确认数据包的响应。 在过去10年里，我可能至少5次从头开始实现这个块系统，每次我都找到新的和令人兴奋的方式来让发送方挂起。我开发和测试块系统的策略是首先进行编码确认它能够跑起来，然后设置一个测试工具在有大量的数据包丢失、数据包乱序和重复的情况下随机发送随机大小的块。这往往会清除任何挂起。我曾经实现过的块系统都至少有一个挂起存在，通常会有2到3个挂起。所以如果你是打算从头开始实现这个块系统的话，请不要轻敌。请设置一个浸泡测试。你会感谢我在这里的提醒的。 我通常遇到的第一次挂起是由于对同一个片段数据包的多次收到不会回复一个确认数据包。它有点像这样：” 哦，这个片段数据包已经收到过了么？已经收到过了就丢弃它”，然后忘记在确认数据包里面设置标记。这对于发送方来说是一个困扰，因为这样的话就不会有一个确认数据包，那么如果出现这种情况的话，又恰好遇到第一次收到这个片段数据包的时候发送的确认数据包出现丢包的情况，发送方根本就不知道这个他在反复发送的片段数据包其实已经被收到了。如果你就是这么不巧，遇上了第一次收到这个片段数据包的时候发送的确认数据包出现丢包的情况，那么就遇上了挂起的情况。如果你想在你的代码里面重现这个情况的话，可以在收到最后一个片段数据包的时候不发送确认数据包，那么出现的情况就是这种挂起了。 下一个挂起会发生在接收方在发送方知道之前就已经知道块发送完毕并切换它的状态变量“readyToRead”来丢弃后续传入的片段数据包。在这种状态下，即使接收方认为块已经完全接收完毕，但是发送方还不知道这一点，所以有必要设置确认数据包对应的标志位，即使块已经完全接收完毕，这样发送方才能一直接收到提示块已经全部发送完毕的确认数据包。 通常遇到的最后一个挂起情况是在读取完块数据以后的状态切换里面，那个时候状态变量“readyToRead”已经切回false而块的id也加一了。让我们举个简单例子来说明一下这个问题，块0已经完成接收，用户已经完成对块0的读取并且块id已经递增到1了，所以我们已经准备好接收块1的片段数据包了（我们会丢弃任何与我们当前正在接收块ID不同的片段数据包）。 再一次出现这种情况，就是这里的发送方因为确认数据包的丢失导致信息有一点滞后，可能是因为没有收到第一个确认数据包。在这种情况下，有必要关注片段数据包，如果我们正处于这么一个状态：我们尚未收到第n个片段数据包，但是前面n – 1个片段数据包都已经收到了，我们必须设置一个特殊的标记位然后我们会发送一个包含所有前面n – 1个片段数据包都已经收到信息的确认数据包，否则发送方不会意识到块数据已经收到并且发送方已经准备挂起了。 正如你所看到的那样，确认数据包的实现是有一点微妙的，这是一个有点奇怪的过程因为当片段数据包在网络的一端收到的时候，需要设置一个标记位来发送确认数据包直到发送方知道都有哪些发送的片段数据包被成功接收为止。如果你打破了片段数据包-&gt;确认数据包这个链接的话，那么整个系统就将挂起。我鼓励你仔细看看这篇文章的源代码搞清楚进一步的细节。 总结 块系统在概念上是很简单的，但是它的具体实现肯定不是微不足道的。在我看来，实现发送者设计这一块是一个很好的学习经验，当你从头开始实现这样的系统的时候一定有很多东西需要学习。 我希望你喜欢这个系统的设计，并试着自己动手从头开始实现它。这是一个很好的学习经历。此外，我鼓励你在patreon上支持我，作为回报，你会得到本文的示例源代码(以及本系列的其他文章的示例源代码)，还包括我在GDC 2015上关于网络物理的演讲的源代码。 如果你觉得这篇文章有价值的话，请在patreon上支持我的写作，这样我会写的更快。你可以在BSD 3.0许可下访问到这篇文章里面的代码。非常感谢你的支持！【版权声明】 原文作者未做权利声明，视为共享知识产权进入公共领域，自动获得授权。]]></content>
      <categories>
        <category>GS</category>
      </categories>
      <tags>
        <tag>GafferOnGames</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[构建游戏网络协议三之数据包的分包和重组]]></title>
    <url>%2Fblog%2F2017%2F02%2F26%2Fpacket_fragmentation_and_reassembly%2F</url>
    <content type="text"><![CDATA[本篇自我总结本篇主要讲了数据包的分包和重组问题, 到底数据包多大才好呢?是不是越大越好呢?包太大了怎么办呢?请看总结, 不明之处再看文中具体讲解. 为什么需要做这个分包和重组系统每台计算机(路由器)会沿着路由强制要求数据包的大小会有一个最大的上限，这个上限就是所谓的最大传输单元MTU。如果任意一个路由器收到一个数据包的大小超过这个最大传输单元的大小，它有这么两个选择，a)在IP层对这个数据包进行分包，并将分包后的数据包继续传递，b)丢弃这个数据包然后告诉你数据包被丢弃了，你自己负责摆平这个问题。 实例 : 这儿有一个我会经常遇到的情况。人们在编写多人在线游戏的时候，数据包的平均大小都会非常的小，让我们假设下，这些数据包的平均大小大概只有几百字节，但时不时会在他们的游戏中同时发生大量的事情并且发出去的数据包会出现丢失的情况，这个时候数据包会比通常的情况下要大。突然之间，游戏的数据包的大小就会超过最大传输单元的大小，这样就只有很少一部分玩家能够收到这个数据包，然后整个通信就崩溃了。 本篇基本术语 数据包packets 分包fragments 分包的数据结构我们将允许一个数据包最多可以分成256个数据包，并且每个分包后的数据包的大小不会超过1024个字节。这样的话，我们就可以通过这样一个系统来发送大小最大为256k的数据包 [protocol id] (32 bits) // not actually sent, but used to calc crc32 [crc32] (32 bits) [sequence] (16 bits) // 数据包序号 [packet type = 0] (2 bits) [fragment id] (8 bits) // 分包ID [num fragments] (8 bits) [pad zero bits to nearest byte index] // 用于字节对齐的bits &lt;fragment data&gt; 发送分包后的数据包发送分包以后的数据包是一件非常容易的事情。如果数据包的大小小于保守估计的最大传输单元的大小。那么就按正常的方法进行发送。否则的话，就计算这个数据包到底该分成多少个1024字节的数据包分包，然后构建这些分包并按照之前发送正常数据包的方法进行发送。 发送出去以后也不记录发送的数据包的内容，这种发送以后不记录发送的数据包的内容的方法有一个后果，就是数据包的任意一个分包如果丢失的话，那么整个数据包就都要丢弃。随着分包数量的增加，整个数据包被丢弃的概率也随之增加.由此可见，当你需要发送要给256K的数据包的时候要发送256个分包，如果有一个分包丢失的话，你就要重新把这个256k的数据包再分一次包然后再发送出去。 什么时候用这个分包和重组系统呢因为发送出去以后也不记录发送的数据包, 随着分包数量的增加，整个数据包被丢弃的概率也随之增加, 而一个片段的丢失就会导致整个数据包都要被丢弃掉.所以我建议你要小心分包以后的数量。 这个分包和重组系统最好是只对2-4个分包的情况进行使用，而且最好是针对那种对时间不怎么敏感的数据使用或者是就算分包lost了也无所谓的情况。绝对不要只是为了省事就把一大堆依赖顺序的事件打到一个大数据包里面然后依赖数据包的分包和重组机制进行发送。这会让事情变得更加麻烦。 数据包分包和重组系统的关键弱点是一个片段的丢失就会导致整个数据包都要被丢弃掉, 想要解决这个弱点得使用大块数据发送策略, 见下一篇文章 构建游戏网络协议四之发送大块数据. 接收分包后的数据包之所以对分包后的数据包进行接收很困难的原因是我们不仅需要给缓冲区建立一个数据结构还要把这些分包重组成原始的数据包，我们也要特别小心如果有人试图让我们的程序产生崩溃而给我们发送恶意的数据包。 要非常小心检查一切可能的情况。除此之外，还有一个非常简单的事情要注意：让分包保存在一个数据结构里面，当一个数据包的所有分包都到达以后（通过计数来判断是否全部到达），将这些分包重组成一个大的数据包，并把这个重组后的大数据包返回给接收方。 什么样的数据结构在这里是有意义的?这里并没有什么特别的数据结构!我使用的是一种我喜欢称之为序列缓冲区的东西。我想和你分享的最核心的技巧是如何让这个数据结构变得高效： 12345678const int MaxEntries = 256; struct SequenceBuffer&#123; bool exists[MaxEntries]; uint16_t sequence[MaxEntries]; Entry entries[MaxEntries];&#125;; 原文原文出处 原文标题 : Packet Fragmentation and Reassembly (How to send and receive packets larger than MTU) Introduction Hi, I’m Glenn Fiedler and welcome to Building a Game Network Protocol. In the previous article we discussed how to unify packet read and write into a single serialize function and added a bunch of safety features to packet read. Now we are ready to start putting interesting things in our packets and sending them over the network, but immediately we run into an interesting question: how big should our packets be? To answer this question properly we need a bit of background about how packets are actually sent over the Internet. Background Perhaps the most important thing to understand about the internet is that there&rsquo;s no direct connection between the source and destination IP address. What actually happens is that packets hop from one computer to another to reach their destination. Each computer along this route enforces a maximum packet size called the maximum transmission unit, or MTU. According to the IP standard, if any computer recieves a packet larger than its MTU, it has the option of a) fragmenting that packet, or b) dropping the packet. So here&rsquo;s how this usually goes down. People write a multiplayer game where the average packet size is quite small, lets say a few hundred bytes, but every now and then when a lot of stuff is happening in their game and a burst of packet loss occurs, packets get a lot larger than usual, going above MTU for the route, and suddenly all packets start getting dropped! Just last year (2015) I was talking with Alex Austin at Indiecade about networking in his game Sub Rosa. He had this strange networking bug he couldn&rsquo;t reproduce. For some reason, players would randomly get disconnected from the game, but only when a bunch of stuff was going on. It was extremely rare and he was unable to reproduce it. Alex told me looking at the logs it seemed like packets just stopped getting through. This sounded exactly like an MTU issue to me, and sure enough, when Alex limited his maximum packet size to a reasonable value the bug went away. MTU in the real world So what&rsquo;s a reasonable maximum packet size? On the Internet today (2016, IPv4) the real-world MTU is 1500 bytes. Give or take a few bytes for UDP/IP packet header and you&rsquo;ll find that the typical number before packets start to get dropped or fragmented is somewhere around 1472. You can try this out for yourself by running this command on MacOS X: ping -g 56 -G 1500 -h 10 -D 8.8.4.4 On my machine it conks out around just below 1500 bytes as expected: 1404 bytes from 8.8.4.4: icmp_seq=134 ttl=56 time=11.945 ms 1414 bytes from 8.8.4.4: icmp_seq=135 ttl=56 time=11.964 ms 1424 bytes from 8.8.4.4: icmp_seq=136 ttl=56 time=13.492 ms 1434 bytes from 8.8.4.4: icmp_seq=137 ttl=56 time=13.652 ms 1444 bytes from 8.8.4.4: icmp_seq=138 ttl=56 time=133.241 ms 1454 bytes from 8.8.4.4: icmp_seq=139 ttl=56 time=17.463 ms 1464 bytes from 8.8.4.4: icmp_seq=140 ttl=56 time=12.307 ms 1474 bytes from 8.8.4.4: icmp_seq=141 ttl=56 time=11.987 ms ping: sendto: Message too long ping: sendto: Message too long Request timeout for icmp_seq 142 Why 1500? That&rsquo;s the default MTU for MacOS X. It&rsquo;s also the default MTU on Windows. So now we have an upper bound for your packet size assuming you actually care about packets getting through to Windows and Mac boxes without IP level fragmentation or a chance of being dropped: 1472 bytes. So what&rsquo;s the lower bound? Unfortunately for the routers in between your computer and the destination the IPv4 standard says 576. Does this mean we have to limit our packets to 400 bytes or less? In practice, not really. MacOS X lets me set MTU values in range 1280 to 1500 so considering packet header overhead, my first guess for a conservative lower bound on the IPv4 Internet today would be 1200 bytes. Moving forward, in IPv6 this is also a good value, as any packet of 1280 bytes or less is guaranteed to get passed on without IP level fragmentation. This lines up with numbers that I&rsquo;ve seen throughout my career. In my experience games rarely try anything complicated like attempting to discover path MTU, they just assume a reasonably conservative MTU and roll with that, something like 1000 to 1200 bytes of payload data. If a packet larger than this needs to be sent, it&rsquo;s split up into fragments by the game protocol and re-assembled on the other side. And that&rsquo;s exactly what I&rsquo;m going to show you how to do in this article. Fragment Packet Structure Let&rsquo;s get started with implementation. The first thing we need to decide is how we&rsquo;re going to represent fragment packets over the network so they are distinct from non-fragmented packets. Ideally, we would like fragmented and non-fragmented packets to be compatible with the existing packet structure we&rsquo;ve already built, with as little overhead as possible in the common case when we are sending packets smaller than MTU. Here&rsquo;s the packet structure from the previous article: [protocol id] (64 bits) // not actually sent, but used to calc crc32 [crc32] (32 bits) [packet type] (2 bits for 3 distinct packet types) (variable length packet data according to packet type) [end of packet serialize check] (32 bits) In our protocol we have three packet types: A, B and C. Let&rsquo;s make one of these packet types generate really large packets: static const int MaxItems = 4096 * 4; struct TestPacketB : public Packet{ int numItems; int items[MaxItems]; TestPacketB() : Packet( TEST_PACKET_B ) { numItems = random_int( 0, MaxItems ); for ( int i = 0; i &amp;lt; numItems; ++i ) items[i] = random_int( -100, +100 ); } template &amp;lt;typename Stream&amp;gt; bool Serialize( Stream &amp;amp; stream ) { serialize_int( stream, numItems, 0, MaxItems ); for ( int i = 0; i &amp;lt; numItems; ++i ) { serialize_int( stream, items[i], -100, +100 ); } return true; }}; This may seem somewhat contrived but these situations really do occur. For example, if you have a strategy where you send all un-acked events from server to client and you hit a burst of packet loss, you can easily end up with packets larger than MTU, even though your average packet size is quite small. Another common case is delta encoded snapshots in a first person shooter. Here packet size is proportional to the amount of state changed between the baseline and current snapshots for each client. If there are a lot of differences between the snapshots the delta packet is large and there&rsquo;s nothing you can do about it except break it up into fragments and re-assemble them on the other side. Getting back to packet structure. It&rsquo;s fairly common to add a sequence number at the header of each packet. This is just a packet number that increases with each packet sent. I like to use 16 bits for sequence numbers even though they wrap around in about 15 minutes @ 60 packets-per-second, because it&rsquo;s extremely unlikely that a packet will be delivered 15 minutes late. Sequence numbers are useful for a bunch of things like acks, reliability and detecting and discarding out of order packets. In our case, we&rsquo;re going to use the sequence number to identify which packet a fragment belongs to: [protocol id] (64 bits) // not actually sent, but used to calc crc32 [crc32] (32 bits) [sequence] (16 bits) [packet type] (2 bits) (variable length packet data according to packet type) [end of packet serialize check] (32 bits) Here&rsquo;s the interesting part. Sure we could just add a bit is_fragment to the header, but then in the common case of non-fragmented packets you&rsquo;re wasting one bit that is always set to zero. What I do instead is add a special fragment packet type: enum TestPacketTypes { PACKET_FRAGMENT = 0, // RESERVED TEST_PACKET_A, TEST_PACKET_B, TEST_PACKET_C, TEST_PACKET_NUM_TYPES }; And it just happens to be free because four packet types fit into 2 bits. Now when a packet is read, if the packet type is zero we know it&rsquo;s a fragment packet, otherwise we run through the ordinary, non-fragmented read packet codepath. Lets design what this fragment packet looks like. We&rsquo;ll allow a maximum of 256 fragments per-packet and have a fragment size of 1024 bytes. This gives a maximum packet size of 256k that we can send through this system, which should be enough for anybody, but please don&rsquo;t quote me on this. With a small fixed size header, UDP header and IP header a fragment packet be well under the conservative MTU value of 1200. Plus, with 256 max fragments per-packet we can represent a fragment id in the range [0,255] and the total number of fragments per-packet [1,256] with 8 bits. [protocol id] (32 bits) // not actually sent, but used to calc crc32 [crc32] (32 bits) [sequence] (16 bits) [packet type = 0] (2 bits) [fragment id] (8 bits) [num fragments] (8 bits) [pad zero bits to nearest byte index] &lt;fragment data&gt; Notice that we pad bits up to the next byte before writing out the fragment data. Why do this? Two reasons: 1) it&rsquo;s faster to copy fragment data into the packet via memcpy than bitpacking each byte, and 2) we can now save a small amount of bandwidth by inferring the fragment size by subtracting the start of the fragment data from the total size of the packet. Sending Packet Fragments Sending packet fragments is easy. For any packet larger than conservative MTU, simply calculate how many 1024 byte fragments it needs to be split into, and send those fragment packets over the network. Fire and forget! One consequence of this is that if any fragment of that packet is lost then the entire packet is lost. It follows that if you have packet loss then sending a 256k packet as 256 fragments is not a very good idea, because the probability of dropping a packet increases significantly as the number of fragments increases. Not quite linearly, but in an interesting way that you can read more about here. In short, to calculate the probability of losing a packet, you must calculate the probability of all fragments being delivered successfully and subtract that from one, giving you the probability that at least one fragment was dropped. 1 - probability_of_fragment_being_delivered ^ num_fragments For example, if we send a non-fragmented packet over the network with 1% packet loss, there is naturally a 1&frasl;100 chance the packet will be dropped. As the number of fragments increase, packet loss is amplified: Two fragments: 1 - (99&frasl;100) ^ 2 = 2% Ten fragments: 1 - (99&frasl;100) ^ 10 = 9.5% 100 fragments: 1 - (99&frasl;100) ^ 100 = 63.4% 256 fragments: 1 - (99&frasl;100) ^ 256 = 92.4% So I recommend you take it easy with the number of fragments. It&rsquo;s best to use this strategy only for packets in the 2-4 fragment range, and only for time critical data that doesn&rsquo;t matter too much if it gets dropped. It&rsquo;s definitely not a good idea to fire down a bunch of reliable-ordered events in a huge packet and rely on packet fragmentation and reassembly to save your ass. Another typical use case for large packets is when a client initially joins a game. Here you usually want to send a large block of data down reliably to that client, for example, representing the initial state of the world for late join. Whatever you do, don&rsquo;t send that block of data down using the fragmentation and re-assembly technique in this article. Instead, check out the technique in next article which handles packet loss by resending fragments until they are all received. Receiving Packet Fragments It&rsquo;s time to implement the code that receives and processed packet fragments. This is a bit tricky because we have to be particularly careful of somebody trying to attack us with malicious packets. Here&rsquo;s a list of all the ways I can think of to attack the protocol: Try to send out of bound fragments ids trying to get you to crash memory. eg: send fragments [0,255] in a packet that has just two fragments. Send packet n with some maximum fragment count of say 2, and then send more fragment packets belonging to the same packet n but with maximum fragments of 256 hoping that you didn&rsquo;t realize I widened the maximum number of fragments in the packet after the first one you received, and you trash memory. Send really large fragment packets with fragment data larger than 1k hoping to get you to trash memory as you try to copy that fragment data into the data structure, or blow memory budget trying to allocate fragments Continually send fragments of maximum size (256&frasl;256 fragments) in hope that it I could make you allocate a bunch of memory and crash you out. Lets say you have a sliding window of 256 packets, 256 fragments per-packet max, and each fragment is 1k. That&rsquo;s 64 mb per-client. Can I fragment the heap with a bunch of funny sized fragment packets sent over and over? Perhaps the server shares a common allocator across clients and I can make allocations fail for other clients in the game because the heap becomes fragmented. Aside from these concerns, implementation is reasonably straightforward: store received fragments somewhere and when all fragments arrive for a packet, reassemble them into the original packet and return that to the user. Data Structure on Receiver Side The first thing we need is some way to store fragments before they are reassembled. My favorite data structure is something I call a sequence buffer: const int MaxEntries = 256; struct SequenceBuffer{ uint32_t sequence[MaxEntries]; Entry entries[MaxEntries];}; Indexing into the arrays is performed with modulo arithmetic, giving us a fast O(1) lookup of entries by sequence number: const int index = sequence % MaxEntries; A sentinel value of 0xFFFFFFFF is used to represent empty entries. This value cannot possibly occur with 16 bit sequence numbers, thus providing us with a fast test to see if an entry exists for a given sequence number, without an additional branch to test if that entry exists. This data structure is used as follows. When the first fragment of a new packet comes in, the sequence number is mapped to an entry in the sequence buffer. If an entry doesn&rsquo;t exist, it&rsquo;s added and the fragment data is stored in there, along with information about the fragment, eg. how many fragments there are, how many fragments have been received so far, and so on. Each time a new fragment arrives, it looks up the entry by the packet sequence number. When an entry already exists, the fragment data is stored and number of fragments received is incremented. Eventually, once the number of fragments received matches the number of fragments in the packet, the packet is reassembled and delivered to the user. Since it&rsquo;s possible for old entries to stick around (potentially with allocated blocks), great care must be taken to clean up any stale entries when inserting new entries in the sequence buffer. These stale entries correspond to packets that didn&rsquo;t receive all fragments. And that&rsquo;s basically it at a high level. For further details on this approach please refer to the example source code for this article. Click here to get the example source code for this article series. Test Driven Development One thing I&rsquo;d like to close this article out on. Writing a custom UDP network protocol is hard. It&rsquo;s so hard that even though I&rsquo;ve done this from scratch at least 10 times, each time I still manage to fuck it up in a new and exciting ways. You&rsquo;d think eventually I&rsquo;d learn, but this stuff is complicated. You can&rsquo;t just write low-level netcode and expect it to just work. You have to test it! My strategy when testing low-level netcode is as follows: Code defensively. Assert everywhere. These asserts will fire and they&rsquo;ll be important clues you need when something goes wrong. Add functional tests and make sure stuff is working as you are writing it. Put your code through its paces at a basic level as you write it and make sure it&rsquo;s working as you build it up. Think hard about the essential cases that need to be property handled and add tests that cover them. But just adding a bunch of functional tests is not enough. There are of course cases you didn&rsquo;t think of! Now you have to get really mean. I call this soak testing and I&rsquo;ve never, not even once, have coded a network protocol that hasn&rsquo;t subsequently had problems found in it by soak testing. When soak testing just loop forever and just do a mix of random stuff that puts your system through its paces, eg. random length packets in this case with a huge amount of packet loss, out of order and duplicates through a packet simulator. Your soak test passes when it runs overnight and doesn&rsquo;t hang or assert. If you find anything wrong with soak testing. You may need to go back and add detailed logs to the soak test to work out how you got to the failure case. Once you know what&rsquo;s going on, stop. Don&rsquo;t fix it immediately and just run the soak test again. Instead, add a unit test that reproduces that problem you are trying to fix, verify your test reproduces the problem, and that it problem goes away with your fix. Only after this, go back to the soak test and make sure they run overnight. This way the unit tests document the correct behavior of your system and can quickly be run in future to make sure you don&rsquo;t break this thing moving forward when you make other changes. Add a bunch of logs. High level errors, info asserts showing an overview of what is going on, but also low-level warnings and debug logs that show what went wrong after the fact. You&rsquo;re going to need these logs to diagnose issues that don&rsquo;t occur on your machine. Make sure the log level can be adjusted dynamically. Implement network simulators and make sure code handles the worst possible network conditions imaginable. 99% packet loss, 10 seconds of latency and +/- several seconds of jitter. Again, you&rsquo;ll be surprised how much this uncovers. Testing is the time where you want to uncover and fix issues with bad network conditions, not the night before your open beta. Implement fuzz tests where appropriate to make sure your protocol doesn&rsquo;t crash when processing random packets. Leave fuzz tests running overnight to feel confident that your code is reasonably secure against malicious packets and doesn&rsquo;t crash. Surprisingly, I&rsquo;ve consistently found issues that only show up when I loop the set of unit tests over and over, perhaps these issues are caused by different random numbers in tests, especially with the network simulator being driven by random numbers. This is a great way to take a rare test that fails once every few days and make it fail every time. So before you congratulate yourself on your tests passing 100%, add a mode where your unit tests can be looped easily, to uncover such errors. Test simultaneously on multiple platforms. I&rsquo;ve never written a low-level library that worked first time on MacOS, Windows and Linux. There are always interesting compiler specific issues and crashes. Test on multiple platforms as you develop, otherwise it&rsquo;s pretty painful fixing all these at the end. This about how people can attack the protocol. Implement code to defend against these attacks. Add functional tests that mimic these attacks and make sure that your code handles them correctly. This is my process and it seems to work pretty well. If you are writing a low-level network protocol, the rest of your game depends on this code working correctly. You need to be absolutely sure it works before you build on it, otherwise it&rsquo;s basically a stack of cards. In my experience, game neworking is hard enough without having suspicions that that your low-level network protocol has bugs that only show up under extreme network conditions. That&rsquo;s exactly where you need to be able to trust your code works correctly. So test it! 译文 译文出处 译者：崔嘉艺(milan21) 审校：崔国军（星际迷航）大家好，我是格伦·菲德勒。欢迎大家阅读系列教程《构建游戏网络协议》的第三篇文章。在之前的文章中，我们讨论了如何将数据包的读取和写入用一个单独的序列化函数来实现。现在我们要开始把一些有趣的事情放到这些数据包中，,但正如你即将开始编码的令人惊叹的多人在线动作、第一人称射击、大型多人在线角色扮演游戏、多人在线战术竞技游戏会发生的那样，当你以每秒120次的频率发送8k大小的数据包，游戏网络中会传来一个声音呼喊着你:“不要发送超过1200字节大小的数据包!”但是都已经2016年了，你真的要注意最大传输单元这个东西么?不幸的是，答案是是的！最大传输单元MTU你可能已经听说过最大传输单元了。在网络程序员中流传着大量有关最大传输单元问题的故事。那么这到底是怎么回事呢？究竟什么是最大传输单元?为什么你要在乎最大传输单元这个事情？当你通过互联网来发送数据包的时候到底背后发生了什么？这些数据包要从一台计算机(路由器)跳到另一个计算机(路由器)上，如此往复多次才能到达自己的目的地。这是一个分组交换网络具体如何运作的方式。在大部分时间里，它的工作方式不像是在源和目的地之间存在一条直接的连接。但意外的是：每台计算机(路由器)会沿着路由强制要求数据包的大小会有一个最大的上限，这个上限就是所谓的最大传输单元。如果任意一个路由器收到一个数据包的大小超过这个最大传输单元的大小，它有这么两个选择，a)在IP层对这个数据包进行分包，并将分包后的数据包继续传递，b)丢弃这个数据包然后告诉你数据包被丢弃了，你自己负责摆平这个问题。这儿有一个我会经常遇到的情况。人们在编写多人在线游戏的时候，数据包的平均大小都会非常的小，让我们假设下，这些数据包的平均大小大概只有几百字节，但时不时会在他们的游戏中同时发生大量的事情并且发出去的数据包会出现丢失的情况，这个时候数据包会比通常的情况下要大。突然之间，游戏的数据包的大小就会超过最大传输单元的大小，这样就只有很少一部分玩家能够收到这个数据包，然后整个通信就崩溃了。就在去年(2015年)，我与亚历克斯·奥斯汀在Indiecade谈论他的游戏” Sub Rosa “中的网络部分。他遇到了一些奇怪的无法重现的网络bug。出于某种原因，一些客户端（在所有玩家里面总是有那么一个或者两个）会随机的从游戏中断开连接并且其他人都能够正常游戏。查看日志的话，亚历克斯又觉得一切都是正常的，只是看上去好像数据包突然停止进行传递了。对我来说，这听上去就像是一个最大传输单元所引起的问题，并且我非常确信。当亚历克斯把他最大的数据包大小限制在一个合理的值之内，这个bug就再也没有出现了。真实世界中的最大传输单元MTU所以什么是“一个合理的数据包的大小”？在今天的互联网上(2016年，还是基于IPv4)，典型的最大传输单元的大小是1500字节。在UDP/IP数据包的包头添加或者去掉几个字节，你会发现在数据包开始出现被丢弃或者被分包情况的一个典型的边界值是1472。你可以自己在MacOS X尝试运行下下面这个命令：ping-g 56 -G 1500 -h 10 -D 8.8.4.4 在我的机器上，这个结果在略微低于1500字节的大小上下浮动，符合预期：1404bytes from 8.8.4.4: icmp_seq=134 ttl=56 time=11.945 ms1414bytes from 8.8.4.4: icmp_seq=135 ttl=56 time=11.964 ms1424bytes from 8.8.4.4: icmp_seq=136 ttl=56 time=13.492 ms1434bytes from 8.8.4.4: icmp_seq=137 ttl=56 time=13.652 ms1444bytes from 8.8.4.4: icmp_seq=138 ttl=56 time=133.241 ms1454bytes from 8.8.4.4: icmp_seq=139 ttl=56 time=17.463 ms1464bytes from 8.8.4.4: icmp_seq=140 ttl=56 time=12.307 ms1474bytes from 8.8.4.4: icmp_seq=141 ttl=56 time=11.987 msping:sendto: Message too longping:sendto: Message too longRequesttimeout for icmp_seq 142 为什么是1500字节？这是MacOS X上默认的最大传输单元的大小。这也是Windows平台上默认的最大传输单元的大小。所以现在我们对数据包的大小有了一个上限（也就是不能超过这个默认的最大传输单元的大小），假如你真的关心数据包通过Windows平台或者Mac平台进行传播而不希望数据包在IP层进行分包或者有被丢弃的可能的话，那么就要保证数据包的大小不能超过这个上限：1472个字节。那么这个最大传输单元的大小的下限是多少呢？MacOS X允许设置的最大传输单元的大小的值是在1280到1500，所以我对现在互联网上通过IPv4进行传播的数据包的最大传输单元的大小的下限有一个比较保守的估计，就是1200字节。如果是在通过IPv4进行传播的情况下这个比较保守的大传输单元的大小的下限也会是一个很好的估计值，任意数据包只要大小小于1280字节都能保证在没有IP层分包的情况下顺利的传播。这个估计与我在职业生涯中感受到的情况是比较一致的。以我的经验来看，很少有游戏会试图做这些复杂的事情，诸如尝试发现路径上的最大传输单元的大小之类的，它们一般都是假定一个合理又保守的最大传输单元的大小然后一直遵循这个值。如果出现一个要发送的数据包比这个保守的最大传输单元的大小要大的情况，游戏协议会将这个数据包分成几个包然后在网络的另外一侧进行重组。而这正是我要在这篇文章里面要向你传授的内容。分包后的数据包的结构让我们从决定该对网络上传输的数据分包后采用什么的结构进行表示来开始构建我们的数据包分包和重组机制。在理想状态下，我们希望分包以后的数据包和未分包的数据包兼容我们现在已经建立好的数据包结构，这样当我们发送小于最大传输单元的大小的数据包的时候，网络协议没有任何多余的负载。下面是在前一篇文章结束的时候得到的数据包的结构： [protocol id] (64 bits) // not actually sent, but used to calc crc32 [crc32] (32 bits) [packet type] (2 bits for 3 distinct packet types) (variable length packet data according to packet type) [end of packet serialize check] (32 bits) 在我们这个例子之中，我们一共有三个数据包的类型，分别是Ａ、Ｂ和Ｃ。让我们用这三个数据包类型中的一个制造一个比最大传输单元的大小还要大一些的数据包：?1234567891011121314151617181920212223static const int MaxItems = 4096 * 4; struct TestPacketB : public protocol2::Packet{ int numItems; int items[MaxItems]; TestPacketB() : Packet( TEST_PACKET_B ) { numItems = random_int( 0, MaxItems ); for ( int i = 0; i &lt; numItems; ++i ) items[i] = random_int( -100, +100 ); } template &lt;typename stream&gt; bool Serialize( Stream &amp; stream ) { serialize_int( stream, numItems, 0, MaxItems ); for ( int i = 0; i &lt; numItems; ++i ) serialize_int( stream, items[i], -100, +100 ); return true; }};这可能看起来不怎么自然，但在现实世界中这些情况真的会发生。举个简单的例子来说，如果你有一个策略，这个策略是从服务器往客户端发送所有的未确认的事件，你会得到一组可信赖也有序的事件，但是也会遇到大量数据包的丢失的情况，你会轻易的遇到数据包比最大传输单元的大小还要大的情况，即使你的数据包的平均大小非常小。（译注：这是由于丢包重传导致的不停重发，而重发的数据包在UDP或者TCP上会进行合并。所以即使数据包的平均大小远小于最大传输单元的大小，但是由于大量这样的数据包的合并，还是很容易出现超过最大传输单元的大小的情况）。在大多数的情况下，通过实现这么一个策略：在一个数据包里面只包含很少一组事件或者状态更新来避免数据包的大小超过最大传输单元的大小，采用这种策略以后可以有效的规避上面的那种情况。这种规划在很多情况下都工作的很棒。。。但是有一种情况下这种策略也是存在问题的，这种情况就是增量编码。由一个增量编码器创建的数据包的大小是与当前状态与之前状态之间所发生的状态变化的数量成正比的。如果这两个状态之间有大量的不同的话，那么增量也将很大并且你对这种情况其实是没有什么办法的。如果出现一个增量恰好比最大传输单元的大小要大的情况，当然这是一个坏运气下才会出现的情况，但是你仍然要发送这个超过最大传输单元的大小的数据包！所以你可以看到，在增量编码的情况下，你真的不能限制数据包的最大大小一定小于最大传输单元的大小，在这种情况下，数据包的分包和重组策略就有了用武之地。让我们回到数据包的结构上面来。在每个数据包的包头的地方添加一个序号是一种非常常见的做法。这并没有什么复杂的。这只是一个数据包的序号，会在每个数据包进行发送的时候依次加一。举个例子来说，就是0、1、2、3这么简单。我喜欢用16比特来表示这个序号，即使在每秒发送60个数据包的情况下，只要15分钟序号就会被用完一遍，需要再次从头开始重用，但是这么做也没有什么关系，主要是因为你在网络上收到一个15分钟之前发送出去的数据包是一个非常罕见的事情，所以你很少会有机会困惑这到底是一个新的数据包还是一个旧的数据包（因为IP层在包头的地方有个跳转计数，超出一定跳转次数的数据包会被丢弃掉，所以基本不用担心这种情况）。如果你确实关心这个问题的话，请使用32比特的序号进行代替。无论你选择用多少比特来表示这个序号，它们对于很多事情都是有用的，比如说可依赖性、检测和丢弃乱序的数据包等等。除此之外，我们需要一个数据包序号的原因是在对数据包进行分包的时候，我们需要某个方法来确定这个数据包的分包到底是属于哪个数据包的。所以，让我们在我们的数据包的结构里面加上序号这个东西： [protocol id] (64 bits) // not actually sent, but used to calc crc32 [crc32] (32 bits) [sequence] (16 bits) [packet type] (2 bits) (variable length packet data according to packet type) [end of packet serialize check] (32 bits) 这是最有趣的部分。我们可以在数据包的头部只添加一个比特的标识 is_fragment，但是对于通常情况下根本不需要分包的数据包来说，你就浪费了一个比特，因为它总是要被置为0。这不是很好。相反，我的选择是在数据包的结构里面添加了一个特殊的”分包后的数据包“的类型：?12345678enum TestPacketTypes{ PACKET_FRAGMENT = 0, // RESERVED TEST_PACKET_A, TEST_PACKET_B, TEST_PACKET_C, TEST_PACKET_NUM_TYPES}; 这恰好不需要占据任何的空间，是因为四个数据包类型正好可以用两个比特来表示，而这两个比特的空间已经用于表示数据包类型了，我们只是在原来的枚举上新加了一个类型。这么处理以后，每次当我们读取一个数据包的时候，如果这个数据包的类型是0的话，我们就知道这个数据包是一个特殊的分包以后的数据包，它的内存布局可以通过数据包的类型得知，否则的话，我们就走回原来的通用的对非分包的数据包进行读取和处理的方法。让我们设计下这个分包后的数据包看起来应该是什么样子的。我们将允许一个数据包最多可以分成256个数据包，并且每个分包后的数据包的大小不会超过1024个字节。这样的话，我们就可以通过这样一个系统来发送大小最大为256k的数据包，这对于任意系统任意情况来说都应该是足够的，但是这只是我个人的一个看法，如果有特殊的情况，还请结合实际情况进行具体分析。有了这么一个不大的固定大小的数据包包头结构，再加上UDP的包头结构以及IP的包头结构，一个分包以后的数据包会小于之前我们保守估计的最大传输单元的大小：1200字节。除此之外，因为一个数据包最多可以分包成256个数据包，我们可以用【0，255】这个范围来表示分包的id和序号，这样每个分包里面还需要有8比特来表示这个序号。 [protocol id] (32 bits) // not actually sent, but used to calc crc32 [crc32] (32 bits) [sequence] (16 bits) [packet type = 0] (2 bits) [fragment id] (8 bits) [num fragments] (8 bits) [pad zero bits to nearest byte index] &lt;fragment data&gt; 请注意，我们把这几个比特对齐到了下一个字节，然后才把对齐以后的数据写入到数据包里面。我们为什么要这么做？这么做其实是有两个原因的: 1) 这么处理以后可以通过memcpy函数更快的把分包的数据拷贝到数据包里面而不需要使用位打包器来对每个字节进行处理。2) 我们通过不发送分包数据的大小节省了一小部分带宽。我们可以通过从数据包的整体大小减去分包数据起始位置的字节序号来推断出这个分包的大小。发送分包以后的数据包发送分包以后的数据包是一件非常容易的事情。如果数据包的大小小于保守估计的最大传输单元的大小。那么就按正常的方法进行发送。否则的话，就计算这个数据包到底该分成多少个1024字节的数据包分包，然后构建这些分包并按照之前发送正常数据包的方法进行发送。发送出去以后也不记录发送的数据包的内容，这没什么困难的！这种发送以后不记录发送的数据包的内容的方法有一个后果，就是数据包的任意一个分包如果丢失的话，那么整个数据包就都要丢弃。由此可见，当你需要发送要给256K的数据包的时候要发送256个分包，如果有一个分包丢失的话，你就要重新把这个256k的数据包再分一次包然后再发送出去。这绝对不是一个好主意。这显然是一个很糟糕的办法，因为随着分包数目的增多，发生丢失的概率肯定是显著的增大。这种增长关系不是线性的，而是一种相当复杂的关系，如果你对这个计算感兴趣的话，你可以读下这篇文章。简而言之，如果你要计算一个数据包会被丢弃的概率，你必须要计算所有分包被成功发送到目的地的概率，然后从1中减去这个概率，得到的结果就是至少有一个分包丢失的概率。下面这个公式可以用来计算因为分包丢失导致整个数据包被丢弃的概率：＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼1- ( probability of fragment being delivered ) ^ num_fragments＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼ 让我们举个简单的例子对上面这个公式进行说明，如果我们发送的一个不需要分包的数据包，如果它在网络上传说的时候丢失的概率是1%，那么只有百分之一的概率会出现这个数据包被丢弃的情况，或者我们不要嫌麻烦，把这些数据代入到上面这个公式里面: 1 – (99/100) ^ 1 = 1/100 = 1%。随着分包数量的增加，整个数据包被丢弃的概率也随之增加：两个分包的情况： 1 – (99/100) ^ 2 = 2%。十个分包的情况： 1 – (99/100) ^ 10 = 9.5%。一百个分包的情况： 1 – (99/100) ^ 100 = 63.4%。二百五十六个分包的情况： 1 – (99/100) ^ 256 = 92.4%。所以我建议你要小心分包以后的数量。这个策略最好是只对2-4个分包的情况进行使用，而且最好是针对那种对时间不怎么敏感的数据使用或者是就算分包lost了也无所谓的情况。绝对不要只是为了省事就把一大堆依赖顺序的事件打到一个大数据包里面然后依赖数据包的分包和重组机制进行发送。这会让事情变得更加麻烦。大数据包的另外一种典型的用途是当客户端新连入一个游戏服务器的时候。游戏服务器通常会把大块的数据以一种可靠的方式下发给客户端。对于后期才加入的客户端而言，这些大块的数据也许代表了世界的初始状态。无论这些数据包含了怎么样的信息，请不要使用本篇文章的分包技巧来给客户端下发大块的数据。相反，请查阅这个系列教程的下篇文章，在那篇文章里面将讲解如何在有数据包可能发生丢失的情况下，快速可靠的发送大块的数据直到这些大块的数据完全被确认接收。对分包后的数据包的接收尽管发送分包以后的数据包是一件相同简单的事情，但是对分包后的数据包进行接收就相对需要技巧了。之所以对分包后的数据包进行接收很困难的原因是我们不仅需要给缓冲区建立一个数据结构还要把这些分包重组成原始的数据包，我们也要特别小心如果有人试图让我们的程序产生崩溃而给我们发送恶意的数据包。 [protocol id] (32 bits) // not actually sent, but used to calc crc32 [crc32] (32 bits) [sequence] (16 bits) [packet type = 0] (2 bits) [fragment id] (8 bits) [num fragments] (8 bits) [pad zero bits to nearest byte index] &lt;fragment data&gt; 下面列出的是我能想到的如何攻击你的协议以便让你的服务器崩溃的所有方法：尝试发送分包ID在一个限制范围内的分包，看看能不能让你的程序崩溃。举个例子来说，也许这个数据包只有两个分包，但是我会发送多个分包ID在【０，２５５】之内的分包。发送一个数据包，让我们假设这个书包的最大分包数量是2，然后发送多个属于这个数据包的分包，但是在分包的数据包报头里面把最大分包的数量改为256，来希望你在接收完第一个分包以后，不会发现分包的最大数量信息被改变了，这样就有可能造成你的内存崩溃。发送非常大的分包，让分包里面的数据超过1k，测试下你在试图把分包数据拷贝到数据结构的时候是否有良好的判断，如果没有良好的判断的话，这也许会让你的内存崩溃，或者占用大量的内容让你在分配新的分包的时候没有空间。 持续的给你的程序发送最大分包数目的分包（也就是如果最大分包数目是256的话，就持续不断的发送分包ID是256的数据包），希望你会分配大量的内容来容纳这些分包然后让你的内存崩溃。让我们假设你的程序中有一个滑动窗口，这个滑动窗口有256个数据包，每个数据包最多可以有256个分包，每个分包预留的空间是1k。那么也就是会给每个客户端预留67,108,864字节或者64mb的空间。我可以通过这种方法让服务器崩溃么？我能用一堆大小有趣的分包来耗尽的你地址空间的heap空间么？因为服务器的程序是你来设计实现的，所以只有你才知道这个问题的确切答案。它取决于你的内存预算以及如何分配内存来存储分包。如果你考虑过了觉得这会是一个问题，那么就限制下缓冲区中分包的数目或考虑减少每个数据包的分包的最大数目。考虑给每个分包静态分配数据结果或者使用一个内存池来减少内存碎片。所以你可以看到，在接收端代码是非常脆弱的，要非常小心检查一切可能的情况。除此之外，还有一个非常简单的事情要注意：让分包保存在一个数据结构里面，当一个数据包的所有分包都到达以后（通过计数来判断是否全部到达），将这些分包重组成一个大的数据包，并把这个重组后的大数据包返回给接收方。什么样的数据结构在这里是有意义的?这里并没有什么特别的数据结构!我使用的是一种我喜欢称之为序列缓冲区的东西。我想和你分享的最核心的技巧是如何让这个数据结构变得高效：?12345678const int MaxEntries = 256; struct SequenceBuffer{ bool exists[MaxEntries]; uint16_t sequence[MaxEntries]; Entry entries[MaxEntries];};这里面有几件事情值得注意。首先，使用类似数组的结构可以允许对给定条目是否存在于一个序列缓冲区的存在性进行快速测试并且将测试结果进行缓存，即使每个数据包的条目结构是非常大的（而且这种结构对于数据包的分包和重组来说是非常棒的）。那么我们该如何使用这个数据结构？当你接收到一个分包以后的数据包以后，这个数据包会携带一个序号指定它是属于哪个数据包的分包。这个序号会随着发送而不停的增大(所有序号全部用完导致发生环绕除外)， 所以最关键的技巧是你要对序号进行散列让散列后的序号进入一个数组中某个给定的位置，具体处理过程如下所示：int index = sequence % MaxEntries; 现在你可以用O(1)的时间复杂度进行一个快速测试，来通过序号看下一个给定的条目是否存在，并且判断下这个条目是否和你想要读取或者写入的数据包序号相匹配。也就是说，你既需要测试存在性又需要测试序号是否是预期的序号，这是因为所有的序号都是有效的数字(比如说是0)，还有就是因为根据一个特定的数据包序号查找到的条目可能是存在的，但是它属于过去的一个序号（比如说，这是某个其他的序号，但是恰巧通过取模的计算得到相同的序号）。所以当一个新的数据包的第一个分包到达的时候，你需要把数据包的序号散列成一个索引，如果发现这个索引对应的内容还不存在的话，你需要设置exists[index] = 1，并设置sequence[index]来匹配你正在处理的数据包，并把这个分包储存在序号缓冲区对应的条目里面。这样当下一次有分包实际到达的时候，你会得到相同的序号，然后得到一个相当的索引，在查找的时候会发现对应这个索引的内容已经存在了，并且这个条目的序号正好能和刚刚接收到的数据包的序号匹配，所以你就可以把这个分包累加到这个条目上，这个过程会一直重复直到这个数据包的所有分包都被接收到为止。如果从比较抽象的层次来解释这个事情的话，基本原理大概就是这样的。这种方法的一个更完整的解释请参阅本文的示例源代码。在地址https://www.patreon.com/gafferongames可以获取本系列文章示例的源代码。网络协议的测试驱动开发还有一件事情我想在文章的末尾进行补充说明。我感觉如果我不向我的读者提及这个方法的话，就是对他们的一个伤害。编写一个定制的网络协议是非常困难的。这个过程是如此的苦难以至于我从头开始至少编写了10次网络协议，但是每次我都觉得我在用一种全新的有趣的方法在做这个事情。也许你会认为是我在挑战自己，用一些新奇的方法来实现这个过程，但其实是这个过程太复杂了，完全没有办法按照预期的那样写完代码就期待它们能够正确的工作。你需要对写出来的代码进行测试！当编写底层网络协议层的代码的时候，我的策略是:1、防御性编程。在一切可能的地方进行断言。当某些地方出现错误的时候这些断言会起作用，并且将成为查找问题的重要线索。2、添加函数的功能测试，确保它们是如你的预期那样工作的。把你的代码放到一个可以运行和测试的环境下，这样可以不时地对它们进行测试以便可以确保它一直会像你起初创建它们时候那样良好的工作。仔细考虑有哪些情况需要正确的处理并给这些情况添加测试。3、虽然函数测试非常有必要，但是只是添加一些函数测试是远远不够的。无论如何都有遇到你没有预料到的情况! 现在你必须把它们放到一个真实的环境下看看到底会发生什么。我把这个称之为浸泡测试，并且在我之前编写网络协议的过程中还从来没有过在浸泡测试的过程中没有发现问题的情况。浸泡测试只是在不停的循环，并会随机的做一些事情让你的系统在它的空间中处理一些情况，让我们举些简单的例子对它进行一些简单的说明，比如说构造出随机长度的数据包，而且这些数据包有一个非常高的丢失率，通过数据包模拟器发出的大量乱序并且重复的数据包等等。如果你的程序能够在一晚上的时间里面不挂起或者遇到断言，那么就算你的程序通过了浸泡测试。4、如果你的程序在浸泡测试的过程中发现了某些问题。你可能需要在代码里面添加一些详细的日志以便下次在浸泡测试的时候如果遇到了同样的问题你可以找到出现问题的原因。一旦你知道发生了什么，就可以停止了。不要立即的修复这个问题并且再次运行浸泡测试。这种做法非常的愚蠢。相反，利用单元测试来不停的重现你需要修复的问题，确保单元测试能够重现问题，而且这个问题因为你的修复已经彻底修好了。只有在这样的处理流程之后，才能回到浸泡测试并确保程序在浸泡测试能正常运转一整夜。通过这种方式，单元测试能够记录你的系统的正确的行为并且在以后需要的时候可以快速的运行起来，确保当你做其他改变的时候不会导致一些原来修过的问题重复的出现。这就是我的数据包分包和重组的处理流程了，它似乎工作的不错。如果你在进行一些底层的网络协议的设计，你的游戏的其他的部分将依赖于底层的网络协议的设计。在你继续构建其他的功能之前，你需要绝对的确认底层的网络协议是否能够正常的工作，否则就像一堆胡乱堆积的卡片，很容易就散架了。多人在线游戏的网络部分是非常困难的，如果不小心设计的话，很容易就会出现底层网络协议可能无法正常的工作或者存在缺陷。所以请确保你是知道你的底层网络协议是如何工作的！即将到来的文章的预告下一篇文章是: 《发送大块的数据》请继续阅读本系列的下一篇文章，在哪篇文章里面我将向你展示如何通过数据包快速可信赖的发送大块的数据，如果其中一块数据丢失了也不需要丢弃整个数据包！如果你觉得这篇文章有价值的话，请在patreon上支持我的写作，这样我会写的更快。你可以在BSD 3.0许可下访问到这篇文章里面的代码。非常感谢你的支持！【版权声明】原文作者未做权利声明，视为共享知识产权进入公共领域，自动获得授权。]]></content>
      <categories>
        <category>GS</category>
      </categories>
      <tags>
        <tag>GafferOnGames</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[构建游戏网络协议二之序列化策略]]></title>
    <url>%2Fblog%2F2017%2F02%2F25%2Fserialization_strategies%2F</url>
    <content type="text"><![CDATA[自我总结本篇概要 读取数据的时候要特别小心， 因为可能会有攻击者发送过来的恶意的数据包以及错误的包， 在写入数据的时候你可能会轻松很多，因为如果有任何事情出错了，那几乎肯定是你自己导致的错误 统一的数据包序列化功能 ：诀窍在于让流类型的序列化函数模板化。在我的系统中有两个流类型：ReadStream类和WriteStream类。每个类都有相同的一套方法，但实际上它们没有任何关系。一个类负责从比特流读取值到变量中，另外一个类负责把变量的值写到流中。在模板里类似这样写, 通过 Stream::IsWriting 和 Stream::IsReading 模板会自动区分,然后帮你生产你想要的代码, 简洁漂亮 1234567891011121314151617181920212223242526272829303132333435363738class WriteStream&#123;public: enum &#123; IsWriting = 1 &#125;; enum &#123; IsReading = 0 &#125;; // ...&#125;;class ReadStream&#123;public: enum &#123; IsWriting = 0 &#125;; enum &#123; IsReading = 1 &#125;; // ..&#125;template &lt;typename Stream&gt;bool serialize( Stream &amp; stream, float &amp; value )&#123; union FloatInt &#123; float float_value; uint32_t int_value; &#125;; FloatInt tmp; if ( Stream::IsWriting ) tmp.float_value = value; bool result = stream.SerializeBits( tmp.int_value, 32 ); if ( Stream::IsReading ) value = tmp.float_value; return result;&#125; 边界检查和终止读取 ： 把允许的大小范围也传给序列化函数而不仅仅是所需的比特数量。 序列化浮点数和向量 ： 计算机根本不知道内存中的这个32位的值到底是一个整数还是一个浮点数还是一个字符串的部分。它知道的就是这仅仅是一个32位的值。代码如下(可以通过一个联合体来访问看上去是整数的浮点数).有些时候，你并不想把一个完整精度的浮点数进行传递。那么该如何压缩这个浮点值？第一步是将它的值限制在某个确定的范围内然后用一个整数表示方式来将它量化。举个例子来说，如果你知道一个浮点类型的值是在区间[-10,+10]，对于这个值来说可以接受的精确度是0.01，那么你可以把这个浮点数乘以100.0让它的值在区间[-1000,+1000]并在网络上将其作为一个整数进行序列化。而在接收的那一端，仅仅需要将它除以100.0来得到最初的浮点值. 123456789union FloatInt&#123; float float_value; uint32_t int_value;&#125;; FloatInt tmp;tmp.float_value= 10.0f;printf(“float value as an integer: %x\n”, tmp.int_value ); 序列化字符串和数组 : 为什么要费那么大精力把一个字节数组按比特打包到你的比特流里?为什么不在序列化写入之前进行按字节进行对齐？Why not align to byte so you can memcpy the array of bytes directly into the packet?如何将比特流按字节对齐？只需要在流的当前位置做些计算就可以了，找出还差写入多少个比特就能让当前比特流的比特数量被8整除，然后按照这个数字插入填充比特（比如当前比特流的比特数量是323，那么323+5才能被8整除，所以需要插入5个填充比特）。对于填充比特来说，填充的比特值都是0，这样当你序列化读取的时候你可以进行检测，如果检测的结果是正确的，那么就确实是在读取填充的部分，并且填充的部分确实是0。一直读取到下一个完整字节的比特起始位置（可以被8整除的位置）。如果检测的结果是在应该填充的地方发现了非0的比特值，那么就中止序列化读取并丢弃这个数据包。 序列化数组的子集 : 当实现一个游戏网络协议的时候，或早或晚总会需要序列化一个对象数组然后在网络上传递。比如说服务器也许需要把所有的物体发送给客户端，或者有时候需要发送一组事件或者消息。如果你要发送所有的物体到客户端，这是相当简单直观的，但是如果你只是想发送一个数组的一个子集怎么办？最先想到也是最容易的办法是遍历数组的所有物体然后序列化一个bool数组，这个bool数组标记的是对应的物体是否通过网络发送。如果bool值为1那么后面会跟着物体的数据，否则就会被忽略然后下一个物体的bool值取决于流的下一个值。如果有大量的物体需要发送，举个例子来说，整个场景中有4000个物体，有一半的物体也就是2000个需要通过网络进行发送。每个物体需要一个序号，那么就需要2000个序号，每个序号需要12比特。。。。这就是说数据包里面24000比特或者说接近30000比特（几乎是30000，不是严格是，译注：原文如此）的数据被序号浪费掉了.可以把序号的编码方式修改下来节省数据，序号不再是全局序号，而是相对上一个物体的相对序号。 如何应对恶意数据包和错误包 : 如果某些人发送一些包含随机信息的恶意数据包给你的服务器。你会不会在解析的时候把服务器弄崩溃掉？有三种技术应对 : 协议ID : 在你的数据包里面包含协议ID。一般典型的做法是，头4个字节你可以设定一些比较罕见而且独特的值，你可以通过这３２比特的数据判断出来根本就不是你的应用程序的包，然后就可以直接丢弃了。 CRC32 : 对你的数据包整体做一个CRC32的校验，并把这个校验码放到数据包的包头。可以不发送这个协议ID，但是发送方和接收方提前确认过这个协议ID是什么，并在计算数据包CRC32值的时候装作这个数据包带上了这个协议ID的前缀来参与计算。这样如果发送方使用的协议ID与接收方不一致的时候，CRC32的校验就会失败，这将为每个数据包节省4个字节. 序列化检测 : 是在包的中间，在一段复杂的序列化写入之前或者之后写上一个已知的32比特整数，并在另外一端序列化读取的时候用相同的值进行检测判断。如果序列化检查值是不正确的，那么就中止序列化读取并丢弃这个数据包。 . . . 原文原文出处 原文标题 : Serialization Strategies (Smart tricks that unify packet read and write) Introduction Hi, I’m Glenn Fiedler and welcome to Building a Game Network Protocol. In the previous article, we created a bitpacker but it required manual checking to make sure reading a packet from the network is safe. This is a real problem because the stakes are particularly high - a single missed check creates a vulnerability that an attacker can use to crash your server. In this article, we&rsquo;re going to transform the bitpacker into a system where this checking is automatic. We&rsquo;re going to do this with minimal runtime overhead, and in such a way that we don&rsquo;t have to code separate read and write functions, performing both read and write with a single function. This is called a serialize function. Serializing Bits Let&rsquo;s start with the goal. Here&rsquo;s where we want to end up: struct PacketA { int x,y,z; template &amp;lt;typename Stream&amp;gt; bool Serialize( Stream &amp;amp; stream ) { serialize_bits( stream, x, 32 ); serialize_bits( stream, y, 32 ); serialize_bits( stream, z, 32 ); return true; }}; Above you can see a simple serialize function. We serialize three integer variables x,y,z with 32 bits each. struct PacketB { int numElements; int elements[MaxElements]; template &amp;lt;typename Stream&amp;gt; bool Serialize( Stream &amp;amp; stream ) { serialize_int( stream, numElements, 0, MaxElements ); for ( int i = 0; i &amp;lt; numElements; ++i ) { serialize_bits( buffer, elements[i], 32 ); } return true; }}; And now something more complicated. We serialize a variable length array, making sure that the array length is in the range [0,MaxElements]. Next, we serialize a rigid body with an simple optimization while it&rsquo;s at rest, serializing only one bit in place of linear and angular velocity: struct RigidBody { vec3f position; quat4f orientation; vec3f linear_velocity; vec3f angular_velocity; template &amp;lt;typename Stream&amp;gt; bool Serialize( Stream &amp;amp; stream ) { serialize_vector( stream, position ); serialize_quaternion( stream, orientation ); bool at_rest = Stream::IsWriting ? ( velocity.length() == 0 ) : 1; serialize_bool( stream, at_rest ); if ( !at_rest ) { serialize_vector( stream, linear_velocity ); serialize_vector( stream, angular_velocity ); } else if ( Stream::IsReading ) { linear_velocity = vec3f(0,0,0); angular_velocity = vec3f(0,0,0); } return true; }}; Notice how we&rsquo;re able to branch on Stream::IsWriting and Stream::IsReading to write code for each case. These branches are removed by the compiler when the specialized read and write serialize functions are generated. As you can see, serialize functions are flexible and expressive. They&rsquo;re also safe, with each serialize_* call performing checks and aborting read if anything is wrong (eg. a value out of range, going past the end of the buffer). Most importantly, this checking is automatic, so you can&rsquo;t forget to do it! Implementation in C++ The trick to making this all work is to create two stream classes that share the same interface: ReadStream and WriteStream. The write stream implementation writes values using the bitpacker: class WriteStream { public: enum { IsWriting = 1 }; enum { IsReading = 0 }; WriteStream( uint8_t * buffer, int bytes ) : m_writer( buffer, bytes ) {} bool SerializeInteger( int32_t value, int32_t min, int32_t max ) { assert( min &lt; max ); assert( value &gt;= min ); assert( value &lt;= max ); const int bits = bits_required( min, max ); uint32_t unsigned_value = value - min; m_writer.WriteBits( unsigned_value, bits ); return true; } // ...private: BitWriter m_writer;}; And the read stream implementation reads values in: class ReadStream { public: enum { IsWriting = 0 }; enum { IsReading = 1 }; ReadStream( const uint8_t * buffer, int bytes ) : m_reader( buffer, bytes ) {} bool SerializeInteger( int32_t &amp; value, int32_t min, int32_t max ) { assert( min &lt; max ); const int bits = bits_required( min, max ); if ( m_reader.WouldReadPastEnd( bits ) ) { return false; } uint32_t unsigned_value = m_reader.ReadBits( bits ); value = (int32_t) unsigned_value + min; return true; } // ...private: BitReader m_reader;}; With the magic of C++ templates, we leave it up to the compiler to specialize the serialize function to the stream class passed in, producing optimized read and write functions. To handle safety serialize_* calls are not actually functions at all. They&rsquo;re actually macros that return false on error, thus unwinding the stack in case of error, without the need for exceptions. For example, this macro serializes an integer in a given range: #define serialize_int( stream, value, min, max ) \ do \ { \ assert( min &lt; max ); \ int32_t int32_value; \ if ( Stream::IsWriting ) \ { \ assert( value &gt;= min ); \ assert( value &lt;= max ); \ int32_value = (int32_t) value; \ } \ if ( !stream.SerializeInteger( int32_value, min, max ) ) \ { \ return false; \ } \ if ( Stream::IsReading ) \ { \ value = int32_value; \ if ( value &lt; min || value &gt; max ) \ { \ return false; \ } \ } \ } while (0) If a value read in from the network is outside the expected range, or we read past the end of the buffer, the packet read is aborted. Serializing Floating Point Values We&rsquo;re used to thinking about floating point numbers as being different to integers, but in memory they&rsquo;re just a 32 bit value like any other. The C++ language lets us work with this fundamental property, allowing us to directly access the bits of a float value as if it were an integer: union FloatInt { float float_value; uint32_t int_value; }; FloatInt tmp;tmp.float_value = 10.0f;printf( “float value as an integer: %x\n”, tmp.int_value ); You may prefer to do this with an aliased uint32_t* pointer, but this breaks with GCC -O2. Friends of mine point out that the only truly standard way to get the float as an integer is to cast a pointer to the float value to char* and reconstruct the integer from the bytes values accessed through the char pointer. Meanwhile in the past 5 years I&rsquo;ve had no problems in the field with the union trick. Here&rsquo;s how I use it to serialize an uncompressed float value: template &lt;typename Stream&gt; bool serialize_float_internal( Stream &amp; stream, float &amp; value ) { union FloatInt { float float_value; uint32_t int_value; }; FloatInt tmp; if ( Stream::IsWriting ) { tmp.float_value = value; } bool result = stream.SerializeBits( tmp.int_value, 32 ); if ( Stream::IsReading ) { value = tmp.float_value; } return result; } This is of course wrapped with a serialize_float macro for error checking: #define serialize_float( stream, value ) \ do \ { \ if ( !serialize_float_internal( stream, value ) ) \ { \ return false; \ } } while (0) We can now transmit full precision floating point values over the network. But what about situations where you don&rsquo;t need full precision? What about a floating point value in the range [0,10] with an acceptable precision of 0.01? Is there a way to send this over the network using less bits? Yes there is. The trick is to simply divide by 0.01 to get an integer in the range [0,1000] and send that value over the network. On the other side, convert back to a float by multiplying by 0.01. Here&rsquo;s a general purpose implementation of this basic idea: template &lt;typename Stream&gt; bool serialize_compressed_float_internal( Stream &amp; stream, float &amp; value, float min, float max, float res ) { const float delta = max - min; const float values = delta / res; const uint32_t maxIntegerValue = (uint32_t) ceil( values ); const int bits = bits_required( 0, maxIntegerValue ); uint32_t integerValue = 0; if ( Stream::IsWriting ) { float normalizedValue = clamp( ( value - min ) / delta, 0.0f, 1.0f ); integerValue = (uint32_t) floor( normalizedValue * maxIntegerValue + 0.5f ); } if ( !stream.SerializeBits( integerValue, bits ) ) { return false; } if ( Stream::IsReading ) { const float normalizedValue = integerValue / float( maxIntegerValue ); value = normalizedValue * delta + min; } return true; } Of course we need error checking, so we wrap this with a macro: #define serialize_compressed_float( stream, value, min, max ) \ do \ { \ if ( !serialize_float_internal( stream, value, min, max ) ) \ { \ return false; \ } \ } while (0) And now the basic interface is complete. We can serialize both compressed and uncompressed floating point values over the network. Serializing Vectors and Quaternions Once you can serialize float values it&rsquo;s trivial to serialize vectors over the network. I use a modified version of the vectorial library in my projects and implement serialization for its vector type like this: template &lt;typename Stream&gt; bool serialize_vector_internal( Stream &amp; stream, vec3f &amp; vector ) { float values[3]; if ( Stream::IsWriting ) { vector.store( values ); } serialize_float( stream, values[0] ); serialize_float( stream, values[1] ); serialize_float( stream, values[2] ); if ( Stream::IsReading ) { vector.load( values ); } return true; } #define serialize_vector( stream, value ) do { if ( !serialize_vector_internal( stream, value ) ) { return false; } } while(0) If your vector is bounded in some range, then you can compress it: template &lt;typename Stream&gt; bool serialize_compressed_vector_internal( Stream &amp; stream, vec3f &amp; vector, float min, float max, float res ) { float values[3]; if ( Stream::IsWriting ) { vector.store( values ); } serialize_compressed_float( stream, values[0], min, max, res ); serialize_compressed_float( stream, values[1], min, max, res ); serialize_compressed_float( stream, values[2], min, max, res ); if ( Stream::IsReading ) { vector.load( values ); } return true; } Notice how we are able to build more complex serialization using the primitives we&rsquo;re already created. Using this approach you can easily extend the serialization to support anything you need. Serializing Strings and Arrays What if you need to serialize a string over the network? Is it a good idea to send a string over the network with null termination? Not really. You&rsquo;re just asking for trouble! Instead, serialize the string as an array of bytes with the string length in front. Therefore, in order to send a string over the network, we have to work out how to send an array of bytes. First observation. Why waste effort bitpacking an array of bytes into your bit stream just so they are randomly shifted by [0,7] bits? Why not align to byte so you can memcpy the array of bytes directly into the packet? To align a bitstream just work out your current bit index in the stream and how many bits of padding are needed until the current bit index divides evenly into 8, then insert that number of padding bits. For bonus points, pad up with zero bits to add entropy so that on read you can verify that yes, you are reading a byte align and yes, it is indeed padded up with zero bits to the next whole byte bit index. If a non-zero bit is discovered in the padding, abort serialize read and discard the packet. Here&rsquo;s my code to align a bit stream to byte: void BitWriter::WriteAlign() { const int remainderBits = m_bitsWritten % 8; if ( remainderBits != 0 ) { uint32_t zero = 0; WriteBits( zero, 8 - remainderBits ); assert( ( m_bitsWritten % 8 ) == 0 ); } } bool BitReader::ReadAlign(){ const int remainderBits = m_bitsRead % 8; if ( remainderBits != 0 ) { uint32_t value = ReadBits( 8 - remainderBits ); assert( m_bitsRead % 8 == 0 ); if ( value != 0 ) return false; } return true;} #define serialize_align( stream ) do { if ( !stream.SerializeAlign() ) return false; } while (0) Now we can align to byte prior to writing an array of bytes, letting us use memcpy for the bulk of the array data. The only wrinkle is because the bitpacker works at the word level, it&rsquo;s necessary to have special handling for the head and tail portions. Because of this, the code is quite complex and is omitted for brevity. You can find it in the sample code for this article. The end result of all this is a serialize_bytes primitive that we can use to serialize a string as a length followed by the string data, like so: template &lt;typename Stream&gt; bool serialize_string_internal( Stream &amp; stream, char * string, int buffer_size ) { uint32_t length; if ( Stream::IsWriting ) { length = strlen( string ); assert( length &lt; buffer_size - 1 ); } serialize_int( stream, length, 0, buffer_size - 1 ); serialize_bytes( stream, (uint8_t*)string, length ); if ( Stream::IsReading ) { string[length] = '\0'; } } #define serialize_string( stream, string, buffer_size ) do { if ( !serialize_string_internal( stream, string, buffer_size ) ) { return false; } } while (0) This is an ideal string format because it lets us quickly reject malicious data, vs. having to scan through to the end of the packet searching for &lsquo;\0&rsquo; before giving up. This is important because otherwise protocol level attacks could be crafted to degrade your server&rsquo;s performance by making it do extra work. Serializing Array Subsets When implemeting a game network protocol, sooner or later you need to serialize an array of objects over the network. Perhaps the server needs to send object state down to the client, or there is an array of messages to be sent. This is straightforward if you are sending all objects in the array - just iterate across the array and serialize each object in turn. But what if you want to send a subset of the array? The simplest approach is to iterate across all objects in the array and serialize a bit per-object if that object is to be sent. If the value of the bit is 1 then the object data follows in the bit stream, otherwise it&rsquo;s ommitted: template &lt;typename Stream&gt; bool serialize_scene_a( Stream &amp; stream, Scene &amp; scene ) { for ( int i = 0; i &lt; MaxObjects; ++i ) { serialize_bool( stream, scene.objects[i].send ); if ( !scene.objects[i].send ) { if ( Stream::IsReading ) { memset( &amp;scene.objects[i], 0, sizeof( Object ) ); } continue; } serialize_object( stream, scene.objects[i] ); } return true; } This approach breaks down as the size of the array gets larger. For example, for an array size of size 4096, then 4096 / 8 = 512 bytes spent on skip bits. That&rsquo;s not good. Can we switch it around so we take overhead propertional to the number of objects sent instead of the total number of objects in the array? We can but now, we&rsquo;ve done something interesting. We&rsquo;re walking one set of objects in the serialize write (all objects in the array) and are walking over a different set of objects in the serialize read (subset of objects sent). At this point the unified serialize function concept starts to breaks down, and in my opinion, it&rsquo;s best to separate the read and write back into separate functions, because they have so little in common: bool write_scene_b( WriteStream &amp; stream, Scene &amp; scene ) { int num_objects_sent = 0; for ( int i = 0; i &lt; MaxObjects; ++i ) { if ( scene.objects[i].send ) num_objects_sent++; } write_int( stream, num_objects_sent, 0, MaxObjects ); for ( int i = 0; i &lt; MaxObjects; ++i ) { if ( !scene.objects[i].send ) { continue; } write_int( stream, i, 0, MaxObjects - 1 ); write_object( stream, scene.objects[i] ); } return true; } bool read_scene_b( ReadStream &amp; stream, Scene &amp; scene ){ memset( &amp;scene, 0, sizeof( scene ) ); int num_objects_sent; read_int( stream, num_objects_sent, 0, MaxObjects ); for ( int i = 0; i &lt; num_objects_sent; ++i ) { int index; read_int( stream, index, 0, MaxObjects - 1 ); read_object( stream, scene.objects[index] ); } return true;} One more point. The code above walks over the set of objects twice on serialize write. Once to determine the number of changed objects and a second time to actually serialize the set of changed objects. Can we do it in one pass instead? Absolutely! You can use another trick, rather than serializing the # of objects in the array up front, use a sentinel value to indicate the end of the array: bool write_scene_c( WriteStream &amp; stream, Scene &amp; scene ) { for ( int i = 0; i &lt; MaxObjects; ++i ) { if ( !scene.objects[i].send ) { continue; } write_int( stream, i, 0, MaxObjects ); write_object( stream, scene.objects[i] ); } write_int( stream, MaxObjects, 0, MaxObjects ); return true; } bool read_scene_c( ReadStream &amp; stream, Scene &amp; scene ){ memset( &amp;scene, 0, sizeof( scene ) ); while ( true ) { int index; read_int( stream, index, 0, MaxObjects ); if ( index == MaxObjects ) { break; } read_object( stream, scene.objects[index] ); } return true;} The above technique works great if the objects sent are a small percentage of total objects. But what if a large number of objects are sent, lets say half of the 4000 objects in the scene. That&rsquo;s 2000 object indices with each index costing 12 bits&hellip; that&rsquo;s 24000 bits or 3000 bytes (almost 3k!) in your packet wasted on indexing. You can reduce this overhead by encoding each object index relative to the previous object index. Think about it, you&rsquo;re walking from left to right along an array, so object indices start at 0 and go up to MaxObjects - 1. Statistically speaking, you&rsquo;re quite likely to have objects that are close to each other and if the next index is +1 or even +10 or +30 from the previous one, on average, you&rsquo;ll need quite a few less bits to represent that difference than an absolute index. Here&rsquo;s one way to encode the object index as an integer relative to the previous object index, while spending less bits on statistically more likely values: template &lt;typename Stream&gt; bool serialize_object_index_internal( Stream &amp; stream, int &amp; previous, int &amp; current ) { uint32_t difference; if ( Stream::IsWriting ) { assert( previous &lt; current ); difference = current - previous; assert( difference &gt; 0 ); } // +1 (1 bit) bool plusOne; if ( Stream::IsWriting ) { plusOne = difference == 1; } serialize_bool( stream, plusOne ); if ( plusOne ) { if ( Stream::IsReading ) { current = previous + 1; } previous = current; return true; } // [+2,5] -&amp;gt; [0,3] (2 bits) bool twoBits; if ( Stream::IsWriting ) { twoBits = difference &amp;lt;= 5; } serialize_bool( stream, twoBits ); if ( twoBits ) { serialize_int( stream, difference, 2, 5 ); if ( Stream::IsReading ) { current = previous + difference; } previous = current; return true; } // [6,13] -&amp;gt; [0,7] (3 bits) bool threeBits; if ( Stream::IsWriting ) { threeBits = difference &amp;lt;= 13; } serialize_bool( stream, threeBits ); if ( threeBits ) { serialize_int( stream, difference, 6, 13 ); if ( Stream::IsReading ) { current = previous + difference; } previous = current; return true; } // [14,29] -&amp;gt; [0,15] (4 bits) bool fourBits; if ( Stream::IsWriting ) { fourBits = difference &amp;lt;= 29; } serialize_bool( stream, fourBits ); if ( fourBits ) { serialize_int( stream, difference, 14, 29 ); if ( Stream::IsReading ) { current = previous + difference; } previous = current; return true; } // [30,61] -&amp;gt; [0,31] (5 bits) bool fiveBits; if ( Stream::IsWriting ) { fiveBits = difference &amp;lt;= 61; } serialize_bool( stream, fiveBits ); if ( fiveBits ) { serialize_int( stream, difference, 30, 61 ); if ( Stream::IsReading ) { current = previous + difference; } previous = current; return true; } // [62,125] -&amp;gt; [0,63] (6 bits) bool sixBits; if ( Stream::IsWriting ) { sixBits = difference &amp;lt;= 125; } serialize_bool( stream, sixBits ); if ( sixBits ) { serialize_int( stream, difference, 62, 125 ); if ( Stream::IsReading ) { current = previous + difference; } previous = current; return true; } // [126,MaxObjects+1] serialize_int( stream, difference, 126, MaxObjects + 1 ); if ( Stream::IsReading ) { current = previous + difference; } previous = current; return true;} template &lt;typename Stream&gt;bool serialize_scene_d( Stream &amp; stream, Scene &amp; scene ){ int previous_index = -1; if ( Stream::IsWriting ) { for ( int i = 0; i &amp;lt; MaxObjects; ++i ) { if ( !scene.objects[i].send ) { continue; } write_object_index( stream, previous_index, i ); write_object( stream, scene.objects[i] ); } write_object_index( stream, previous_index, MaxObjects ); } else { while ( true ) { int index; read_object_index( stream, previous_index, index ); if ( index == MaxObjects ) { break; } read_object( stream, scene.objects[index] ); } } return true;} But what about the worst case? Won&rsquo;t we spent more bits when indices are &gt;= +126 apart than on an absolute index? Yes we do, but how many of these worst case indices fit in an array of size 4096? Just 32. It&rsquo;s nothing to worry about. Protocol IDs, CRC32 and Serialization Checks We are nearly at the end of this article, and you can see by now that we are sending a completely unattributed binary stream. It&rsquo;s essential that read and write match perfectly, which is of course why the serialize functions are so great, it&rsquo;s hard to desync something when you unify read and write. But accidents happen, and when they do this system can seem like a stack of cards. What if you somehow desync read and write? How can you debug this? What if somebody tries to connect to your latest server code with an old version of your client? One technique to protect against this is to include a protocol id in your packet. For example, it could be a combination of a unique number for your game, plus the hash of your protocol version and a hash of your game data. Now if a packet comes in from an incompatible game version, it&rsquo;s automatically discarded because the protocol ids don&rsquo;t match: [protocol id] (64bits) (packet data) The next level of protection is to pass a CRC32 over your packet and include that in the header. This lets you pick up corrupt packets (these do happen, remember that the IP checksum is just 16 bits&hellip;). Now your packet header looks like this: [protocol id] (64bits) [crc32] (32bits) (packet data) At this point you may be wincing. Wait. I have to take 8+4 = 12 bytes of overhead per-packet just to implement my own checksum and protocol id? Well actually, you don&rsquo;t. You can take a leaf out of how IPv4 does their checksum, and make the protocol id a magical prefix. This means you don&rsquo;t actually send it, and rely on the fact that if the CRC32 is calculated as if the packet were prefixed by the protocol id, then the CRC32 will be incorrect if the sender does not have the same protocol id as the receiver, thus saving 8 bytes per-packet: [protocol id] (64bits) // not actually sent, but used to calc crc32 [crc32] (32bits) (packet data) One final technique, perhaps as much a check against programmer error on your part and malicious senders (although redundant once you encrypt and sign your packet) is the serialization check. Basically, somewhere mid-packet, either before or after a complicated serialization section, just write out a known 32 bit integer value, and check that it reads back in on the other side with the same value. If the serialize check value is incorrect abort read and discard the packet. I like to do this between sections of my packet as I write them, so at least I know which part of my packet serialization has desynced read and write as I&rsquo;m developing my protocol. Another cool trick I like to use is to always serialize a protocol check at the very end of the packet, to detect accidental packet truncation (which happens more often than you would think). Now the packet looks something like this: [protocol id] (64bits) // not actually sent, but used to calc crc32 [crc32] (32bits) (packet data) [end of packet serialize check] (32 bits) This is great packet structure to use during development. 译文 注意 ：这篇译文对应的是下面的原作者原文旧版本。 译文出处 译者：崔嘉艺（milan21） 审校：陈敬凤(nunu) 在这个系列文章中，我将完全从头开始构建一个专业级别的客户端/服务器游戏网络协议，只使用了C++编译器和一组UDP套接字。如果你正在寻找一个关于如何实现你自己的游戏网络协议方面的详细、实用实现，那么这个系列的文章对你来说就再适合不过了。大家好，我是Glenn Fiedler，欢迎阅读《构建游戏网络协议》系列教程的第二篇文章。在前面的文章里，我们讨论了在多人在线网络游戏里面读取和写入网络包的不同方法。我们很快就否决了通过文本的格式比如XML和JSON来发送游戏状态的办法因为它们确实在效率上存在比较大的问题，因此我们决定用自定义的二进制格式进行代替。我们实现了一个位打包器(bitpacker)，所以我们无需手动将几个布尔变量聚成一个8位比特值(以便为了节省空间)，也无需考虑大端小端问题，可以每次写入一个完整的单词而不需要将单词拆成一个个字符，再考虑如何用字节表示它们，这使得位打包器既非常简单也工作的非常快，也无需考虑与平台有关的细节。 但是我们仍然遗留了以下这些问题需要解决：1. 我们需要实现一个方法来判断整数值是否超出预期范围，如果超出了就要中止网络包的读取和解析，因为会有一些不怀好意的人给我们发送恶意网络包希望我们的程序和内存崩溃掉。网络包的读取和解析的中止必须是自动化的，而且不能使用异常处理，因为异常处理太慢了会拖累我们的程序。2. 如果独立的读取和写入函数是手动编解码的，那么维护它们真的是一个噩梦。我们希望能够为包一次性的编写好序列化代码并且没有任何运行时的性能消耗（主要是额外的分支、虚化等等）。我们应该如何实现上面的这些目标? 请继续阅读，我将向你展示如何用Ｃ＋＋来实现这些功能。开发和完善这些技术花费了我不少时间，所以我希望这些内容对你来说是有帮助的，至少是一个很好的选择值得考虑是否要替换你目前采用的方案，或者可以与你在其他游戏看到的这个问题解决方案相结合，看是否能得到更好的解决方案。 统一的数据包序列化功能让我们从我们的目标开始。这就是我们在本文结束的时候希望得到的东西：?1234567891011121314151617181920212223242526272829303132333435363738394041struct PacketA{ int x,y,z; template &lt;typename Stream&gt; bool Serialize( Stream &amp; stream ) { serialize_bits( stream, x, 32 ); serialize_bits( stream, y, 32 ); serialize_bits( stream, z, 32 ); return true; }}; struct PacketB{ int numElements; int elements[MaxElements]; template &lt;typename Stream&gt; bool Serialize( Stream &amp; stream ) { serialize_int( stream, numElements, 0, MaxElements ); for ( int i = 0; i &lt; numElements; ++i ) serialize_bits( buffer, elements[i], 32 ); return true; }}; struct PacketC{ bool x; short y; int z; template &lt;typename Stream&gt; bool Serialize( Stream &amp; stream ) { serialize_int( stream, x, 8 ); serialize_int( stream, y, 16 ); serialize_int( stream, z, 32 ); return true; }};看下上面的代码片段，可以看到每个数据包结构里面都只有一个单独的序列化函数，而不是有互相独立的序列化读取和序列化写入函数。这非常的棒！它把整个序列化代码一分为二，你可能需要做很多努力来实现序列化读取和写入（因为读取的过程是数据解析并装入本地内存，写入的的过程是将本地的数据写到消息体，会有比较大的差异，所以代码基本是一分为二，一半用于读取，一半用于写入）。如果要让这项工作变得有效，诀窍在于让流类型的序列化函数模板化。在我的系统中有两个流类型：ReadStream类和WriteStream类。每个类都有相同的一套方法，但实际上它们没有任何关系。一个类负责从比特流读取值到变量中，另外一个类负责把变量的值写到流中。ReadStream和WriteStream只是上一篇文章中BitReader和BitWriter类的一个高层次封装。当然也有其他方法可以用来代替。如果你不喜欢用模板的话，你可以使用一个纯虚的基类作为流的接口，然后分别实现读取和写入类来实现这个流接口。但是如果你这么做的话，就要在每个序列化调用的时候发生了一次虚函数调用。这种方法对我来说开销似乎比较大。实现这个功能的另外一种方法是实现一个超级棒的流类型，可以通过配置而在运行时进入读取或者写入模式。这种方法会比虚函数方法快一些，但是仍然会在每次序列化调用的时候存在分支判断到底应该是读还是写，所以它不如硬编码读取和写入函数那么快。我更喜欢模板方法，因为它可以让编译器为项目产生经过优化的读取/写入函数。你甚至可以把序列化代码也这样实现以便让编译器为特定的读取和写入优化一大堆东西：?123456789101112131415161718192021222324252627282930struct RigidBody{ vec3f position; quat3f orientation; vec3f linear_velocity; vec3f angular_velocity; template &lt;typename Stream&gt; bool Serialize( Stream &amp; stream ) { serialize_vector( stream, position ); serialize_quaternion( stream, orientation ); bool at_rest = Stream::IsWriting ? velocity.length() == 0 : 1; serialize_bool( stream, at_rest ); if ( !at_rest ) { serialize_vector( stream, linear_velocity ); serialize_vector( stream, angular_velocity ); } else if ( Stream::IsReading ) { linear_velocity = vec3f(0,0,0); angular_velocity = vec3f(0,0,0); } return true; }};虽然这看起来很没有效率，但是实际上并不是！这个函数经过模板特化以后会根据流的类型优化了所有分支。这很整齐漂亮吧？ 而ReadStream和WriteStream是这样的 ： 1234567891011121314151617class WriteStream&#123;public: enum &#123; IsWriting = 1 &#125;; enum &#123; IsReading = 0 &#125;; // ...&#125;;class ReadStream&#123;public: enum &#123; IsWriting = 0 &#125;; enum &#123; IsReading = 1 &#125;; // ..&#125; 边界检查和终止读取现在我们通过编译器实现了生成优化过的序列化读取/写入函数，我们还需要一些方法来做序列化读取时候的自动错误检测以便让我们不受恶意网络包的影响。我们要做的第一件事情是把允许的大小范围也传给序列化函数而不仅仅是所需的比特数量。试想一下如果有了最小和最大的范围，序列化函数就能自己算出所需的比特数量：serialize_int(stream, numElements, 0, MaxElements );这种做法开辟了一类新的方法，可以非常容易地支持带符号整数的序列化并且序列化函数可以检测从网络中读取的值并确保这个值一定在期望的范围内。如果这个值超出范围了，就立即中止序列化读取并丢弃这个数据包。因为我们没有办法使用异常来处理这种中止（因为异常太慢了），所以上面的方式是我比较喜欢的处理方式。在我的环境中，serialize_int其实并不是一个函数，它实际是一个如下面代码所示的宏：?1234567891011121314151617181920#define serialize_int( stream, value, min, max) \ do \ { \ assert( min &lt; max); \ int32_tint32_value; \ if ( Stream::IsWriting) \ { \ assert( value &gt;= min); \ assert( value &lt;= max); \ int32_value = (int32_t)value; \ } \ if ( !stream.SerializeInteger( int32_value, min, max ) ) \ return false; \ if ( Stream::IsReading) \ { \ value =int32_value; \ if ( value &lt; min || value &gt; max) \ return false; \ } \ } while (0)我让人觉得恐怖害怕的原因是我竟然使用了宏来插入代码来检测SerializeInteger函数的结果以及在发生错误的时候返回false。这会让人感觉到这种行为和异常处理很像，它会在出错的时候回溯堆栈到序列化调用堆栈的最顶上，但是这种处理不会带来任何的问题比如性能的消耗。在回溯的时候出现分支是非常罕见的（序列化错误非常少见）所以分支预测应该不会带来什么性能上的问题。还有一种情况我们也需要中止序列化读取：如果流读取超出了结尾。这种情况其实也是非常罕见的，但是我们必须在每次序列化操作都进行这个检查，这是因为流读取超出结尾会造成的影响是未定义的（也就是说我们对于它能造成什么样子的结果完全是未知的，最糟糕的情况并不是代码崩溃，而是把我们的内容数据完全搞乱了，相信我，你会无比痛恨这件事情）。如果我们没有做这个检测，可能会出现程序无限循环的情况，因为读取的位置超出了缓冲区的结尾。虽然在读取的时候如果发现超出比特流结尾的时候返回0值是很常见的做法（如以前的文章提到的那样），但是返回0值也不能保证序列化函数在有循环的时候能够正确的中止。如果要确保程序是有良好定义的行为，那么这种缓冲溢出检测总是必须的。最后一点，在序列化写入的时候如果遇到范围检测失败或者写入的地址超出流的结尾的时候，我并没有采用中止这种做法。在写入数据的时候你可能会轻松很多，因为如果有任何事情出错了，那几乎肯定是你自己导致的错误。在序列化写入的时候我们只是对每个序列化写入做了断言来确保一切是符合预期的（在范围内、写入的地址没有超出流的结尾），其他的一切都任由你来发挥。序列化浮点数和向量这个比特流现在只序列化类型整数的值。如果我们要序列化一个类型为浮点数的值，我们该怎么做？我们的做法虽然看上去有点投机取巧但实际上并不是。在内存中浮点数也是像整数那样保存成一个32位的值。你的计算机根本不知道内存中的这个32位的值到底是一个整数还是一个浮点数还是一个字符串的部分。它知道的就是这仅仅是一个32位的值。幸运的是，C++语言使得我们可以直接对这个基础属性进行控制（其他语言不行，因为底层被封装掉了，这也是C++被认为不好的地方之一，很多现代语言都禁止了这种做法）。你可以通过一个联合体来访问看上去是整数的浮点数：?123456789union FloatInt{ float float_value; uint32_t int_value;}; FloatInt tmp;tmp.float_value= 10.0f;printf("float value as an integer: %x\n", tmp.int_value );你也可以通过别名uint32_t*的指针来做到这一点，但是因为GCC -O2会导致这种做法有些性能问题，所以我更倾向于使用联合体这种做法。我的朋友们指出（很有可能是正确的）从一个整数类型的值转换到浮点值的唯一真正标准的做法是将浮点数指针转换成uint8_t*指针然后通过这个字节指针来分别引用4个字节来对这个值进行重建。虽然这对我来说似乎是一个非常愚蠢的做法。女士们，先生们。。。这毕竟是C++啊！（作者的意思是C++提供了很多接触底层的方法，我们可以尽情利用这一优势，只要能保证结果是正确的就可以了，哪怕使用一些取巧的办法也无所谓！）。 与此同时，在过去的5年里，在使用联合体这个技巧方面我还没有遇到过什么问题。下面是我如何序列化一个未压缩的浮点值：?123456789101112131415161718192021template &lt;typename Stream&gt;bool serialize_float_internal( Stream &amp; stream, float &amp; value ){ union FloatInt { float float_value; uint32_t int_value; }; FloatInt tmp; if ( Stream::IsWriting ) tmp.float_value = value; bool result = stream.SerializeBits( tmp.int_value, 32 ); if ( Stream::IsReading ) value = tmp.float_value; return result;}通过一个serialize_float宏来包装这个部分以方便在序列化读取的时候方便进行一致的错误检测：?123456#define serialize_float( stream, value) \ do \ { \ if ( !protocol2::serialize_float_internal( stream, value )) \ return false; \ } while(0)有些时候，你并不想把一个完整精度的浮点数进行传递。那么该如何压缩这个浮点值？第一步是将它的值限制在某个确定的范围内然后用一个整数表示方式来将它量化。举个例子来说，如果你知道一个浮点类型的值是在区间[-10,+10]，对于这个值来说可以接受的精确度是0.01，那么你可以把这个浮点数乘以100.0让它的值在区间[-1000,+1000]并在网络上将其作为一个整数进行序列化。而在接收的那一端，仅仅需要将它除以100.0来得到最初的浮点值。下面是这个概念用序列化实现的版本：?12345678910111213141516171819202122232425262728293031323334template &lt;typename Stream&gt;boolserialize_compressed_float_internal( Stream &amp; stream, float &amp; value, float min, float max, float res ){ const float delta = max - min; const float values = delta / res; const uint32_t maxIntegerValue = (uint32_t) ceil( values ); const int bits = bits_required( 0, maxIntegerValue ); uint32_t integerValue = 0; if ( Stream::IsWriting ) { float normalizedValue = clamp( ( value - min ) / delta, 0.0f, 1.0f ); integerValue = (uint32_t) floor( normalizedValue * maxIntegerValue + 0.5f ); } if ( !stream.SerializeBits( integerValue, bits ) ) return false; if ( Stream::IsReading ) { const float normalizedValue = integerValue / float( maxIntegerValue ); value = normalizedValue * delta + min; } return true;}一旦你实现了对浮点数的序列化，那么将方法拓展下通过网络序列化向量和四元数就非常容易了。我在我自己的项目中使用了这个超赞的针对向量数学的向量库（https://github.com/scoopr/vectorial）的一个修改版本，并且我对这些类型实现的序列化方法如下所示：?12345678910111213141516171819202122232425262728293031323334353637383940414243444546template &lt;typename Stream&gt;boolserialize_vector_internal( Stream &amp; stream, vec3f &amp; vector ){ float values[3]; if ( Stream::IsWriting ) vector.store( values ); serialize_float( stream, values[0] ); serialize_float( stream, values[1] ); serialize_float( stream, values[2] ); if ( Stream::IsReading ) vector.load( values ); return true;} template &lt;typename Stream&gt;boolserialize_quaternion_internal( Stream &amp; stream, quat4f &amp; quaternion ){ float values[4]; if ( Stream::IsWriting ) quaternion.store( values ); serialize_float( stream, values[0] ); serialize_float( stream, values[1] ); serialize_float( stream, values[2] ); serialize_float( stream, values[3] ); if ( Stream::IsReading ) quaternion.load( values ); return true;} #defineserialize_vector( stream, value) \ do \ { \ if ( !serialize_vector_internal( stream, value )) \ return false; \ } \ while(0) #defineserialize_quaternion( stream, value) \ do \ { \ if ( !serialize_quaternion_internal( stream, value ) ) \ return false; \ } \ while(0)如果你知道你的向量的取值会限制在某个范围内，你可以像下面这样对它进行压缩：?123456789101112131415161718192021222324252627282930313233template &lt;typename Stream&gt; &lt;typename stream=""&gt; bool serialize_compressed_vector_internal( Stream &amp; stream, vec3f &amp; vector, float min, float max, float res ) { float values[3]; if ( Stream::IsWriting ) vector.store( values ); serialize_compressed_float( stream, values[0], min, max, res ); serialize_compressed_float( stream, values[1], min, max, res ); serialize_compressed_float( stream, values[2], min, max, res ); if ( Stream::IsReading ) vector.load( values ); return true; }&lt;/typename&gt;你如果想要在网络上压缩一个方向，不要把它视为四个取值范围在[-1,+1]的成员变量的结构。如果使用这个四元数的三个最小值来表示它效果会好的多，请看下这篇文章的示例代码(地址在https://www.patreon.com/gafferongames?ty=h)来得到一个这方面的实现。四元数是简单的超复数。 复数是由实数加上虚数单位 i 组成，其中i^2 = -1。 相似地，四元数都是由实数加上三个虚数单位 i、j、k 组成，而且它们有如下的关系： i^2 = j^2 = k^2 = -1， i^0 = j^0 = k^0 = 1 , 每个四元数都是 1、i、j 和 k 的线性组合，即是四元数一般可表示为a + bk+ cj + di，其中a、b、c 、d是实数。对于i、j、k本身的几何意义可以理解为一种旋转，其中i旋转代表X轴与Y轴相交平面中X轴正向向Y轴正向的旋转，j旋转代表Z轴与X轴相交平面中Z轴正向向X轴正向的旋转，k旋转代表Y轴与Z轴相交平面中Y轴正向向Z轴正向的旋转，-i、-j、-k分别代表i、j、k旋转的反向旋转。 序列化字符串和数组如果你想序列化字符串并通过网络传输该怎么办？在网络上发送字符串的时候用Null作为终止符是个好主意么？我不这么认为。我认为这么做只是在自找麻烦！我们应该把字符串作为带长度作为前缀的字符数组。所以，要通过网络发送字符串，我们必须解决如何有效的发送字符数组的问题。观察到的第一个事情：为什么要费那么大精力把一个字节数组按比特打包到你的比特流里?只是为了让它们随机的偏移[0,7]比特？为什么不在序列化写入之前进行按字节进行对齐？Why not align to byte so you can memcpy the array of bytes directly into the packet?如果这么处理的话，数据包里面的字节数组数据就很对齐的很准，数组的每个字节都对应着数据包里面的一个实际字节。对于每个要序列化的字节数组，你只损失了[0,7]个比特，这取决于对齐的方式，但是以我的观点来看这没什么好在意的。如何将比特流按字节对齐？只需要在流的当前位置做些计算就可以了，找出还差写入多少个比特就能让当前比特流的比特数量被8整除，然后按照这个数字插入填充比特（比如当前比特流的比特数量是323，那么323+5才能被8整除，所以需要插入5个填充比特）。对于填充比特来说，填充的比特值都是0，这样当你序列化读取的时候你可以进行检测，如果检测的结果是正确的，那么就确实是在读取填充的部分，并且填充的部分确实是0。一直读取到下一个完整字节的比特起始位置（可以被8整除的位置）。如果检测的结果是在应该填充的地方发现了非0的比特值，那么就中止序列化读取并丢弃这个数据包。下面是我用来将比特流按比特对齐的代码：?123456789101112131415161718192021222324252627282930void BitWriter::WriteAlign(){ const int remainderBits = m_bitsWritten % 8; if ( remainderBits != 0 ) { uint32_t zero = 0; WriteBits( zero, 8 - remainderBits ); assert( ( m_bitsWritten % 8 ) == 0 ); }} bool BitReader::ReadAlign(){ const int remainderBits = m_bitsRead % 8; if ( remainderBits != 0 ) { uint32_t value = ReadBits( 8 - remainderBits ); assert( m_bitsRead % 8 == 0 ); if ( value != 0 ) return false; } return true;} #define serialize_align( stream) \ do \ { \ if ( !stream.SerializeAlign() ) \ return false; \ } while(0)现在我们可以使用这个对齐操作来有效率的将字节数组写入比特流：因为我们已经将比特流按照字节对齐了，所以我们可以使用memcpy方法来做大部分的工作。唯一的问题在于比特读取器和比特写入器是按照双字进行工作的，所以需要一些特殊的代码来处理字节数组的头部和尾部，以确保头部的零散比特会被写入内存，并且在头部处理完毕以后，读取的位置会被正确设置到下一个字节。?1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798void BitWriter::WriteBytes( const uint8_t* data, int bytes ){ assert( GetAlignBits() == 0 ); assert( m_bitsWritten + bytes * 8 &lt;= m_numBits ); assert( ( m_bitsWritten % 32 ) == 0 || ( m_bitsWritten % 32 ) == 8|| ( m_bitsWritten % 32 ) == 16 || ( m_bitsWritten % 32 ) == 24 ); int headBytes = ( 4 - ( m_bitsWritten % 32 ) / 8 ) % 4; if ( headBytes &gt; bytes ) headBytes = bytes; for ( int i = 0; i &lt; headBytes; ++i ) WriteBits( data[i], 8 ); if ( headBytes == bytes ) return; assert( GetAlignBits() == 0 ); int numWords = ( bytes - headBytes ) / 4; if ( numWords &gt; 0 ) { assert( ( m_bitsWritten % 32 ) == 0 ); memcpy( &amp;m_data[m_wordIndex], data+headBytes, numWords*4 ); m_bitsWritten += numWords * 32; m_wordIndex += numWords; m_scratch = 0; } assert( GetAlignBits() == 0 ); int tailStart = headBytes + numWords * 4; int tailBytes = bytes - tailStart; assert( tailBytes &gt;= 0 &amp;&amp; tailBytes &lt; 4 ); for ( int i = 0; i &lt; tailBytes; ++i ) WriteBits( data[tailStart+i], 8 ); assert( GetAlignBits() == 0 ); assert( headBytes + numWords * 4 + tailBytes == bytes );} void ReadBytes( uint8_t* data, int bytes ){ assert( GetAlignBits() == 0 ); assert( m_bitsRead + bytes * 8 &lt;= m_numBits ); assert( ( m_bitsRead % 32 ) == 0 || ( m_bitsRead % 32 ) == 8 || ( m_bitsRead % 32 ) == 16 || ( m_bitsRead % 32 ) == 24 ); int headBytes = ( 4 - ( m_bitsRead % 32 ) / 8 ) % 4; if ( headBytes &gt; bytes ) headBytes = bytes; for ( int i = 0; i &lt; headBytes; ++i ) data[i] = ReadBits( 8 ); if ( headBytes == bytes ) return; assert( GetAlignBits() == 0 ); int numWords = ( bytes - headBytes ) / 4; if ( numWords &gt; 0 ) { assert( ( m_bitsRead % 32 ) == 0 ); memcpy( data + headBytes, &amp;m_data[m_wordIndex], numWords * 4 ); m_bitsRead += numWords * 32; m_wordIndex += numWords; m_scratchBits = 0; } assert( GetAlignBits() == 0 ); int tailStart = headBytes + numWords * 4; int tailBytes = bytes - tailStart; assert( tailBytes &gt;= 0 &amp;&amp; tailBytes &lt; 4 ); for ( int i = 0; i &lt; tailBytes; ++i ) data[tailStart+i] = ReadBits( 8 ); assert( GetAlignBits() == 0 ); assert( headBytes + numWords * 4 + tailBytes == bytes );} template &lt;typename Stream&gt;bool serialize_bytes_internal( Stream &amp; stream, uint8_t* data, int bytes ){ return stream.SerializeBytes( data, bytes );} #define serialize_bytes( stream, data, bytes) \ do \ { \ if ( !serialize_bytes_internal( stream, data, bytes ) ) \ return false; \ } while(0) 现在，我们可以通过先序列化字符串长度然后序列化字符串数据的方法来序列化一个字符串：?123456789101112131415161718192021222324template &lt;typename Stream&gt;bool serialize_string_internal(Stream &amp; stream, char* string, int buffer_size ){ uint32_t length; if ( Stream::IsWriting ) { length = strlen( string ); assert( length &lt; buffer_size - 1 ); } serialize_int( stream, length, 0, buffer_size - 1 ); serialize_bytes( stream, (uint8_t*)string, length ); if ( Stream::IsReading ) string[length] = '\0';} #define serialize_string( stream, string, buffer_size) \do \{ \ if ( !serialize_string_internal(stream, \ string,buffer_size ) ) \ return false; \} while (0) 正如你看到的那样，可以从基本元素的序列化开始构建一个相当复杂的序列化体系。序列化数组的子集当实现一个游戏网络协议的时候，或早或晚总会需要序列化一个对象数组然后在网络上传递。比如说服务器也许需要把所有的物体发送给客户端，或者有时候需要发送一组事件或者消息。如果你要发送所有的物体到客户端，这是相当简单直观的，但是如果你只是想发送一个数组的一个子集怎么办？最先想到也是最容易的办法是遍历数组的所有物体然后序列化一个bool数组，这个bool数组标记的是对应的物体是否通过网络发送。如果bool值为1那么后面会跟着物体的数据，否则就会被忽略然后下一个物体的bool值取决于流的下一个值。?12345678910111213141516171819template &lt;typename Stream&gt;bool serialize_scene_a( Stream &amp; stream, Scene &amp; scene ){ for ( int i = 0; i &lt; MaxObjects; ++i ) { serialize_bool( stream, scene.objects[i].send ); if ( !scene.objects[i].send ) { if ( Stream::IsReading ) memset( &amp;scene.objects[i], 0, sizeof( Object ) ); continue; } serialize_object( stream, scene.objects[i] ); } return true;}但是如果物体的数组很大怎么办？举个例子，比如场景中有4000个物体。4000 / 8 = 500。光是标记物体是否发送的BOOL数组就要500个字节的开销，即使你只发送了一两个物体也是这样！这种方法。。。。不是太好。所以我们是否能够找到一种办法来让额外的开销正比于发送的物体数目而不是正比于数组中的物体数目？我们可以找到这么一个方法，但是现在我们已经做了一些有意思的事情。我们在序列化写入的时候遍历一个物体的集合（数组里面的所有物体）但是序列化读取的时候遍历的是一个不同的物体集合（发送物体数组的子集）。在这一点上统一的序列化函数概念就不能维系了。对于这种情况最好是把读取和写入分解成单独的函数：?123456789101112131415161718192021222324252627282930313233343536373839bool write_scene_b( protocol2::WriteStream &amp; stream, Scene &amp; scene ){ int num_objects_sent = 0; for ( int i = 0; i &lt; MaxObjects; ++i ) { if ( scene.objects[i].send ) num_objects_sent++; } write_int( stream, num_objects_sent, 0, MaxObjects ); for ( int i = 0; i &lt; MaxObjects; ++i ) { if ( !scene.objects[i].send ) continue; write_int( stream, i, 0, MaxObjects - 1 ); write_object( stream, scene.objects[i] ); } return true;} bool read_scene_b( protocol2::ReadStream &amp; stream, Scene &amp; scene ){ memset( &amp;scene, 0, sizeof( scene ) ); int num_objects_sent; read_int( stream, num_objects_sent, 0, MaxObjects ); for ( int i = 0; i &lt; num_objects_sent; ++i ) { int index; read_int( stream, index, 0, MaxObjects - 1 ); read_object( stream, scene.objects[index] ); } return true;}此外，你可以用发生变化的对象集合来生成一个单独的数据结构，并且针对发生变化的对象集合实现序列化。但是对每个你期望能够序列化的数据结构都产生C++代码对应的数据结构体是一件非常痛苦的事情。最终你可能想要同时遍历几个数据结构然后高效的将一个动态数据结构写入比特流。这在写一些更高级的序列化方法比如增量编码的时候是一种非常平常的做法。只要你采用了这种做法，统一序列化这种做法就不再有什么意义。我对此的建议是如果任何时候你想这么做，那么请不要担心，就把序列化读取和序列化写入分开好了。将序列化读取和序列化写入统一起来是一种非常简单的方式，但是这种方式带来的简单易用与序列化写入时动态生成数据结构的痛苦相比是不划算的。我的经验是复杂的序列化功能有时候可能会需要单独的序列化读取和序列化写入功能，但是如果可能的话，尽量让具体的序列化函数是统一读取和写入的（举个例子来说，实际的物体和事件无论何时序列化都尽量保持序列化读取和序列化写入是统一的）。多说一点。上面的代码在一次序列化写入的时候对物体集合进行了两次遍历。一次遍历用来确定发生变化的物体数目，第二次遍历用来对发生变化的物体集合进行实际的序列化。我们是否能只用一次遍历就能处理好发生变化的物体集合的序列化？当然可以！你可以使用另外一个技巧，用一个哨兵值（sentinel value）来标记数组的结尾位置，而不是一直序列化数组中的物体直到遇到#。使用这种方法你可以在发送的时候只遍历整个数组一遍，当没有更多物体需要发送的时候，就把哨兵值序列化进数据包以表示数组结束了：?1234567891011121314151617181920212223242526272829bool write_scene_c( protocol2::WriteStream &amp; stream, Scene &amp; scene ){ for ( int i = 0; i &lt; MaxObjects; ++i ) { if ( !scene.objects[i].send ) continue; write_int( stream, i, 0, MaxObjects ); write_object( stream, scene.objects[i] ); } write_int( stream, MaxObjects, 0, MaxObjects ); return true;} bool read_scene_c( protocol2::ReadStream &amp; stream, Scene &amp; scene ){ memset( &amp;scene, 0, sizeof( scene ) ); while ( true ) { int index; read_int( stream, index, 0, MaxObjects ); if ( index == MaxObjects ) break; read_object( stream, scene.objects[index] ); } return true;} 这种做法非常的简单，并且在发送的物体集合相比较全部物体集合比例非常小的时候工作的很棒。但是如果有大量的物体需要发送，举个例子来说，整个场景中有4000个物体，有一半的物体也就是2000个需要通过网络进行发送。每个物体需要一个序号，那么就需要2000个序号，每个序号需要12比特。。。。这就是说数据包里面24000比特或者说接近30000比特（几乎是30000，不是严格是，译注：原文如此）的数据被序号浪费掉了。可以把序号的编码方式修改下来节省数据，序号不再是全局序号，而是相对上一个物体的相对序号。想下这个问题，我们从左到右遍历一个数组，所以数组中物体的序号从0开始并且逐步增大到MaxObjects – 1。从统计学的角度来说，要发送的物体有可能是挨着很近的，这样下一个序号可能就是+1或者+10再或者是+30这样的小数字，因为我们这里用的序号是相对上一个发送的物体的，所以数字从统计意义上来说都会比较小，所以平均来讲，相比较之前的解决方案你可能需要更少的比特来表示物体的序号。（其实最差情况下我们所需的比特位也只是和前一个方案相同而已，可以证明每个序号，后一方案都比前一方案的要小，那么每个序号花费的比特位无疑不会更多，但是这种方案的主要问题在于健壮性，需要确保关于数据集合的数据包中间都不能丢，一旦中间某个包被丢掉了，那么后面的解析就完全乱掉了，实现起来更加困难一些)。下面就是这么一种编码物体序号的方式，每个序号都是相对上一个物体序号而言的，不再是全局序号，从统计的角度来讲它们会消耗更少的比特位（但是如果非常大的集合，但是发送的数组所占的比例很小，那么两种方法的差异其实是比较小的）：?123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133template &lt;typename Stream&gt;bool serialize_object_index_internal( Stream &amp; stream, int &amp; previous, int &amp; current ){ uint32_t difference; if ( Stream::IsWriting ) { assert( previous &lt; current ); difference = current - previous; assert( difference &gt; 0 ); } // +1 (1 bit) bool plusOne; if ( Stream::IsWriting ) plusOne = difference == 1; serialize_bool( stream, plusOne ); if ( plusOne ) { if ( Stream::IsReading ) current = previous + 1; previous = current; return true; } // [+2,5] -&gt; [0,3] (2 bits) bool twoBits; if ( Stream::IsWriting ) twoBits = difference &lt;= 5; serialize_bool( stream, twoBits ); if ( twoBits ) { serialize_int( stream, difference, 2, 5 ); if ( Stream::IsReading ) current = previous + difference; previous = current; return true; } // [6,13] -&gt; [0,7] (3 bits) bool threeBits; if ( Stream::IsWriting ) threeBits = difference &lt;= 13; serialize_bool( stream, threeBits ); if ( threeBits ) { serialize_int( stream, difference, 6, 13 ); if ( Stream::IsReading ) current = previous + difference; previous = current; return true; } // [14,29] -&gt; [0,15] (4 bits) bool fourBits; if ( Stream::IsWriting ) fourBits = difference &lt;= 29; serialize_bool( stream, fourBits ); if ( fourBits ) { serialize_int( stream, difference, 14, 29 ); if ( Stream::IsReading ) current = previous + difference; previous = current; return true; } //[30,61] -&gt; [0,31] (5 bits) bool fiveBits; if ( Stream::IsWriting ) fiveBits = difference &lt;= 61; serialize_bool( stream, fiveBits ); if ( fiveBits ) { serialize_int( stream, difference, 30, 61 ); if ( Stream::IsReading ) current = previous + difference; previous = current; return true; } // [62,125] -&gt; [0,63] (6 bits) bool sixBits; if ( Stream::IsWriting ) sixBits = difference &lt;= 125; serialize_bool( stream, sixBits ); if ( sixBits ) { serialize_int( stream, difference, 62, 125 ); if ( Stream::IsReading ) current = previous + difference; previous = current; return true; } // [126,MaxObjects+1] serialize_int( stream, difference, 126, MaxObjects + 1 ); if ( Stream::IsReading ) current = previous + difference; previous = current; return true;} template &lt;typename Stream&gt;bool serialize_scene_d( Stream &amp; stream, Scene &amp; scene ){ int previous_index = -1; if ( Stream::IsWriting ) { for ( int i = 0; i &lt; MaxObjects; ++i ) { if ( !scene.objects[i].send ) continue; write_object_index( stream, previous_index, i ); write_object( stream, scene.objects[i] ); } write_object_index( stream, previous_index, MaxObjects ); } else { while ( true ) { int index; read_object_index( stream, previous_index, index ); if ( index == MaxObjects ) break; read_object( stream, scene.objects[index] ); } } return true;}通常情况下，这将节省大量的带宽，因为要发送的物体的序号往往倾向于聚在一起。在这种情况下，如果下一个物体也要被发送，那么它的的序号就是+1只需要一个比特大小。如果是+2到+5的情况每个序号需要5个比特。平均下来序号所占的数据大小方面可以降低2-3倍。但是要注意的是如果是间隔序号比较大的序号的消耗将比不相关序号编码方案（每个序号都是占12比特空间）要大。这看上去非常糟糕，但是实际上并不会这么差，试想一下，即使你遇到了“最差情况”（要发送的物体的序号间隔均匀都是相差128），那么在一个4000物体的大数组里面你实际才发送几个物体？只有32个而已，所以不用担心这个问题! 协议ID和CRC32和序列化检测 阅读到这里，你可能会有一个疑惑。“哦哦哦，整个体系看上去非常脆弱啊，只有一个完全不带任何属性信息的二进制流。流里面只有一个个数据协议。你该怎么对这些信息进行反序列化读取和写入？如果某些人发送一些包含随机信息的数据包给你的服务器。你会不会在解析的时候把服务器弄崩溃掉？”确实大部分游戏服务器就是这样工作的，但是我有个好消息告诉你和其他之前是这么做服务器的人，存在这样的技术可以减少或者几乎杜绝由于序列化层传过来的数据导致的崩溃可能性。第一种技术是在你的数据包里面包含协议ID。一般典型的做法是，头4个字节你可以设定一些比较罕见而且独特的值，比如0x12345678，反正是这种其他人不会想着去使用的值就好了。但是说真的，把你的序列ID和协议版本的数字用散列得到一个散列值放到每个数据包的前面32比特的位置，这种方法真的工作的很好。至少如果是其他应用程序的数据包发送到了你的端口（要记住，UDP的数据包可以从任何IP任何端口在任何时间发送过来），你可以通过这３２比特的数据判断出来根本就不是你的应用程序的包，然后就可以直接丢弃了。 12[protocol id] (32bits)(packet data) 下一个级别的防护是对你的数据包整体做一个CRC32的校验，并把这个校验码放到数据包的包头。这可以让你在接收的时候偶然会放过一些错误的数据包进来处理（这确实是会发生，IP的校验和是16位的，所以一堆东西不会使用16位的校验和。。。其实是通过协议ID来避免这种小概率事件的）。现在你的数据包头文件看起来像下面这样： 12345[protocol id](32bits)[crc32](32bits)(packet data) 如果你按着这个顺序做下来的话，现在你可能会有点畏惧。”请等一下，我需要为每个数据包花费8个额外的字节来实现我自己的校验和以及协议ID么？“事实上，你可以不这么做。你可以学习下看看IPv4是如何进行校验的，并让协议ID变成一个魔术前缀(Magical Prefix)。也就是说你可以不发送这个协议ID，但是发送方和接收方提前确认过这个协议ID是什么，并在计算数据包CRC32值的时候装作这个数据包带上了这个协议ID的前缀来参与计算。这样如果发送方使用的协议ID与接收方不一致的时候，CRC32的校验就会失败，这将为每个数据包节省4个字节： 12345[protocol id] (32bits) // not actually sent, but used to calc crc32[crc32](32bits)(packet data) 当然，CRC32只是防止有些随机的数据包误打误撞的情况，但是对于可以轻易修改或者构建恶意数据包的头4个字节以便修正CRC32值的那些恶意发送者来说它起不到什么防护作用。要防止那些恶意发送者，你需要使用一个保密性更好的密码哈希函数，同时还需要一个密钥，这个密钥最好是在客户端尝试登陆游戏服务器之前就通过HTTPS协议在客户端和服务器之间统一好(而且要确保每个客户端的密钥都不一样，只有服务器和对应的客户端才知道密钥是什么)。最后一项技术，也可能是最有效的阻止恶意发送者的技术了（虽然会导致数据包的加密和签名有很多冗余信息），这就是序列化检查（serialization check）。这个技术基本上来说是在包的中间，在一段复杂的序列化写入之前或者之后写上一个已知的32比特整数，并在另外一端序列化读取的时候用相同的值进行检测判断。如果序列化检查值是不正确的，那么就中止序列化读取并丢弃这个数据包。我喜欢在我的数据包每个部分之间写入一些序列化检查值，这样我至少知道我的数据包那部分已经被成功的序列化读取和写入（有些问题无论你如何努力避免都很难完全避免的）。我喜欢使用的另外一个很酷的技巧是在数据包的结尾序列化一个协议检查值，这非常非常的有用，因为它能够帮我判断是否遇到了数据包截断（非常像上一篇文章最后提到的臭名昭著的大端截断和小端截断，在开发的时候也是很让人头疼的地方）。所以现在网络包看起来应该是像这样： 1234567[protocol id] (32bits) // not actually sent, but used to calc crc32[crc32](32bits)(packet data)[end of packet serialize check] (32 bits) 如果你喜欢的话，你可以把这些协议编译出来然后在你的发布版本中检查这些数据包的内容，特别是在有非常棒的数据包加密和数据包签名支持的情况下，不过不编译也没关系，反正不再需要它们了。下一篇预告：数据包的分包和重组请继续阅读这个系列的下一篇文章，在这篇文章里我将向大家介绍如何拓展本章中实现的网络协议来实现数据包的分包和重组以确保你的网络包的大小在MTU限制以下。MTU：最大传输单元，Maximum Transmission Unit，是指一种通信协议的某一层上面所能通过的最大数据包大，以字节为单位。最大传输单元这个参数通常与通信接口有关，比如网络接口卡、串口等。因为协议数据单元的包头和包尾的长度是固定的，MTU越大，则一个协议数据单元的承载的有效数据就越长，通信效率也越高。MTU越大，传送相同的用户数据所需的数据包个数也越低。MTU也不是越大越好，因为MTU越大， 传送一个数据包的延迟也越大；并且MTU越大，数据包中 bit位发生错误的概率也越大。MTU越大，通信效率越高而传输延迟增大，所以要权衡通信效率和传输延迟选择合适的MTU。以以太网传送IPv4报文为例。MTU表示的长度包含IP包头的长度，如果IP层以上的协议层发送的数据报文的长度超过了MTU，则在发送者的IP层将对数据报文进行分片，在接收者的IP层对接收到的分片进行重组。 如果你觉得这篇文章有价值的话，请在patreon上支持我的写作，这样我会写的更快。你可以在BSD 3.0许可下访问到这篇文章里面的代码。非常感谢你的支持！ 【版权声明】原文作者未做权利声明，视为共享知识产权进入公共领域，自动获得授权。 原文旧版本 Hi, I’mGlenn Fiedler and welcome to the second article in Building a GameNetwork Protocol.In the previous article we discussed different ways to read and write packets in multiplayer games. Wequickly shot down sending game state via text formats like XML and JSON becausethey’re really inefficient and decided to write own binary protocolinstead. We implemented a bitpacker so we don’t have to round boolsup to 8 bits, solved endianness issues, wrote words at a time instead of bytesand pretty much made the bitpacker as simple and as fast as possible withoutplatform specific tricks.Where weleft off we still had the following problems to solve:1. Weneed a way to check if integer values are outside the expected rangeand abort packet read because people will send malicious packets tryingto make us trash memory. The packet read abort must be automatic and notuse exceptions because they’re really slow.2. Separateread and write functions are a maintainance nightmare if those functions arecoded manually. We’d like to write the serialization code for a packet once but not pay any runtime cost (in terms of additional branching, virtuals and soon) when doing so.How can wedo this? Read on and I’ll show you how exactly I do it in C++. It’s taken awhile for me to develop and refine this technique so I hope you’ll find ituseful and at least a good alternative to consider vs. the way youcurrently do it or how you’ve seen it done in other game engines.Unified Packet Serialize FunctionLets startwith the goal. Here’s where we want to end up:struct PacketA{ int x,y,z; template &lt;typename Stream&gt; bool Serialize( Stream & stream ) { serialize_bits( stream, x, 32 ); serialize_bits( stream, y, 32 ); serialize_bits( stream, z, 32 ); return true; }}; struct PacketB{ int numElements; int elements[MaxElements]; template &lt;typename Stream&gt; bool Serialize( Stream & stream ) { serialize_int( stream, numElements, 0, MaxElements ); for ( int i = 0; i &lt; numElements; ++i ) serialize_bits( buffer, elements[i], 32 ); return true; }}; struct PacketC{ bool x; short y; int z; template &lt;typename Stream&gt; bool Serialize( Stream & stream ) { serialize_int( stream, x, 8 ); serialize_int( stream, y, 16 ); serialize_int( stream, z, 32 ); return true; }};Noticethere is a single serialize function per-packet struct instead of separate readand write functions. This is great! It halves the amount of serialization codeand now you have put in some serious effort in order to desync read andwrite.The trickto making this work efficiently is having thestream class templated in the serialize function. There are two stream types inmy system: ReadStream and WriteStream. Each class has the same set of methods,but otherwise are not related in any way. One class reads values in from a bitstream to variables, and the other writes variables values out to a bit stream.ReadStream and WriteStream are just wrappers on top of BitReader andBitWriter classes from the previous article.There areof course alternatives to this approach. If you dislike templates you couldhave a pure virtual base stream interface and implement that interfacewith read and write stream classes. But now you’re taking a virtualfunction for each serialize call. Seems like an excessive amount of overhead tome.Anotheroption is to have an uber-stream class that can be configured to act in read orwrite mode at runtime. This can be faster than the virtual functionmethod, but you still have to branch per-serialize call to decide if you shouldread or write so it’s not going to be as fast as hand-coded read and write.I preferthe templated method because it lets the compiler do the work of generatingoptimized read/write functions for you. You can even code serializefunctions like this and let the compiler optimize out a bunch of stuff whenspecializing read and write:structRigidBody{ vec3f position; quat3f orientation; vec3f linear_velocity; vec3f angular_velocity; template &lt;typename Stream&gt; bool Serialize( Stream & stream ) { serialize_vector( stream, position ); serialize_quaternion( stream, orientation ); bool at_rest = Stream::IsWriting ? velocity.length() == 0 : 1; serialize_bool( stream, at_rest ); if ( !at_rest ) { serialize_vector( stream, linear_velocity ); serialize_vector( stream, angular_velocity ); } else if ( Stream::IsReading ) { linear_velocity = vec3f(0,0,0); angular_velocity = vec3f(0,0,0); } return true; }}; While thismay look inefficient, it’s actually not! The templatespecialization of this function optimizes out all of the branchesaccording to the stream type. Pretty neat huh?Bounds Checking and Abort ReadNow thatwe’ve twisted the compiler’s arm to generate optimized read/writefunctions, we need some way to automate error checking on readso we’re not vulnerable to malicious packets.The firststep is to pass in the range of the integer to the serialize functioninstead of just the number of bits required. Think about it. The serializefunction can work out the number of bits required from the min/max values:serialize_int(stream, numElements, 0, MaxElements );This opensup the interface to support easy serialization of signed integer quantities andthe serialize function can check the value read in from the networkand make sure it’s within the expected range. If the value is outsiderange, abort serialize read immediately and discard the packet.Since wecan’t use exceptions to handle this abort (too slow), here’s how I like to doit.In mysetup serialize_int is not actually a function, it’s a sneaky macro likethis:#defineserialize_int( stream, value, min, max) \ do \ { \ assert( min &lt; max); \ int32_tint32_value; \ if ( Stream::IsWriting) \ { \ assert( value &gt;= min); \ assert( value &lt;= max); \ int32_value = (int32_t)value; \ } \ if ( !stream.SerializeInteger( int32_value, min, max ) ) \ return false; \ if ( Stream::IsReading) \ { \ value =int32_value; \ if ( value &lt; min || value &gt; max) \ return false; \ } \ } while (0) The reasonI’m being a terrible person here is that I’m using the macro to insert codethat checks the result of SerializeInteger and returns false onerror. This gives you exception-like behavior in the sense that itunwinds the stack back to the top of the serialization callstack on error, butyou don’t pay anything like the cost of exceptions to do this. The branch tounwind is super uncommon (serialization errors are rare) so branchprediction should have no trouble at all.Anothercase where we need to abort is if the stream reads past the end. This is also arare branch but it’s one we do have to check on each serialization operationbecause reading past the end is undefined. If we fail to do this check, weexpose ourselves to infinite loops as we read past the end of the buffer.While it’s common to return 0 values when reading past the end of a bit stream(as per-the previous article) there is no guarantee that reading zerovalues will always result in the serialize function terminating correctlyif it has loops. This overflow check is necessary for well defined behavior.One finalpoint. On serialize write I don’t do any abort on range checksor write past the end of the stream. You can be a lot more relaxed on thewrite since if anything goes wrong it’s pretty much guaranteed to be yourfault. Just assert that everything is as expected (in range, not past theend of stream) for each serialize write and you’re good to go.Serializing Floats and VectorsThe bitstream only serializes integer values. How can we serialize a float value?Seemstrickly but it’s not actually. A floating point number stored inmemory is just a 32 bit value like any other. Your computer doesn’t knowif a 32 bit word in memory is an integer, a floating point value or partof a string. It’s just a 32 bit value. Luckily, the C++ language (unlikea few others) lets us work with this fundamental property.You canaccess the integer value behind a floating point number with a union:union FloatInt{ float float_value; uint32_t int_value;}; FloatInt tmp;tmp.float_value= 10.0f;printf("float value as an integer: %x\n", tmp.int_value ); You canalso do it via an aliased uint32_t* pointer, but I’ve experienced thisbreak with GCC -O2, so I prefer the union trick instead. Friends of minepoint out (likely correctly) that the only truly standard way to get the float as an integer is to cast a pointer to the float value touint8_t* and reconstruct the integer value from the four bytevalues accessed individually through the byte pointer. Seems a prettydumb way to do it to me though. Ladies and gentlemen… C++!Meanwhilein the past 5 years I’ve had no actual problems in the field with the uniontrick. Here’s how I serialize an uncompressed float value:template &lt;typename Stream&gt; boolserialize_float_internal( Stream & stream, float & value ){ union FloatInt { float float_value; uint32_t int_value; }; FloatInt tmp; if ( Stream::IsWriting ) tmp.float_value = value; bool result = stream.SerializeBits( tmp.int_value, 32 ); if ( Stream::IsReading ) value = tmp.float_value; return result;}Wrap thiswith a serialize_float macro for convenient error checking on read:#define serialize_float( stream, value) \ do \ { \ if ( !protocol2::serialize_float_internal( stream, value )) \ return false; \ } while(0)Sometimesyou don’t want to transmit a full precision float. How can you compress a floatvalue? The first step is to bound that value in some known rangethen quantize it down to an integer representation.Forexample, if you know a floating point number in is range [-10,+10] and anacceptable resolution for that value is 0.01, then you can just multiply thatfloating point number by 100.0 to get it in the range [-1000,+1000] andserialize that as an integer over the network. On the other side, justdivide by 100.0 to get back to the floating point value.Here is ageneralized version of this concept:template &lt;typename Stream&gt; boolserialize_compressed_float_internal( Stream & stream, float & value, float min, float max, float res ){ const float delta = max - min; const float values = delta / res; const uint32_t maxIntegerValue = (uint32_t) ceil( values ); const int bits = bits_required( 0, maxIntegerValue ); uint32_t integerValue = 0; if ( Stream::IsWriting ) { float normalizedValue = clamp( ( value - min ) / delta, 0.0f, 1.0f ); integerValue = (uint32_t) floor( normalizedValue * maxIntegerValue + 0.5f ); } if ( !stream.SerializeBits( integerValue, bits ) ) return false; if ( Stream::IsReading ) { const float normalizedValue = integerValue / float( maxIntegerValue ); value = normalizedValue * delta + min; } return true;}Once youcan serialize float values it’s trivial extend to serialize vectorsand quaternions over the network. I use a modified version of the awesome vectorial library for vector math in my projects and I implement serialization for thosetypes like this:template &lt;typename Stream&gt; boolserialize_vector_internal( Stream & stream, vec3f & vector ){ float values[3]; if ( Stream::IsWriting ) vector.store( values ); serialize_float( stream, values[0] ); serialize_float( stream, values[1] ); serialize_float( stream, values[2] ); if ( Stream::IsReading ) vector.load( values ); return true;} template &lt;typename Stream&gt; boolserialize_quaternion_internal( Stream & stream, quat4f & quaternion ){ float values[4]; if ( Stream::IsWriting ) quaternion.store( values ); serialize_float( stream, values[0] ); serialize_float( stream, values[1] ); serialize_float( stream, values[2] ); serialize_float( stream, values[3] ); if ( Stream::IsReading ) quaternion.load( values ); return true;} #defineserialize_vector( stream, value) \ do \ { \ if ( !serialize_vector_internal( stream, value )) \ return false; \ } \ while(0) #defineserialize_quaternion( stream, value) \ do \ { \ if ( !serialize_quaternion_internal( stream, value ) ) \ return false; \ } \ while(0)If youknow your vector is bounded in some range, you can compress it like this:template &lt;typename Stream&gt; boolserialize_compressed_vector_internal( Stream & stream, vec3f & vector, float min, float max, float res ){ float values[3]; if ( Stream::IsWriting ) vector.store( values ); serialize_compressed_float( stream, values[0], min, max, res ); serialize_compressed_float( stream, values[1], min, max, res ); serialize_compressed_float( stream, values[2], min, max, res ); if ( Stream::IsReading ) vector.load( values ); return true;}If youwant to compress an orientation over the network, don’t just compress it as avector with 8.8.8.8 bounded in the range [-1,+1]. You can do much better if youuse the smallest three representation of the quaternion. See the sample code for this article for an implementation.Serializing Strings and ArraysWhat ifyou want to serialize a string over the network?Is it agood idea to send a string over the network with null termination? I don’tthink so. You’re just asking for trouble! Instead, treat the string as an arrayof bytes with length prefixed. So, in order to send a string over thenetwork, we have to work out how to efficiently send an array of bytes.Firstobservation: why waste effort bitpacking an array of bytes into your bit streamjust so they are randomly shifted by shifted by [0,7] bits? Why not just alignto byte before writing the array, so the array data sits in the packetnicely aligned, each byte of the array corresponding to an actual byte in thepacket. You lose only [0,7] bits for each array of bytes serialized,depending on the alignment, but that’s nothing to be too concerned about in myopinion.How toalign the bit stream to byte? Just work out your current bit index in thestream and how many bits are left to write until the current bit number in thebit stream divides evenly into 8, then insert that number of paddingbits. For bonus points, pad up with zero bits to add entropy so that onread you can verify that yes, you are reading a byte align and yes, it isindeed padded up with zero bits to the next whole byte bit index. If a non-zerobit is discovered in the pad bits, abort serialize read and discard thepacket.Here’s mycode to align a bit stream to byte:void BitWriter::WriteAlign(){ const int remainderBits = m_bitsWritten % 8; if ( remainderBits != 0 ) { uint32_t zero = 0; WriteBits( zero, 8 - remainderBits ); assert( ( m_bitsWritten % 8 ) == 0 ); }} bool BitReader::ReadAlign(){ const int remainderBits = m_bitsRead % 8; if ( remainderBits != 0 ) { uint32_t value = ReadBits( 8 - remainderBits ); assert( m_bitsRead % 8 == 0 ); if ( value != 0 ) return false; } return true;} #define serialize_align( stream) \ do \ { \ if ( !stream.SerializeAlign() ) \ return false; \ } while(0)Now we canuse this align operation to write an array of bytes into the bit streamefficiently: since we are aligned to bytes we can do most of the workusing memcpy. The only wrinkle is because the bit reader and bit writer work atthe word level, so it’s neccessary to have special code to handle the head andtail portion of the byte array, to make sure any previous scratch bits areflushed to memory at the head, and the scratch is properly setup for the nextbytes after the array in the tail section. void BitWriter::WriteBytes( const uint8_t* data, int bytes ){ assert( GetAlignBits() == 0 ); assert( m_bitsWritten + bytes * 8 &lt;= m_numBits ); assert( ( m_bitsWritten % 32 ) == 0 || ( m_bitsWritten % 32 ) == 8|| ( m_bitsWritten % 32 ) == 16 || ( m_bitsWritten % 32 ) == 24 ); int headBytes = ( 4 - ( m_bitsWritten % 32 ) / 8 ) % 4; if ( headBytes &gt; bytes ) headBytes = bytes; for ( int i = 0; i &lt; headBytes; ++i ) WriteBits( data[i], 8 ); if ( headBytes == bytes ) return; assert( GetAlignBits() == 0 ); int numWords = ( bytes - headBytes ) / 4; if ( numWords &gt; 0 ) { assert( ( m_bitsWritten % 32 ) == 0 ); memcpy( &m_data[m_wordIndex], data+headBytes, numWords*4 ); m_bitsWritten += numWords * 32; m_wordIndex += numWords; m_scratch = 0; } assert( GetAlignBits() == 0 ); int tailStart = headBytes + numWords * 4; int tailBytes = bytes - tailStart; assert( tailBytes &gt;= 0 && tailBytes &lt; 4 ); for ( int i = 0; i &lt; tailBytes; ++i ) WriteBits( data[tailStart+i], 8 ); assert( GetAlignBits() == 0 ); assert( headBytes + numWords * 4 + tailBytes == bytes );} void ReadBytes( uint8_t* data, int bytes ){ assert( GetAlignBits() == 0 ); assert( m_bitsRead + bytes * 8 &lt;= m_numBits ); assert( ( m_bitsRead % 32 ) == 0 || ( m_bitsRead % 32 ) == 8 || ( m_bitsRead % 32 ) == 16 || ( m_bitsRead % 32 ) == 24 ); int headBytes = ( 4 - ( m_bitsRead % 32 ) / 8 ) % 4; if ( headBytes &gt; bytes ) headBytes = bytes; for ( int i = 0; i &lt; headBytes; ++i ) data[i] = ReadBits( 8 ); if ( headBytes == bytes ) return; assert( GetAlignBits() == 0 ); int numWords = ( bytes - headBytes ) / 4; if ( numWords &gt; 0 ) { assert( ( m_bitsRead % 32 ) == 0 ); memcpy( data + headBytes, &m_data[m_wordIndex], numWords * 4 ); m_bitsRead += numWords * 32; m_wordIndex += numWords; m_scratchBits = 0; } assert( GetAlignBits() == 0 ); int tailStart = headBytes + numWords * 4; int tailBytes = bytes - tailStart; assert( tailBytes &gt;= 0 && tailBytes &lt; 4 ); for ( int i = 0; i &lt; tailBytes; ++i ) data[tailStart+i] = ReadBits( 8 ); assert( GetAlignBits() == 0 ); assert( headBytes + numWords * 4 + tailBytes == bytes );} template &lt;typename Stream&gt; bool serialize_bytes_internal( Stream & stream, uint8_t* data, int bytes ){ return stream.SerializeBytes( data, bytes );} #define serialize_bytes( stream, data, bytes) \ do \ { \ if ( !serialize_bytes_internal( stream, data, bytes ) ) \ return false; \ } while(0) Now we canserialize a string by by serializing its length followed by the stringdata:template &lt;typename Stream&gt; bool serialize_string_internal(Stream & stream, char* string, int buffer_size ){ uint32_t length; if ( Stream::IsWriting ) { length = strlen( string ); assert( length &lt; buffer_size - 1 ); } serialize_int( stream, length, 0, buffer_size - 1 ); serialize_bytes( stream, (uint8_t*)string, length ); if ( Stream::IsReading ) string[length] = '\0';} #define serialize_string( stream, string, buffer_size) \do \{ \ if ( !serialize_string_internal(stream, \ string,buffer_size ) ) \ return false; \} while (0) As you cansee, you can build up quite complicated serialization from basicprimitives.Serializing Array SubsetsWhenimplemeting a game network protocol, sooner or later you need to serialize anarray of objects over the network. Perhaps the server needs to send all objectsdown to the client, or an array of events or messages to be sent. This isfairly straightforward if you are sending all objects in the array downto the client, but what if you want to send only a subset of the array?The firstand simplest approach is to iterate across all objects in the array andserialize a bool per-object if that object is to be sent. If the value of thatbool 1 then the object data follows, otherwise it’s ommitted and the bool forthe next object is up next in the stream.template &lt;typename Stream&gt; bool serialize_scene_a( Stream & stream, Scene & scene ){ for ( int i = 0; i &lt; MaxObjects; ++i ) { serialize_bool( stream, scene.objects[i].send ); if ( !scene.objects[i].send ) { if ( Stream::IsReading ) memset( &scene.objects[i], 0, sizeof( Object ) ); continue; } serialize_object( stream, scene.objects[i] ); } return true;}But whatif the array of objects is very large, like 4000 objects in the scene? 4000 / 8= 500. Ruh roh. That’s an overhead of 500 bytes, even if you only send oneor two objects! That’s… not good. Can we switch it around so we take overheadpropertional to the number of objects sent instead of the total number ofobjects in the array?Wecan but now, we’ve done something interesting. We’re walking one set of objectsin the serialize write (all objects in the array) and are walking over adifferent set of objects in the serialize read (subset of objects sent). Atthis point the unified serialize function concept breaks down. It’s best toseparate the read and write back into separate functions in cases like this:bool write_scene_b( protocol2::WriteStream & stream, Scene & scene ){ int num_objects_sent = 0; for ( int i = 0; i &lt; MaxObjects; ++i ) { if ( scene.objects[i].send ) num_objects_sent++; } write_int( stream, num_objects_sent, 0, MaxObjects ); for ( int i = 0; i &lt; MaxObjects; ++i ) { if ( !scene.objects[i].send ) continue; write_int( stream, i, 0, MaxObjects - 1 ); write_object( stream, scene.objects[i] ); } return true;} bool read_scene_b( protocol2::ReadStream & stream, Scene & scene ){ memset( &scene, 0, sizeof( scene ) ); int num_objects_sent; read_int( stream, num_objects_sent, 0, MaxObjects ); for ( int i = 0; i &lt; num_objects_sent; ++i ) { int index; read_int( stream, index, 0, MaxObjects - 1 ); read_object( stream, scene.objects[index] ); } return true;} Alternativelyyou could generate a separate data structure with the set of changed objects,and implement a serialize for that array of changed objects. But having togenerate a C++ data structure for each data structure you want serialized is ahuge pain in the ass. Eventually you want to walk several datastructures at the same time and effectively write out a dynamic data structureto the bit stream. This is a really common thing to do when writing moreadvanced serialization methods like delta encoding. As soon as you do it thisway, unified serialize no longer makes sense.My adviceis that when you want to do this, don’t worry, just separate read and write.Unifying read and write are simply not worth the hassle when dynamicallygenerating a data structure on write. My rule of thumb is that complicatedserialization probably justifies separate read and writefunctions, but if possible, try to keep the leaf nodes unified if you can (eg.the actual objects / events, whatever being serialized).One morepoint. The code above walks over the set of objects twice onserialize write. Once to determine the number of changed objects and a secondtime to actually serialize the set of changed objects. Can we do it in onepass instead? Absolutely! You can use another trick, a sentinel value toindicate the end of the array, rather than serializing the # of objects in thearray up front. This way you can iterate over the array only once onsend, and when there are no more objects to send, serialize the sentinalvalue to indicate the end of the array:bool write_scene_c( protocol2::WriteStream & stream, Scene & scene ){ for ( int i = 0; i &lt; MaxObjects; ++i ) { if ( !scene.objects[i].send ) continue; write_int( stream, i, 0, MaxObjects ); write_object( stream, scene.objects[i] ); } write_int( stream, MaxObjects, 0, MaxObjects ); return true;} bool read_scene_c( protocol2::ReadStream & stream, Scene & scene ){ memset( &scene, 0, sizeof( scene ) ); while ( true ) { int index; read_int( stream, index, 0, MaxObjects ); if ( index == MaxObjects ) break; read_object( stream, scene.objects[index] ); } return true;} This ispretty simple and it works great if the set of objects sent is a smallpercentage of total objects. But what if a large number of objects are sent,lets say half of the 4000 objects in the scene. That’s 2000 object indices witheach index costing 12 bits… that’s 24000 bits or 3000 bytes (almost 3k!) inyour packet wasted indexing objects.You canreduce this by encoding each object index relative to the previous objectindex. Think about it, we’re walking left to right along an array, so objectindices start at 0 and go up to MaxObjects – 1. Statistically speaking, you’relikely to have objects that are close to each other and if the next index is +1or even +10 or +30 from the previous one, on average, you’ll need quite a fewless bits to represent that difference than youneed to represent an absolute index.Here’s oneway to encode the object index as an integer relative to the previous objectindex, while spending less bits on statistically more likely values (eg. smalldifferences between successive object indices, vs. large ones):template &lt;typename Stream&gt; bool serialize_object_index_internal( Stream & stream, int & previous, int & current ){ uint32_t difference; if ( Stream::IsWriting ) { assert( previous &lt; current ); difference = current - previous; assert( difference &gt; 0 ); } // +1 (1 bit) bool plusOne; if ( Stream::IsWriting ) plusOne = difference == 1; serialize_bool( stream, plusOne ); if ( plusOne ) { if ( Stream::IsReading ) current = previous + 1; previous = current; return true; } // [+2,5] -&gt; [0,3] (2 bits) bool twoBits; if ( Stream::IsWriting ) twoBits = difference &lt;= 5; serialize_bool( stream, twoBits ); if ( twoBits ) { serialize_int( stream, difference, 2, 5 ); if ( Stream::IsReading ) current = previous + difference; previous = current; return true; } // [6,13] -&gt; [0,7] (3 bits) bool threeBits; if ( Stream::IsWriting ) threeBits = difference &lt;= 13; serialize_bool( stream, threeBits ); if ( threeBits ) { serialize_int( stream, difference, 6, 13 ); if ( Stream::IsReading ) current = previous + difference; previous = current; return true; } // [14,29] -&gt; [0,15] (4 bits) bool fourBits; if ( Stream::IsWriting ) fourBits = difference &lt;= 29; serialize_bool( stream, fourBits ); if ( fourBits ) { serialize_int( stream, difference, 14, 29 ); if ( Stream::IsReading ) current = previous + difference; previous = current; return true; } //[30,61] -&gt; [0,31] (5 bits) bool fiveBits; if ( Stream::IsWriting ) fiveBits = difference &lt;= 61; serialize_bool( stream, fiveBits ); if ( fiveBits ) { serialize_int( stream, difference, 30, 61 ); if ( Stream::IsReading ) current = previous + difference; previous = current; return true; } // [62,125] -&gt; [0,63] (6 bits) bool sixBits; if ( Stream::IsWriting ) sixBits = difference &lt;= 125; serialize_bool( stream, sixBits ); if ( sixBits ) { serialize_int( stream, difference, 62, 125 ); if ( Stream::IsReading ) current = previous + difference; previous = current; return true; } // [126,MaxObjects+1] serialize_int( stream, difference, 126, MaxObjects + 1 ); if ( Stream::IsReading ) current = previous + difference; previous = current; return true;} template &lt;typename Stream&gt; bool serialize_scene_d( Stream & stream, Scene & scene ){ int previous_index = -1; if ( Stream::IsWriting ) { for ( int i = 0; i &lt; MaxObjects; ++i ) { if ( !scene.objects[i].send ) continue; write_object_index( stream, previous_index, i ); write_object( stream, scene.objects[i] ); } write_object_index( stream, previous_index, MaxObjects ); } else { while ( true ) { int index; read_object_index( stream, previous_index, index ); if ( index == MaxObjects ) break; read_object( stream, scene.objects[index] ); } } return true;} In thecommon case this saves a bunch of bandwidth because object indices tend to beclustered together. In the case where the next object is sent, that’s just onebit for the next index being +1 and 5 bits per-index for +2 to +5. On averagethis gives somewhere between a 2-3X reduction in indexing overhead. But noticethat larger indices far apart cost a lot more for each index than thenon-relative encoding (12 bits per index). This seems bad but it’snot because think about it, even if you hit the ‘worst case’ (objectsindices spaced apart evenly with by +128 apart) how many of these can youactually fit into an object array 4000 large? Just 32. No worries!Protocol IDs, CRC32 and Serialization ChecksAt thispoint you may wonder. Wow. This whole thing seems reallyfragile. It’s a totally unattributed binary stream. A stack of cards. Whatif you somehow desync read and write? What if somebody just sent packetscontaining random bytes to your server. How long until you hit a sequence ofbytes that crashes you out?I havegood news for you and the rest of the game industry since most game serversbasically work this way. There are techniques you can use to reduce orvirtually eliminate the possibility of corrupt data getting past theserialization layer.The firsttechnique is to include a protocol id in your packet. Typically, the first 4bytes you can set to some reasonable rare and unique value, maybe 0x12345678because nobody else will ever think to use that. But seriously, put in a hashof your protocol id and your protocol version number in the first 32 bits ofeach packet and you’re doing pretty good. At least if a random packet gets sentto your port from some other application (remember UDP packets can come in fromany IP/port combination at any time) you can trivially discard it:[protocol id] (32bits)(packet data)The nextlevel of protection is to pass a CRC32 over your packet and include that in theheader. This lets you pick up corrupt packets (these do happen, rememberthat the IP checksum is just 16 bits, and a bunch of stuff will not get pickedup by a checksum of 16bits…). Now your packet header looks ilke this:[protocol id](32bits)[crc32](32bits)(packet data)At thispoint you may be wincing. Wait. I have to take 8 bytes of overhead per-packetjust to implement my own checksum and protocol id? Well actually, you don’t.You can take a leaf out of how IPv4 does their checksum, and make the protocolid a magical prefix. eg: you don’t actually send it, but if bothsender and receiver knows the protocol id and the CRC32 iscalculated as if the packet were prefixed by the protocol id, the CRC32will be incorrect if the sender does not have the same protocol id as thereceiver, saving 4 bytes per-packet:[protocol id] (32bits) // not actually sent, but used to calc crc32[crc32](32bits)(packet data)Of courseCRC32 is only protection against random packet correction, and is no actualprotection against a malicious sender who can easily modify or construct amalicious packet and then properly adjust the CRC32 in the first four bytes. Toprotect against this you need to use a more cryptographically secure hashfunction combined with a secret key perhaps exchanged between client andserver over HTTPS by the matchmaker prior to the client attempting to connectto the game server (different key for each client, known only by the server andthat particular client).One finaltechnique, perhaps as much a check against programmer error on your part andmalicious senders (although redundant once you encrypt and sign your packet) isthe serialization check. Basically, somewhere mid-packet, either beforeor after a complicated serialization section, just write out a known 32 bitinteger value, and check that it reads back in on the other side with the samevalue. If the serialize check value is incorrect abort read and discardthe packet.I like todo this between sections of my packet as I write them, so at least I know whichpart of my packet serialization has desynced read and write as I’m developingmy protocol (it’s going to happen no matter how hard you try to avoid it…).Another cool trick I like to use is to serialize a protocol check at the veryend of the packet, this is super, super useful because it helps pick up packettruncations (like the infamous, little endian vs. big endian truncation of thelast word from the previous article).So now thepacket looks something like this:[protocol id] (32bits) // not actually sent, but used to calc crc32[crc32](32bits)(packet data)[end of packetserialize check] (32 bits)You canjust compile these protocol checks out in your retail build if you like,especially if you have a good encryption and packet signature, as they shouldno longer be necessary.Up next: Packet Fragmentation and ReassemblyRead onfor the next article in this series where I show you how to extend your networkprotocol to perform packet fragmentation and reassembly so you can keep yourpacket payload under MTU.Pleasesupport my writing on patreon, and I’llwrite new articles faster, plus you get access to example source code for thisarticle under BSD 3.0 licence. Thanks for your support!]]></content>
      <categories>
        <category>GS</category>
      </categories>
      <tags>
        <tag>GafferOnGames</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[构建游戏网络协议一之数据包的读取和写入]]></title>
    <url>%2Fblog%2F2017%2F02%2F24%2Freading_and_writing_packets%2F</url>
    <content type="text"><![CDATA[自我总结这篇文章只是介绍, 之后的文章才是正题. 此篇文章大体介绍了 : 文本格式传输的低效率问题， 为了可读性而产生了太多冗余无用数据 为什么不用目前已经有了的库比如Protocol Buffers：因为我们不需要版本信息，也不需要什么跨语言的支持。所以让我们直接忽略掉这些功能并用我们自己的不带属性的二进制流进行代替，在这个过程中我们可以获得更多的控制性和灵活性 要注意大小端的问题 实现一个位打包器， 工作在32位或者64位的级别， 而不是是工作在字节这个级别。因为现代机器对这个长度进行了专门的优化而不应该像1985年那样在字节的级别对缓冲区进行处理。 要注意防止恶意数据包的问题 ： 我们需要实现一个方法来判断整数值是否超出预期范围，如果超出了就要中止网络包的读取和解析，因为会有一些不怀好意的人给我们发送恶意网络包希望我们的程序和内存崩溃掉。网络包的读取和解析的中止必须是自动化的，而且不能使用异常处理，因为异常处理太慢了会拖累我们的程序。 如果独立的读取和写入函数是手动编解码的，那么维护它们真的是一个噩梦。我们希望能够为包一次性的编写好序列化代码并且没有任何运行时的性能消耗（主要是额外的分支、虚化等等）。 我们为了不想自己手动检查各种可能会被攻击的地方， 需要实现检查自动化， 在下一篇文章 构建游戏网络协议二之序列化策略 里将会说。 . . . 原文原文出处 原文标题 : Reading and Writing Packets (Best practices for reading and writing packets) Introduction Hi, I’m Glenn Fiedler and welcome to Building a Game Network Protocol. In this article we&rsquo;re going to explore how AAA multiplayer games like first person shooters read and write packets. We&rsquo;ll start with text based formats then move into binary hand-coded binary formats and bitpacking. At the end of this article and the next, you should understand exactly how to implement your own packet read and write the same way the pros do it. Background Consider a web server. It listens for requests, does some work asynchronously and sends responses back to clients. It’s stateless and generally not real-time, although a fast response time is great. Web servers are most often IO bound. Game server are different. They&rsquo;re a headless version of the game running in the cloud. As such they are stateful and CPU bound. The traffic patterns are different too. Instead of infrequent request/response from tens of thousands of clients, a game server has far fewer clients, but processes a continuous stream of input packets sent from each client 60 times per-second, and broadcasts out the state of the world to clients 10, 20 or even 60 times per-second. And this state is huge. Thousands of objects with hundreds of properties each. Game network programmers spend a lot of their time optimizing exactly how this state is sent over the network with crazy bit-packing tricks, hand-coded binary formats and delta encoding. What would happen if we just encoded this world state as XML? &lt;world_update world_time="0.0"&gt; &lt;object id="1" class="player"&gt; &lt;property name="position" value="(0,0,0)"&lt;/property&gt; &lt;property name="orientation" value="(1,0,0,0)"&lt;/property&gt; &lt;property name="velocity" value="(10,0,0)"&lt;/property&gt; &lt;property name="health" value="100"&gt;&lt;/property&gt; &lt;property name="weapon" value="110"&gt;&lt;/property&gt; ... 100s more properties per-object ... &lt;/object&gt; &lt;object id="100" class="grunt"&gt; &lt;property name="position" value="(100,100,0)"&lt;/property&gt; &lt;property name="health" value="10"&lt;/property&gt; &lt;/object&gt; &lt;object id="110" class="weapon"&gt; &lt;property type="semi-automatic"&gt;&lt;/property&gt; &lt;property ammo_in_clip="8"&gt;&lt;/property&gt; &lt;property round_in_chamber="true"&gt;&lt;/property&gt; &lt;/object&gt; ... 1000s more objects ... &lt;/world_update&gt; Pretty verbose&hellip; it&rsquo;s hard to see how this would be practical for a large world. JSON is a bit more compact: { "world_time": 0.0, "objects": { 1: { "class": "player", "position": "(0,0,0)", "orientation": "(1,0,0,0)", "velocity": "(10,0,0)", "health": 100, "weapon": 110 } 100: { "class": "grunt", "position": "(100,100,0)", "health": 10 } 110: { "class": "weapon", "type: "semi-automatic" "ammo_in_clip": 8, "round_in_chamber": 1 } // etc... } } But it still suffers from the same problem: the description of the data is larger than the data itself. What if instead of fully describing the world state in each packet, we split it up into two parts? A schema that describes the set of object classes and properties per-class, sent only once when a client connects to the server. Data sent rapidly from server to client, which is encoded relative to the schema. The schema could look something like this: { "classes": { 0: "player" { "properties": { 0: { "name": "position", "type": "vec3f" } 1: { "name": "orientation", "type": "quat4f" } 2: { "name": "velocity", "type": "vec3f" } 3: { "name": "health", "type": "float" } 4: { "name": "weapon", "type": "object", } } } 1: "grunt": { "properties": { 0: { "name": "position", "type": "vec3f" } 1: { "name": "health", "type": "float" } } } 2: "weapon": { "properties": { 0: { "name": "type", "type": "enum", "enum_values": [ "revolver", "semi-automatic" ] } 1: { "name": "ammo_in_clip", "type": "integer", "range": "0..9", } 2: { "name": "round_in_chamber", "type": "integer", "range": "0..1" } } } } } The schema is quite big, but that&rsquo;s beside the point. It&rsquo;s sent only once, and now the client knows the set of classes in the game world and the number, name, type and range of properties per-class. With this knowledge we can make the rapidly sent portion of the world state much more compact: { "world_time": 0.0, "objects": { 1: [0,"(0,0,0)","(1,0,0,0)","(10,0,0)",100,110], 100: [1,"(100,100,0)",10], 110: [2,1,8,1] } } And we can compress it even further by switching to a custom text format: 0.0 1:0,0,0,0,1,0,0,0,10,0,0,100,110 100:1,100,100,0,10 110:2,1,8,1 As you can see, it’s much more about what you don’t send than what you do. The Inefficiencies of Text We’ve made good progress on our text format so far, moving from a highly attributed stream that fully describes the data (more description than actual data) to an unattributed text format that&rsquo;s an order of magnitude more efficient. But there are inherent inefficiencies when using text format for packets: We are most often sending data in the range A-Z, a-z and 0-1, plus a few other symbols. This wastes the remainder of the 0-255 range for each character sent. From an information theory standpoint, this is an inefficient encoding. The text representation of integer values are in the general case much less efficient than the binary format. For example, in text format the worst case unsigned 32 bit integer 4294967295 takes 10 bytes, but in binary format it takes just four. In text, even the smallest numbers in 0-9 range require at least one byte, but in binary, smaller values like 0, 11, 31, 100 can be sent with fewer than 8 bits if we know their range ahead of time. If an integer value is negative, you have to spend a whole byte on &rsquo;-&rsquo; to indicate that. Floating point numbers waste one byte specifying the decimal point. The text representation of numerical values are variable length: “5”, “12345”, “3.141593”. Because of this we need to spend one byte on a separator after each value so we know when it ends. Newlines &lsquo;\n&rsquo; or some other separator are required to distinguish between the set of variables belonging to one object and the next. When you have thousands of objects, this really adds up. In short, if we wish to optimize any further, it&rsquo;s necessary to switch to a binary format. Switching to a Binary Format In the web world there are some really great libraries that read and write binary formats like BJSON, Protocol Buffers, Flatbuffers, Thrift, Cap’n Proto and MsgPack. In manay cases, these libraries are great fit for building your game network protocol. But in the fast-paced world of first person shooters where efficiency is paramount, a hand-tuned binary protocol is still the gold standard. There are a few reasons for this. Web binary formats are designed for situations where versioning of data is extremely important. If you upgrade your backend, older clients should be able to keep talking to it with the old format. Data formats are also expected to be language agnostic. A backend written in Golang should be able to talk with a web client written in JavaScript and other server-side components written in Python or Java. Game servers are completely different beasts. The client and server are almost always written in the same language (C++), and versioning is much simpler. If a client with an incompatible version tries to connect, that connection is simply rejected. There&rsquo;s simply no need for compatibility across different versions. So if you don’t need versioning and you don’t need cross-language support what are the benefits for these libraries? Convenience. Ease of use. Not needing to worry about creating, testing and debugging your own binary format. But this convenience is offset by the fact that these libraries are less efficient and less flexible than a binary protocol we can roll ourselves. So while I encourage you to evaluate these libraries and see if they suit your needs, for the rest of this article, we&rsquo;re going to move forward with a custom binary protocol. Getting Started with a Binary Format One option for creating a custom binary protocol is to use the in-memory format of your data structures in C/C++ as the over-the-wire format. People often start here, so although I don’t recommend this approach, lets explore it for a while before we poke holes in it. First define the set of packets, typically as a union of structs: struct Packet { enum PacketTypeEnum { PACKET_A, PACKET_B, PACKET_C }; uint8_t packetType; union { struct PacketA { int x,y,z; } a; struct PacketB { int numElements; int elements[MaxElements]; } b; struct PacketC { bool x; short y; int z; } c; }; }; When writing the packet, set the first byte in the packet to the packet type number (0, 1 or 2). Then depending on the packet type, memcpy the appropriate union struct into the packet. On read do the reverse: read in the first byte, then according to the packet type, copy the packet data to the corresponding struct. It couldn’t get simpler. So why do most games avoid this approach? The first reason is that different compilers and platforms provide different packing of structs. If you go this route you’ll spend a lot of time with #pragma pack trying to make sure that different compilers and different platforms lay out the structures in memory exactly the same way. The next one is endianness. Most computers are mostly little endian these days but historically some architectures like PowerPC were big endian. If you need to support communication between little endian and big endian machines, the memcpy the struct in and out of the packet approach simply won’t work. At minimum you need to write a function to swap bytes between host and network byte order on read and write for each variable in your struct. There are other issues as well. If a struct contains pointers you can’t just serialize that value over the network and expect a valid pointer on the other side. Also, if you have variable sized structures, such as an array of 32 elements, but most of the time it’s empty or only has a few elements, it&rsquo;s wasteful to always send the array at worst case size. A better approach would support a variable length encoding that only sends the actual number of elements in the array. But ultimately, what really drives a stake into the heart of this approach is security. It’s a massive security risk to take data coming in over the network and trust it, and that&rsquo;s exactly what you do if you just copy a block of memory sent over the network into your struct. Wheee! What if somebody constructs a malicious PacketB and sends it to you with numElements = 0xFFFFFFFF? You should, no you must, at minimum do some sort of per-field checking that values are in range vs. blindly accepting what is sent to you. This is why the memcpy struct approach is rarely used in professional games. Read and Write Functions The next level of sophistication is read and write functions per-packet. Start with the following simple operations: void WriteInteger( Buffer &amp; buffer, uint32_t value ); void WriteShort( Buffer &amp; buffer, uint16_t value ); void WriteChar( Buffer &amp; buffer, uint8_t value ); uint32_t ReadInteger( Buffer &amp; buffer );uint16_t ReadShort( Buffer &amp; buffer );uint8_t ReadByte( Buffer &amp; buffer ); These operate on a structure which keeps track of the current position: struct Buffer { uint8_t * data; // pointer to buffer data int size; // size of buffer data (bytes) int index; // index of next byte to be read/written }; The write integer function looks something like this: void WriteInteger( Buffer &amp; buffer, uint32_t value ) { assert( buffer.index + 4 &lt;= size ); #ifdef BIG_ENDIAN *((uint32_t*)(buffer.data+buffer.index)) = bswap( value ); #else // #ifdef BIG_ENDIAN *((uint32_t*)(buffer.data+buffer.index)) = value; #endif // #ifdef BIG_ENDIAN buffer.index += 4; } And the read integer function looks like this: uint32_t ReadInteger( Buffer &amp; buffer ) { assert( buffer.index + 4 &lt;= size ); uint32_t value; #ifdef BIG_ENDIAN value = bswap( *((uint32_t*)(buffer.data+buffer.index)) ); #else // #ifdef BIG_ENDIAN value = *((uint32_t*)(buffer.data+buffer.index)); #endif // #ifdef BIG_ENDIAN buffer.index += 4; return value; } Now, instead of copying across packet data in and out of structs, we implement read and write functions for each packet type: struct PacketA { int x,y,z; void Write( Buffer &amp;amp; buffer ) { WriteInteger( buffer, x ); WriteInteger( buffer, y ); WriteInteger( buffer, z ); } void Read( Buffer &amp;amp; buffer ) { ReadInteger( buffer, x ); ReadInteger( buffer, y ); ReadInteger( buffer, z ); }}; struct PacketB{ int numElements; int elements[MaxElements]; void Write( Buffer &amp;amp; buffer ) { WriteInteger( buffer, numElements ); for ( int i = 0; i &amp;lt; numElements; ++i ) WriteInteger( buffer, elements[i] ); } void Read( Buffer &amp;amp; buffer ) { ReadInteger( buffer, numElements ); for ( int i = 0; i &amp;lt; numElements; ++i ) ReadInteger( buffer, elements[i] ); }}; struct PacketC{ bool x; short y; int z; void Write( Buffer &amp;amp; buffer ) { WriteByte( buffer, x ); WriteShort( buffer, y ); WriteInt( buffer, z ); } void Read( Buffer &amp;amp; buffer ) { ReadByte( buffer, x ); ReadShort( buffer, y ); ReadInt( buffer, z ); }}; When reading and writing packets, start the packet with a byte specifying the packet type via ReadByte/WriteByte, then according to the packet type, call the read/write on the corresponding packet struct in the union. Now we have a system that allows machines with different endianness to communicate and supports variable length encoding of elements. Bitpacking What if we have a value in the range [0,1000] we really only need 10 bits to represent all possible values. Wouldn&rsquo;t it be nice if we could write just 10 bits, instead of rounding up to 16? What about boolean values? It would be nice to send these as one bit instead of 8! One way to implement this is to manually organize your C++ structures into packed integers with bitfields and union tricks, such as grouping all bools together into one integer type via bitfield and serializing them as a group. But this is tedious and error prone and there’s no guarantee that different C++ compilers pack bitfields in memory exactly the same way. A much more flexible way that trades a small amount of CPU on packet read and write for convenience is a bitpacker. This is code that reads and writes non-multiples of 8 bits to a buffer. Writing Bits Many people write bitpackers that work at the byte level. This means they flush bytes to memory as they are filled. This is simpler to code, but the ideal is to read and write words at a time, because modern machines are optimized to work this way instead of farting across a buffer at byte level like it’s 1985. If you want to write 32 bits at a time, you&rsquo;ll need a scratch word twice that size, eg. uint64_t. The reason is that you need the top half for overflow. For example, if you have just written a value 30 bits long into the scratch buffer, then write another value that is 10 bits long you need somewhere to store 30 + 10 = 40 bits. uint64_t scratch; int scratch_bits; int word_index; uint32_t * buffer; When we start writing with the bitpacker, all these variables are cleared to zero except buffer which points to the start of the packet we are writing to. Because we&rsquo;re accessing this packet data at a word level, not byte level, make sure packet buffers lengths are a multiple of 4 bytes. Let’s say we want to write 3 bits followed by 10 bits, then 24. Our goal is to pack this tightly in the scratch buffer and flush that out to memory, 32 bits at a time. Note that 3 + 10 + 24 = 37. We have to handle this case where the total number of bits don’t evenly divide into 32. This is actually the common case. At the first step, write the 3 bits to scratch like this: xxx scratch_bits is now 3. Next, write 10 bits: yyyyyyyyyyxxx scratch_bits is now 13 (3+10). Next write 24 bits: zzzzzzzzzzzzzzzzzzzzzzzzyyyyyyyyyyxxx scratch_bits is now 37 (3+10+24). We’re straddling the 32 bit word boundary in our 64 bit scratch variable and have 5 bits in the upper 32 bits (overflow). Flush the lower 32 bits of scratch to memory, advance word_index by one, shift scratch right by 32 and subtract 32 from scratch_bits. scratch now looks like this: zzzzz We&rsquo;ve finished writing bits but we still have data in scratch that&rsquo;s not flushed to memory. For this data to be included in the packet we need to make sure to flush any remaining bits in scratch to memory at the end of writing. When we flush a word to memory it is converted to little endian byte order. To see why this is important consider what happens if we flush bytes to memory in big endian order: DCBA000E Since we fill bits in the word from right to left, the last byte in the packet E is actually on the right. If we try to send this buffer in a packet of 5 bytes (the actual amount of data we have to send) the packet catches 0 for the last byte instead of E. Ouch! But when we write to memory in little endian order, bytes are reversed back out in memory like this: ABCDE000 And we can write 5 bytes to the network and catch E at the end. Et voilà! Reading Bits To read the bitpacked data, start with the buffer sent over the network: ABCDE The bit reader has the following state: uint64_t scratch; int scratch_bits; int total_bits; int num_bits_read; int word_index; uint32_t * buffer; To start all variables are cleared to zero except total_bits which is set to the size of the packet as bytes * 8, and buffer which points to the start of the packet. The user requests a read of 3 bits. Since scratch_bits is zero, it’s time to read in the first word. Read in the word to scratch, shifted left by scratch_bits (0). Add 32 to scratch_bits. The value of scratch is now: zzzzzzzzzzzzzzzzzzzyyyyyyyyyyxxx Read off the low 3 bits, giving the expected value of: xxx Shift scratch to the right 3 bits and subtract 3 from scratch_bits: zzzzzzzzzzzzzzzzzzzyyyyyyyyyy Read off another 10 bits in the same way, giving the expected value of: yyyyyyyyyy Scratch now looks like: zzzzzzzzzzzzzzzzzzz The next read asks for 24 bits but scratch_bits is only 19 (=32-10-3). It’s time to read in the next word. Shifting the word in memory left by scratch_bits (19) and or it on top of scratch. Now we have all the bits necessary for z in scratch: zzzzzzzzzzzzzzzzzzzzzzzz Read off 24 bits and shift scratch right by 24. scratch is now all zeros. We&rsquo;re done! Beyond Bitpacking Reading and writing integer values into a packet by specifying the number of bits to read/write is not the most user friendly option. Consider this example: const int MaxElements = 32; struct PacketB{ int numElements; int elements[MaxElements]; void Write( BitWriter &amp;amp; writer ) { WriteBits( writer, numElements, 6 ); for ( int i = 0; i &amp;lt; numElements; ++i ) WriteBits( writer, elements[i] ); } void Read( BitReader &amp;amp; reader ) { ReadBits( reader, numElements, 6 ); for ( int i = 0; i &amp;lt; numElements; ++i ) ReadBits( reader, elements[i] ); }}; This code looks fine at first glance, but let’s assume that some time later you, or somebody else on your team, increases MaxElements from 32 to 200 but forget to update the number of bits required to 7（注意看`WriteBits( writer, numElements, 6 )的6， 现在需要7了`）. Now the high bit of numElements are being silently truncated on send. It&rsquo;s pretty hard to track something like this down after the fact. The simplest option is to just turn it around and define the maximum number of elements in terms of the number of bits sent: const int MaxElementBits = 7; const int MaxElements = ( 1 &lt;&lt; MaxElementBits ) - 1; Another option is to get fancy and work out the number of bits required at compile time: template &lt;uint32_t x&gt; struct PopCount { enum { a = x - ( ( x &gt;&gt; 1 ) &amp; 0x55555555 ), b = ( ( ( a &gt;&gt; 2 ) &amp; 0x33333333 ) + ( a &amp; 0x33333333 ) ), c = ( ( ( b &gt;&gt; 4 ) + b ) &amp; 0x0f0f0f0f ), d = c + ( c &gt;&gt; 8 ), e = d + ( d &gt;&gt; 16 ), result = e &amp; 0x0000003f }; }; template &lt;uint32_t x&gt; struct Log2{ enum { a = x | ( x &gt;&gt; 1 ), b = a | ( a &gt;&gt; 2 ), c = b | ( b &gt;&gt; 4 ), d = c | ( c &gt;&gt; 8 ), e = d | ( d &gt;&gt; 16 ), f = e &gt;&gt; 1, result = PopCount&lt;f&gt;::result };}; template &lt;int64_t min, int64_t max&gt; struct BitsRequired{ static const uint32_t result = ( min == max ) ? 0 : ( Log2&lt;uint32_t(max-min)&gt;::result + 1 );}; #define BITS_REQUIRED( min, max ) BitsRequired&lt;min,max&gt;::result Now you can’t mess up the number of bits, and you can specify non-power of two maximum values and it everything works out. const int MaxElements = 32; const int MaxElementBits = BITS_REQUIRED( 0, MaxElements ); But be careful when array sizes aren&rsquo;t a power of two! In the example above MaxElements is 32, so MaxElementBits is 6. This seems fine because all values in [0,32] fit in 6 bits. The problem is that there are additional values within 6 bits that are outside our array bounds: [33,63]. An attacker can use this to construct a malicious packet that corrupts memory! This leads to the inescapable conclusion that it’s not enough to just specify the number of bits required when reading and writing a value, we must also check that it is within the valid range: [min,max]. This way if a value is outside of the expected range we can detect that and abort read. I used to implement this using C++ exceptions, but when I profiled, I found it to be incredibly slow. In my experience, it’s much faster to take one of two approaches: set a flag on the bit reader that it should abort, or return false from read functions on failure. But now, in order to be completely safe on read you must to check for error on every read operation. const int MaxElements = 32; const int MaxElementBits = BITS_REQUIRED( 0, MaxElements ); struct PacketB{ int numElements; int elements[MaxElements]; void Write( BitWriter &amp;amp; writer ) { WriteBits( writer, numElements, MaxElementBits ); for ( int i = 0; i &amp;lt; numElements; ++i ) { WriteBits( writer, elements[i], 32 ); } } void Read( BitReader &amp;amp; reader ) { ReadBits( reader, numElements, MaxElementBits ); if ( numElements &amp;gt; MaxElements ) { reader.Abort(); return; } for ( int i = 0; i &amp;lt; numElements; ++i ) { if ( reader.IsOverflow() ) break; ReadBits( buffer, elements[i], 32 ); } }}; If you miss any of these checks, you expose yourself to buffer overflows and infinite loops when reading packets. Clearly you don’t want this to be a manual step when writing a packet read function. You want it to be automatic, just read the NEXT ARTICLE. 译文 译文出处 译者：陈敬凤（nunu） 审校：崔国军（飞扬971） 导论 大家好，我是格伦·菲德勒。欢迎大家阅读我新开的这个系列教程：《构建游戏网络协议》。 在这个系列文章中，我将完全从头开始为动作游戏（比如说第一人称射击游戏、近身格斗和实时多人在线战斗竞技场游戏都是动作游戏）基于UDP协议构建一个专业级别的游戏网络协议。我所使用的工具只包括我的Macbook Pro、 Sublime Text 3（这是一个很好用的编辑器，非常值得一试）、我信赖的C++编译器和一组UDP套接字。 我写这个系列文章的目的是分享在过去十年里我在这个领域学到的有关游戏网络方面的专业知识，因为似乎没有人写过这些方面的东西。所以其他的网络程序员到底是如何想如何做的，我想通过这个系列文章来进行一定的分享。如果你觉得这篇文章有价值的话，请在patreon上支持我的写作，这样我可以有机会来写更多的文章。如果你对我的工作进行支持和捐赠的话，你还可以访问到这个系列文章的示例源代码(这些源代码都是开源的,所以您可以使用任何你想用的地方甚至是商业内容上！)以及一个密码以便访问这个网站上还没有发表的一些文章。 关于我的情况，已经说得足够多了，现在让我们开始这个系列文章把！ 对数据包的读取和写入 你是否有想过多人在线游戏是如何读取和写入数据包的？ 如果你有web开发的背景话，你可能会使用XML、JSON或YAML以文本格式来通过网络发送数据。但大多数游戏网络程序员会嘲笑这种对游戏数据进行编码的建议。他们可能会说：”哈哈哈，这真的很有趣。你不是认真的对吧？“哦。你…是认真的。你被解雇了。你可以收拾东西回家了。 我只是开了一个玩笑，但是玩笑归玩笑，为什么这种方法不好？ 一个网页服务器位于网络上的某个位置，监听请求并发送响应。这些请求和响应都是无状态的并且对实时性要求非常非常低（当然有个快速的响应是非常重要的，但是如果不是这种情况也没什么关系）。因为无状态和请求/响应的频率非常低，所以具有非常良好的扩展性。但是多人在线游戏的服务器与这种网页服务器完全是不同的。它是以每秒60次的速度来对游戏世界的状态进行模拟。它是实时的并且有状态的，并且需要把所有的状态以每秒20次或者30次的频率下发给客户端。因为整个游戏世界有几千个物体，每个物体可能会有几百个状态，所以下发给客户端的状态可能是海量的。 如果你使用文本格式(如XML或JSON)对这个游戏状态进行编码的话，有可能这种方法是非常低效的。 让我们举个简单的例子，看下下面这个XML文档： ?1234567891011121314151617181920&lt;world_update world_time="0.0"&gt; &lt;object id="1" class="player"&gt; &lt;property name="position" value="(0,0,0)" &lt;="" property=""&gt; &lt;property name="orientation" value="(1,0,0,0)" &lt;="" property=""&gt; &lt;property name="velocity" value="(10,0,0)" &lt;="" property=""&gt; &lt;property name="health" value="100"&gt;&lt;/property&gt; &lt;property name="weapon" value="110"&gt;&lt;/property&gt; ... 100s more properties per-object ... &lt;/property&gt;&lt;/property&gt;&lt;/property&gt;&lt;/object&gt; &lt;object id="100" class="grunt"&gt; &lt;property name="position" value="(100,100,0)" &lt;="" property=""&gt; &lt;property name="health" value="10" &lt;="" property=""&gt; &lt;/property&gt;&lt;/property&gt;&lt;/object&gt; &lt;object id="110" class="weapon"&gt; &lt;property type="semi-automatic"&gt;&lt;/property&gt; &lt;property ammo_in_clip="8"&gt;&lt;/property&gt; &lt;property round_in_chamber="true"&gt;&lt;/property&gt; &lt;/object&gt; ... 1000s more objects ...&lt;/world_update&gt; 这看上去真的很让人讨厌。我们可以做得更好吗?当然。。。使用JSON来编码的话文本量可以少一些： ?123456789101112131415161718192021222324{ "world_time": 0.0, "objects": { 1: { "class": "player", "position": "(0,0,0)", "orientation": "(1,0,0,0)", "velocity": "(10,0,0)", "health": 100, "weapon": 110 } 100: { "class": "grunt", "position": "(100,100,0)", "health": 10 } 110: { "class": "weapon", "type: "semi-automatic" "ammo_in_clip": 8, "round_in_chamber": 1 } }}但要注意的是描述数据的属性比实际要发送的数据还大。这糟透了。 但是如果我们把数据分成两个部分呢? 1. 一个模式来描述物体类的几何以及每个类的属性。 2. 数据相对于该模式进行编码来快速下发给各个客户端。 下面是JSON的一个模式。它只会被下发一次： ?1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859{ "classes": { 0: "player" { "properties": { 0: { "name": "position", "type": "vec3f" } 1: { "name": "orientation", "type": "quat4f" } 2: { "name": "velocity", "type": "vec3f" } 3: { "name": "health", "type": "float" } 4: { "name": "weapon", "type": "object", } } } 1: "grunt": { "properties": { 0: { "name": "position", "type": "vec3f" } 1: { "name": "health", "type": "float" } } } 2: "weapon": { "properties": { 0: { "name": "type", "type": "enum", "enum_values": [ "revolver", "semi-automatic" ] } 1: { "name": "ammo_in_clip", "type": "integer", "range": "0..9", } 2: { "name": "round_in_chamber", "type": "integer", "range": "0..1" } } } }} 现在我们可以更加紧凑的进行状态更新(每秒进行20次或30次状态更新…)： ?12345678{ "world_time": 0.0, "objects": { 1: [0,"(0,0,0)","(1,0,0,0)","(10,0,0)",100,110], 100: [1,"(100,100,0)",10], 110: [2,1,8,1] }} 这确实更好一点了。但是我们为什么不能直接下发一个简单的文本格式而没有任何废话吗? ?12340.01:0,0,0,0,1,0,0,0,10,0,0,100,110100:1,100,100,0,10110:2,1,8,1 这比原来紧凑多了。当然它在没有模式的情况下是完全不可读的，但是这也是整个问题的所在。 文本格式存在的问题 我们确实取得了一些进展，但是我们仍然在使用文本格式，所以就继承了文本格式固有的低效。 为什么？ 让我们以一个浮点数为例来分析这个问题。在二进制格式中一个浮点数需要占据4个字节，但是在文本格式下一个浮点数需要多的多的空间。让我们举个简单的例子：“3.141592653589793”一共用了17个字节（并没有包括终止符‘\0’）。当然你可以说让我们把这个浮点数缩短到6位有效数字的情况。但是就算我们这样处理了，所得到的“3.141593”所占据的空间仍然是二进制表示法的2倍大小。更糟糕的是如果你有一个很大的表示位置的数值，比如”12134.534112”。 你仍然需要6位精度的准确性所以现在你需要使用12个字节来表示这个浮点数。而这样所占据的空间是二进制表示法的3倍大小。你还会为每一个‘,’分隔符浪费一个字节。 我希望你能看到，在多人在线游戏中使用文本格式进行通信是完全不可行的。“但是格伦，整个互联网都是构建于文本格式之上的，为什么在多人在线游戏中使用文本格式进行通信完全不可行？”。恩，我认为这是那些创造互联网的人在这个过程中犯下的一些错误。(译注：原作者的这个结论下的太武断了，我觉得主要是因为互联网和游戏通信协议的面向对象不同，游戏通信协议面向的目标是非常特定而且固定的，这使得他们可以提前沟通到底以什么格式来传输协议，或者说不需要传输任何解读的标签就能够互相理解，而这种互相理解的基础是通过其他方式已经进行了沟通，但是互联网的通信则是完全不同的，它要非常的通用，兼顾更多的需求，而且根本就不知道是谁来解读这些信息，所以信息的可读性就非常非常的重要，因为我们没有其他的管道进行通信，只能凭借传输过去的信息本身来解读，所以互联网的协议才是设计成这样。)。可读性可能在这些早期协议很重要，但我可以向你保证，让人们很容易来阅读这些协议和修改你的游戏的网络协议正好是硬币的两面，根本没有办法兼得。 而且因为带宽是非常关键的要素，并且你希望在每秒用尽可能少的比特数来传递尽可能多的游戏内容，所以你完全没有办法使用文本格式，必须切换成二进制格式。 切换成二进制格式 在网络世界中存在一些用来读取和写入二进制格式的库，比如BJSON,、Protocol Buffers、 Flatbuffers、 Thrift、 Cap’n Proto 和 MsgPack。这几乎都是常识了。 这些库其实都还不错。借助接口描述语言和一些代码生成工具的帮助能够读取和写入（这通常被称为序列化）你的游戏数据结构成为二进制格式，而这些二进制数据会在网络上进行传递。在网络的另外一侧，这些数据会被反序列化并且转换回原始的数据结构。 使用二进制格式进行传说网络数据的优点有：1) 你不必自己动手写一个序列化层。2）这些库往往是语言无关的，所以你可以在程序的前端（也就是客户端）使用一种语言而在程序的后端（也就是服务端）使用另外一种语言，而在这种情况下手，程序的前端和程序的后端仍然是可以进行通信的。3)提供了版本信息，这样的话，如果你的客户端使用的是一个比较旧版本的协议，而服务器使用的是一个比较新版本的协议的话，它们仍然是可以进行通信的。反过来也是一样。 这些库看上去很棒，但是它们是实现你的游戏的网络协议的一种很棒的方法么？ 其实情况并不总是如此。如果你有一个协议，这个协议是负责向网页服务器进行请求和接受响应，但是需要支持多种语言并且版本信息对你来说非常的重要，那么在这种情况下，因为有了可以支持这些功能的库的存在，你再自己去实现自己的带版本信息的序列化层和多语言支持就是一件特别傻的事情。如果你的游戏需要从一台机器上远程调用另外一台机器的函数。那么可能只用这些已有的库就是完全没问题的。。。 但是在性能至关重要的网络通信的情况下，比如我们在这篇文章中讨论的这种情况（对于第一人称射击游戏游戏、动作游戏等等），游戏网络的基本单位是状态而不是远程函数调用。同时,跨语言支持二进制格式所能提供的好处很小，这是因为客户端和服务器通常情况下都是使用一种语言进行开发的（比如C++）。这些游戏也不需要复杂的版本信息和版本验证机制，因为如何客户端试图以一个不同的协议版本与服务器进行连接的话，那么服务器可以直接拒绝这个连接就好。有一种特别简单但是有效的办法可以解决版本的问题，就是绕过客户端永远用和服务器相同的协议版本进行连接。 所以如果你的协议不是大部分使用远程函数调用的话，你其实根本就不需要版本信息，也不需要什么跨语言的支持，到底这么做有什么好处？从我的观点来看好处其实并不多。所以让我们直接忽略掉这些功能并用我们自己的不带属性的二进制流进行代替，在这个过程中我们可以获得更多的控制性和灵活性。 如何开始使用二进制格式 如果你在用C或C++对你的游戏进行编码，你可能想知道为什么不能直接使用memcpy（这是一个函数，可以直接进行内存拷贝, 将n字节长的内容从一个内存地址复制到另一个地址）把我的结构拷贝到数据包里面？很多人会经常从这里开始，因此尽管我不推荐这种方法，还是让我们看下这个方法，看看如果用这个方法来进行网络包传递的话会有哪些问题。 首先要定义你的数据包的集合，通常情况下这是结构的”联合“（C语言的一种语法名字）： ?123456789101112131415161718192021222324252627struct Packet{ enum PacketTypeEnum { PACKET_A, PACKET_B, PACKET_C }; uint8_t packetType; union { struct PacketA { int x,y,z; } a; struct PacketB { int numElements; int elements[MaxElements]; } b; struct PacketC { bool x; short y; int z; } c; }; }; 当对数据包进行写入的时候，设置数据包的第一个字节为数据包的类型（0或者1或者2）。然后依据数据包的类型，使用memcpy函数将合适的联合结构拷贝到数据包里面。在读取数据包的时候进行完全相反的操作：读取的数据包的第一个字节，然后根据数据包的类型就能判断出这个数据包还有多少字节没有读取。然后使用memcpy函数将数据包的数据拷贝到合适的联合结构里面。 这种方法没有办法更加简化了，所以这就完了么？不完全是这样的！我不推荐这种方法，因为它有一些很讨厌的问题。 首先，不同的编译器和平台对于数据结构的打包方式是完全不同的。如果你走这条路的话，你会花很多时间来使用#pragma来努力确保在不同平台的不同的编译器上布局结构在内存中使用完全相同的方式。让这种方式可以在32位和64位架构上都能顺利工作是一个很有“意思”的过程。这并不是说这是不可能的，但它肯定不是一件容易的事情。 下一个比较大的问题是字节顺序。现代计算机大多是使用小端这种字节顺序（英特尔的中央处理器都是如此），但PowerPC的内核使用大端这种字节顺序。历史上的网络数据通过用大端这种字节顺序（网络字节顺序）来进行发送，但没有理由让你遵循这一传统。在我的代码里面，我使用的是小端这种字节顺序，这么做的原因是使用这个字节顺序在最常见的平台（英特尔）上在打包和解包中所要做的工作量最少。 如果你需要支持使用小端字节顺序的机器和使用大端字节顺序的机器之间进行通信的话，只是使用memcpy函数来将结构体拷贝到数据包的方法根本行不通。你至少需要编写一个函数来在结构被读取以后对它的每个属性的字节顺序进行调整，然后才能顺利的读取和写入。 还有一些其他的小问题。很显然，如果你的结构中包含指针，你不能直接把指针进行序列化然后在网络上进行传递然后期望在网络的另外一端反序列化这个指针以后，这个指针在那边还能够正常的使用。另外，如果你有可变大小的结构，比如说一个可以多达32个元素的数组，但是它在大多数时间里面都是空的或者只有很少一些元素，但是为了防止最差的情况你总是需要假设它有32个元素并且进行序列化和反序列化，这非常非常的浪费。一个更好的办法是让你可以对你的可变长度的结构进行编码，能够把长度信息编码到数据结构本身，这样在序列化和反序列化的时候都能妥善处理这个问题。 我觉得真正影响这个方法的可用性的最后一个问题是安全性。如果使用这种方法，你相当于直接把整个C++结构中的数据包直接拷贝，然后在网络上进行发送。你到底在想什么？!!如果有人构造了一个恶意的数据包，并把这个包的长度标记为0xFFFFFFFF发送给你的话，这样你在处理这个数据包的时候，将导致你耗费尽所有的内存空间。 这是一个巨大的安全风险，让你的原始数据直接在网络上进行传输并且选择相信这些在网络上接收到的数据。。你应该，至少，对这些值进行一个取值范围的检查，让这些值确保落在你期望的范围之内，而不是盲目的相信所有发送给你的数据。 读取和写入函数 这个复杂度导致的下一个问题就是每个数据包的读取和写入函数。让我们先从以下几个简单的操作开始： ?1234567void WriteInteger( Buffer &amp; buffer, uint32_t value );void WriteShort( Buffer &amp; buffer, uint16_t value );void WriteChar( Buffer &amp; buffer, uint8_t value ); uint32_t ReadInteger( Buffer &amp; buffer );uint16_t ReadShort( Buffer &amp; buffer );uint8_t ReadByte( Buffer &amp; buffer ); 这几个函数是对一个缓冲区结构进行操作，有点类似这样： ?123456struct Buffer{ uint8_t * data; // pointer to buffer data int size; // size of buffer data (bytes) int index; // index of next byte to be read/written};举个简单的例子来说，写入整数的函数会像是如下这样： ?12345678910void WriteInteger( Buffer &amp; buffer, uint32_t value ){ assert( buffer.index + 4 &lt;= size );#ifdef BIG_ENDIAN *((uint32_t*)(buffer.data+buffer.index)) = bswap( value ); #else // #ifdef BIG_ENDIAN *((uint32_t*)(buffer.data+buffer.index)) = value; #endif // #ifdef BIG_ENDIAN buffer.index += 4;} 而读取整数的函数会像是如下这样： ?123456789101112uint32_t ReadInteger( Buffer &amp; buffer ){ assert( buffer.index + 4 &lt;= size ); uint32_t value;#ifdef BIG_ENDIAN value = bswap( *((uint32_t*)(buffer.data+buffer.index)) );#else // #ifdef BIG_ENDIAN value = *((uint32_t*)(buffer.data+buffer.index));#endif // #ifdef BIG_ENDIAN buffer.index += 4; return value;} 现在就不仅仅是直接使用memcpy函数把内存中的数据结构直接拷贝到数据包里面，而是为每个数据包类型使用了单独的读取和写入函数： ?1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859struct PacketA{ int x,y,z; void Write( Buffer &amp; buffer ) { WriteInteger( buffer, x ); WriteInteger( buffer, y ); WriteInteger( buffer, z ); } void Read( Buffer &amp; buffer ) { ReadInteger( buffer, x ); ReadInteger( buffer, y ); ReadInteger( buffer, z ); }}; struct PacketB{ int numElements; int elements[MaxElements]; void Write( Buffer &amp; buffer ) { WriteInteger( buffer, numElements ); for ( int i = 0; i &lt; numElements; ++i ) WriteInteger( buffer, elements[i] ); } void Read( Buffer &amp; buffer ) { ReadInteger( buffer, numElements ); for ( int i = 0; i &lt; numElements; ++i ) ReadInteger( buffer, elements[i] ); }}; struct PacketC{ bool x; short y; int z; void Write( Buffer &amp; buffer ) { WriteByte( buffer, x ); WriteShort( buffer, y ); WriteInt( buffer, z ); } void Read( Buffer &amp; buffer ) { ReadByte( buffer, x ); ReadShort( buffer, y ); ReadInt( buffer, z ); }}; 当对数据包进行读取和写入的时候，通过ReadByte / WriteByte函数来在数据包的数据之前加上一个字节，用来表明数据包的类型，然后根据数据包的类型，调用联合体里面对应数据包结构里面读取或者写入函数。 所以，现在我们有了一个简单的系统，允许使用不同的字节顺序的机器可以进行通信，并支持可变长度的编码。举个简单的例子来说，现在可以对数组的长度进行序列化并且只对存在的数据进行遍历和序列化，而不用像以前那样，总是要按照最坏情况来发送整个队列。 读取和写入函数存在的问题 有了读取和写入函数，是相比较之前的memcpy方法进行数据包的打包和解包前进了一大步，但是这种方法也存在一些问题。让我们一起看下具体都有些什么问题。 第一个存在的问题：如果有一个值，它的取值范围是【0，100】，那么它只需要10个比特就能表示所有可能的值，但是因为我们只能序列化一个字节的值【0，255】，一个short类型的整数【0,655325】或者是一个32比特的整数【0，2^32-1】，所以我们必须凑够16位。这样的话就浪费了6位！ 同样的，如果是一个布尔值（只有真和假两种情况）只需要一位就能表示，但是它们必须取整为一个字节，这就浪费了7位！现在你可以通过用C++结构来实现你的数据包以及对C++的位域进行序列化并使用一些联合方面的技巧来对这个值进行取整，但是你真的能保证两个完全不同的C ++编译器用完全相同的方式来在内存对位域进行打包？据我所知，应该是不可能的。 第二个存在的问题：怀有恶意的人仍然可以构造一个恶意的数据包。举个简单的例子来说，让我们假设MaxElements是32，所以数据包PacketB的元素的数目肯定是在范围【0，32】之间。由于我们是在字节级别对数据包进行读取和吸入，所以元素的数目是存在一个字节里面，可能的取值范围是【0，255】。这导致字节里面【33,255】这个范围的值是完全没有定义的。 如果有人构造了一个elementCount = 255的恶意的数据包，会发生什么？会发生内存崩溃！当然你可以手动设置一个阈值，对读入的数据的大小进行限制，或者手动对它们进行检查，如果出现问题就中止读取，但是你真正想要的一个知道每个要读取的域的最小/最大值到底是多少的系统，并且在任何数据包中对应的值如果超出预期的范围就会自动中止读取，这样在你的代码看到这些不合法的值之前，这些值就已经被丢弃了。在我看来，最后一个存在的问题是，维护这些单独的读取和写入函数真的很让人讨厌。随着这些函数变得越来越复杂，它很容易对其中一个函数进行改变而忘记相应的改变另外一个函数（读取和写入函数是成对出现的，如果要改变一个的话，需要同时改变另外一个，要么就会出现问题，发送方和写入方根本就没法正确的得到数据包里面的信息）。导致之间的读写不同步。而这些不同步可能非常难以追查。 我们要解决所有这些问题，但是首先我们要通过实现一个位打包器来朝这个方向努力，这样我们才不会继续浪费位来存储一些没必要的数据。 实现一个位打包器 如果我们要在数据包写入一个布尔值的话，它应该只需要在数据包里面占据一个位的大小。如果我们要在数据包写入一个取值范围在【0，31】的值，它应该在数据包里面占据五个位的大小，而不是八个位的大小。如果我们要在数据包写入一个取值范围在【0，100000】的值，它应该在数据包里面占据十七位的大小，而不是二十四位的大小或者三十二位的大小。要做到这一点，我们需要写一个位打包器。 很多人所写的位打包器是工作在字节这个级别上的，举个例子来说明的话就是他们会把生成的字节刷新到流里面，但是我不喜欢这种方法，因为如果它们是工作在word这个级别的话会快的多。我的目标是每次是写下很多word的时候（32位或者64位），然后每次读取32位或者64位，因为现代机器对这个长度进行了专门的优化而不应该像1985年那样在字节的级别对缓冲区进行处理。 我的位打包技术的原理是类似这样的：如果你想在一次在数据包写入32位的话，你需要一个两倍大小的临时word，比如说uint64_t。如果你想在一次在数据包写入64位的话，你需要128位。这样做的原因是你需要整个区域的上半部分进行溢出，因为你可以只写一个30位大小的值到临时缓冲区，然后需要写入一个10位大小的值到临时缓冲区中。如果这个临时字的上半部分没有额外的空间的话，你需要额外的分支和逻辑来处理溢出情况。既然你想在位打包里面的循环里面产生尽可能少的分支，那么这种方法将有很大的意义。 以位来写入数据包 对于位打包器的数据吸入，你需要一些缓冲区以及一个变量来记录目前在缓冲区里面的位的数目。在这个例子之中，让我们把字长选为32位，这样，位打包器的变量看上去就会像是这样： ?1234uint64_t scratch;int scratch_bits;int word_index;uint32_t * buffer; 当你开始启动你的位打包器进行写入的时候，所有这些变量都将被清零，缓冲区的指针会指向缓冲区的起始位置，这个指针用于数据包实际开始写入的位置指示。这个缓冲区的长度是以字节为单位的，必须要是4的整数倍，因为我们工作的字长是32位。 比方说，我们要写入3位数据，然后是10位数据，再然后是24位数据。你的目标是在临时缓冲区对这块数据使用一个比较紧密的打包方式并把整理好的数据刷新到内存中去，一次是32位（一个字长）。需要注意的是3 + 10 + 24 = 37，这是故意设计的。你必须处理这种情况。 在第一步中，向临时缓冲区写入3位数据就像下面这样： xxx 临时缓冲区的长度现在就是3位。 接下来，向临时缓冲区写入10位数据就像下面这样： yyyyyyyyyyxxx 临时缓冲区的长度现在就是13位（10+3）。为什么要以这种方式进行打包而不是使用xxxyyyyyyyyyy这种方式？我用这种字节顺序写入的原因是因为我用小端字节顺序来存储网络数据。如果我没有按照这个方向对位数据进行存储的话，那么在发送数据包的时候我将不得不将数据包的大小对齐到下一个双字的长度那里，或者我只能对我的数据包的尾端进行截断。如果你想以大端字节顺序来发送刷新的数据的话，你应该按照另外一个方向对数据包的数据进行打包。 接下来，向临时缓冲区写入24位数据就像下面这样： zzzzzzzzzzzzzzzzzzzzzzzzyyyyyyyyyyxxx 临时缓冲区的长度现在就是37位（10+3+24）。我们正在跨越32位字的边界，并有5位会写入到uint64_t结构的上半32位中（所以这实际是一个溢出）。现在bit_index &gt;= 32，刷新低32位的数据到内存中去，并对word_index加1，然后从临时缓冲区的长度减去32并向对临时缓冲区向右偏移32位。 临时缓冲区现在看上去就像是下面这样： zzzzz 我们可以继续下去，但是我们决定在停下来对这一点进行解释。我们必须在整个数据包的最后放置的是一个32位的字。对字这个级别进行处理的位打包器的一个微妙的一点是你需要在整体写入的最后进行一个刷新处理，这样才能保证最后的这些位会写入到内存中去。当我们把一个字刷新到内存中去的时候，我们要确保这个字会被正确的转换成小端字节顺序。举个简单的例子来说：ＡＢＣＤ当被写入内存中的时候需要被有效的转换成ＤＣＢＡ这个顺序。如果要想明白为什么这种做法非常重要，需要考虑下如果我们用大端字节顺序来把字刷新到内存中去会发生什么事情？最后一个字会截断，因为这个字会以在整数相同的字节顺序写入到内存中去。因为我们会把开始的那些位写到字的低的那个字节中去，我们的内存中的顺序看上去是这样的：对于刚才那串比特值，写入内存的时候会是ＡＢＣＤＥ这个顺序。DCBA000E现在，如果我们尝试以５个字节的大小来用一个数据包来发送缓冲区（我们要发送的数据的实际大小）它捕获的最后一个字节会是０而不是Ｅ。（作者在这里打了一个笑脸）。 但是，当我们以小端这种字节顺序把上面的内容写入内存的时候，字节在内存中的布局是这样的： ABCDE000 我们可以只在网络上写5个字节，这样就节省了3个字节，并且仍然是以Ｅ来作为数据包的结尾。 实际上，我们所做的是开关字周围的字节，因为我们通过这种方法进行构造来避免小端字节顺序的重新排序，所以我们希望是以它们在内存中的顺序直接写入到网络的数据包中，字节顺序是很难处理的。 其代码类似于 ： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144class BitWriter&#123;public: BitWriter( void* data, int bytes ) : m_data( (uint32_t*)data ), m_numWords( bytes / 4 ) &#123; assert( data ); assert( ( bytes % 4 ) == 0 ); // buffer size must be a multiple of four m_numBits = m_numWords * 32; m_bitsWritten = 0; m_wordIndex = 0; m_scratch = 0; m_scratchBits = 0; &#125; void WriteBits( uint32_t value, int bits ) &#123; assert( bits &gt; 0 ); assert( bits &lt;= 32 ); assert( m_bitsWritten + bits &lt;= m_numBits ); value &amp;= ( uint64_t(1) &lt;&lt; bits ) - 1; m_scratch |= uint64_t( value ) &lt;&lt; m_scratchBits; m_scratchBits += bits; if ( m_scratchBits &gt;= 32 ) &#123; assert( m_wordIndex &lt; m_numWords ); m_data[m_wordIndex] = host_to_network( uint32_t( m_scratch &amp; 0xFFFFFFFF ) ); m_scratch &gt;&gt;= 32; m_scratchBits -= 32; m_wordIndex++; &#125; m_bitsWritten += bits; &#125; void WriteAlign() &#123; const int remainderBits = m_bitsWritten % 8; if ( remainderBits != 0 ) &#123; uint32_t zero = 0; WriteBits( zero, 8 - remainderBits ); assert( ( m_bitsWritten % 8 ) == 0 ); &#125; &#125; void WriteBytes( const uint8_t* data, int bytes ) &#123; assert( GetAlignBits() == 0 ); assert( m_bitsWritten + bytes * 8 &lt;= m_numBits ); assert( ( m_bitsWritten % 32 ) == 0 || ( m_bitsWritten % 32 ) == 8 || ( m_bitsWritten % 32 ) == 16 || ( m_bitsWritten % 32 ) == 24 ); int headBytes = ( 4 - ( m_bitsWritten % 32 ) / 8 ) % 4; if ( headBytes &gt; bytes ) headBytes = bytes; for ( int i = 0; i &lt; headBytes; ++i ) WriteBits( data[i], 8 ); if ( headBytes == bytes ) return; FlushBits(); assert( GetAlignBits() == 0 ); int numWords = ( bytes - headBytes ) / 4; if ( numWords &gt; 0 ) &#123; assert( ( m_bitsWritten % 32 ) == 0 ); memcpy( &amp;m_data[m_wordIndex], data + headBytes, numWords * 4 ); m_bitsWritten += numWords * 32; m_wordIndex += numWords; m_scratch = 0; &#125; assert( GetAlignBits() == 0 ); int tailStart = headBytes + numWords * 4; int tailBytes = bytes - tailStart; assert( tailBytes &gt;= 0 &amp;&amp; tailBytes &lt; 4 ); for ( int i = 0; i &lt; tailBytes; ++i ) WriteBits( data[tailStart+i], 8 ); assert( GetAlignBits() == 0 ); assert( headBytes + numWords * 4 + tailBytes == bytes ); &#125; void FlushBits() &#123; if ( m_scratchBits != 0 ) &#123; assert( m_wordIndex &lt; m_numWords ); m_data[m_wordIndex] = host_to_network( uint32_t( m_scratch &amp; 0xFFFFFFFF ) ); m_scratch &gt;&gt;= 32; m_scratchBits -= 32; m_wordIndex++; &#125; &#125; int GetAlignBits() const &#123; return ( 8 - ( m_bitsWritten % 8 ) ) % 8; &#125; int GetBitsWritten() const &#123; return m_bitsWritten; &#125; int GetBitsAvailable() const &#123; return m_numBits - m_bitsWritten; &#125; const uint8_t* GetData() const &#123; return (uint8_t*) m_data; &#125; int GetBytesWritten() const &#123; return ( m_bitsWritten + 7 ) / 8; &#125; int GetTotalBytes() const &#123; return m_numWords * 4; &#125;private: uint32_t* m_data; uint64_t m_scratch; int m_numBits; int m_numWords; int m_bitsWritten; int m_wordIndex; int m_scratchBits;&#125;; 以位来读取数据包 我们该如何在网络的另外一侧读取已经通过位打包器打包好的数据？ 要我们从要在网络上进行发送的缓冲区开始，假设我们刚刚从recvfrom.函数返回。它里面的内容有５位这么长。 ABCDE 因为我们是按照字这个等级进行读取，我们必须要把数据的长度截断到双字这个长度，比如说８个字节： ABCDE000 现在，这里有一点很微妙的地方。当我在网络上发送一个数据包的时候，我真的不知道它到底包含了多少位的数据（否则的话我将不得不将这个数据包的大小直接记录在数据包的包头里面），而且这是一个带宽的浪费。但是通过在网络的另外一侧使用recvfrom函数我确实知道到底这个数据包的内容的长度是多少，因此当５个字节大小的数据包从网络上到达的时候，我可以直接认为缓冲区中的位的大小是数据包字节大小乘以８。因此，实际上，位读取器认为这个分组中要被读取的比特数为5 * 8 = 40，而不是37。 你真的想在这里做的事情是确保如果位读取器读取的位置已经超过缓冲区实际位数的结尾，在这个例子中，就是发送的３７位数据，那么它将读取的是零而不是未定义数据。这会自动发生在数据包最后一个字节的最后3位上，因为它们是由位写入器写入的零，但是对于在缓冲区中的最后3个比特你必须确保它们被读为零，以及未来的任何可能位的读取也是返回零。这是一个非常重要的安全步骤以防止你读取的时候超过缓冲区或者数据流的结尾。 现在让我们开始吧。你将在位读取器里面有如下这些变量： ?123456uint64_t scratch;int scratch_bits;int total_bits;int num_bits_read;int word_index;uint32_t * buffer;这比位写入器要复杂一些，因为你需要做更多的检测。但是请记住，永远不要相信来自客户端的数据。 要开始进行读取的时候，字的序号是０，临时缓冲区的内容和临时缓冲区的位数都是０。 然后用户请求３位的空间。因为临时缓冲区的位数现在是０，现在是时候来读取第一个字了。读取第一个字然后把它放到临时缓冲区，并对临时缓冲区的位数（目前是0）向右进行偏移。把临时缓冲区的位数添加32。 现在通过把临时缓冲区的内容拷贝到另外一个变量里面来读取前面的3位并通过&amp; ( (1&lt;&lt;3 ) – 1 )进行遮罩处理，来给出最后输出的结果： xxx 现在将临时缓冲区的内容向右偏移3位，并从临时缓冲区的位数中减去3： zzzzzzzzzzzzzzzzzzzyyyyyyyyyy 现在用完全相同的办法读取后面的10位。临时缓冲区看上去就是下面这样的： zzzzzzzzzzzzzzzzzzz 恩。接下来的读取调用请求24位数据的内容，但是临时缓冲区的位数只有19了（19 = 32-10-3 ）。。。现在是时候来读取下一个字了。这个字多半是零因为我们已经对它们进行了清除，但是它还有多余的5位我们需要在接下来进行读取。现在我们准备读取临时缓冲区里面有关z的比特位了： zzzzzzzzzzzzzzzzzzzzzzzz 接下来读取24位并向右位移24位。临时缓冲区现在全部都是0了。 理论上位读取器认为还有27位数据留在缓冲区没有进行读取，如果我们继续进行读取的话，这些位会被作为零弹出来，因为最后一个字节的前3位是零，并且我们把临时缓冲区的最后3个字节全部清位0，因为我们为了按照字进行对齐。（为了对齐，所以填充了3个全部是0的字节。所以位读取器这时候看的话还有3个完整的字节没有读。） 但是让我们假设，由于某种原因，越过了这个点以后用户还是一直尝试进行读取。为了处理这种情况，检查缓冲区中你需要读取的字的数目，以及每次你需要从缓冲区实际读取的字的情况。如果你已经读完了要读的字以后，不要继续增加word_index并继续读取数据这会让你的内存崩溃的，给你的缓冲区填充0来对齐并在缓冲区的位数添加32在每次给缓冲区的末尾添加一个新的字的情况下。通过这种方法，在读取位数的每次内部循环调用的时候能确保分支最后总是返回零，并把它放在你每次需要读取一个新字的地方，但是你读取的位置已经超过了缓冲区结尾的地方你仍然是安全的并且返回0个比特。 这似乎有点过于谨慎了，但是在对网络数据进行读取的时候这种谨慎是特别重要的。这是通过网络传输过来的数据。不要相信它们！如果你的读取和写入是不同步的，或者有人给你发送了一个恶意的缓冲区数据，你可能会被困在一个循环里面并不断尝试读取数据。确保内部循环里面所有的遍历里面的读取都会检测缓冲区是否溢出或者有损坏，如果用户读取的位置已经超出了缓冲区的结尾这种策略总是返回定义的值（0），通过这种行为你可以确保大多数情况都是符合预期的。 其代码类似于： 123456789101112131415161718192021222324252627282930313233343536class BitReader&#123;public: // ... uint32_t ReadBits( int bits ) &#123; assert( bits &gt; 0 ); assert( bits &lt;= 32 ); assert( m_bitsRead + bits &lt;= m_numBits ); m_bitsRead += bits; assert( m_scratchBits &gt;= 0 &amp;&amp; m_scratchBits &lt;= 64 ); if ( m_scratchBits &lt; bits ) &#123; assert( m_wordIndex &lt; m_numWords ); m_scratch |= uint64_t( network_to_host( m_data[m_wordIndex] ) ) &lt;&lt; m_scratchBits; m_scratchBits += 32; m_wordIndex++; &#125; assert( m_scratchBits &gt;= bits ); const uint32_t output = m_scratch &amp; ( (uint64_t(1)&lt;&lt;bits) - 1 ); m_scratch &gt;&gt;= bits; m_scratchBits -= bits; return output; &#125; //...&#125; 如何使位打包器更棒 位打包器这种方法非常的棒，但是直接用于读取和写入数据包时，这并不是最有用的方法。我见过有团队直接使用位打包器进行读取和写入数据包，但是这并不是最佳的方法。你会拥有很多复杂的代码，并且非常容易出错。 让我们一起看下这个例子： ?123456789101112131415161718192021const int MaxElements = 32; struct PacketB{ int numElements; int elements[MaxElements]; void Write( BitWriter &amp; writer ) { WriteBits( writer, numElements, 6 ); for ( int i = 0; i &lt; numElements; ++i ) WriteBits( writer, elements[i] ); } void Read( BitReader &amp; reader ) { ReadBits( reader, numElements, 6 ); for ( int i = 0; i &lt; numElements; ++i ) ReadBits( reader, elements[i] ); }};第一种方法很容易出错，让我们假设在一段时间以后，你把MaxElements的值从32提高到100，但是你没有修改需要序列化的比特的数目，也就是需要序列化的比特的数目还是6（注意看`WriteBits( writer, numElements, 6 )的6， 现在需要7了`）。哎呦。因为你忘记了在读取和写入函数里面更新比特的数目，那么现在当你在发送数据的时候你会把高位进行截断。事后追查这样的事情是相当困难的。让我们通过增加一些编译时候的检测来计算出所需要的比特的位数： ? 12345678910111213141516171819202122232425262728template &lt;uint32_t x&gt; struct PopCount&#123; enum &#123; a = x - ( ( x &gt;&gt; 1 ) &amp; 0x55555555 ), b = ( ( ( a &gt;&gt; 2 ) &amp; 0x33333333 ) + ( a &amp; 0x33333333 ) ), c = ( ( ( b &gt;&gt; 4 ) + b ) &amp; 0x0f0f0f0f ), d = c + ( c &gt;&gt; 8 ), e = d + ( d &gt;&gt; 16 ), result = e &amp; 0x0000003f &#125;; &#125;;template &lt;uint32_t x&gt; struct Log2&#123; enum &#123; a = x | ( x &gt;&gt; 1 ), b = a | ( a &gt;&gt; 2 ), c = b | ( b &gt;&gt; 4 ), d = c | ( c &gt;&gt; 8 ), e = d | ( d &gt;&gt; 16 ), f = e &gt;&gt; 1, result = PopCount&lt;f&gt;::result &#125;;&#125;;template &lt;int64_t min, int64_t max&gt; struct BitsRequired&#123; static const uint32_t result = ( min == max ) ? 0 : ( Log2&lt;uint32_t(max-min)&gt;::result + 1 );&#125;;#define BITS_REQUIRED( min, max ) BitsRequired&lt;min,max&gt;::result 哦，太好了。模板元编程和宏。感谢格伦！ 但是这么做真的真棒，相信我！因为你现在没有办法弄乱需要的比特的数目了： ?12345678910111213141516171819202122232425const int MaxElements = 32;const int MaxElementBits = BITS_REQUIRED( 0, MaxElements ); struct PacketB{ int numElements; int elements[MaxElements]; void Write( BitWriter &amp; writer ) { WriteBits( writer, numElements, MaxElementBits ); for ( int i = 0; i &lt; numElements; ++i ) WriteBits( writer, elements[i], 32 ); } void Read( BitReader &amp; reader ) { ReadBits( reader, numElements, MaxElementBits ); for ( int i = 0; i &lt; numElements; ++i ) ReadBits( buffer, elements[i], 32 ); }}; 当然现在也有机会犯错。MaxElements的值是32所以BITS_REQUIRED(0,32) 返回的是6，因为5比特所能得到的取值范围只有【0，31】。这没什么问题，但是现在我们有概率把未定义的值进行插入，如果有一个恶意发送者发送了一个取值范围在【33,63】之间的值的话。当你在长度为32的数组里面读入63个整数的时候会发生什么？当然，你可以对取值范围进行限制来修正这个问题，但是仔细想想。。。如果你从位读取器得到了一个值但是它超出了取值范围，你要么就是让读取和写入操作完全不同步（这显然是你自己的错误）或者有人试图坑你。所以，不要对取值范围进行限制。如果遇到这种情况，直接停止对数据包的读取并且丢弃这个数据包。我还想在这里提到一个陷阱，因为这个陷阱看上去很方便，但是其实它会让代码运行的很慢。我有一次使用异常实现了数据包的读取中断。它看上去很棒，因为在一个递归的位打包器读取函数里面你可能会有28层调用堆栈，而你想要做的不过是展开堆栈然后回到数据包读取函数调用的地方，但是异常真的太慢太慢了。为了取代异常，有两种方法可以运行的快得多：1)在位读取器那里设置一个值表明这个数据包应该被丢弃，2）升级数据包的读取函数让它在读取失败的时候返回false。但是现在，你可以使用这里面的任意一种方法，为了实现读取时候的安全性，你需要检测上面说的标记或者在每一次读取的时候返回一个值，否则如果遇到读取失败的情况，你还是会一直继续读取直到把内存全部耗光。?123456789101112131415161718192021222324252627282930313233343536const int MaxElements = 32;const int MaxElementBits = BITS_REQUIRED( 0, MaxElements ); struct PacketB{ int numElements; int elements[MaxElements]; void Write( BitWriter &amp; writer ) { WriteBits( writer, numElements, MaxElementBits ); for ( int i = 0; i &lt; numElements; ++i ) WriteBits( writer, elements[i], 32 ); } void Read( BitReader &amp; reader ) { ReadBits( reader, numElements, MaxElementBits ); if ( numElements &gt; MaxElements ) { reader.Abort(); return; } for ( int i = 0; i &lt; numElements; ++i ) { if ( reader.IsOverflow() ) break; ReadBits( buffer, elements[i], 32 ); } }}; 但是这么做了以后，整个读取函数就开始变得非常的复杂，而且如果有什么地方你漏过了这些检查的话，你就把自己置于缓冲区溢出和无线循环这种危险的境地。你不会希望在写数据包读取函数的时候全部变成一个手动的过程，你肯定是希望这个过程是自动化的。因为这给了程序员太多犯错误的机会。 请继续关注下一篇文，在那篇文章里面我将向你展现如何使用C++来用非常简洁的方式实现。 这个系列的下一篇文章是《序列化策略》。 在这个系列的下一篇文章，我将向你展示如何将读取和写入函数统一到一个单独的序列化函数里面，它可以在不增加任何运行时损耗的情况同时处理读取和写入。 如果你觉得这篇文章有价值的话，请在patreon上支持我的写作，这样我会写的更快。你可以在BSD 3.0许可下访问到这篇文章里面的代码。非常感谢你的支持！ 【版权声明】 原文作者未做权利声明，视为共享知识产权进入公共领域，自动获得授权。]]></content>
      <categories>
        <category>GS</category>
      </categories>
      <tags>
        <tag>GafferOnGames</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kbe之ubuntu下的编译]]></title>
    <url>%2Fblog%2F2017%2F02%2F10%2Fkbe%E4%B9%8Bubuntu%E4%B8%8B%E7%9A%84%E7%BC%96%E8%AF%91%2F</url>
    <content type="text"><![CDATA[感觉之前的博客已经整理了大多数之前的关于基础的私人笔记, 现在应该可以讨论一下实操的东西了.先来一发之前的kbe在ubuntu下的编译笔记吧, 因为官方对于ubuntu下的kbe编译文档是有问题的. . . . 编译步骤 安装openssl : sudo apt-get install libssl-dev 安装mysql : sudo apt-get install libmysqld-dev sudo apt-get install mysql-server 编译kbe : cd kbengine/kbe/src chmod -R 755 . make 编译出错解决方法查看是否是以下问题导致的 内存不足 硬盘容量不足(编译完要占2个G左右的硬盘空间)]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>KBE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kbe之1分钟完成安装]]></title>
    <url>%2Fblog%2F2017%2F02%2F09%2Fkbe_installation_tutorial%2F</url>
    <content type="text"><![CDATA[KBEngine概绍根据之前的博文 游戏服务端常用架构 属于第三代服务端框架，可能类似于图10。（这个理解不确定）Kbengine引擎应该是对图10中的Gate服务器和NODE和OBJ进行了细分。在功能上大体划分为与位置有关（在Kbengine中称为Cellapp）和与位置无关（在Kbengine中称为Baseapp）。类似于下面的示图架构。 KBE安装介绍 官方是有自动化的安装py脚本的, 不过还是有很多小坑的.不过其实脚本主要也就是只做两件事, 其他都是可选的: 配置环境变量 安装mysql . . . 安装步骤 安装kbe之前请提前在mysql里 建一个数据库(比如建一个数据库kbe_database) 一个拥有所有权限(免得多事…)的用户(比如这个用户是kbe_user) (具体详情请谷歌, 本篇文章是讲kbe的安装的, 不讨论mysql, 弄完mysql之后就可以开始下面的1分钟kbe安装教程啦) 找到你的kbe根目录, 然后进入根目录, 比如你的kbe根目录是kbengine, 则 : cd kbengine sudo python kbengine/kbe/tools/server/install/installer.py install 然后它就会问你 :Install KBEngine to Linux-account(No input is kbe): 为了简单起见, 建议直接填写你当前的linux用户名称, 比如我的是”b” 然后就是开始配置环境变量了, 它就会显示 12345678Check the dependences:- kbe_environment: checking...ERROR: KBE_ROOT: is error! The directory or file not found:KBE_ROOT//kbeKBE_ROOT=KBE_ROOT current: reset KBE_ROOT(No input is [/home/b/kbengine-0.9.18/]): KBE_ROOT这里填写你的kbe根目录所在路径, 比如像我的是~/kbengine-0.9.18, 那就填~/kbengine-0.9.18 他之后显示的都直接敲回车, 用默认的就可以, 如果直到他开始问你mysql的东西都没有弹出Check to some problems, if you are sure this is not a problem please skip: [yes|no]yes, 说明基本没填错 到mysql他会问 12- MySQL: checking...- MySQL is installed on the remote machine?[yes/no] 这里我们直接填yes, 然后就直接填我们之前建立好的数据库kbe_database和用户kbe_user即可, 它会显示 : 12345678- Enter mysql ip-address:127.0.0.1- Enter mysql ip-port:3306- Enter mysql-account:kbe_user- Enter mysql-password:123456- Enter mysql-databaseName:kbe_database- MySQL: yesModified: /home/b/kbengine-0.9.18//kbe/res/server/kbengine_defs.xmlKBEngine has been successfully installed! 是否安装成功 找到你的kbe根目录, 然后进入根目录, 比如你的kbe根目录是kbengine, 则1. 进入kbe根目录下的assets目录 : cd kbengine/assets 2. 运行启动脚本 : sh ./start_server.sh 用ps检查一下是否有以下进程再跑 : 12345678910b@b-VirtualBox:~/kbengine-0.9.18/assets$ ps -ef | grep -v grep | grep -i kbeb 15504 1372 0 04:28 pts/1 00:00:01 /home/b/kbengine-0.9.18/kbe/bin/server//machine --cid=2129652375332859700 --gus=1b 15505 1372 0 04:28 pts/1 00:00:05 /home/b/kbengine-0.9.18/kbe/bin/server//logger --cid=1129653375331859700 --gus=2b 15506 1372 0 04:28 pts/1 00:00:02 /home/b/kbengine-0.9.18/kbe/bin/server//interfaces --cid=1129652375332859700 --gus=3b 15507 1372 0 04:28 pts/1 00:00:06 /home/b/kbengine-0.9.18/kbe/bin/server//dbmgr --cid=3129652375332859700 --gus=4b 15508 1372 0 04:28 pts/1 00:00:07 /home/b/kbengine-0.9.18/kbe/bin/server//baseappmgr --cid=4129652375332859700 --gus=5b 15509 1372 0 04:28 pts/1 00:00:07 /home/b/kbengine-0.9.18/kbe/bin/server//cellappmgr --cid=5129652375332859700 --gus=6b 15510 1372 0 04:28 pts/1 00:00:03 /home/b/kbengine-0.9.18/kbe/bin/server//baseapp --cid=6129652375332859700 --gus=7b 15511 1372 0 04:28 pts/1 00:00:03 /home/b/kbengine-0.9.18/kbe/bin/server//cellapp --cid=7129652375332859700 --gus=8b 15512 1372 0 04:28 pts/1 00:00:06 /home/b/kbengine-0.9.18/kbe/bin/server//loginapp --cid=8129652375332859700 --gus=9 检查我们mysql中的kbe_database数据库里是否多了几个表 : 1234567891011mysql&gt; show tables;+---------------------------+| Tables_in_b_test_database |+---------------------------+| kbe_accountinfos || kbe_email_verification || kbe_entitylog || kbe_serverlog || tbl_Account |+---------------------------+5 rows in set (0.00 sec) 好, 如果都有基本安装完成!]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>KBE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多级指针与多维数组详解]]></title>
    <url>%2Fblog%2F2017%2F02%2F08%2F%E5%A4%9A%E7%BA%A7%E6%8C%87%E9%92%88%E4%B8%8E%E5%A4%9A%E7%BB%B4%E6%95%B0%E7%BB%84%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[指针与数组是 C/C++ 编程中非常重要的元素，同时也是较难以理解的。其中，多级指针与 “多维” 数组更是让很多人云里雾里，其实，只要掌握一定的方法，理解多级指针和 “多维” 数组完全可以像理解一级指针和一维数组那样简单。 基础知识首先，先声明一些常识，如果你对这些常识还不理解，请先去弥补一下基础知识： 实际上并不存在多维数组，所谓的多维数组本质上是用一维数组模拟的。 数组名是一个常量（意味着不允许对其进行赋值操作），其代表数组首元素的地址。 数组与指针的关系是因为数组下标操作符[]，比如，int a[3][2]相当于((a+3)+2) 。 指针是一种变量，也具有类型，其占用内存空间大小和系统有关，一般32位系统下，sizeof(指针变量)=4。 指针可以进行加减算术运算，加减的基本单位是sizeof(指针所指向的数据类型)。 对数组的数组名进行取地址(&amp;)操作，其类型为整个数组类型。 对数组的数组名进行sizeof运算符操作，其值为整个数组的大小(以字节为单位)。 数组作为函数形参时会退化为指针。 指针一个指针包含两方面： 地址值； 所指向的数据类型。 解引用操作符（dereference operator）会根据指针当前的地址值，以及所指向的数据类型，访问一块连续的内存空间（大小由指针所指向的数据类型决定），将这块空间的内容转换成相应的数据类型，并返回左值。 有时候，两个指针的值相同，但数据类型不同，解引用取到的值也是不同的，例如， 12345char str[] =&#123;0, 1, 2, 3&#125;; /* 以字符的 ASCII 码初始化 */ char * pc = &amp;str[0]; /* pc 指向 str[0]，即 0 */ int * pi = (int *) pc; /* 指针的 “值” 是个地址，32 位。 */ 此时，pc 和 pi 同时指向 str[0]，但 * pc 的值为 0（即，ASCII 码值为 0 的字符）；而 * pi 的值为 50462976。或许把它写成十六进制会更容易理解：0x03020100（4 个字节分别为 3,2,1,0）。我想你已经明白了，因为小端字节序, 且指针 pi 指向的类型为 int，因此在解引用时，需要访问 4 个字节的连续空间，并将其转换为 int 返回。 一维数组与数组指针假如有一维数组如下： char a[3]; 该数组一共有 3 个元素，元素的类型为 char，如果想定义一个指针指向该数组，也就是如果想把数组名 a 赋值给一个指针变量，那么该指针变量的类型应该是什么呢？前文说过，一个数组的数组名代表其首元素的地址，也就是相当于 &amp; a[0]，而 a[0] 的类型为 char，因此 &amp; a[0] 类型为 char *，因此，可以定义如下的指针变量： char * p = a;//相当于char * p = &amp;a[0] 以上文字可用如下内存模型图表示。 大家都应该知道，a 和 &amp; a[0] 代表的都是数组首元素的地址，而如果你将 &amp; a 的值打印出来，会发现该值也等于数组首元素的地址。请注意我这里的措辞，也就是说，&amp;a 虽然在数值上也等于数组首元素地址的值，但是其类型并不是数组首元素地址类型，也就是char *p = &amp;a是错误的。 前文第 6 条常识已经说过，对数组名进行取地址操作，其类型为整个数组，因此，&amp;a 的类型是 char (*)[3]，所以正确的赋值方式如下: char (*p)[3] = &amp;a; 注意： 很多人对类似于a+1,&amp;a+1,&amp;a[0]+1,sizeof(a),sizeof(&amp;a)等感到迷惑，其实只要搞清楚指针的类型就可以迎刃而解。比如在面对 a+1 和 &amp; a+1 的区别时，由于 a 表示数组首元素地址，其类型为 char *，因此 a+1 相当于数组首地址值 + sizeof(char)；而 &amp; a 的类型为char (*)[3]，代表整个数组，因此 &amp; a+1 相当于数组首地址值 + sizeof(a)。 sizeof(a) 代表整个数组大小，前文第 7 条说明，但是无论数组大小如何，sizeof(&amp;a) 永远等于一个指针变量占用空间的大小，具体与系统平台有关 二维数组与数组指针假如有如下二维数组： char a[3][2]; 由于实际上并不存在多维数组，因此，可以将 a[3][2] 看成是一个具有 3 个元素的一维数组，只是这三个元素分别又是一个一维数组。实际上，在内存中，该数组的确是按照一维数组的形式存储的，存储顺序为 (低地址在前)：a[0][0]、a[0][1]、a[1][0]、a[1][1]、a[2][0]、a[2][1]。(此种方式也不是绝对，也有按列优先存储的模式) 为了方便理解，我画了一张逻辑上的内存图，之所以说是逻辑上的，是因为该图只是便于理解，并不是数组在内存中实际的存储模型（实际模型为前文所述）。 如上图所示，我们可以将数组分成两个维度来看，首先是第一维，将 a[3][2] 看成一个具有三个元素的一维数组，元素分别为：a[0]、a[1]、a[2]，其中，a[0]、a[1]、a[2] 又分别是一个具有两个元素的一维数组 (元素类型为 char)。从第二个维度看，此处可以将 a[0]、a[1]、a[2] 看成自己代表” 第二维” 数组的数组名，以 a[0]为例，a[0](数组名)代表的一维数组是一个具有两个 char 类型元素的数组，而 a[0]是这个数组的数组名 (代表数组首元素地址)，因此 a[0] 类型为 char *，同理 a[1]和 a[2]类型都是 char *。而 a 是第一维数组的数组名，代表首元素地址，而首元素是一个具有两个 char 类型元素的一维数组，因此 a 就是一个指向具有两个 char 类型元素数组的数组指针，也就是 char(*)[2]。 也就是说，如下的赋值是正确的: 123char (*p)[2] = a; //a为第一维数组的数组名，类型为char (*)[2]char * p = a[0]; //a[0]维第二维数组的数组名，类型为char * 同样，对 a 取地址操作代表整个数组的首地址，类型为数组类型 (请允许我暂且这么称呼)，也就是 char (*)[3][2]，所以如下赋值是正确的： char (*p)[3][2] = &amp;a; 若做如下定义： 12345int a[3][4] = &#123;0,1,2,3,4,5,6,7,8,9,10,11&#125;; int ** p; p = (int**)a; // 不做强制类型转换会报错 说明： p 是一个二级指针，它首先是一个指针，指向一个 int*； a 是二维数组名，它首先是一个指针，指向一个含有 4 个元素的 int 数组； 由此可见，a 和 p 的类型并不相同，如果想将 a 赋值给 p，需要强制类型转换。 为什么二维数组名传递给二级指针是不安全的？假如我们将 a 强制转换之后赋值给 p : p = (int**)a; 既然 p 是二级指针，那么 当 **p 时会出什么问题呢？ 首先看一下 p 的值，p 指向 a[0][0]，即 p 的值为 a[0][0] 的地址； 再看一下 * p 的值，p 所指向的类型是 int*，占 4 字节，根据前面所讲的解引用操作符的过程：从 p 指向的地址开始，取连续 4 个字节的内容。 * p得到的正式 a[0][0] 的值，即 0。 再看一下 **p 的值，诶，报错了？当然报错了，因为你访问了地址为 0 的空间，而这个空间你是没有权限访问的。 二维数组和二级指针相关的参数匹配 三维数组与数组指针假设有三维数组： char a[3][2][2]; 同样，为了便于理解，特意画了如下的逻辑内存图。分析方法和二维数组类似，首先，从第一维角度看过去，a[3][2][2] 是一个具有三个元素 a[0]、a[1]、a[2] 的一维数组，只是这三个元素分别又是一个 “二维” 数组, a 作为第一维数组的数组名，代表数组首元素的地址，也就是一个指向一个二维数组的数组指针，其类型为 char ()[2][2]。从第二维角度看过去，a[0]、a[1]、a[2] 分别是第二维数组的数组名，代表第二维数组的首元素的地址，也就是一个指向一维数组的数组指针，类型为 char()[2]；同理，从第三维角度看过去，a[0][0]、a[0][1]、a[1][0]、a[1][1]、a[2][0]、a[2][1] 又分别是第三维数组的数组名，代表第三维数组的首元素的地址，也就是一个指向 char 类型的指针，类型为 char *。 由上可知，以下的赋值是正确的： 1234char (*p)[3][2][2] = &amp;a;//对数组名取地址类型为整个数组char (*p)[2][2] = a;char (*p) [2] = a[0];//或者a[1]、a[2]char *p = a[0][0];//或者a[0][1]、a[1][0]... 多级指针所谓的多级指针，就是一个指向指针的指针，比如: 12345char *p = "my name is chenyang.";char **pp = &amp;p;//二级指针char ***ppp = &amp;pp;//三级指针 假设以上语句都位于函数体内，则可以使用下面的简化图来表达多级指针之间的指向关系。 多级指针通常用来作为函数的形参，比如常见的 main 函数声明如下: int main(int argc,char ** argv) 因为当数组用作函数的形参的时候，会退化为指针来处理，所以上面的形式和下面是一样的。 int main(int argc,char* argv[]) argv 用于接收用户输入的命令参数，这些参数会以字符串数组的形式传入，类似于: 12345//模拟用户传入的参数char * parm[] = &#123;"parm1","parm2","parm3","parm4"&#125;;//模拟调用main函数，实际中main函数是由入口函数调用的(glibc中的入口函数默认为_start)main(sizeof(parm)/sizeof(char *),parm); 多级指针的另一种常见用法是，假设用户想调用一个函数分配一段内存，那么分配的内存地址可以有两种方式拿到：第一种是通过函数的返回值，该种方式的函数声明如下： 12345void * get_memery(int size)&#123; void *p = malloc(size); return p;&#125; 第二种获取地址的方法是使用二级指针，代码如下： 12345678910int get_memery(int** buf,int size)&#123; *buf = (int *)malloc(size); if(*buf == NULL) return -1; else return 0;&#125;int *p = NULL;get_memery(&amp;p,10);]]></content>
      <categories>
        <category>CPP</category>
      </categories>
      <tags>
        <tag>CPP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[进程间的通信与同步]]></title>
    <url>%2Fblog%2F2017%2F01%2F27%2Fipc%2F</url>
    <content type="text"><![CDATA[概绍IPC 即 Inter Process Communication, 大概有以下几种方式(排序已打乱) : 6.共享内存( shared memory, 非常实用, 后文将说一下比较常用的两种方式, 分别是 mmap 和 System V共享内存 ) ：共享内存就是映射一段能被其他进程所访问的内存，这段共享内存由一个进程创建，但多个进程都可以访问。共享内存是最快的 IPC 方式，它是针对其他进程间通信方式运行效率低而专门设计的。它往往与其他通信机制，如信号量，配合使用，来实现进程间的同步和通信。 3.信号量( semophore, 主要用来进程/线程间同步, 后文将会说 System V信号量) ：信号量是一个计数器，可以用来控制多个进程对共享资源的访问。它常作为一种锁机制，防止某进程正在访问共享资源时，其他进程也访问该资源。因此，主要作为进程间以及同一进程内不同线程之间的同步手段。 7.套接字( socket ) ：套接字也是一种进程间通信机制，与其他通信机制不同的是，它可用于不同机器间的进程通信。 1.匿名管道( 英文为pipe, 这种IPC很原始 )：匿名管道是一种半双工的通信方式，通常是在父子进程间使用。 2.命名管道 ( named pipe或FIFO, 这种IPC很原始 ) ：命名管道也是半双工的通信方式，但是它允许无亲缘关系进程间的通信。 4.消息队列( message queue, 正在被淘汰 ) ：消息队列是消息的链表，存放在内核中并由消息队列标识符标识。消息队列克服了信号传递信息少、管道只能承载无格式字节流以及缓冲区大小受限等缺点。 5.信号 ( sinal ) ：信号是一种比较复杂的通信方式，用于通知接收进程某个事件已经发生。 共享内存概绍采用共享内存通信的一个显而易见的好处是效率高，因为进程可以直接读写内存，而不需要任何数据的拷贝。对于像管道和消息队列等通信方式，则需要在内核和用户空间进行四次的数据拷贝，而共享内存则只拷贝两次数据 [1]：一次从输入文件到共享内存区，另一次从共享内存区到输出文件。实际上，进程之间在共享内存时，并不总是读写少量数据后就解除映射，有新的通信时，再重新建立共享内存区域。而是保持共享区域，直到通信完毕为止，这样，数据内容一直保存在共享内存中，并没有写回文件。共享内存中的内容往往是在解除映射时才写回文件的。因此，采用共享内存的通信方式效率是非常高的。 Linux 的 2.2.x 内核支持多种共享内存方式，如 mmap() 系统调用，Posix 共享内存，以及系统 V 共享内存。linux 发行版本如 Redhat 8.0 支持 mmap() 系统调用及系统 V 共享内存，但还没实现 Posix 共享内存，本文将主要介绍 mmap() 系统调用及系统 V 共享内存 API 的原理及应用。 内核怎样保证各个进程寻址到同一个共享内存区域的内存页面 1、page cache 及 swap cache 中页面的区分：一个被访问文件的物理页面都驻留在 page cache 或 swap cache 中，一个页面的所有信息由 struct page 来描述。struct page 中有一个域为指针 mapping ，它指向一个 struct address_space 类型结构。page cache 或 swap cache 中的所有页面就是根据 address_space 结构以及一个偏移量来区分的。 2、文件与 address_space 结构的对应：一个具体的文件在打开后，内核会在内存中为之建立一个 struct inode 结构，其中的 i_mapping 域指向一个 address_space 结构。这样，一个文件就对应一个 address_space 结构，一个 address_space 与一个偏移量能够确定一个 page cache 或 swap cache 中的一个页面。因此，当要寻址某个数据时，很容易根据给定的文件及数据在文件内的偏移量而找到相应的页面。 3、进程调用 mmap() 时，只是在进程空间内新增了一块相应大小的缓冲区，并设置了相应的访问标识，但并没有建立进程空间到物理页面的映射。因此，第一次访问该空间时，会引发一个缺页异常。 4、对于共享内存映射情况，缺页异常处理程序首先在 swap cache 中寻找目标页（符合 address_space 以及偏移量的物理页），如果找到，则直接返回地址；如果没有找到，则判断该页是否在交换区 (swap area)，如果在，则执行一个换入操作；如果上述两种情况都不满足，处理程序将分配新的物理页面，并把它插入到 page cache 中。进程最终将更新进程页表。注：对于映射普通文件情况（非共享映射），缺页异常处理程序首先会在 page cache 中根据 address_space 以及数据偏移量寻找相应的页面。如果没有找到，则说明文件数据还没有读入内存，处理程序会从磁盘读入相应的页面，并返回相应地址，同时，进程页表也会更新。 5、所有进程在映射同一个共享内存区域时，情况都一样，在建立线性地址与物理地址之间的映射之后，不论进程各自的返回地址如何，实际访问的必然是同一个共享内存区域对应的物理页面。注：一个共享内存区域可以看作是特殊文件系统 shm 中的一个文件，shm 的安装点在交换区上。 上面涉及到了一些数据结构，围绕数据结构理解问题会容易一些。 共享内存的优缺点使用共享内存的优缺点如下所述 。 优点：使用共享内存进行进程间的通信非常方便，而且函数的接口也简单，数据的共享还使进程间的数据不用传送，而是直接访问内存，也加快了程序的效率。 同时，它也不像无名管道那样要求通信的进程有一定的父子关系 。 缺点：共享 内存没有提供同步的机制，这使得在使用共享 内存进行进程间通信时，往往要借助其他的手段来进行进程间的同步工作 。 mmapmmap() 系统调用使得进程之间通过映射同一个普通文件实现共享内存。普通文件被映射到进程地址空间后，进程可以向访问普通内存一样对文件进行访问，不必再调用 read()，write（）等操作。 注：实际上，mmap() 系统调用并不是完全为了用于共享内存而设计的。它本身提供了不同于一般对普通文件的访问方式，进程可以像读写内存一样对普通文件的操作。而 Posix 或系统 V 的共享内存 IPC 则纯粹用于共享目的，当然 mmap() 实现共享内存也是其主要应用之一。 mmap() 系统调用形式如下void* mmap (void * addr , size_t len , int prot , int flags , int fd , off_t offset) 参数 fd 为即将映射到进程空间的文件描述字，一般由 open() 返回，同时，fd 可以指定为 - 1，此时须指定 flags 参数中的 MAP_ANON，表明进行的是匿名映射（不涉及具体的文件名，避免了文件的创建及打开，很显然只能用于具有亲缘关系的进程间通信）。 len 是映射到调用进程地址空间的字节数，它从被映射文件开头 offset 个字节开始算起。 prot 参数指定共享内存的访问权限。可取如下几个值的或：PROT_READ（可读） , PROT_WRITE （可写）, PROT_EXEC （可执行）, PROT_NONE（不可访问）。 flags 由以下几个常值指定：MAP_SHARED , MAP_PRIVATE , MAP_FIXED，其中，MAP_SHARED , MAP_PRIVATE 必选其一，而 MAP_FIXED 则不推荐使用。 offset 参数一般设为 0，表示从文件头开始映射。 参数 addr 指定文件应被映射到进程空间的起始地址，一般被指定一个空指针，此时选择起始地址的任务留给内核来完成。 函数的返回值为最后文件映射到进程空间的地址，进程可直接操作起始地址为该值的有效地址。这里不再详细介绍 mmap() 的参数，读者可参考 mmap() 手册页获得进一步的信息。 系统调用 mmap() 用于共享内存的两种方式 （1）使用普通文件提供的内存映射：适用于任何进程之间； 此时，需要打开或创建一个文件，然后再调用 mmap()；典型调用代码如下： fd=open(name, flag, mode); ptr=mmap(NULL, len , PROT_READ|PROT_WRITE, MAP_SHARED , fd , 0); 通过 mmap() 实现共享内存的通信方式有许多特点和要注意的地方，我们将在范例中进行具体说明。 （2）使用特殊文件提供匿名内存映射：适用于具有亲缘关系的进程之间； 由于父子进程特殊的亲缘关系，在父进程中先调用 mmap()，然后调用 fork()。 那么在调用 fork() 之后，子进程继承父进程匿名映射后的地址空间，同样也继承 mmap() 返回的地址，这样，父子进程就可以通过映射区域进行通信了。 注意，这里不是一般的继承关系。 一般来说，子进程单独维护从父进程继承下来的一些变量。 而 mmap() 返回的地址，却由父子进程共同维护。 对于具有亲缘关系的进程实现共享内存最好的方式应该是采用匿名内存映射的方式。 此时，不必指定具体的文件，只要设置相应的标志即可，参见范例 2。 系统调用 munmap()int munmap(void * addr, size_t len)该调用在进程地址空间中解除一个映射关系，addr 是调用 mmap() 时返回的地址，len 是映射区的大小。当映射关系解除后，对原来映射地址的访问将导致段错误发生。 系统调用 msync()int msync (void * addr , size_t len, int flags)一般说来，进程在映射空间的对共享内容的改变并不直接写回到磁盘文件中，往往在调用 munmap（）后才执行该操作。可以通过调用 msync() 实现磁盘上文件内容与共享内存区的内容一致。 mmap() 范例下面将给出使用 mmap() 的两个范例： 范例 1 给出两个进程通过映射普通文件实现共享内存通信； 范例 2 给出父子进程通过匿名映射实现共享内存。 系统调用 mmap() 有许多有趣的地方，下面是通过 mmap（）映射普通文件实现进程间的通信的范例，我们通过该范例来说明 mmap() 实现共享内存的特点及注意事项。 范例1两个进程通过映射普通文件实现共享内存通信范例1 包含两个子程序：map_normalfile1.c 及 map_normalfile2.c。编译两个程序，可执行文件分别为 map_normalfile1 及 map_normalfile2。两个程序通过命令行参数指定同一个文件来实现共享内存方式的进程间通信。 map_normalfile2 试图打开命令行参数指定的一个普通文件，把该文件映射到进程的地址空间，并对映射后的地址空间进行写操作。map_normalfile1 把命令行参数指定的文件映射到进程地址空间，然后对映射后的地址空间执行读操作。这样，两个进程通过命令行参数指定同一个文件来实现共享内存方式的进程间通信。 下面是两个程序代码： map_normalfile1.c12345678910111213141516171819202122232425262728293031323334353637/*-------------map_normalfile1.c-----------*/#include &lt;sys/mman.h&gt;#include &lt;sys/types.h&gt;#include &lt;fcntl.h&gt;#include &lt;unistd.h&gt;typedef struct&#123; char name[4]; int age;&#125;people;main(int argc, char** argv) // map a normal file as shared mem:&#123; int fd,i; people *p_map; char temp; fd=open(argv[1],O_CREAT|O_RDWR|O_TRUNC,00777); lseek(fd,sizeof(people)*5-1,SEEK_SET); write(fd,"",1); p_map = (people*) mmap( NULL,sizeof(people)*10,PROT_READ|PROT_WRITE, MAP_SHARED,fd,0 ); close( fd ); temp = 'a'; for(i=0; i&lt;10; i++) &#123; temp += 1; memcpy( ( *(p_map+i) ).name, &amp;temp,2 ); ( *(p_map+i) ).age = 20+i; &#125; printf(" initialize over \n "); sleep(10); munmap( p_map, sizeof(people)*10 ); printf( "umap ok \n" );&#125; map_normalfile2.c12345678910111213141516171819202122232425/*-------------map_normalfile2.c-----------*/#include &lt;sys/mman.h&gt;#include &lt;sys/types.h&gt;#include &lt;fcntl.h&gt;#include &lt;unistd.h&gt;typedef struct&#123; char name[4]; int age;&#125;people;main(int argc, char** argv) // map a normal file as shared mem:&#123; int fd,i; people *p_map; fd=open( argv[1],O_CREAT|O_RDWR,00777 ); p_map = (people*)mmap(NULL,sizeof(people)*10,PROT_READ|PROT_WRITE, MAP_SHARED,fd,0); for(i = 0;i&lt;10;i++) &#123; printf( "name: %s age %d;\n",(*(p_map+i)).name, (*(p_map+i)).age ); &#125; munmap( p_map,sizeof(people)*10 );&#125; map_normalfile1.c 首先定义了一个 people 数据结构，（在这里采用数据结构的方式是因为，共享内存区的数据往往是有固定格式的，这由通信的各个进程决定，采用结构的方式有普遍代表性）。map_normfile1 首先打开或创建一个文件，并把文件的长度设置为 5 个 people 结构大小。然后从 mmap() 的返回地址开始，设置了 10 个 people 结构。然后，进程睡眠 10 秒钟，等待其他进程映射同一个文件，最后解除映射。 map_normfile2.c 只是简单的映射一个文件，并以 people 数据结构的格式从 mmap() 返回的地址处读取 10 个 people 结构，并输出读取的值，然后解除映射。 分别把两个程序编译成可执行文件 map_normalfile1 和 map_normalfile2 后，在一个终端上先运行./map_normalfile2 /tmp/test_shm，程序输出结果如下： initialize over umap ok在 map_normalfile1 输出 initialize over 之后，输出 umap ok 之前，在另一个终端上运行 map_normalfile2 /tmp/test_shm，将会产生如下输出 (为了节省空间，输出结果为稍作整理后的结果)： name: b age 20; name: c age 21; name: d age 22; name: e age 23; name: f age 24; name: g age 25; name: h age 26; name: I age 27; name: j age 28; name: k age 29;在 map_normalfile1 输出 umap ok 后，运行 map_normalfile2 则输出如下结果： name: b age 20; name: c age 21; name: d age 22; name: e age 23; name: f age 24; name: age 0; name: age 0; name: age 0; name: age 0; name: age 0;从程序的运行结果中可以得出的结论 1、 最终被映射文件的内容的长度不会超过文件本身的初始大小，即映射不能改变文件的大小； 2、 可以用于进程通信的有效地址空间大小大体上受限于被映射文件的大小，但不完全受限于文件大小。打开文件被截短为 5 个 people 结构大小，而在 map_normalfile1 中初始化了 10 个 people 数据结构，在恰当时候（map_normalfile1 输出 initialize over 之后，输出 umap ok 之前）调用 map_normalfile2 会发现 map_normalfile2 将输出全部 10 个 people 结构的值，后面将给出详细讨论。 注：在 linux 中，内存的保护是以页为基本单位的，即使被映射文件只有一个字节大小， 内核也会为映射分配一个页面大小的内存。当被映射文件小于一个页面大小时， 进程可以对从 mmap() 返回地址开始的一个页面大小进行访问， 而不会出错；但是， 如果对一个页面以外的地址空间进行访问， 则导致错误发生， 后面将进一步描述。因此， 可用于进程间通信的有效地址空间大小不会超过文件大小及一个页面大小的和。 3、 文件一旦被映射后，调用 mmap() 的进程对返回地址的访问是对某一内存区域的访问，暂时脱离了磁盘上文件的影响。所有对 mmap() 返回地址空间的操作只在内存中有意义，只有在调用了 munmap() 后或者 msync() 时，才把内存中的相应内容写回磁盘文件，所写内容仍然不能超过文件的大小。 范例2父子进程通过匿名映射实现共享内存并用semaphore同步主要介绍下在多进程中使用信号量semaphore的方法。在上一文中，我们已经知道semaphore和mutex对临界区访问控制的一个最主要区别就是semaphore可以跨进程使用，而mutex只能在一个进程中使用。我们再来看下sem_init的原型，熟悉决定进程共享或者线程共享的方法： 12#include &lt;semaphore.h&gt;int sem_init(sem_t *sem, int pshared, unsigned int value); 通过设置pshared的值来控制信号量是属于进程间共享还是线程间共享，若pshared为0表明是多线程共享，否则就是多进程间共享。 接下来我们实验思路是：创建两个进程，一个进程负责读取用户在界面输入的数据，然后存入本地的test.txt文件；另一个进程负责读取该文件，然后在标准输出上显示读取的内容。 为此，我们需要创建两个个支持两个进程访问的信号量sem1和sem2，读文件时需要获取sem1信号，读取结束后释放sem2信号；写文件需要获取sem2信号，写文件结束后方式sem1信号。 sem2的初始值为1，sem1的初始值为0，以保证先写入再进行读取，源代码如下，稍后挑关键内容进行解释： mmap_fork_sync.c12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788#include&lt;stdio.h&gt;#include&lt;stdlib.h&gt;#include&lt;pthread.h&gt;#include&lt;semaphore.h&gt;#include&lt;string.h&gt;#include&lt;sys/mman.h&gt;#include&lt;unistd.h&gt;#include&lt;sys/types.h&gt;#include&lt;sys/stat.h&gt;#include&lt;fcntl.h&gt;#define BUF_SIZE 30 void readfile(sem_t* psem1,sem_t* psem2)&#123; FILE* fp; char buf[BUF_SIZE]; int str_len,str_seek=0; while(1) &#123; sem_wait(psem1); fp=fopen("data.txt","r+"); if(fp==NULL) return ; memset(buf,0,sizeof(BUF_SIZE)); fseek(fp,str_seek,SEEK_SET); str_len=fread(buf,sizeof(char),BUF_SIZE-1,fp); buf[str_len]=0; str_seek+=str_len; fputs("output:",stdout); puts(buf); fclose(fp); sem_post(psem2); &#125;&#125;void writefile(sem_t* psem1,sem_t* psem2)&#123; FILE* fp; char buf[BUF_SIZE]; while(1) &#123; sem_wait(psem2); fp=fopen("data.txt","a"); if(fp==NULL) return; memset(buf,0,BUF_SIZE); fputs("Input:",stdout); fgets(buf,BUF_SIZE,stdin); fwrite(buf,sizeof(char),strlen(buf),fp); fclose(fp); sem_post(psem1); &#125;&#125; int main()&#123; int pid; int fd1,fd2; void* pv; sem_t* psem1; sem_t* psem2; fd1=open("data1",O_CREAT|O_RDWR|O_TRUNC,0666); fd2=open("data2",O_CREAT|O_RDWR|O_TRUNC,0666);\ ftruncate(fd1,8192); ftruncate(fd2,8192); //lseek(fd,5000,SEEK_SET); psem1=(sem_t*)mmap(NULL,sizeof(sem_t),PROT_READ|PROT_WRITE,MAP_SHARED,fd1,0); psem2=(sem_t*)mmap(NULL,sizeof(sem_t),PROT_READ|PROT_WRITE,MAP_SHARED,fd2,0); sem_init(psem1,1,0); sem_init(psem2,1,1); pid=fork(); if(pid==0) &#123; puts("进入子进程"); writefile(psem1,psem2); &#125; else &#123; puts("进入父进程"); readfile(psem1,psem2); &#125; sem_destroy(psem1); sem_destroy(psem2); munmap(psem1,sizeof(sem_t)); munmap(psem2,sizeof(sem_t)); close(fd1); close(fd2); return 0; &#125; 为了能够跨进程使用 semaphore ，我们引入了跨进程的技术mmap,第61、第62行分别打开了两个mmap需要映射的文件，和我们平时用的open函数不同，这里面为程序赋予了该文件的666权限。这点很重要，因为mmap需要映射的本地文件必须明确赋予其可读写的权限，否则无法通信。 第63行和第64行分别设置两个本地映射文件的大小，以保证有充分的空间在mmap中映射并容纳我们定义的sem_t变量。这点也很重要，如果空间不够会造成总线错误。 第66行和第67行分别利用mmap在共享内存中映射了两个sem_t类型的指针，这就是我们需要sem_init的信号量。 第68、69行开始初始化信号量。 70行fork了两个进程，在子进程中我们进行写操作，在主进程中我们进行读操作。读写操作的代码比较简单，在这里不再多说。 第81到86行在使用完信号量后分别是销毁信号量、释放共享内存、关闭文件操作符。 程序写到这里基本上完成了这个实验，可以考察程序的输出结果，编译命令 : gcc mmap_fork_sync.c -o mmap_fork_sync -pthread , 体会父子进程匿名共享内存： b@b-VirtualBox:~/tc/mmap_test$ ./mmap_fork_sync 进入父进程 进入子进程 Input:4 output:4 Input:5 output:5 Input:6 output:6 Input:7 output:7 Input:7 output:7 ...我们可以顺便可以简单总结下在多进程中使用信号量的步骤： （1）open()用于进行mmap映射的文件，得到文件操作符fd；（2）把映射文件用ftruncate或者fseek重新设置大小，以保证有足够的空间容纳我们需要传递的sem_t变量；（3）利用mmap函数在共享内存中创建sen_t类型的指针。（4）用sem_init()函数初始化第（3）步中创建的指针，也就得到了我们需要的信号量。（5）用sem_wait()和sem_post()函数进行信号量的等待和释放。（6）用sem_destroy()销毁信号量。（7）用munmap()释放共享内存以及用close()函数关闭文件操作符。 理解页式管理机制前面对范例运行结构的讨论中已经提到，linux 采用的是页式管理机制。对于用 mmap() 映射普通文件来说，进程会在自己的地址空间新增一块空间，空间大小由 mmap() 的 len 参数指定，注意，进程并不一定能够对全部新增空间都能进行有效访问。进程能够访问的有效地址大小取决于文件被映射部分的大小。简单的说，能够容纳文件被映射部分大小的最少页面个数决定了进程从 mmap() 返回的地址开始，能够有效访问的地址空间大小。超过这个空间大小，内核会根据超过的严重程度返回发送不同的信号给进程。可用如下图示说明： 注意：文件被映射部分而不是整个文件决定了进程能够访问的空间大小，另外，如果指定文件的偏移部分，一定要注意为页面大小的整数倍。下面是对进程映射地址空间的访问范例： 12345678910111213141516171819202122232425262728293031323334353637#include &lt;sys/mman.h&gt;#include &lt;sys/types.h&gt;#include &lt;fcntl.h&gt;#include &lt;unistd.h&gt;typedef struct&#123; char name[4]; int age;&#125;people;main(int argc, char** argv)&#123; int fd,i; int pagesize,offset; people *p_map; pagesize = sysconf(_SC_PAGESIZE); printf("pagesize is %d\n",pagesize); fd = open(argv[1],O_CREAT|O_RDWR|O_TRUNC,00777); lseek(fd,pagesize*2-100,SEEK_SET); write(fd,"",1); offset = 0; //此处offset = 0编译成版本1；offset = pagesize编译成版本2 p_map = (people*)mmap(NULL,pagesize*3,PROT_READ|PROT_WRITE,MAP_SHARED,fd,offset); close(fd); for(i = 1; i&lt;10; i++) &#123; (*(p_map+pagesize/sizeof(people)*i-2)).age = 100; printf("access page %d over\n",i); (*(p_map+pagesize/sizeof(people)*i-1)).age = 100; printf("access page %d edge over, now begin to access page %d\n",i, i+1); (*(p_map+pagesize/sizeof(people)*i)).age = 100; printf("access page %d over\n",i+1); &#125; munmap(p_map,sizeof(people)*10);&#125; 如程序中所注释的那样，把程序编译成两个版本，两个版本主要体现在文件被映射部分的大小不同。文件的大小介于一个页面与两个页面之间（大小为：pagesize2-99），版本 1 的被映射部分是整个文件，版本 2 的文件被映射部分是文件大小减去一个页面后的剩余部分，不到一个页面大小 (大小为：pagesize-99)。程序中试图访问每一个页面边界，两个版本都试图在进程空间中映射 pagesize3 的字节数。 版本 1 的输出结果如下： pagesize is 4096 access page 1 over access page 1 edge over, now begin to access page 2 access page 2 over access page 2 over access page 2 edge over, now begin to access page 3 Bus error //被映射文件在进程空间中覆盖了两个页面，此时，进程试图访问第三个页面版本 2 的输出结果如下： pagesize is 4096 access page 1 over access page 1 edge over, now begin to access page 2 Bus error //被映射文件在进程空间中覆盖了一个页面，此时，进程试图访问第二个页面结论：采用系统调用 mmap() 实现进程间通信是很方便的，在应用层上接口非常简洁。内部实现机制区涉及到了 linux 存储管理以及文件系统等方面的内容，可以参考一下相关重要数据结构来加深理解。在本专题的后面部分，将介绍系统 v 共享内存的实现。 System V共享内存说一下System V共享内存. 顾名思义，共享内存就是允许两个不相关的进程访问同一个逻辑内存。 共享内存是在两个正在运行的进程之间共享和传递数据的一种非常有效的方式 。 不同进程之间共享的内存通常安排在同－段物理内存中 。 进程可以将同一段共享内存连接到它们 自己 的地址空间中，所有进程都可以访问共享内存中的地址，就好像它们是由用 C 语言 函数 malloc 分配的内存一样。 而如果某个进程向共享内存写入数据，所做的改动将立即影响到可以访问同一段共享内存的任何其他进程 。 不过，共享内存并未提供同步机制，也就是说，在第一个进程对共享内存的写操作结束之前，并无自动机制可以阻止第二个进程对它进行读取。 所以通常需要用其他的机制来同步对共享内存的访问 。 shmget在 Linux 中也提供了一组函数接口用于使用共享 内存， 首先常用的函数是 shmget ， 该函数用来创建共享内存，它用到的头文件是 ： #include &lt;sys/shm .h&gt; 函数原型是：int shmget(key_ t key, int size , int flag) ; 第一个参数，程序需要提供一个参数 key （非 0 整数），它有效地为共享内存段命名，shmget 函数运行成功时会返回一个与 key 相关的共享内存标识符（非负整数），用于后续的共享内存函数；调用失败返回－ 1 。不相关的进程可以通过该函数的返回值访问同一共享内存，它代表程序可能要使用的某个资源，程序对所有共享内存的访问都是间接的 。 程序先通过调用 shmget 函数并提供一个键，再由系统生成一个相应的共享内存标识符（ shmget 函数的返回值） 。 第二个参数， size 以字节为单位指定需要共享的内存容量。 第三个参数， shmfl.g 是权限标志，它的作用与 open 函数的 mode 参数一样，如果要想在key 标识的共享 内存不存在的条件下创建它的话，可以与 IPC_CREAT 做或操作 。 共享内存的权限标志与文件的读写权限一样，举例来说， 0644 表示允许一个进程创建的共享内存被内存创建者所拥有的进程向共享内存读取和写人数据，同时其他用户创建的进程只能读取共享内存 。 shmat当共享 内存创建后，其余进程可以调用 shmat 将其连接到自身的地址空间中，它的函数原型是 ：void *shmat(int shmid , void *addr , int flag) ; shmid 为 shmget 函数返回的共享存储标识符， addr 和 flag 参数决定了以什么方式来确定连接的地址，函数的返回值即是该进程数据段所连接的实际地址， 其他进程可以对此进程进行读写操作 。 shmdtshmdt 函数用于将共享 内存从当前进程中分离 。 注意，将共享内存分离并不是删除它，只是使该共享内存对当前进程不再可用 。 它的原型如下：int shmdt(const void *shmaddr) ; 参数 shmaddr 是 shmat 函数返回的地址指针，调用成功时返回 0 ，失败时返回－ 1 。 例子程序共享 内存是进程间通信的最快的方式，但是共享 内存的同步问题自身无法解决（即进程该何时去共享内存取得数据，而何时不能取），但用信号量即可轻易解决这个问题 。 下面使用例来说明如何使用信号量解决共享内存的同步问题 。 这个例子的主要功能是writer 向 reader 传递数据，并且只有在 writer 发送完毕后， reader 才取数据，否则阻塞等待 。 reader.cpp12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849#include &lt;sys/types.h&gt;#include &lt;sys/ipc.h&gt;#include &lt;sys/sem.h&gt;#include &lt;stdio.h&gt;#include &lt;unistd.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;#include &lt;sys/shm.h&gt;#include &lt;errno.h&gt;#define SEM_KEY 4001#define SHM_KEY 5678union semun &#123; int val;&#125;;int main(void)&#123; /*create a shm*/ int semid,shmid; shmid = shmget(SHM_KEY,sizeof(int),IPC_CREAT|0666); if(shmid&lt;0)&#123; printf("create shm error\n"); return -1; &#125; void * shmptr; shmptr =shmat(shmid,NULL,0); if(shmptr == (void *)-1)&#123; printf("shmat error:%s\n",strerror(errno)); return -1; &#125; int * data = (int *)shmptr; semid = semget(SEM_KEY,2,IPC_CREAT|0666);/*这里是创建一个semid，并且有两个信号量*/ union semun semun1;/*下面这四行就是初始化那两个信号量，一个val=0,另一个val=1*/ semun1.val=0; semctl(semid,0,SETVAL,semun1); semun1.val=1; semctl(semid,1,SETVAL,semun1); struct sembuf sembuf1; while(1)&#123; sembuf1.sem_num=0;/*sem_num=0指的是下面操作指向第一个信号量，上面设置可知其 val=0*/ sembuf1.sem_op=-1; /*初始化值为0，再-1的话就会等待*/ sembuf1.sem_flg=SEM_UNDO; semop(semid,&amp;sembuf1,1);/*reader在这里会阻塞,直到收到信号*/ printf("the NUM:%d\n",*data);/*输出结果*/ sembuf1.sem_num=1;/*这里让writer再次就绪，就这样循环*/ sembuf1.sem_op=1; sembuf1.sem_flg=SEM_UNDO; semop(semid,&amp;sembuf1,1); &#125; return 0;&#125; 然后是writer writer.cpp12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455#include &lt;sys/types.h&gt;#include &lt;sys/ipc.h&gt;#include &lt;sys/sem.h&gt;#include &lt;stdio.h&gt;#include &lt;unistd.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;#include &lt;sys/shm.h&gt;#include &lt;errno.h&gt;#define SEM_KEY 4001#define SHM_KEY 5678union semun &#123; int val;&#125;;int main(void)&#123; /*create a shm*/ int semid,shmid; shmid = shmget(SHM_KEY,sizeof(int),IPC_CREAT|0666); if(shmid&lt;0) &#123; printf("create shm error\n"); return -1; &#125; void * shmptr; shmptr =shmat(shmid,NULL,0); if(shmptr == (void *)-1) &#123; printf("shmat error:%s\n",strerror(errno)); return -1; &#125; int * data = (int *)shmptr; semid = semget(SEM_KEY,2,0666); struct sembuf sembuf1; union semun semun1; while(1) &#123; sembuf1.sem_num=1;//这里指向第2个信号量（sem_num=1） sembuf1.sem_op=-1;//操作是-1，因为第2个信号量初始值为1，所以下面不会阻塞 sembuf1.sem_flg=SEM_UNDO; semop(semid,&amp;sembuf1,1);/*继续*/ scanf("%d",data); /*用户在终端输入数据*/ sembuf1.sem_num=0;/*这里指向第一个信号量*/ sembuf1.sem_op=1;/*操作加1*/ sembuf1.sem_flg=SEM_UNDO; semop(semid,&amp;sembuf1,1); //执行+1后，我们发现，reader阻塞正是由于第一个信号量为0， //无法减一，而现在writer先为其加1，那reader就绪！writer继续循环， //发现第二个信号量已经减为0，则阻塞了，我们回到reader*/ &#125; return 0;&#125; 输出多打开几个终端，同时执行 writer 程序，看是否 reader 能够正确地读到数据 writer : [b@host 1105]$ ./writer 51 09 977writer : [b@host 1105]$ ./writer 22 11 55 55 5reader : [b@host 1105]$ ./reader the NUM:22 the NUM:11 the NUM:55 the NUM:55 the NUM:5 the NUM:51 the NUM:9 the NUM:977要想让程序安全地执行，就要有一种进程同步的进制，保证在进入临界区的操作是原子操作 。例如，使用信号量来进行进程的同步 。 因为对信号量的操作都是原子性的 。 System V信号量在 Linux 中提供了一组函数接口用于使用System V信号量 ，首先常用的函数是 semget，该函数用来创建和打开信号量 ，它用到的头文件是： 123#include &lt;sys / types . h&gt;#include &lt; sys / ipc . h &gt;#include &lt;sys/sem. h &gt; semget函数原型是：int semget( key_ t key , int nsems , int semflg) ; 该函数执行成功返回信号量标示符，失败则返回－ 1 。 参数 key 是函数通过调用负ok 函数得到的键值， nsems 代表创建信号量的个数，如果只是访问而不创建则可以指定该参数为0 ；但一旦创建了该信号量 ，就不能更改其信号量个数。 只要不删除该信号量 ，就可以重新调用该函数创建该键值的信号量 ，该函数只是返回以前创建的值，而不会重新创建。 semflg指定该信号茸的读写权限， 当创建信号量时不许加 IPC_C阻AT ，若指定 IPC CREAT IIPC_EXCL 后创建时发现存在该信号量 ，创建失败 。 semopsemop 函数，用于改变信号量的值，原型是：int semop(int semid, struct sembuf *sops , unsigned nsops) ; sem_id 是 由 semget 返回的信号量标识符， sembuf 结构的定义如下： 1234567struct sembuf &#123; short sem_num; // 除非使用一组信号量，否则它为 O short sem_op ; // 信号量在一次操作中需要改变的数据，通常是两个数， // 一个是－ 1 ，即 p （等待）操作，一个是＋ 1 ，即 v （发送信号）操作 。 short sem_flg; // 通常为 SEM_UNDO ， 使操作系统跟踪信号， // 并在进程没有释放该信号量而终止时 ， 操作系统释放信号量&#125; semctlsemctl 函数，该函数用来直接控制信号量信息，它的原型是：int semctl (int semid, int semnum, int cmd , ... ) ; 如果有第 4 个参数，它通常是一个 union semum 结构，定义如下： 12345union semun&#123; int val ; struct semid_ds *buf; unsigned short *arry ;&#125; 前两个参数与前面一个函数中的一样， cmd 通常是 SETVAL 或 IPC RMID 。 SETVAL用来把信号量初始化为一个己知的值 。 p 值通过 union semun 中的 val 成员设置，其作用是在信号量第一次使用前对它进行设置 。 IPC_RMID 用于删除一个已经无须继续使用的信号量标识符 ipcs命令ipcs 是一个 UINX/Linux 的命令 ，用于报告系统的消息队列、信号量、共享内存等 。 下面列举一些常用命令。 ipcs -a 用于列出本用户所有相关的 ipcs 参数，结果如下所示 : [b@host ~]$ ipcs -a ------ Shared Memory Segments -------- key shmid owner perms bytes nattch status 0x000004d1 32768 b 666 2052 0 0x000004d2 65537 b 666 2052 0 ------ Semaphore Arrays -------- key semid owner perms nsems ------ Message Queues -------- key msqid owner perms used-bytes messages ipcs -l 用于列出系统的限额 [b@host ~]$ ipcs -l ------ Shared Memory Limits -------- max number of segments = 4096 max seg size (kbytes) = 4194303 max total shared memory (kbytes) = 1073741824 min seg size (bytes) = 1 ------ Semaphore Limits -------- max number of arrays = 32000 max semaphores per array = 32000 max semaphores system wide = 1024000000 max ops per semop call = 500 semaphore max value = 32767 ------ Messages: Limits -------- max queues system wide = 32000 max size of message (bytes) = 65536 default max size of queue (bytes) = 65536 ipcs -u 用于列出当前的使用情况 [b@host ~]$ ipcs -u ------ Shared Memory Status -------- segments allocated 2 pages allocated 2 pages resident 2 pages swapped 0 Swap performance: 0 attempts 0 successes ------ Semaphore Status -------- used arrays = 3 allocated semaphores = 3 ------ Messages: Status -------- allocated queues = 0 used headers = 0 used space = 0 bytes ipcs -t 用于列出最后的访问时间 [b@host ~]$ ipcs -t ------ Shared Memory Attach/Detach/Change Times -------- shmid owner attached detached changed 32768 b May 18 06:46:54 May 18 06:47:43 May 18 06:45:48 65537 b May 18 06:45:57 May 18 06:46:08 May 18 06:45:57 ------ Semaphore Operation/Change Times -------- semid owner last-op last-changed ------ Message Queues Send/Recv/Change Times -------- msqid owner send recv change]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>noodle</tag>
        <tag>IPC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络物理模拟六之状态同步]]></title>
    <url>%2Fblog%2F2017%2F01%2F26%2Fstate_synchronization%2F</url>
    <content type="text"><![CDATA[自我总结状态同步的要点为 : input+state : 既通过网络发送输入信息又会发送状态信息来进行同步 发送端 优先级累加器 : 只发送一些重要的实体状态更新, 而不是所有都发. 如果遇到一个物体的状态更新信息不合适放到这个数据包里面，那么就跳过这个物体并尝试下一个。当你序列化完这个数据包以后，将那些已经在这帧更新过的物体在优先级累加器里面的值重置为0，但是那些没有在这帧更新过的物体在优先级累加器里面的值则保持不变。 接收端 抗网络抖动 : 做一个jitter buffer来缓冲数据, 然后以相同时间的间隔均匀取出 应用状态更新 : 一旦你的数据包从抖动缓冲器里面出来，你该在状态更新直接应用这些信息进行仿真。 对两边都量化(这里的量化指的是&lt;&lt;网络物理模拟五之快照压缩&gt;&gt;说的量化压缩技术) : 如果只有接收端用了量化的数据, 那接收端模拟的结果很可能与发送端不同, 所以要对两边都量化来避免发送端和接收端模拟的差异 长时间丢包的平滑处理 : 对于不同的网络断开时间用不同的平滑因子, 来自适应误差 增量压缩 : 相对编码 : 在数据包的包头里面发送最近确认的数据包的序列号（这个数据是从可靠的确认系统里面得到的）然后对每个物体编码相对这个基准帧的偏移量 绝对编码 原文原文出处 原文标题 : State Synchronization (Keeping simulations in sync by sending state) Introduction Hi, I&rsquo;m Glenn Fiedler and welcome to Networked Physics. In the previous article we discussed techniques for compressing snapshots. In this article we round out our discussion of networked physics strategies with state synchronization, the third and final strategy in this article series. State Synchronization What is state synchronization? The basic idea is that, somewhat like deterministic lockstep, we run the simulation on both sides but, unlike deterministic lockstep, we don&rsquo;t just send input, we send both input and state. This gives state synchronization interesting properties. Because we send state, we don&rsquo;t need perfect determinism to stay in sync, and because the simulation runs on both sides, objects continue moving forward between updates. This lets us approach state synchronization differently to snapshot interpolation. Instead of sending state updates for every object in each packet, we can now send updates for only a few, and if we&rsquo;re smart about how we select the objects for each packet, we can save bandwidth by concentrating updates on the most important objects. So what&rsquo;s the catch? State synchronization is an approximate and lossy synchronization strategy. In practice, this means you&rsquo;ll spend a lot of time tracking down sources of extrapolation divergence and pops. But other than that, it&rsquo;s a quick and easy strategy to get started with. Implementation Here&rsquo;s the state sent over the network per-object: struct StateUpdate { int index; vec3f position; quat4f orientation; vec3f linear_velocity; vec3f angular_velocity; }; Unlike snapshot interpolation, we&rsquo;re not just sending visual quantities like position and orientation, we&rsquo;re also sending non-visual state such as linear and angular velocity. Why is this? The reason is that state synchronization runs the simulation on both sides, so it&rsquo;s always extrapolating from the last state update applied to each object. If linear and angular velocity aren&rsquo;t synchronized, this extrapolation is done with incorrect velocities, leading to pops when objects are updated. While we must send the velocities, there&rsquo;s no point wasting bandwidth sending (0,0,0) over and over while an object is at rest. We can fix this with a trivial optimization, like so: void serialize_state_update( Stream &amp; stream, int &amp; index, StateUpdate &amp; state_update ) { serialize_int( stream, index, 0, NumCubes - 1 ); serialize_vector( stream, state_update.position ); serialize_quaternion( stream, state_update.orientation ); bool at_rest = stream.IsWriting() ? state_update.AtRest() : false; serialize_bool( stream, at_rest ); if ( !at_rest ) { serialize_vector( stream, state_update.linear_velocity ); serialize_vector( stream, state_update.angular_velocity ); } else if ( stream.IsReading() ) { state_update.linear_velocity = vec3f(0,0,0); state_update.angular_velocity = vec3f(0,0,0); } } What you see above is a serialize function. It&rsquo;s a trick I like to use to unify packet read and write. I like it because it&rsquo;s expressive while at the same time it&rsquo;s difficult to desync read and write. You can read more about them here. Packet Structure Now let&rsquo;s look at the overall structure of packets being sent: const int MaxInputsPerPacket = 32; const int MaxStateUpdatesPerPacket = 64; struct Packet{ uint32_t sequence; Input inputs[MaxInputsPerPacket]; int num_object_updates; StateUpdate state_updates[MaxStateUpdatesPerPacket];}; First we include a sequence number in each packet so we can determine out of order, lost or duplicate packets. I recommend you run the simulation at the same framerate on both sides (for example 60HZ) and in this case the sequence number can work double duty as the frame number. Input is included in each packet because it&rsquo;s needed for extrapolation. Like deterministic lockstep we send multiple redundant inputs so in the case of packet loss it&rsquo;s very unlikely that an input gets dropped. Unlike deterministic lockstep, if don&rsquo;t have the next input we don&rsquo;t stop the simulation and wait for it, we continue extrapolating forward with the last input received. Next you can see that we only send a maximum of 64 state updates per-packet. Since we have a total of 901 cubes in the simulation so we need some way to select the n most important state updates to include in each packet. We need some sort of prioritization scheme. To get started each frame walk over all objects in your simulation and calculate their current priority. For example, in the cube simulation I calculate priority for the player cube as 1000000 because I always want it to be included in every packet, and for interacting (red cubes) I give them a higher priority of 100 while at rest objects have priority of 1. Unfortunately if you just picked objects according to their current priority each frame you&rsquo;d only ever send red objects while in a katamari ball and white objects on the ground would never get updated. We need to take a slightly different approach, one that prioritizes sending important objects while also distributing updates across all objects in the simulation. Priority Accumulator You can do this with a priority accumulator. This is an array of float values, one value per-object, that is remembered from frame to frame. Instead of taking the immediate priority value for the object and sorting on that, each frame we add the current priority for each object to its priority accumulator value then sort objects in order from largest to smallest priority accumulator value. The first n objects in this sorted list are the objects you should send that frame. You could just send state updates for all n objects but typically you have some maximum bandwidth you want to support like 256kbit/sec. Respecting this bandwidth limit is easy. Just calculate how large your packet header is and how many bytes of preamble in the packet (sequence, # of objects in packet and so on) and work out conservatively the number of bytes remaining in your packet while staying under your bandwidth target. Then take the n most important objects according to their priority accumulator values and as you construct the packet, walk these objects in order and measure if their state updates will fit in the packet. If you encounter a state update that doesn&rsquo;t fit, skip over it and try the next one. After you serialize the packet, reset the priority accumulator to zero for objects that fit but leave the priority accumulator value alone for objects that didn&rsquo;t. This way objects that don&rsquo;t fit are first in line to be included in the next packet. The desired bandwidth can even be adjusted on the fly. This makes it really easy to adapt state synchronization to changing network conditions, for example if you detect the connection is having difficulty you can reduce the amount of bandwidth sent (congestion avoidance) and the quality of state synchronization scales back automatically. If the network connection seems like it should be able to handle more bandwidth later on then you can raise the bandwidth limit. Jitter Buffer The priority accumulator covers the sending side, but on the receiver side there is much you need to do when applying these state updates to ensure that you don&rsquo;t see divergence and pops in the extrapolation between object updates. The very first thing you need to consider is that network jitter exists. You don&rsquo;t have any guarantee that packets you sent nicely spaced out 60 times per-second arrive that way on the other side. What happens in the real world is you&rsquo;ll typically receive two packets one frame, 0 packets the next, 1, 2, 0 and so on because packets tend to clump up across frames. To handle this situation you need to implement a jitter buffer for your state update packets. If you fail to do this you&rsquo;ll have a poor quality extrapolation and pops in stacks of objects because objects in different state update packets are slightly out of phase with each other with respect to time. All you do in a jitter buffer is hold packets before delivering them to the application at the correct time as indicated by the sequence number (frame number) in the packet. The delay you need to hold packets for in this buffer is a much smaller amount of time relative to interpolation delay for snapshot interpolation but it&rsquo;s the same basic idea. You just need to delay packets just enough (say 4-5 frames @ 60HZ) so that they come out of the buffer properly spaced apart. Applying State Updates Once the packet comes out of the jitter how do you apply state updates? My recommendation is that you should snap the physics state hard. This means you apply the values in the state update directly to the simulation. I recommend against trying to apply some smoothing between the state update and the current state at the simulation level. This may sound counterintuitive but the reason for this is that the simulation extrapolates from the state update so you want to make sure it extrapolates from a valid physics state for that object rather than some smoothed, total bullshit made-up one. This is especially important when you are networking large stacks of objects. Surprisingly, without any smoothing the result is already pretty good: Your browser does not support the video tag. As you can see it&rsquo;s already looking quite good and barely any bandwidth optimization has been performed. Contrast this with the first video for snapshot interpolation which was at 18mbit/sec and you can see that using the simulation to extrapolate between state updates is a great way to use less bandwidth. Of course we can do a lot better than this and each optimization we do lets us squeeze more state updates in the same amount of bandwidth. The next obvious thing we can do is to apply all the standard quantization compression techniques such as bounding and quantizing position, linear and angular velocity value and using the smallest three compression as described in snapshot compression. But here it gets a bit more complex. We are extrapolating from those state updates so if we quantize these values over the network then the state that arrives on the right side is slightly different from the left side, leading to a slightly different extrapolation and a pop when the next state update arrives for that object. Your browser does not support the video tag. Quantize Both Sides The solution is to quantize the state on both sides. This means that on both sides before each simulation step you quantize the entire simulation state as if it had been transmitted over the network. Once this is done the left and right side are both extrapolating from quantized state and their extrapolations are very similar. Because these quantized values are being fed back into the simulation, you&rsquo;ll find that much more precision is required than snapshot interpolation where they were just visual quantities used for interpolation. In the cube simulation I found it necessary to have 4096 position values per-meter, up from 512 with snapshot interpolation, and a whopping 15 bits per-quaternion component in smallest three (up from 9). Without this extra precision significant popping occurs because the quantization forces physics objects into penetration with each other, fighting against the simulation which tries to keep the objects out of penetration. I also found that softening the constraints and reducing the maximum velocity which the simulation used to push apart penetrating objects also helped reduce the amount of popping. Your browser does not support the video tag. With quantization applied to both sides you can see the result is perfect once again. It may look visually about the same as the uncompressed version but in fact we&rsquo;re able to fit many more state updates per-packet into the 256kbit/sec bandwidth limit. This means we are better able to handle packet loss because state updates for each object are sent more rapidly. If a packet is lost, it&rsquo;s less of a problem because state updates for those objects are being continually included in future packets. Be aware that when a burst of packet loss occurs like 1&frasl;4 a second with no packets getting through, and this is inevitable that eventually something like this will happen, you will probably get a different result on the left and the right sides. We have to plan for this. In spite of all effort that we have made to ensure that the extrapolation is as close as possible (quantizing both sides and so on) pops can and will occur if the network stops delivering packets. Visual Smoothing We can cover up these pops with smoothing. Remember how I said earlier that you should not apply smoothing at the simulation level because it ruins the extrapolation? What we&rsquo;re going to do for smoothing instead is calculating and maintaining position and orientation error offsets that we reduce over time. Then when we render the cubes in the right side we don&rsquo;t render them at the simulation position and orientation, we render them at the simulation position + error offset, and orientation * orientation error. Over time we work to reduce these error offsets back to zero for position error and identity for orientation error. For error reduction I use an exponentially smoothed moving average tending towards zero. So in effect, I multiply the position error offset by some factor each frame (eg. 0.9) until it gets close enough to zero for it to be cleared (thus avoiding denormals). For orientation, I slerp a certain amount (0.1) towards identity each frame, which has the same effect for the orientation error. The trick to making this all work is that when a state update comes in you take the current simulation position and add the position error to that, and subtract that from the new position, giving the new position error offset which gives an identical result to the current (smoothed) visual position. The same process is then applied to the error quaternion (using multiplication by the conjugate instead of subtraction) and this way you effectively calculate on each state update the new position error and orientation error relative to the new state such that the object appears to have not moved at all. Thus state updates are smooth and have no immediate visual effect, and the error reduction smoothes out any error in the extrapolation over time without the player noticing in the common case. I find that using a single smoothing factor gives unacceptable results. A factor of 0.95 is perfect for small jitters because it smooths out high frequency jitter really well, but at the same time it is too slow for large position errors, like those that happen after multiple seconds of packet loss: Your browser does not support the video tag. The solution I use is two different scale factors at different error distances, and to make sure the transition is smooth I blend between those two factors linearly according to the amount of positional error that needs to be reduced. In this simulation, having 0.95 for small position errors (25cms or less) while having a tighter blend factor of 0.85 for larger distances (1m error or above) gives a good result. The same strategy works well for orientation using the dot product between the orientation error and the identity matrix. I found that in this case a blend of the same factors between dot 0.1 and 0.5 works well. The end result is smooth error reduction for small position and orientation errors combined with a tight error reduction for large pops. As you can see above you don&rsquo;t want to drag out correction of these large pops, they need to be fast and so they&rsquo;re over quickly otherwise they&rsquo;re really disorienting for players, but at the same time you want to have really smooth error reduction when the error is small hence the adaptive error reduction approach works really well. Your browser does not support the video tag. Delta Compression Even though I would argue the result above is probably good enough already it is possible to improve the synchronization considerably from this point. For example to support a world with larger objects or more objects being interacted with. So lets work through some of those techniques and push this technique as far as it can go. There is an easy compression that can be performed. Instead of encoding absolute position, if it is within a range of the player cube center, encode position as a relative offset to the player center position. In the common cases where bandwidth is high and state updates need to be more frequent (katamari ball) this provides a large win. Next, what if we do want to perform some sort of delta encoding for state synchronization? We can but it&rsquo;s quite different in this case than it is with snapshots because we&rsquo;re not including every cube in every packet, so we can&rsquo;t just track the most recent packet received and say, OK all these state updates in this packet are relative to packet X. What you actually have to do is per-object update keep track of the packet that includes the base for that update. You also need to keep track of exactly the set of packets received so that the sender knows which packets are valid bases to encode relative to. This is reasonably complicated and requires a bidirectional ack system over UDP. Such a system is designed for exactly this sort of situation where you need to know exactly which packets definitely got through. You can find a tutorial on how to implement this in this article. So assuming that you have an ack system you know with packet sequence numbers get through. What you do then is per-state update write one bit if the update is relative or absolute, if absolute then encode with no base as before, otherwise if relative send the 16 bit sequence number per-state update of the base and then encode relative to the state update data sent in that packet. This adds 1 bit overhead per-update as well as 16 bits to identify the sequence number of the base per-object update. Can we do better? Yes. In turns out that of course you&rsquo;re going to have to buffer on the send and receive side to implement this relative encoding and you can&rsquo;t buffer forever. In fact, if you think about it you can only buffer up a couple of seconds before it becomes impractical and in the common case of moving objects you&rsquo;re going to be sending the updates for same object frequently (katamari ball) so practically speaking the base sequence will only be from a short time ago. So instead of sending the 16 bit sequence base per-object, send in the header of the packet the most recent acked packet (from the reliability ack system) and per-object encode the offset of the base sequence relative to that value using 5 bits. This way at 60 packets per-second you can identify an state update with a base half a second ago. Any base older than this is unlikely to provide a good delta encoding anyway because it&rsquo;s old, so in that case just drop back to absolute encoding for that update. Now lets look at the type of objects that are going to have these absolute encodings rather than relative. They&rsquo;re the objects at rest. What can we do to make them as efficient as possible? In the case of the cube simulation one bad result that can occur is that a cube comes to rest (turns grey) and then has its priority lowered significantly. If that very last update with the position of that object is missed due to packet loss, it can take a long time for that object to have its at rest position updated. We can fix this by tracking objects which have recently come to rest and bumping their priority until an ack comes back for a packet they were sent in. Thus they are sent at an elevated priority compared with normal grey cubes (which are at rest and have not moved) and keep resending at that elevated rate until we know that update has been received, thus &ldquo;committing&rdquo; that grey cube to be at rest at the correct position. Conclusion And that&rsquo;s really about it for this technique. Without anything fancy it&rsquo;s already pretty good, and on top of that another order of magnitude improvement is available with delta compression, at the cost of significant complexity! 译文 译文出处 译者：陈敬凤（nunu） 审校：崔国军（飞扬971） 介绍 大家好，我是格伦·菲德勒。欢迎阅读《网络物理模拟》的系列文章，这个系列文章的主题是关于如何将一个物理模拟通过网络通信进行同步。 在这篇文章，我们将讨论第三种也是最后一种同步的策略：状态同步。 状态同步概念 在我看来，这是最简单的同步策略也是最容易理解的同步策略。事实上，这是我开始实现《雇佣兵2：战火纷飞》的网络物理部分的时候我首先尝试的同步策略。这个同步策略的基本想法是我们在网络的两侧同时运行仿真，但是与具有确定性的帧同步不同的是帧同步会通过网络发送输入信息并且依赖完美的确定性来保持同步，这种同步策略是既通过网络发送输入信息又会发送状态信息来进行同步。 这就赋予了状态同步与之前的同步策略完全不同的属性。与具有确定性的帧同步不同，这种同步策略不需要要求确定性来保持同步，因为我们可以迅速的通过网络发送状态来纠正任何的偏差。这种同步策略也跟快照信息的插值不同，如果一个对象不在数据包里面的话，这个物体还是会继续移动，因为网络两侧的仿真都在持续的运行。 正是由于这一特性，状态同步的实现方法才会与快照信息的插值有差别。不再是在每个数据包里面发送每个物体的状态更新信息，我们可以只对几个对象进行更新。如果我们在每个数据包选择要同步的物体的时候方案比较聪明的话，我们可以更有效地利用带宽，把注意力主要集中在最重要的物体的更新上，而那些不那么重要的物体，他们的更新信息可以以一个较低的速率进行发送。这样的话，相比较快照信息插值这种要在一个快照里面包括所有物体的方法，状态同步这种方法使用的带宽可以减少一个数量级。此外，状态同步这种方法不会在网络延迟之上还要附加插值带来的延迟，因为它相比较于快照信息插值这种方法，延迟也更低。 这样做的代价是状态同步是一个近似和有损的同步策略。如果推送信息的时候出现了一些问题导致大量的数据包丢失的话，远程的模拟仿真使用的是过期的数据进行预测。根据我的经验，如果使用这个同步策略的话，你会花很多时间追踪由于进行预测所带来的差异。如果使用这个同步策略的话，在大量物体堆叠的时候，会看到很多物体的移动不正常，并且很难精确地追查。在这篇文章中，我会告诉你如何追踪并通过网络发送量化和压缩的物理状态来减少分歧的根源。 实现 让我们从实际的实现来看下这个同步策略。这里是每个发送的对象的网络状态： 12345678struct StateUpdate &#123; int index; vec3f position; quat4f orientation; vec3f linear_velocity; vec3f angular_velocity;&#125;; 需要注意的是，我们发送的不仅仅是一些像位置、方向这样的视觉信息，这个地方与快照信息插值那种方法相同，我们还发送了很多非视觉的物体状态信息，比如线性速度和角速度，这是与快照信息插值那种方法不同的地方。这么做是必要的是因为物理仿真需要对每个物体最近的状态进行外推。因此，状态更新需要提供所有进行外推所需的信息，以便能够正确的进行推测。如果一个物体的速度信息没有发送的话，在预测物体前进的时候，就会使用一个不正确的速度信息，这将导致下一次物体信息进行更新的时候有一个拉扯。 当我们在网络上对状态更新进行序列化的时候，没有必要对不动的物体浪费网络带宽，为这些不动的物体发送什么(0,0,0)来表示线性速度和角速度。我们可以做一个简单的优化，通过把物体的静止状态包含在内来给每个静止的物体节省24字节的带宽： 123456789101112131415161718192021void serialize_state_update( Stream &amp; stream, int &amp; index, StateUpdate &amp; state_update )&#123; serialize_int( stream, index, 0, NumCubes - 1 ); serialize_vector( stream, state_update.position ); serialize_quaternion( stream, state_update.orientation ); bool at_rest = stream.IsWriting() ? state_update.AtRest() : false; serialize_bool( stream, at_rest ); if ( !at_rest ) &#123; serialize_vector( stream, state_update.linear_velocity ); serialize_vector( stream, state_update.angular_velocity ); &#125; else if ( stream.IsReading() ) &#123; state_update.linear_velocity = vec3f(0,0,0); state_update.angular_velocity = vec3f(0,0,0); &#125;&#125; 上面的代码就是我所谓的序列化功能。这里面有一个我喜欢的小技巧来统一位打包器的读取和写入函数，它们通常是分开实现的。这个函数会在两种不同的上下文中进行调用：写入的时候和读取的时候。你可以通过IsReading/IsWriting函数来知道自己目前处在哪个上下文。我喜欢这个技巧的原因是如何读取和写入功能统一在一个函数的时候，读取和写入的不同步就会很少发生。如果你希望读取和写入功能统一在一起并且像我这样进行数据包的数据，请参考这里。 数据包结构体 当把状态更新写入的时候，如果这个物体是静止不动的话，这个函数其实只序列化了一比特的信息而不会更新线性速度和角速度的信息。如果这个物体不是静止不动的话，会把线性速度和角速度的信息写入之前先写入一比特的信息。在从数据包进行读取的时候，代码会读取这个比特位，如果这个比特位是0的话，会从这个比特流里面读取线性速度和角速度的信息，否则的话，会把物体的线性速度和角速度全部清为（0,0,0）。这是一个非常简单而有效的无损带宽压缩策略能够针对静止不动的物体进行数据的压缩，能够节省将近一半的带宽。接下来让我们看一下被发送的数据包的结构： 1234567891011const int MaxInputsPerPacket = 32;const int MaxStateUpdatesPerPacket = 64; struct Packet&#123; uint32_t sequence; Input inputs[MaxInputsPerPacket]; int num_object_updates; StateUpdate state_updates[MaxStateUpdatesPerPacket];&#125;; 从上面的数据包结构中，你可以看到，首先登场的是我们在每个数据包包含的序列号，通过这个数据信息我们可以判断数据包是否出故障、丢失或者重复。我强烈建议你在网络两侧的运行都按照相同的帧速率（比如说60fps）进行仿真。在这种情况下，你还可以给序列号赋予另外一重任务：作为状态更新的帧号。 输入信息被包含在每个数据包里面，这是因为仿真需要输入信息才能进行外推。当仿真在网络的另外一侧运行的时候，我们希望通过状态更新的信息以及运行玩家相同的输入信息来往前预测后续状态的信息，并且希望预测出来的状态能够和真实的状态尽可能的接近。就跟具有确定性的帧同步一样，我们发送了多个冗余输入信息，这样即使在有包丢失的情况下，输入信息也不太可能完全被丢弃而不能到达网络的另外一端，但是跟具有确定性的帧同步不一样的是，就算是最坏的情况下（也就是我们没有收到后续的输入信息的情况），我们本地的仿真也不会停止并且等待后续的输入信息的到来，我们还是会根据最后收到的输入信息继续往前模拟。接下来，你可以看到，在一个数据包里面我们最多发送64个状态更新。我们在仿真的场景中一共有901个立方体，所以我们需要一些方法来从这901个立方体里面选出一些最重要的立方体，在每一个数据包进行数据更新。我们需要某种优先级方案，这样我们才能找到最重要的物体，允许我们只会很频繁的发送最重要的物体的状态信息，而那些不怎么重要的物体的更新就会不那么的频繁，零零散散的更新，这样保证所有对象都有机会进行更新和发送，但是又能让最重要的那些物体始终得到更新。这样就要求在仿真的每一帧开始的时候遍历所有的物体并且计算它们当前的优先级。让我们举个简单的例子来说，在立方体模拟这个场景中，我把玩家立方体的优先级设为100000，因为我希望玩家立方体的更新信息能够包含在每个数据包里面，而对于发生交互的立方体（那些红色的立方体），我赋予它们的优先级为100，而所有静止不动的立方体，优先级为1。非常遗憾的是如果只有这一个机制的话，是不足以公平分配对象的更新的，这是因为如果你刚刚仅仅是在每一帧对物体的当前优先级进行了排序，这样的话，如果人物立方体和红色立方体有交互的话，那么就永远只有红色立方体的信息会被发送，而地面上的白色立方体则永远不会更新。我们需要一个稍微不同的方法，优先发送重要的对象，同时也会在仿真的过程让那些不重要的物体也有更新的机会。 优先级累加器 你可以通过优先级累加器做到这一点。这是一组浮点数值，每个对象都会有一个对应的浮点数值，在帧与帧之间会一直保留。有了这个值以后，不再是根据当前帧中这个物体的重要性进行排序，而是在每一帧中将每个物体的重要性加到这个优先级累加器中，然后对优先级累加器的值进行从大到小进行排序。这个排序的顺序中前面的物体就是这一帧中你应该发送的物体。你可以为所有的N个物体发送状态更新信息，但是通常情况下你的带宽会有一些限制，比如说你需要控制在256k比特每秒。尊重这个带宽限制是很容易的。只要计算出你的数据包Header有多大，并且计算下数据包的preamble部分有多大（这主要是指序号、标记哪些物体在这个数据包等信息），这样就能计算出来你的数据包还剩下多少字节，可以通过计划传递多少个物体的更新信息来确保带宽小于约定的最大带宽。然后根据它们的优先级累加器里面的值选取数目和上面计算相符合的n个最重要的物体，然后用这n个最重要的物体的更新信息来构建你的数据包。对这n个最重要的物体进行依次遍历并测试它们的数据更新信息是否应该放在这个数据包里面。如果遇到一个物体的状态更新信息不合适放到这个数据包里面，那么就跳过这个物体并尝试下一个。当你序列化完这个数据包以后，将那些已经在这帧更新过的物体在优先级累加器里面的值重置为0，但是那些没有在这帧更新过的物体在优先级累加器里面的值则保持不变。通过使用这个办法，刚才检测不合适放到数据包的物体的更新信息会被首先包含在下一帧的数据包里面。使用这种同步策略，所需的带宽甚至可以动态调整。这使得这种同步策略可以很容易的根据不断变化的网络条件来调整状态同步的信息量，让我们举个简单的例子来说，如果你发现连接有困难，就可以减少发送所占的带宽（拥塞避免），这样状态同步的规模会逐步的自动回复回来。如果网络连接似乎可以处理更多的带宽，那么就可以把带宽限制提高。我们这里所做的处理主要是在网络发送这一端。 抖动缓冲区 对物体做了优先级的排序，并且在每帧里面更新物体在优先级累加器里面的值，并在每次发送数据包的时候只发送n个最重要的物体。但是在网络接收这一端，还有很多需要做的事情，比如当应用接收过来的状态更新信息的时候，如何避免与之前预测的物体状态信息之间的差异会被玩家感觉到。你需要考虑的第一件事情是，网络抖动的存在。你没有任何办法来确保你发送的数据包就是完美的是每秒60次的频率抵达网络的另外一侧。在现实世界中会发生的事情是你可能在一帧中收到两个数据包，然后在下一帧一个数据包也收不到，然后下面几帧可能是1个，2个或者0个。为了处理这种情况，你需要实现一个抖动缓冲器来保存你的状态更新数据包。如果你不这样做，所做的推测质量就很难保证而且会出现对象堆叠的情况，这是因为在不同的状态更新数据包里面，每一个物体的信息都会有些轻微的变化。 在抖动缓冲器你所要做的事情就是保存这些数据包，然后根据数据包里面的序号（其实也就是帧号了）来在正确的时间将数据包发给应用程序处理。你需要在这个缓冲区来保存数据包所导致的延迟相比较快照信息插值所带来的延迟是一段非常微小的时间，但是这两种同步策略的基本思想是一致的。你只要稍微延迟一下数据包让时间刚刚好就好（比如说每秒60次更新的情况延迟个4-5帧），这样数据包就能够以合适的间隔从缓冲区里面出来到达应用程序。 应用状态更新 一旦你的数据包从抖动缓冲器里面出来，你该如何使用数据包的信息进行状态更新呢？我的建议是，你要努力对齐这种状态。在状态更新直接应用这些信息进行仿真。我反对在状态更新和当前仿真的目前状态之间做一些平滑的更替。这听起来可能有悖常理，但这样做的原因是，当前有些物体的状态可能是根据之前状态推测出来的，所以你要保证这个物体的预测信息是从物体有效的物理状态出发，而不是一些平滑出来的完全是虚假的数据出发。这在你有大量的物体对象的时候会格外的重要。到目前为止，我们已迅速建立了一个实用的同步策略而没做有太多的工作。事实上，这种同步策略已经足够好了，已经完全可以在互联网上进行游戏对战了，并且它对丢包、抖动和带宽限制都处理的相当不错。【视频1：cube_state_sync_uncompressed】 正如你可以看到的那样，这种同步策略已经看上去相当不错了，并且几乎没有做任何的带宽优化。与快照信息插值那种策略的第一次18m每秒的信息量相比，你可以看到在状态更新之间进行状态的推测是使用更少的带宽的好方法。当然，我们可以做得比现在的状态好的多，每次我们做优化的时候我们都可以使用相同的带宽来传递更多的物体状态更新信息。我们可以做的下一个明显有效果的事情是应用所有的标准量化压缩技术，比如压缩边界和量化位置、线性速度和角速度的值以及如同《网络物理模拟五之快照压缩》里面的描述的“最小的三个变量”方法。但在这里它变得更复杂一点。我们从这些状态更新向前进行推测，所以如果我们量化这些通过网络传递的值的话，那么到达右侧的状态将与左侧的状态稍有不同，这会让推测变得更加不准，并且当下一个状态更新到达的时候会出现一些拉扯现象。【视频2： cube_state_sync_compressed】 对两边都量化 解决的办法是量化两侧的状态。这意味着，在每一个仿真进行一次更新之前，你要对网络的两侧同时量化整个模拟状态，就好像它们都已经在网络上传输了一样。一旦这样做了的话，左侧和右侧都是从量化的状态进行推测，这样它们的推测结果就会非常的接近。由于这些量化以后的值会被反馈到仿真中去，你会发现这种方法对精确度的要求比快照信息插值的方法所要求的精确度要高的多，因为在快照信息插值的方法里面，使用的数据只是用来插值的视觉信息。在立方体模拟这个情况下，我发现有必要对于每米要有4096个位置的精度，而在快照信息插值的方法里每米只要有512个位置的精度就可以了，所以四元数最小的三个分量每个要15比特位（在快照信息插值的方法里四元数最小的三个分量每个只要9个比特位的信息就行）。如果没有这种额外的精度出现，就非常容易出现物体的拉扯的情况，这是因为量化以后的数据会迫使物理对象相互渗透，这与模拟要所求的尽量保持物理对象不相互渗透的努力是背道而驰的。我还发现，软化约束以及减少模拟用于推开相互渗透的物理对象的最大速度也有助于减少出现物体拉扯的情况。【视频3： cube_state_sync_quantize_both_sides】 在量化应用于网络两侧的模拟以后，就可以再次看到结果是比较完美的。这种处理以后，看起来视觉效果与未压缩版本差不多，但事实上通过这种方案我们能够适应每个包进行更多的状态更新，同时还能满足每秒256比特的带宽限制。这意味着我们能够更好地处理数据包的丢失，因为每一个对象的状态更新可以更迅速的发送。如果出现数据包丢失的情况，对整个模拟来说也会引发更少的问题，这是因为通过未来到来的书包正在持续不断地对这些物体进行状态更新。 请注意如果出现数据包的集中丢弃的情况，比如说在四分之一秒的时间没有数据包通过，这种情况是不可避免的，总是会发生一些这样的事情，你可能会在网络的两侧得到完全不同的结果。我们必须为这种情况进行规划。我们会尽一切努力来确保外推是尽可能与实际结果相接近的（采用在网络的两侧进行量化以及其他一些方案），但是由于网络停止传输数据包，还是会发生各种拉扯和不准确的情况。 视觉平滑 还记得我之前说过的那个事情么？你不应该对模拟这一侧使用平滑算法，因为它会对外推有不好的影响吗? 我们要做的不是平滑而是计算和维护位置和方向的误差补偿，这个量会随着时间而减少。然后当我们在网络的右侧渲染立方体的时候，我们并不是用模拟的位置和方向对这些立方体进行渲染，我们是用模拟的位置和方向再加上误差补偿来对这些立方体进行渲染。位置信息是模拟位置信息加上误差补偿，方向信息是模拟的方向信息再乘以方向的误差补偿。 随着时间的推移，我们努力减少这些误差补偿，让位置的误差补偿尽量趋近于0，而方向的误差补偿尽量趋近于一致。为了减少误差，我使用了一个指数平滑的移动，平均线趋近零。所以实际上，我用每一帧的位置误差乘以某个系数（比如说是0.9），直到它接近于零而被清除(这样就避免了突变)。对于方向而言，我用某一个固定的量（比如说是0.1）来对每一帧的标准向量进行球面插值，这个可以达到方向误差相同的效果。 让所有事情都能够正常运行的诀窍在于当一个状态更新数据包到达的时候，你获取当前的模拟位置信息，并把位置误差添加上去，然后再从新的位置里面减去这个值，这样就可以让新位置的位置误差和当前的视觉位置比较一致（平滑）。然后把相同的过程应用于四元数误差（使用乘法的共轭而不是减法来与基准方向信息进行叠加），通过这种方法你就可以有效的计算在每个状态更新数据包到达的时候，相对于新的状态下新的位置误差和方向误差，这样处理的话物体看上去就根本没有进行任何的移动。因此状态更新的非常平滑，没有任何突然移动的视觉效果，而且可以随着时间慢慢减少由于推断带来的误差而通常情况下这么处理不会让玩家注意到。 我发现只使用一个单独的平滑因子会产生不可接受的结果。平滑因子0.95对于那些小的抖动来说是非常完美的，因为它对那些高频抖动的平滑是非常完美的，但是它对于大的位置误差来说平滑的太慢了，比如说发生了好几秒数据包丢失以后，物体的位置和实际位置差的比较大，这时候用这个因子来平滑就太慢了： 【视频4：cube_state_sync_basic_smoothing】 我使用的解决方案是针对不同的误差距离使用两个不同的平滑因子，并且我会根据需要减少的位置误差的大小来对这两个平滑因子进行线性的混合来让过渡非常的平滑。在这个模拟中，我使用的是0.95来平滑小的位置误差(针对25厘米或者误差更小的情况)，而对于大一点的距离而言会使用一个更严格的混合系数0.85(针对1米或者误差更大的情况)，这给出了一个非常好的结果。对于方向而言，相同的策略适用于对方向误差和单位矩阵使用点积的情况。我发现在这种情况下，混合系数分别采用0.1和0.5的效果就非常的好。 最终的结果是对小的位置误差和方向误差的平滑操作与对大的位置误差和方向误差的快速收敛很好的结合在了一起。正如你在上面看到的那样，你不想拖着一直不处理这些大的位置误差和方向误差，这些大的位置误差和方向误差需要被快速的解决否则它们会给玩家造成非常大的困扰，但是同时当位置误差和方向误差很小的时候你希望这个误差减少的过程能够非常的平滑，因此自适应误差减少方法效果很好。 【视频5：cube_state_sync_adaptive_smoothing.mp4】 增量压缩 尽管我认为上述结果可能已经足够好了，从这一点上来看可以大大提高同步的质量。让我们举些简单的例子来说明，比如支持一个有大量对象的世界或者有更多的对象与之交互。所以让我们通过一些技术上的改进，来推动这项技术尽可能的完美。 有一种简单的压缩，可以立刻执行。不再是编码绝对位置，如果位置是在玩家立方体中心的某个范围之内的话，就会以玩家的中心位置的偏移量来进行编码。如果是常见情况下，带宽很高而且状态更新需要非常的频繁（katamari球），通过这种方法就能节省下很多带宽。 接下来，如果我们想对状态同步执行某种增量编码怎么办? 我们可以做到但是具体的方法会和快照里面的增量编码方法差别很大，这是因为在这种情况下我们的每个数据包不会包含每一个立方体的信息，所以我们不能跟踪最新收到的数据包，并且自以为地觉得这个数据包的所有这些状态更新都是相对于X这个数据包的。 你实际要做的就是逐对象的进行更新，对数据包进行跟踪包括更新的基础值。你还需要跟踪收到的数据包的准备的数量，这样发送方才能知道哪些数据包可以作为增量编码有效的基础值。这是相当复杂的，并且是需要通过UDP协议进行双向确认的系统。这样一个系统是专为这种情况设计的，因为你肯定需要知道哪些数据包确定是到达了另外一侧。你可以在这个教程里面找到具体如何实现这个功能的指南。 所以假设你有一个确认系统，这样你就知道已经发送到网络另外一侧的数据包的序列号。你所要做的就是在每个状态更新的时候，用一位数据来记录下这个更新到底是相对更新还是绝对更新，如果是绝对更新就没有针对基础编码这回事，否则就是一个相对更新，所以要发送16位序列号来标记每个状态相对应的基础状态，然后相对于基础状态对更新数据进行编码并通过数据包进行发送。这为每次更新增加了1比特开销，以及需要增加16位序列号的开销来标记每个物体更新的基准帧。我们可以做得更好吗? 是的。确实可以做的更好。你要在发送和接收端进行缓冲来实现这个相对编码机制，但是你不可能永远缓冲。事实上，如果你仔细想想，你只能缓冲几秒钟然后整个缓冲就变得不切实际，对于物体在移动这个常见的情况，你会经常发送相同对象的更新信息（比如说katamari球），所以实际上基准帧只能是很短时间之前的一帧状态。 所以对每个物体发送16位的序列号来表明基准帧，在数据包的包头里面发送最近确认的数据包的序列号（这个数据是从可靠的确认系统里面得到的）然后对每个物体编码相对这个基准帧的偏移量，这个偏移量使用5位信息。通过这种方式在每秒60个数据包的情况下，你可以识别相对于基准帧半秒前的状态更新。任何比这个值更老的基准帧不太可能提供一个良好的增量编码的基准，主要是因为它们太老了，所以在这种情况下就要切回到绝对编码进行状态更新。 现在让我们看看会使用这些绝对编码而不是相对编码的对象的类型。他们是静止的对象。我们能做什么来让他们的更新尽可能的高效?在这种立方体模拟的情况，一个可能发生的很糟糕的结果是一个立方体进行停止状态（变成灰色）然后它的优先级显著降低。如果由于数据包的丢失，导致最后对象的位置更新信息被错过的话，可能需要很长时间才会轮到这个物体来更新它的停止位置信息。 我们可以通过跟踪哪些最近变成停止状态的对象来解决这个问题，并且会提高这些对象的优先级直到一个确认包返回来标记这些对象的位置更新信息已经被成功的发送了。因此他们的发送优先级会相对于正常的灰色立方体（那些处于静止状态没有移动的立方体）的发送优先级有一定的提高，并且会在这个提高后的优先级上一直发送，直到我们知道对这些立方体的更新信息已经收到，也就是网络的另外一侧会“承诺”把这些灰色的立方体放在正确的位置上停止。 最后 这就是有关于这种技术的全部内容。它非常的有趣，不需要任何花哨的内容就已经足够好了，然后在此基础上可以做一个数量级的带宽节省（通过增量编码），但是这个方案的复杂性非常的高。 【版权声明】 原文作者未做权利声明，视为共享知识产权进入公共领域，自动获得授权。]]></content>
      <categories>
        <category>GS</category>
      </categories>
      <tags>
        <tag>GafferOnGames</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络物理模拟五之快照压缩]]></title>
    <url>%2Fblog%2F2017%2F01%2F24%2Fsnapshot_compression%2F</url>
    <content type="text"><![CDATA[自我总结快照压缩技术的要点为 : 压缩Orientation数据 : 利用四元数的”最小的三个分量”性质:x^2+y^2+z^2+w^2 = 1 来在传输的时候丢弃一个分量并在网络的另外一端对整个四元数进行重建 压缩线性速度和Postion数据 : 把他们限制在某个范围内, 就可以用这个范围的最大值所占用的比特位数来保存这两种数据了, 而不用一个超大的数来保证可以保存他们的最大值了(占用超多bit位) 增量压缩 . . . 原文原文出处 原文标题 : Snapshot Compression (Advanced techniques for optimizing bandwidth) Introduction Hi, I&rsquo;m Glenn Fiedler and welcome to Networked Physics. In the previous article we sent snapshots of the entire simulation 10 times per-second over the network and interpolated between them to reconstruct a view of the simulation on the other side. The problem with a low snapshot rate like 10HZ is that interpolation between snapshots adds interpolation delay on top of network latency. At 10 snapshots per-second, the minimum interpolation delay is 100ms, and a more practical minimum considering network jitter is 150ms. If protection against one or two lost packets in a row is desired, this blows out to 250ms or 350ms delay. This is not an acceptable amount of delay for most games, but when the physics simulation is as unpredictable as ours, the only way to reduce it is to increase the packet send rate. Unfortunately, increasing the send rate also increases bandwidth. So what we&rsquo;re going to do in this article is work through every possible bandwidth optimization (that I can think of at least) until we get bandwidth under control. Our target bandwidth is 256 kilobits per-second. Starting Point @ 60HZ Life is rarely easy, and the life of a network programmer, even less so. As network programmers we&rsquo;re often tasked with the impossible, so in that spirit, let&rsquo;s increase the snapshot send rate from 10 to 60 snapshots per-second and see exactly how far away we are from our target bandwidth. That&rsquo;s a LOT of bandwidth: 17.37 megabits per-second! Let&rsquo;s break it down and see where all the bandwidth is going. Here&rsquo;s the per-cube state sent in the snapshot: struct CubeState { bool interacting; vec3f position; vec3f linear_velocity; quat4f orientation; }; And here&rsquo;s the size of each field: quat orientation: 128 bits vec3 linear_velocity: 96 bits vec3 position: 96 bits bool interacting: 1 bit This gives a total of 321 bits bits per-cube (or 40.125 bytes per-cube). Let&rsquo;s do a quick calculation to see if the bandwidth checks out. The scene has 901 cubes so 901x40.125 = 36152.625 bytes of cube data per-snapshot. 60 snapshots per-second so 36152.625 x 60 = 2169157.5 bytes per-second. Add in packet header estimate: 2169157.5 + 32x60 = 2170957.5. Convert bytes per-second to megabits per-second: 2170957.5 x 8 / ( 1000 x 1000 ) = 17.38mbps. Everything checks out. There&rsquo;s no easy way around this, we&rsquo;re sending a hell of a lot of bandwidth, and we have to reduce that to something around 1-2% of it&rsquo;s current bandwidth to hit our target of 256 kilobits per-second. Is this even possible? Of course it is! Let&rsquo;s get started :) Optimizing Orientation We&rsquo;ll start by optimizing orientation because it&rsquo;s the largest field. (When optimizing bandwidth it&rsquo;s good to work in the order of greatest to least potential gain where possible&hellip;) Many people when compressing a quaternion think: &ldquo;I know. I&rsquo;ll just pack it into 8.8.8.8 with one 8 bit signed integer per-component!&rdquo;. Sure, that works, but with a bit of math you can get much better accuracy with fewer bits using a trick called the &ldquo;smallest three&rdquo;. How does the smallest three work? Since we know the quaternion represents a rotation its length must be 1, so x^2+y^2+z^2+w^2 = 1. We can use this identity to drop one component and reconstruct it on the other side. For example, if you send x,y,z you can reconstruct w = sqrt( 1 - x^2 - y^2 - z^2 ). You might think you need to send a sign bit for w in case it is negative, but you don&rsquo;t, because you can make w always positive by negating the entire quaternion if w is negative (in quaternion space (x,y,z,w) and (-x,-y,-z,-w) represent the same rotation.) Don&rsquo;t always drop the same component due to numerical precision issues. Instead, find the component with the largest absolute value and encode its index using two bits [0,3] (0=x, 1=y, 2=z, 3=w), then send the index of the largest component and the smallest three components over the network (hence the name). On the other side use the index of the largest bit to know which component you have to reconstruct from the other three. One final improvement. If v is the absolute value of the largest quaternion component, the next largest possible component value occurs when two components have the same absolute value and the other two components are zero. The length of that quaternion (v,v,0,0) is 1, therefore v^2 + v^2 = 1, 2v^2 = 1, v = 1/sqrt(2). This means you can encode the smallest three components in [-0.707107,+0.707107] instead of [-1,+1] giving you more precision with the same number of bits. With this technique I&rsquo;ve found that minimum sufficient precision for my simulation is 9 bits per-smallest component. This gives a result of 2 + 9 + 9 + 9 = 29 bits per-orientation (down from 128 bits). Your browser does not support the video tag. This optimization reduces bandwidth by over 5 megabits per-second, and I think if you look at the right side, you&rsquo;d be hard pressed to spot any artifacts from the compression. Optimizing Linear Velocity What should we optimize next? It&rsquo;s a tie between linear velocity and position. Both are 96 bits. In my experience position is the harder quantity to compress so let&rsquo;s start here. To compress linear velocity we need to bound its x,y,z components in some range so we don&rsquo;t need to send full float values. I found that a maximum speed of 32 meters per-second is a nice power of two and doesn&rsquo;t negatively affect the player experience in the cube simulation. Since we&rsquo;re really only using the linear velocity as a hint to improve interpolation between position sample points we can be pretty rough with compression. 32 distinct values per-meter per-second provides acceptable precision. Linear velocity has been bounded and quantized and is now three integers in the range [-1024,1023]. That breaks down as follows: [-32,+31] (6 bits) for integer component and multiply 5 bits fraction precision. I hate messing around with sign bits so I just add 1024 to get the value in range [0,2047] and send that instead. To decode on receive just subtract 1024 to get back to signed integer range before converting to float. 11 bits per-component gives 33 bits total per-linear velocity. Just over 1&frasl;3 the original uncompressed size! We can do even better than this because most cubes are stationary. To take advantage of this we just write a single bit &ldquo;at rest&rdquo;. If this bit is 1, then velocity is implicitly zero and is not sent. Otherwise, the compressed velocity follows after the bit (33 bits). Cubes at rest now cost just 127 bits, while cubes that are moving cost one bit more than they previously did: 159 + 1 = 160 bits. Your browser does not support the video tag. But why are we sending linear velocity at all? In the previous article we decided to send it because it improved the quality of interpolation at 10 snapshots per-second, but now that we&rsquo;re sending 60 snapshots per-second is this still necessary? As you can see below the answer is no. Your browser does not support the video tag. Linear interpolation is good enough at 60HZ. This means we can avoid sending linear velocity entirely. Sometimes the best bandwidth optimizations aren&rsquo;t about optimizing what you send, they&rsquo;re about what you don&rsquo;t send. Optimizing Position Now we have only position to compress. We&rsquo;ll use the same trick we used for linear velocity: bound and quantize. I chose a position bound of [-256,255] meters in the horizontal plane (xy) and since in the cube simulation the floor is at z=0, I chose a range of [0,32] meters for z. Now we need to work out how much precision is required. With experimentation I found that 512 values per-meter (roughly 2mm precision) provides enough precision. This gives position x and y components in [-131072,+131071] and z components in range [0,16383]. That&rsquo;s 18 bits for x, 18 bits for y and 14 bits for z giving a total of 50 bits per-position (originally 96). This reduces our cube state to 80 bits, or just 10 bytes per-cube. This is approximately 1&frasl;4 of the original cost. Definite progress! Your browser does not support the video tag. Now that we&rsquo;ve compressed position and orientation we&rsquo;ve run out of simple optimizations. Any further reduction in precision results in unacceptable artifacts. Delta Compression Can we optimize further? The answer is yes, but only if we embrace a completely new technique: delta compression. Delta compression sounds mysterious. Magical. Hard. Actually, it&rsquo;s not hard at all. Here&rsquo;s how it works: the left side sends packets to the right like this: &ldquo;This is snapshot 110 encoded relative to snapshot 100&rdquo;. The snapshot being encoded relative to is called the baseline. How you do this encoding is up to you, there are many fancy tricks, but the basic, big order of magnitude win comes when you say: &ldquo;Cube n in snapshot 110 is the same as the baseline. One bit: Not changed!&rdquo; To implement delta encoding it is of course essential that the sender only encodes snapshots relative to baselines that the other side has received, otherwise they cannot decode the snapshot. Therefore, to handle packet loss the receiver has to continually send &ldquo;ack&rdquo; packets back to the sender saying: &ldquo;the most recent snapshot I have received is snapshot n&rdquo;. The sender takes this most recent ack and if it is more recent than the previous ack updates the baseline snapshot to this value. The next time a packet is sent out the snapshot is encoded relative to this more recent baseline. This process happens continuously such that the steady state becomes the sender encoding snapshots relative to a baseline that is roughly RTT (round trip time) in the past. There is one slight wrinkle: for one round trip time past initial connection the sender doesn&rsquo;t have any baseline to encode against because it hasn&rsquo;t received an ack from the receiver yet. I handle this by adding a single flag to the packet that says: &ldquo;this snapshot is encoded relative to the initial state of the simulation&rdquo; which is known on both sides. Another option if the receiver doesn&rsquo;t know the initial state is to send down the initial state using a non-delta encoded path, eg. as one large data block, and once that data block has been received delta encoded snapshots are sent first relative to the initial baseline in the data block, then eventually converge to the steady state of baselines at RTT. Your browser does not support the video tag. As you can see above this is a big win. We can refine this approach and lock in more gains but we&rsquo;re not going to get another order of magnitude improvement past this point. From now on we&rsquo;re going to have to work pretty hard to get a number of small, cumulative gains to reach our goal of 256 kilobits per-second. Incremental Improvements First small improvement. Each cube that isn&rsquo;t sent costs 1 bit (not changed). There are 901 cubes so we send 901 bits in each packet even if no cubes have changed. At 60 packets per-second this adds up to 54kbps of bandwidth. Seeing as there are usually significantly less than 901 changed cubes per-snapshot in the common case, we can reduce bandwidth by sending only changed cubes with a cube index [0,900] identifying which cube it is. To do this we need to add a 10 bit index per-cube to identify it. There is a cross-over point where it is actually more expensive to send indices than not-changed bits. With 10 bit indices, the cost of indexing is 10xn bits. Therefore it&rsquo;s more efficient to use indices if we are sending 90 cubes or less (900 bits). We can evaluate this per-snapshot and send a single bit in the header indicating which encoding we are using: 0 = indexing, 1 = changed bits. This way we can use the most efficient encoding for the number of changed cubes in the snapshot. This reduces the steady state bandwidth when all objects are stationary to around 15 kilobits per-second. This bandwidth is composed entirely of our own packet header (uint16 sequence, uint16 base, bool initial) plus IP and UDP headers (28 bytes). Next small gain. What if we encoded the cube index relative to the previous cube index? Since we are iterating across and sending changed cube indices in-order: cube 0, cube 10, cube 11, 50, 52, 55 and so on we could easily encode the 2nd and remaining cube indices relative to the previous changed index, e.g.: +10, +1, +39, +2, +3. If we are smart about how we encode this index offset we should be able to, on average, represent a cube index with less than 10 bits. The best encoding depends on the set of objects you interact with. If you spend a lot of time moving horizontally while blowing cubes from the initial cube grid then you hit lots of +1s. If you move vertically from initial state you hit lots of +30s (sqrt(900)). What we need then is a general purpose encoding capable of representing statistically common index offsets with less bits. After a small amount of experimentation I came up with this simple encoding: [1,8] =&gt; 1 + 3 (4 bits) [9,40] =&gt; 1 + 1 + 5 (7 bits) [41,900] =&gt; 1 + 1 + 10 (12 bits) Notice how large relative offsets are actually more expensive than 10 bits. It&rsquo;s a statistical game. The bet is that we&rsquo;re going to get a much larger number of small offsets so that the win there cancels out the increased cost of large offsets. It works. With this encoding I was able to get an average of 5.5 bits per-relative index. Now we have a slight problem. We can no longer easily determine whether changed bits or relative indices are the best encoding. The solution I used is to run through a mock encoding of all changed cubes on packet write and count the number of bits required to encode relative indices. If the number of bits required is larger than 901, fallback to changed bits. Here is where we are so far, which is a significant improvement: Your browser does not support the video tag. Next small improvement. Encoding position relative to (offset from) the baseline position. Here there are a lot of different options. You can just do the obvious thing, eg. 1 bit relative position, and then say 8-10 bits per-component if all components have deltas within the range provided by those bits, otherwise send the absolute position (50 bits). This gives a decent encoding but we can do better. If you think about it then there will be situations where one position component is large but the others are small. It would be nice if we could take advantage of this and send these small components using less bits. It&rsquo;s a statistical game and the best selection of small and large ranges per-component depend on the data set. I couldn&rsquo;t really tell looking at a noisy bandwidth meter if I was making any gains so I captured the position vs. position base data set and wrote it to a text file for analysis. I wrote a short ruby script to find the best encoding with a greedy search. The best bit-packed encoding I found for the data set works like this: 1 bit small per delta component followed by 5 bits if small [-16,+15] range, otherwise the delta component is in [-256,+255] range and is sent with 9 bits. If any component delta values are outside the large range, fallback to absolute position. Using this encoding I was able to obtain on average 26.1 bits for changed positions values. Delta Encoding Smallest Three Next I figured that relative orientation would be a similar easy big win. Problem is that unlike position where the range of the position offset is quite small relative to the total position space, the change in orientation in 100ms is a much larger percentage of total quaternion space. I tried a bunch of stuff without good results. I tried encoding the 4D vector of the delta orientation directly and recomposing the largest component post delta using the same trick as smallest 3. I tried calculating the relative quaternion between orientation and base orientation, and since I knew that w would be large for this (rotation relative to identity) I could avoid sending 2 bits to identify the largest component, but in turn would need to send one bit for the sign of w because I don&rsquo;t want to negate the quaternion. The best compression I could find using this scheme was only 90% of the smallest three. Not very good. I was about to give up but I run some analysis over the smallest three representation. I found that 90% of orientations in the smallest three format had the same largest component index as their base orientation 100ms ago. This meant that it could be profitable to delta encode the smallest three format directly. What&rsquo;s more I found that there would be no additional precision loss with this method when reconstructing the orientation from its base. I exported the quaternion values from a typical run as a data set in smallest three format and got to work trying the same multi-level small/large range per-component greedy search that I used for position. The best encoding found was: 5-8, meaning [-16,+15] small and [-128,+127] large. One final thing: as with position the large range can be extended a bit further by knowing that if the component value is not small the value cannot be in the [-16,+15] range. I leave the calculation of how to do this as an exercise for the reader. Be careful not to collapse two values onto zero. The end result is an average of 23.3 bits per-relative quaternion. That&rsquo;s 80.3% of the absolute smallest three. That&rsquo;s just about it but there is one small win left. Doing one final analysis pass over the position and orientation data sets I noticed that 5% of positions are unchanged from the base position after being quantized to 0.5mm resolution, and 5% of orientations in smallest three format are also unchanged from base. These two probabilities are mutually exclusive, because if both are the same then the cube would be unchanged and therefore not sent, meaning a small statistical win exists for 10% of cube state if we send one bit for position changing, and one bit for orientation changing. Yes, 90% of cubes have 2 bits overhead added, but the 10% of cubes that save 20+ bits by sending 2 bits instead of 23.3 bit orientation or 26.1 bits position make up for that providing a small overall win of roughly 2 bits per-cube. Your browser does not support the video tag. As you can see the end result is pretty good. Conclusion And that&rsquo;s about as far as I can take it using traditional hand-rolled bit-packing techniques. You can find source code for my implementation of all compression techniques mentioned in this article here. It&rsquo;s possible to get even better compression using a different approach. Bit-packing is inefficient because not all bit values have equal probability of 0 vs 1. No matter how hard you tune your bit-packer a context aware arithmetic encoding can beat your result by more accurately modeling the probability of values that occur in your data set. This implementation by Fabian Giesen beat my best bit-packed result by 25%. It&rsquo;s also possible to get a much better result for delta encoded orientations using the previous baseline orientation values to estimate angular velocity and predict future orientations rather than delta encoding the smallest three representation directly. Chris Doran from Geomerics wrote also wrote an excellent article exploring the mathematics of relative quaternion compression that is worth reading. 译文 译文出处 译者：张大伟（卡卡是我）/ 许春(conan) 审校：崔国军（飞扬971） 介绍 大家好，我是格伦·菲德勒。欢迎阅读《网络物理模拟》的系列文章，这个系列文章的主题是关于如何将一个物理模拟通过网络通信进行同步。在前面的文章中，我们会通过网络以每秒10次的速度发送整个模拟状态的快照，并在网络通信的另外一侧对这些快照进行插值来重建整个模拟的世界状态。因为我们发送的快照的频率比较低，这样会带来的一个问题就是对快照进行插值的话会在网络延迟的基础上还要增加插值带来的延迟。如果是每秒10次快照这个情况，最小的插值延迟是100毫秒，考虑到网络抖动的话一个比较实际的最小延迟是150毫秒。如果需要在连续丢失一个到两个包的情况进行保护处理的话，可能延迟就会高达250毫秒甚至350毫秒。这种程度的延迟对于大多数游戏来说都是不能接受的量。减少这种延迟的唯一方法是增加快照发送的频率。由于许多游戏是以60fps的频率进行更新，可以尝试以每秒60次的频率发送快照而不是我们正在使用的每秒10次。但是很不幸的是，这样做的话会带来网络带宽的损耗，不仅是因为我们更加频繁的发送相同大小的数据包，而且还因为发送每个数据包都会有包头数据的负担。这个的原因听起来很明显，如果以每秒60次的频率来发送数据包，那么相比以每秒10次的频率来发送数据包，我们发送的UDP/IP数据包头的数据量很明显将是6倍。在计算数据包头的带宽消耗的时候我使用了一个经验法则，大概每个数据包头带来的带宽损耗大概是32字节。这个估计并不十分准确但是能让我们对一个典型的数据包头到底是多大有个粗略的概念。把这个大小乘以60就是每秒的带宽损耗，你会发现这样损耗的带宽其实不是一个小数目。而且这样带来带宽损耗是一个基础大小，你根本就没有办法来减少。如果是采用IPv6的话，数据包头的大小可能会更大，每秒的带宽损耗也会跟着变大。对于数据包的包头带来开销，我们基本是没有办法进行优化的，但是数据包的其他所有部分我们都可以进行优化。所以我们将在本文中要做的事情就是遍历一切可能的带宽优化方法（至少是我能想到的一切带宽优化方法）直到我们把带宽的消耗控制在我们能接受的范围内。对于这个应用程序，我们把带宽控制的目标设置为每秒256kb。 从60HZ为起点开始优化吧 这看起来似乎是比较小的一个流量，可能你的网络连接能够支持更大的流量，但是我们要明白的是当你对视频游戏或者物理模拟进行网络通信的时候，你的目标是减少延迟和确保对玩家来说最好的网络连接情况。为了实现这一目标，最好不要让连接一直饱和工作，做到这一点的办法是使用一个比较保守的带宽量，这样就比较不容易给你的玩家造成困扰和麻烦。让我们看下当我们以每秒60次的频率发送未压缩的快照的时候，我们到底使用了多大的带宽。 这一切的带宽都是从哪里来的呢？这个数据包包含了一个有901个立方体的数组，其它的东西就没有什么了。很显然，立方体的数据是造成高带宽的原因，但是为什么发送每个立方体的数据会这么昂贵呢？每个立方体具有以下这些属性：· 用quat表示的立方体朝向：128比特。· 用vec3表示的立方体速度：96比特。· 用vec3表示的立方体位置：96比特。· 用bool表示的是否相互作用：1比特。所以一个立方体一共要占据321比特的大小（40.125字节）让我们做下数学计算并确保一切的东西都包括在内了。这个场景有901个立方体所以每次快照的立方体数据大概有901x40.125 = 36152.625字节。每秒60张快照就一共是，每秒6152.625 x 60 = 2169157.5字节。再加入报文头部大小的估计：2169157.5 + 32x60 = 2170957.5字节。将单位从每秒比特转换成每秒Mb：2170957.5 x 8 / ( 1000 x 1000 ) =17.38mbps。这与刚才的得到的结果就足够接近了！ 优化Orientation数据 正如你看到的那样，所有的东西我们都考虑进来了。让我们先开始从立方体的方向数据进行优化，因为它是最大的一块数据。（当对带宽进行优化的时候，最有效的办法是从大数据到小数进行优化，这样才能收益最大）。在压缩四元数的时候很多人会这么想：“我知道了，我可以把四元数的每一位用8个比特的有符号整数来表示，这样大小就用一个32位的整数来表示四元数了“。当然，这是一个可行的方法，但是如果使用一些数学方面的技巧，你可以用更少的位数得到一个更加准确的表示方法，这个数学技巧被称为”最小的三个分量“。最小的三个分量这种方法是如何起作用的？因为我们知道代表旋转的四元数的长度一定是1，因此代表旋转的四元数会有这么一个性质：x^2+y^2+z^2+w^2 = 1。我们可以利用四元数的这个性质来在传输的时候丢弃一个分量并在网络的另外一端对整个四元数进行重建。举个简单的例子来说，如果你通过网络发送的是x、y、z，你就可以利用这样一个公式来重建w分量：w = sqrt( 1 – x^2 – y^2 – z^2 )。你可能觉得需要发送一个符号位来标识w的正负，以防止w是负的情况，但是事实上，你根本不需要这么做，因为总是可以保证w是正的，如果w是负的话，可以把整个四元数的四个分量取反就好了（在四元数空间中，(x,y,z,w)和(-x,-y,-z,-w)代表的是相同的旋转）。不要总是丢弃同一个分量，因为不总是丢弃同一个分量会得到更高的精度。，相反，要找到四个分量中最大的那一个（以绝对值的大小来衡量）并把这个分量的序号编码进一个2比特[0,3]的信息中（0=x,1=y, 2=z, 3=w)），然后通过网络发送最小的三个分量以及最大分量的序号（这样的话，通过最大的分量的序号，我们就知道了发送过来的三个分量的名字）。在网络的另外一端，我们会从2比特的最大分量的序号信息中解码出来我们需要重建的分量是哪一个，然后就可以利用传过来的三个分量对其进行重建。最后一个改进。如果v是四元数四个分量中最大的那个分量，可能会出现两个分量是0而另外两个分量的绝对值一样大的情况，这个四元数（v，v，0，0）的长度是1，因此v^2 + v^2= 1，2v^2 = 1，v = 1/sqrt(2)。这意味着你将会在[-0.707107，+0.707107]的大小区间里面对最小的三个分量进行编码而不是在[-1，+1]这个完整的可用空间，这将使你得到更高的精度。通过这种方法，我发现对于我的模拟情况来说保证最小的精度要求也需要用9个比特来表示一个分量。这样的话，结果就是要对每个方向需要用2 + 9 + 9 + 9= 29比特。（而原来是需要128个比特位！）。 优化线性速度数据 接下来我们应该优化什么？线性速度和位置数据均可（需要96个比特位）。根据我的经验来看，位置信息是非常难以压缩的，所以让我们从线性速度开始。要压缩线性速度的话，我们首先需要把线性速度的分量限制在某个范围内，这样的话我们就不需要发送完整的浮点值。我发现最大速度定为32米每秒的话就会非常好，正好是2的平方数，并且在立方体模拟的情况下不会影响玩家的体验。由于我们实际上只是使用线性速度作为一个辅助信息来提高位置采样点之间的插值，所以我们可以极大地压缩线性速度值。我发现，其实是只使用32个离散的值（0到31）也是一个可以接受的精度。线性速度已经被限定在某个范围内并进行了量化，现在三个整数会分布在【-1024，1023】内。这种分解具体如下：【-32，+ 31】（6位）用来表示分量的整数部分，然后会乘以5位的小数部分。我不喜欢用符号位乱搞，所以我只是整体都加了1024来让值的范围在【0，2047】，并把新得到的值发送出去。在接收的时候如果想要解码的话，先要减去1024，这样才会得到原始的有符号数，然后才是转换成浮点数。每个分量占据11比特，加起来就是每个线性速度要一共占据33比特的大小，只比未压缩前的数据量的1/3稍微多一点点！因为大多数立方体是固定的，我们可以处理的更好。为了利用这一点，我们可以在立方体“处于休息状态”的时候只写一个单独的比特位。如果这个比特的值是1，那么我们就知道了速度其实是0并且并不发送。否则，压缩好的速度会跟着这个比特值（压缩好的速度值一共有33比特）。“处于休息状态”的立方体现在一共占据了127比特，而正处于移动状态的立方体消耗的带宽大小比之前的方法要多一个比特：159 +1 = 160 比特。 但是我们为什么要发送线性速度呢？在前一篇文章中，我们决定发送线性速度是因为它对于每秒10次快照的情况能显著的提高插值的质量。但是，现在我们每秒发送60次快照，那么是否还需要发送线性速度呢？你可以在下面看到，答案是不需要。在高发送频率的时候线性插值的效果是足够好的。 优化Position数据 那么现在我们只有一个位置信息需要压缩了。我们将使用用于线性速度一样的小技巧：限定在某个范围内并进行量化。大多数的游戏世界其实是相当大的所以我选择的位置限制是在水平平面上的【-256,256】米之内，因为在我的立方体模拟情形中，地板的高度是z=0，所以我选择的z的范围是【0,32】米。现在我们需要确定我们对精度的要求到底是多少。通过一些实验，我发现每米有512个值（大概精度是2mm）的情况就能够提供足够的精度。这样做的话会让x和y分量的值可以分布在区间【131072,+131071】，让z分量的值分布在区间【0, 16383】。所以这么处理的话，x分量占据18比特的大小，y分量占据18比特的大小，而z分量占据14比特的大小，加起来每个位置一共占据50比特的大小（原先是96比特的大小）。这可以把我们的每个立方体的信息减至80比特，也就是10字节。（4倍的提升，原先每个立方体的状态大概需要40字节的大小）。 现在我们对位置信息和方向信息进行了压缩，我们已经通过减少我们发送的数据的精度来完成了简单的压缩。并且压缩率已经到了如果任何方向上进一步压缩都会导致精度进一步受损到不可接受的程度。我们还可以进行进一步的优化么？ 增量压缩 答案是肯定的，但是我们需要使用一种全新的技术：增量压缩。增量压缩听起来让人感觉神秘、神奇、很艰深。实际上，这个技术根本就不难。下面是它具体如何工作的：网络连接的一侧给另外一侧发送数据包，像这样：“这是快照110相对于快照100的编码”。快照是基于某个被称为基线的东西进行编码的。具体你如何实现这种编码方式完全取决于你，这里面有很多花哨的技巧，但是基础是一样的，当你说出“在快照110里面的第n个立方体相对于基线是没有任何改变，所以它只用1个比特位表示：没有变化！”的时候，大量的数据传输就被节省下来了。为了实现增量压缩的编码，当然有一点是非常关键的就是发送方必须只编码那些相对基线发生了变化的东西，这样就要求它要知道网络连接的另外一侧已经接收了什么，否则发送方根本就没法对快照进行编码。因此，为了处理数据包丢失的情况，接收方必须持续发送“ack”（接收确认）的数据包给发送方，这个数据包是说“我已经收到的最新的快照是快照n”。发送方解析最近收到最近的接收确认包，如果这个接收确认包比之前的接收确认包记录的快照更新的话，就会把基线的值调整成最近的接收确认包里面记载的快照。下一次发送数据包的时候，快照就会根据最新更新的基线进行编码。这个过程会持续的进行，这样的话稳定状态下发送者编码快照时候与基线的差距基本就是由过去这段时间的往返时间决定的。这里面会有一个小问题：在刚开始连接的时候，因为发送方没有一次通信所需要的往返时间以及并没有从接收方收到任何的确认包，所以发送方根本就没有任何基线进行相对编码。我是通过在数据包里面添加了一个单独的标志来解决这个问题的，这个标记的意思是“这个快照是相对于模拟的初始状态进行编码的”，而这个标志位是双方都明白意思的。另外一个解决方案是如果发送方不知道发送的初始状态的话，就使用一个非增量的路径进行发送初始状态。所以有可能最初发送的是一个非常大的数据块，一旦这个数据块被接收确认的话，后续发送就会以这个大的数据块作为机箱来发送增量编码的快照，然后最终收敛到以往返时延作为基准的稳定状态。 正如你可以在上面看到的那样，这是一个巨大的胜利。我们可以完善这一做法，并来获得更多的收益，但是这个收益是有限的，不会是像刚才这个做法这样带来这么大幅度的提升。从现在开始，我们将需要努力工作来获得一些比较小幅度能累积的收益来达到我们设定的256kb每秒的目标。 增量的一些优化 第一个小的提升。每个未发送的立方体需要花费1比特的带宽（如果立方体没有变化的话）。因为场景中一共有901个立方体，所以即使所有的数据包都没有变化的话，我们还是要在每个数据包要发送901个比特。如果是每秒发送60个数据包的频率的话，这就将增加54kb的带宽。可以看到在通常情况下会在每次快照的时候发生变化的立方体数目明显小于901个，所以我们可以通过只发送变化过的立方体来减小消耗的带宽。我们创建了一个立方体索引【0，900】来标记哪一个立方体是什么。为了做到这一点，我们需要给每个立方体增加10位的索引来标识它。这里面其实是有一个权衡点的，就是发送索引比发送一个位来表示立方体未发生变化要浪费更多的带宽。因为每个立方体的索引是10位，所以索引的消耗是10xn位。因此如果我们发送的立方体数目小于90个的话（也就是小于900位的话），使用索引是更加有效率的。我们可以依照这个数值对每个快照进行评估，并在数据包的头部发送一个单独的位来进行标示我们该使用哪种方法，我们使用如下的定义：0=索引，1=用单独的1位进行标示是否发生变化。通过这种方法我们根据快照中要发送的发生改变的立方体数目来进行最有效的编码。这种方法会在所有物体都是固定不发生变化的情况下可以减少稳定状态下的带宽到大约15kb每秒。这种情况下带宽是完全由我们自己的数据包包头（16位无符号的顺序号、16位无符号的基准线编号、用了标记是否是初始状态的布尔值）外加IP 和UDP的包头（28位）来占据的。 下一个小的提升。如果我们相对于之前的立方体索引进行当前立方体索引编码怎么样？因为我们在遍历所有的立方体的时候是按照顺序进行遍历的并会按照顺序发送发生改变的立方体：比如说像立方体0、立方体10,、立方体11、立方体50、立方体52、立方体55这样，所以我们可以很容易根据前一个立方体的索引对当前立方体的编号进行相对索引，这样的话，前面的例子就会变成：+10、+1、 +39、 +2、+3。如果我们可以很聪明的利用相对index编码的话，从平均情况来说，我们可以用少于10位的数据来表示一个立方体的索引。最好的编码方法取决于和你进行交互的物体集合。如果你花了很多时间进行水平移动的同时还将很多立方体从最初的立方体位置上推开，那么就会在相对index编码的方法得到很多的+1。如果你从最初的状态开始垂直移动，那么就会在相对index编码的方法得到很多的+30(sqrt(900))。我们需要的是一种通用的编码方法能够用很少的位来表示统计学下通用的index偏移。在进行少量的实验之后，我想出了这个简单的编码方式：· [1,8] =&gt; 1 + 3 (4位)· [9,40] =&gt; 1 + 1 + 5 (7位)· [41,900] =&gt; 1 + 1 + 10 (12位)需要注意下相对偏移具体是有多大，这个大小可能超过10位的大小。这是一个统计意义的游戏。赌注是我们可能得到一个大得多的偏差，这样如果赢的话会消除大偏移带来的增加的消耗。这确实起作用了。有了这个编码方法，我的每个相对序号的大小平均下来是5.5位这么大。现在我们会有一个小问题。我们再也不能很容易地确定到底是用一位来表示是否发生了更改还是使用相对序号才是最好的编码方法。我使用的解决方案就是通过将所有发生改变的立方体通过一个模拟编码的方式写入一个数据包里面，然后计算相对序号这种编码方式所需要的位数。如果所需的位数比901大，那么我们就切换回用一位来表示是否发生了更改的方法。 接下来我们要做这样的一个小的提升。利用基线时候立方体所在的位置对位置信息进行编码。这里有很多不同的选择。你可以做那些有明显效果的事情。举个简单的例子来说，用1位来表示这是相对位置的信息，然后用8-10位来表示每个分量的相对位置信息，如果所有分量的位置正好在这些位数提供的数据范围之内，否则的话就该发送绝对位置（需要使用50位）。这给出了一个还不错的编码方法，但是我们可以做的更好。如果你仔细想想的话，就会发现有如下一个情况，一个位置分量很大但是其他位置分量很小。如果我们可以利用这一点的话，就能得到更好的效果，并且用更少的位数来发送这些大小比较小的分量信息。这是一个统计游戏，到底是给每个分量选择一个比较小的数据范围还是选择一个比较大的数据范围依赖于数据集本身。我没有办法只是看带宽的大小就能告诉一些真正有用的信息，所以我捕获了位置以及基准位置数据，并把它们写入一个文件进行分析。这个格式是每一行表示一个立方体的数据信息，依次是x、y、z、base_x、base_y、base_z。目标是在每一行用基准线状态下位置的x、y、z分量来对当前状态下的x、y、z分量进行编码。如果你有兴趣的话,你可以在这里（地址是http://gafferongames.com/wp-content/uploads/2015/02/position_values.txt）下载这个数据集。我写了一个简短的ruby脚本来使用暴力搜索找到最优的编码方案。我发现最好的位打包编码的数据集是这样运作的：先对每个分量使用1位数据标识，然后如果确实是小数据的话(也就是区间在【-16.15】之内的话)，就使用5位的数据，否则的话，分量的增量的取值范围在【-256，256】之内，并用9位数据进行发送。如果任意分量的增量超出了这个范围的话就会换回使用绝对位置。通过使用这种编码方法，我可以对每个变化的位置只用平均下来26.1位来进行表示。 增量编码最小的三个分量 接下来，我将指出如同相对方向变化的话能几乎取得相对位置变化相同的效果。这里的问题可能会有些不一样，就是位置变化的取值范围相对于整个位置空间而言是非常非常小的，但是100毫秒内方向上的变化可能占整个四元数空间的话，是一个非常大的比例。我尝试了很多方法但并没有得到好的结果。我尝试直接对方向的增量这个四维向量进行编码并使用那个“最小的三个分量”这个技巧来隐含的表示最大的分量。我还尝试计算基线状态下的方向和当前方向之间的相对四元数。因为我知道w分量将是最大的分量（因为这个四元数表示的是旋转），我可以不用发送2位数据来确定最大的分量，但反过来将需要发送一位数据来标识w分量的正负，因为我不想对整个四元数取反。通过这种方案我能找到的最好的压缩方法只有“最小的三个分量”方法的数据量的90%。这个结果并不是太令人满意。我几乎都要放弃这个方向的优化了，但是我对“最小的三个分量”方法跑了一些分析结果。我发现按照“最小的三个分量”的格式90%的情况下方向里面最大的那个分量都和100毫秒之前基线状态下方向里面最大的分量是相同的。这意味着如果直接对最小的三个分量进行增量编码的话有可能是能获得更大收益的。更重要的是，我发现如果使用这种方法的话，在从基线数据重建整个方向信息的话不会有任何额外的精度损失。我从场景中运行一些方向信息并把这些方向信息以最小的三个分量”的格式导出来，并使用我曾经对位置信息使用过的暴力穷举方法来对每个分量的取值范围进行评估。所找到的最佳编码方法是：5-8，这意味着对于比较小的数值使用的是【-16.15】这个区间，对于比较大的数值，使用的是【-128,+127】这个区间。最后一件要做的事情：就跟位置信息的处理一样，大的取值范围可以通过单独的一位来提前知道分量值不会落在【-16.15】这个区间而进行更一步的拓展。我把如何做到这一点作为一个练习留给读者作为一个练习。要小心，不要让这个区间的首尾值最后都成为0。最后的结果是每个相对四元数平均下来只需要23.3位的数据来表示。这是“最小的三个分量”方法的数据量的80.3%。我们所做的优化大概就是这些内容了。但是还有一个小的优化还没有做。通过对传过来的位置和方向数据集进行一个了最终的分析，我注意到如果是在0.5毫米这个精确度的话大概有5%的位置信息是相比于基线状态的位置是没有任何变化的，而且有5%以“最小的三个分量”格式表示的方向信息也是相比于基线状态的方向信息是没有任何变化的。这两种概率是相互排斥的，因为这两种情况同时满足的话那么对应的立方体根本就不会发生变化进而根本就不会发送这个立方体的信息过来，这意味这在这个小规模的统计里面存在10%的立方体，它们的状态我们可以发送一个单独的位来表明是否有位置变化，再用另外一个单独的位来表明是否有方向变化。是的，如果这么做的话，就会给90%的立方体带来2位的额外负担，但是10%的立方体可以节省20多位的带宽（或者用2位信息换取了23.3位的方向信息，或者用2位信息换取了26.1位的位置信息），这种方法大概能给每个立方体在每次快照时候要发送的数据量减少大概2位。 带宽优化有很多的选择方案，而且可以通过一点点工作一些看上去不可能的事情事实上也会变得可能。通过文中的种种方法，我们大概降低了20M比特下来，最后平均下来只有不到0.25m比特。这相比原来未压缩的带宽，大概只有原来的1.25％！ 总结 好了, 这就是我用传统的手动位打包技术能压缩优化到的最大程度了，你可以看到做一些优化之后可以得到多么大的提升。你可以在这里（地址在这里）找到文中提到的所有压缩技术的代码实现。可以使用不同的方法得到更好的压缩比例。位打包这种方法并不是非常有效率的，是因为并不是所有的位的值取0或者1的概率都是相同的。无论根据具体的环境来如何努力的调整位打包技术，我们都可以根据数据集里面的取值的概率情况来建立一个更精确的模型来轻松的打败之前的调整结果。Fabian Giesen的实现（地址在这里）可以比我最好的位打包结果还能提升25%。Geomerics的克里斯·多兰（地址在这里）写了一篇很好的文章来探索数学上如何对四元数进行压缩，非常值得一读。如果不是直接使用”最小的三个分量“这种方法，而是利用之前的基线数据来估计下角速度和预测未来的方向应该能对增量编码的方向信息得到好的多的结果。下一章要讲的是：《状态同步》。【版权声明】原文作者未做权利声明，视为共享知识产权进入公共领域，自动获得授权；]]></content>
      <categories>
        <category>GS</category>
      </categories>
      <tags>
        <tag>GafferOnGames</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络物理模拟四之快照插值]]></title>
    <url>%2Fblog%2F2017%2F01%2F23%2Fsnapshot_interpolation%2F</url>
    <content type="text"><![CDATA[自我总结快照插值这种游戏同步技术的要点为 : 视觉模拟 : 每帧从网络的发送侧捕获所有相关状态的快照，并将其传输到网络的接收侧，在那里我们将试图重建一个视觉上近似合理的模拟 缓冲区 : 内插值之前会缓冲一段合适的时间来处理网络抖动 内插值Interpolation : 处理快照之间的拉扯 线性插值 Hermite插值 外插值Extrapolation (文中翻译为”预测”或”推测”) : 不可行, 因为外插值无法精准预测刚体运动以及各种物理 降低延迟 : 因为我们发送的快照的频率比较低，这样会带来的一个问题就是对快照进行插值的话会在网络延迟的基础上还要增加插值带来的延迟. 所以我们需要增加发送速率, 为了提高发送速度我们需要压缩快照数据技术的配合, 不然占用太多带宽了 减少宽带占用 : 因为所有需要在快照中包含所有实体信息, 所以数据量相当大, 得用各种方法压缩快照数据(网络物理模拟五之快照压缩) . . . 原文原文出处 原文标题 : Snapshot Interpolation (Interpolating between snapshots of visual state) Introduction Hi, I&rsquo;m Glenn Fiedler and welcome to Networked Physics. In the previous article we networked a physics simulation using deterministic lockstep. Now, in this article we&rsquo;re going to network the same simulation with a completely different technique: snapshot interpolation. Background While deterministic lockstep is very efficient in terms of bandwidth, it&rsquo;s not always possible to make your simulation deterministic. Floating point determinism across platforms is hard. Also, as the player counts increase, deterministic lockstep becomes problematic: you can&rsquo;t simulate frame n until you receive input from all players for that frame, so players end up waiting for the most lagged player. Because of this, I recommend deterministic lockstep for 2-4 players at most. So if your simulation is not deterministic or you want higher player counts then you need a different technique. Snapshot interpolation fits the bill nicely. It is in many ways the polar opposite of deterministic lockstep: instead of running two simulations, one on the left and one on the right, and using perfect determinism and synchronized inputs keep them in sync, snapshot interpolation doesn&rsquo;t run any simulation on the right side at all! Snapshots Instead, we capture a snapshot of all relevant state from the simulation on the left and transmit it to the right, then on the right side we use those snapshots to reconstruct a visual approximation of the simulation, all without running the simulation itself. As a first pass, let&rsquo;s send across the state required to render each cube: struct CubeState { bool interacting; vec3f position; quat4f orientation; }; I&rsquo;m sure you&rsquo;ve worked out by now that the cost of this technique is increased bandwidth usage. Greatly increased bandwidth usage. Hold on to your neckbeards, because a snapshot contains the visual state for the entire simulation. With a bit of math we can see that each cube serializes down to 225 bits or 28.1 bytes. Since there are 900 cubes in our simulation that means each snapshot is roughly 25 kilobytes. That&rsquo;s pretty big! At this point I would like everybody to relax, take a deep breath, and imagine we live in a world where I can actually send a packet this large 60 times per-second over the internet and not have everything explode. Imagine I have FIOS (I do), or I&rsquo;m sitting over a backbone link to another computer that is also on the backbone. Imagine I live in South Korea. Do whatever you need to do to suspend disbelief, but most of all, don&rsquo;t worry, because I&rsquo;m going to spend the entire next article showing you how to optimize snapshot bandwidth. When we send snapshot data in packets, we include at the top a 16 bit sequence number. This sequence number starts at zero and increases with each packet sent. We use this sequence number on receive to determine if the snapshot in a packet is newer or older than the most recent snapshot received. If it&rsquo;s older then it&rsquo;s thrown away. Each frame we just render the most recent snapshot received on the right: Look closely though, and even though we&rsquo;re sending the data as rapidly as possible (one packet per-frame) you can still see hitches on the right side. This is because the internet makes no guarantee that packets sent 60 times per-second arrive nicely spaced 1&frasl;60 of a second apart. Packets are jittered. Some frames you receive two snapshot packets. Other frames you receive none. Jitter and Hitches This is actually a really common thing when you first start networking. You start out playing your game over LAN and notice you can just slam out packets really fast (60pps) and most of the time your game looks great because over the LAN those packets actually do tend to arrive at the same rate they were sent&hellip; and then you start trying to play your game over wireless or the internet and you start seeing hitches. Don&rsquo;t worry. There are ways to handle this! First, let&rsquo;s look at how much bandwidth we&rsquo;re sending with this naive approach. Each packet is 25312.5 bytes plus 28 bytes for IP + UDP header and 2 bytes for sequence number. That&rsquo;s 25342.5 bytes per-packet and at 60 packets per-second this gives a total of 1520550 bytes per-second or 11.6 megabit/sec. Now there are certainly internet connections out there that can support that amount of traffic&hellip; but since, let&rsquo;s be honest, we&rsquo;re not really getting a lot of benefit blasting packets out 60 times per-second with all the jitter, let&rsquo;s pull it back a bit and send only 10 snapshots per-second: You can see how this looks above. Not so great on the right side but at least we&rsquo;ve reduced bandwidth by a factor of six to around 2 megabit/sec. We&rsquo;re definitely headed in the right direction. Linear Interpolation Now for the trick with snapshots. What we do is instead of immediately rendering snapshot data received is that we buffer snapshots for a short amount of time in an interpolation buffer. This interpolation buffer holds on to snapshots for a period of time such that you have not only the snapshot you want to render but also, statistically speaking, you are very likely to have the next snapshot as well. Then as the right side moves forward in time we interpolate between the position and orientation for the two slightly delayed snapshots providing the illusion of smooth movement. In effect, we&rsquo;ve traded a small amount of added latency for smoothness. You may be surprised at just how good it looks with linear interpolation @ 10pps: Look closely though and you can see some artifacts on the right side. The first is a subtle position jitter when the player cube is hovering in the air. This is your brain detecting 1st order discontinuity at the sample points of position interpolation. The other artifact occurs when a bunch of cubes are in a katamari ball, you can see a sort of &ldquo;pulsing&rdquo; as the speed of rotation increases and decreases. This occurs because attached cubes interpolate linearly between two sample points rotating around the player cube, effectively interpolating through the player cube as they take the shortest linear path between two points on a circle. Hermite Interpolation I find these artifacts unacceptable but I don&rsquo;t want to increase the packet send rate to fix them. Let&rsquo;s see what we can do to make it look better at the same send rate instead. One thing we can try is upgrading to a more accurate interpolation scheme for position, one that interpolates between position samples while considering the linear velocity at each sample point. This can be done with an hermite spline (pronounced &ldquo;air-mitt&rdquo;) Unlike other splines with control points that affect the curve indirectly, the hermite spline is guaranteed to pass through the start and end points while matching the start and end velocities. This means that velocity is smooth across sample points and cubes in the katamari ball tend to rotate around the cube rather than interpolate through it at speed. Above you can see hermite interpolation for position @ 10pps. Bandwidth has increased slightly because we need to include linear velocity with each cube in the snapshot, but we&rsquo;re able to significantly increase the quality at the same send rate. I can no longer see any artifacts. Go back and compare this with the raw, non-interpolated 10pps version. It really is amazing that we&rsquo;re able to reconstruct the simulation with this level of quality at such a low send rate. As an aside, I found it was not necessary to perform higher order interpolation for orientation quaternions to get smooth interpolation. This is great because I did a lot of research into exactly interpolating between orientation quaternions with a specified angular velocity at sample points and it seemed difficult. All that was needed to achieve an acceptable result was to switch from linear interpolation + normalize (nlerp) to spherical linear interpolation (slerp) to ensure constant angular speed for orientation interpolation. I believe this is because cubes in the simulation tend to have mostly constant angular velocity while in the air and large angular velocity changes occur only discontinuously when collisions occur. It could also be because orientation tends to change slowly while in the air vs. position which changes rapidly relative to the number of pixels affected on screen. Either way, it seems that slerp is good enough and that&rsquo;s great because it means we don&rsquo;t need to send angular velocity in the snapshot. Handling Real World Conditions Now we have to deal with packet loss. After the discussion of UDP vs. TCP in the previous article I&rsquo;m sure you can see why we would never consider sending snapshots over TCP. Snapshots are time critical but unlike inputs in deterministic lockstep snapshots don&rsquo;t need to be reliable. If a snapshot is lost we can just skip past it and interpolate towards a more recent snapshot in the interpolation buffer. We don&rsquo;t ever want to stop and wait for a lost snapshot packet to be resent. This is why you should always use UDP for sending snapshots. I&rsquo;ll let you in on a secret. Not only were the linear and hermite interpolation videos above recorded at a send rate of 10 packets per-second, they were also recorded at 5% packet loss with +/- 2 frames of jitter @ 60fps. How I handled packet loss and jitter for those videos is by simply ensuring that snapshots are held in the interpolation buffer for an appropriate amount of time before interpolation. My rule of thumb is that the interpolation buffer should have enough delay so that I can lose two packets in a row and still have something to interpolate towards. Experimentally I&rsquo;ve found that the amount of delay that works best at 2-5% packet loss is 3X the packet send rate. At 10 packets per-second this is 300ms. I also need some extra delay to handle jitter, which in my experience is typically only one or two frames @ 60fps, so the interpolation videos above were recorded with a delay of 350ms. Adding 350 milliseconds delay seems like a lot. And it is. But, if you try to skimp you end up hitching for 1/10th of a second each time a packet is lost. One technique that people often use to hide the delay added by the interpolation buffer in other areas (such as FPS, flight simulator, racing games and so on) is to use extrapolation. But in my experience, extrapolation doesn&rsquo;t work very well for rigid bodies because their motion is non-linear and unpredictable. Here you can see an extrapolation of 200ms, reducing overall delay from 350 ms to just 150ms: Problem is it&rsquo;s just not very good. The reason is that the extrapolation doesn&rsquo;t know anything about the physics simulation. Extrapolation doesn&rsquo;t know about collision with the floor so cubes extrapolate down through the floor and then spring back up to correct. Prediction doesn&rsquo;t know about the spring force holding the player cube up in the air so it the cube moves slower initially upwards than it should and has to snap to catch up. It also doesn&rsquo;t know anything about collision and how collision response works, so the cube rolling across the floor and other cubes are also mispredicted. Finally, if you watch the katamari ball you&rsquo;ll see that the extrapolation predicts the attached cubes as continuing to move along their tangent velocity when they should rotate with the player cube. Conclusion You could conceivably spend a great deal of time to improve the quality of this extrapolation and make it aware of various movement modes for the cubes. You could take each cube and make sure that at minimum the cube doesn&rsquo;t go through the floor. You could add some approximate collision detection or response using bounding spheres between cubes. You could even take the cubes in the katamari ball and make them predict motion to rotate around with the player cube. But even if you do all this there will still be misprediction because you simply can&rsquo;t accurately match a physics simulation with an approximation. If your simulation is mostly linear motion, eg. fast moving planes, boats, space ships &ndash; you may find that a simple extrapolation works well for short time periods (50-250ms or so), but in my experience as soon as objects start colliding with other non-stationary objects, extrapolation starts to break down. How can we reduce the amount of delay added for interpolation? 350ms still seems unacceptable and we can&rsquo;t use extrapolation to reduce this delay without adding a lot of inaccuracy. The solution is simple: increase the send rate! If we send 30 snapshots per-second we can get the same amount of packet loss protection with a delay of 150ms. 60 packets per-second needs only 85ms. In order to increase the send rate we&rsquo;re going to need some pretty good bandwidth optimizations. But don&rsquo;t worry, there&rsquo;s a lot we can do to optimize bandwidth. So much so that there was too much stuff to fit in this article and I had to insert an extra unplanned article just to cover all of it! 译文 译文出处 翻译：崔国军（飞扬971） 审校：张乾光(星际迷航) 介绍 大家好，我是格伦·菲德勒。欢迎阅读《网络物理模拟》的系列文章，这个系列文章的主题是关于如何将一个物理模拟通过网络通信进行同步。在之前的文章中，我们通过具有确定性的帧同步将物理模拟通过网络通信进行同步。在这一篇文章中我们将使用一种完全不同的技术来将物理模拟通过网络通信进行同步，这个技术就是：快照信息插值方法。 背景 为什么需要一种不同的技术？这是因为虽然具有确定性的帧同步这种同步策略在节省带宽方面非常有效，但是要保证你的仿真具有完美的确定性这个事情并不总是可行的，有些时候是不实际的。此外，具有确定性的帧同步这种同步策略在玩家数目增多的情况下会遇到一些问题，因为你要收到所有玩家对应帧的输入才能对这一帧进行模拟。在实践中，这意味着每个人必须等待最滞后的那个玩家。以我的经验来说，我建议在联网环境下只在2到4个玩家的时候使用具有确定性的帧同步这种同步策略。（译者注：其实国内现在已经有20个玩家在互联网环境下使用具有确定性的帧同步这种同步策略的游戏了，就算dota也是支持5v5对战的，原作者太过于谨慎了）。如果你想要支持更多数目的玩家或者你的模拟并不具有完美的确定性，那么你就需要一种不同的技术了。快照信息插值技术在许多方面都站在具有确定性的帧同步技术的对立面。它不再需要在网络的两侧同时运行仿真，使用程序的确定性以及同步输入信息来保证网络的两侧的仿真始终保持同步。。。快照信息插值技术根本不需要在接收侧运行任何的模拟！ 快照 我们所要做的就是每帧从网络的发送侧捕获所有相关状态的快照，并将其传输到网络的接收侧，在那里我们将试图重建一个视觉上近似合理的模拟。作为第一步，让我们把所需的状态直接发送给网络的接收侧，让它可以渲染每一个立方体： 123456struct CubeState&#123; bool interacting; vec3f position; quat4f orientation;&#125;; 需要注意的是，我们发送了一个布尔值用来标记这个立方体是否与玩家存在交互。为什么需要这个布尔值？这是因为在网络的接收侧并没有运行一个模拟，因此并不会有碰撞检测来告诉我们什么时候一个立方体应该被标红而什么时候不需要对一个立方体进行标红。如果我们想要一个立方体在它与玩家存在互动的情况下变红的话，我们需要在快照中包含此信息。我敢肯定这时候你已经明白这项技术的消耗在于增大了带宽的使用。其实是大大增大了对带宽的占用。这是因为快照包含了整个仿真的状态。通过一点数学计算我们可以看到每个立方体序列化下来的话大概占据225比特或者28.1字节。因为在我们的仿真中有大概900个立方体，这意味着每个快照大约需要25k的字节。这个数据量相当大了！在这一点上，我想每个人都放松、深呼吸，想象我们生活在一个世界里，在这个世界里面我可以在互联网上以60次每秒的速度来实际发送数据包，而不会有什么意外。想象一下我有光纤服务光纤服务或者我坐在骨干网的后面，与另外一台位于骨干网的电脑相连。。想象一下，我使用IPv6，而最大传输单元的大小是100k。可以想象一下，我住在韩国。做你任意想做的，不要有任何的怀疑，而且最重要的是，不用担心网络有任何的问题，这是因为我将在下一篇文章中向你展示如何快速优化快照信息插值这种方法带来的带宽负担。当我们用数据包的方式发送快照的时候，我们会在数据包的头部包括16位的序列号。这个序列号会从零开始并且随着每个快照的发送而增大。我们在接收的时候使用这个序列号来决定数据包中的快照到底比我们最近收到的快照更新还是更旧。如果比我们最近收到的快照更旧的话，我们就会丢弃这个快照。然后，在网络的接收侧我们只会渲染我们接收到的最新的快照上的信息。 请注意，即使我们尽一切可能的快速发送数据（一帧一个数据包），我们仍然会在网络的接收侧看到物体发生抖动现象。这是因为在互联网上根本就不会保证数据包会按照六十分之一秒的间隔到达网络的另外一侧。数据包的到达时间会发生抖动。在某些帧你会收到两个帧的快照，而在另外一些帧则会根本收不到。 抖动与拉扯 这实际上是当你第一次启动网络一个非常常见的事情。你开始通过局域网来玩你的游戏并注意到你可以以一个非常高的速度来发送数据包并且你的游戏会看上表现的非常非常不错，这是因为你的数据包几乎在发送的同时就会到达网络的接收侧。。。然后你开始尝试通过无线网络或者互联网来玩你的游戏，然后你就看到各种各样的抖动。不用担心，有办法来处理这个问题！首先，让我们看看使用这种幼稚的方法进行发送数据包，我们会占据多少的带宽。每个数据包是25312.5字节加上IP 和UDP包头所占的28个字节并且还要有2个字节用来表示网络包的序号。这就是一个数据包的大小：25342.5字节，每秒60个数据包的话就一共要1520550字节，或者换算下就是11.6M每秒。现在当然也有互联网连接可以支持这种规模的流量。。。但是既然我们每秒发送60个数据包也没有什么太大的益处，还是充满了这种抖动，让我们稍微稳妥一点，只要每秒发送10次快照就可以了。 你可以通过上面的效果看到修改以后的表现效果如何。这对网络的接收侧并没有什么太大的影响，起码我们将带宽减少到了六分之一，大概是2M每秒。我们正在朝着正确的方向继续前进。 线性插值 现在是关于处理快照的一些技巧。我们所做的不是在我们接收到快照数据立刻开始进行渲染，而是利用插值缓冲区缓冲了快照数据一小段时间。这个插值缓冲区会持有快照数据一段时间，这样你不仅会持有你准备渲染的这一帧的快照数据，而且从统计数字上看，你很有可能会持有下一帧你需要的快照数据。然后随着网络的接收侧在时间上向前移动，我们会对这两帧轻微延迟的快照数据进行物体位置和方向的插值，提供平滑运动的错觉。实际上，我们通过增加了一小段延迟来交换物体的平滑运动。你可以会很吃惊，通过每秒10帧的快照数据以及一个简单的线性插值，就可以得到表现如此只好的一个表现： 但是如果你仔细观察的话，你还是可以看到在网络的接收侧里面存在大量的瑕疵。首先是当玩家立方体在空中盘旋的时候，玩家立方体的位置有一个轻微的抖动。这是因为你的大脑在位置插值的采样点那里检测到了1秒左右的中断。另外一个瑕疵出现在一大推立方体出于katamari球之中的时候，你可以看到某种“脉冲“的存在，立方体旋转的速度会出现升高或者降低的情况。这种情况的出现是是因为附加的这些立方体会在围绕玩家立方体的周围旋转的时候在两个采样点之间进行线性的插值，通过玩家立方体进行插值是有效的，这是因为这样两点之间就是最短线性距离。 Hermite插值 我发现这些瑕疵是不能接受的，但是不希望增加数据包的发送速率来解决这些问题。让我们看看我们能做些什么，让数据包的发送速率不变的情况让整个效果能够看起来更棒。我们可以尝试升级的一个事情是对于位置的一种更加精确的插值方案：这种方案在位置采样点之间进行插值的同时还要考虑每个采样点的线性速度。可以用于执行这种插值的一种曲线是厄米特曲线。这种曲线和其他需要控制点间接的影响曲线不同，厄米特曲线保证一定会通过起点和终点的同时还能匹配起点和终点的速度。这意味着立方体在通过采样点的速度是平滑的，而且katamari球中的立方体会倾向于围绕立方体旋转，而不是通过它的速度进行插值。 上面你所看到的效果是通过每秒10次的快照数据进行的厄米特曲线插值。我们的带宽略有增加，但我们能够在显著提高质量的同时保证数据包的发送速率不变。我再也看不到任何的瑕疵。让我们重新回到厄米特曲线插值之前的效果进行下对比。我们能够在如此低的数据包发送速率下重建出来如此质量水平的模拟，这真的是令人惊讶的改变。顺便说一句，我发现没有必要对方向四元数进行更高阶的插值来得到平滑的插值。因为我做了很多的研究关于对方向四元数使用一个特定的线性速度进行准确的插值，然后我发现这其实非常的困难。为了达到一个可以接收的效果我们所有要做的事情就是从线性插值加归一化（nlerp函数）到球面线性插值插值（slerp函数）来确保方向四元数的插值得到一个固定的角速度。我相信这是因为在仿真中空中的立方体试图保持大体固定的角速度，而在发生碰撞的时候，比较高的角速度会突然变得不连续。它也可能是因为在空中的时候方向趋于变化缓慢而位置会会相对与屏幕上受影响的像素的数目而快速的变化。无论哪种方式，似乎球面插值都表现的不错，这就是很棒的方法因为这意味着我们并不需要在快照数据中发送角速度。 处理真实世界的情况 现在，我们必须处理的数据包丢失的问题了。在上一篇文章讨论完了UDP和TCP的优劣之后，我敢肯定你可以知道了为什么我们从来不考虑通过TCP来发送我们的快照数据。快照数据是时间敏感的，但是和有确定性的帧同步中的输入信息不一样，这个数据没有必要是可靠的。如果一个快照数据丢失了，我们可以跳过这一帧，并使用一个缓冲区中更新的快照数据进行插值。们再也不想停下来，等待丢失的快照数据包的重发。这就是为什么你总是应该使用UDP发送快照数据的原因。我要告诉你一个秘密。上面这两个视频不仅仅是在每秒10个数据包的发送频率下用线性插值或厄米特插值的效果，它们也记录了在有2帧抖动、5%丢包率下的效果。我该如何处理丢包和抖动？我的做法是确保快照数据会被保存在插值缓冲区，这样在插值之前会缓冲一段合适的时间。我的经验法则是，插值缓冲区应该有足够的延迟，这样就算我连续丢失了两个数据包，我仍然有快照数据可以进行插值。在实验中我发现在有2-5%的数据包丢包率的情况下，延迟量大概是数据包发送速度的3倍左右比较合适。如果是每秒发送10个数据包的情况，这个延迟量就是大概300毫秒。我还需要一些额外的延迟来处理抖动的情况，以我的经验来说，如果是在60fps的条件下，大概预留一到两帧就可以了，所以上面的插值效果的视频使用了一个大概350毫秒的延迟。添加350毫秒的延迟，似乎带来了很大的延迟。但是如果因为觉得不舍得而不在这里使用350毫秒的延迟的话，要么就得到一个充满拉扯的效果，要么就会遇到每秒有十分之一的数据包丢失。人们在其他领域（比如说第一人称设计游戏、飞行模拟游戏、赛车游戏以及其他的游戏）经常使用的用于隐藏由于插值缓冲区带来的延迟的方法是使用预测方法。以我的经验来看，预测方法对于缸体来说效果似乎不太好，这是因为它们的运动不是线性的并且无法预测。在这里，你可以看到使用了200毫秒的预测方法理论上可以将延迟从350毫秒减少到150毫秒： 问题是，它工作起来效果似乎不是太好。当然这个原因在于预测方法根本不知道任何有关物理模拟方面的内容。预测方法不知道立方体需要与地面发生碰撞，立方体在向下遇到地板的时候会反弹回来，当然这是正确的表现。预测方法不知道有关施加在空中的玩家立方体上的反弹力，因此立方体起初会比实际的情况下移动的慢一些，然后开始加速追上自己应该在的位置。预测方法也不知道任何有关碰撞方面的内容以及如果发生碰撞了该发生何种反应，所以如果立方体在地板上或者其他立方体上滚过的时候就会发生预测上的错误。最后，如果你仔细观察katamari球的话，你会看到预测方法会让附着于其上的立方体继续沿着它们的切线速度运动，而这个时候它们应该跟玩家立方体一起转动。 结论 现在，你可以持续的花费大量的时间来改善推测的质量，并通过认识立方体的各种运动模式来持续提高。你可以对每个立方体进行推断，并确保最低程度是立方体不会穿越地面。你还可以添加碰撞检测的一些推断并且利用立方体的球型包围盒做出响应。你甚至可以选取katamari球体里面的立方体并预测当它们在玩家立方体周围的时候会如何运动。但是，即使你做了所有的这一切，仍然会有预测失误，因为你根本无法精确匹配物理模拟与近似估计之间的差距。如果你的仿真中大多数是直线运动，比如说，快速移动的飞机、轮船、太空飞船等等-你会发现一个简单的外推法非常适用于短时间内（50-250ms左右）的这种运动。以我的经验来看，只要物体开始与非静止的物体开始发生碰撞，这种预测就将完全的不成立。我们怎样才能减少由于插值带来的延迟？350毫秒似乎仍然是不能接受的延迟，我们不能使用预测和外推来减少这种延迟，因为这样增加了大量的不确定性。解决的办法其实很简单：增加发送速率。如果我们每秒发送30次快照的话，我们可以在相同的数据包丢失率情况将延迟降低到150ms。如果我们把发送速率增大到每秒60个数据包的程度，延迟将只有85ms。为了提高发送速度，我们会需要进行一些不错的带宽优化。不过不用担心，我们可以做很多很多的事情来优化带宽。但是我们如果把这些东西都放到这篇文章的话，这篇文章就会有太多太多的东西，我不得不插入一个计划之外的文章来涵盖带宽优化方面的东西！【版权声明】原文作者未做权利声明，视为共享知识产权进入公共领域，自动获得授权。]]></content>
      <categories>
        <category>GS</category>
      </categories>
      <tags>
        <tag>GafferOnGames</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络物理模拟三之具有确定性的帧同步]]></title>
    <url>%2Fblog%2F2017%2F01%2F22%2Fdeterministic_lockstep%2F</url>
    <content type="text"><![CDATA[自我总结帧同步要点如下 : 确定性 : 去除随机数 缓冲 : 因为数据包并不是均匀地到达, 所以要做一个缓冲区, 然后再均匀地取出 不用TCP : 因为我们的数据对时间非常敏感, 不接受到第n个输入包就无法继续模拟第n帧, 而TCP的确认机制以及重传机制当我们丢包时, 我们只能暂停等待它重发造成卡顿 用UDP : 发送冗余数据 : 因为帧同步只发送玩家input数据, 而input包是很小的, 所以发冗余也不会很大 增量包 : 加一个bit来标志跟上一个包的比较结果, 如果这个包跟上个包一致则只发送一个1, 如果不一致则发送0和这个包的完整数据 帧同步的缺点 : 等的人太多 : 因为你要收到所有玩家对应帧的输入才能对这一帧进行模拟.在实践中，这意味着每个人必须等待最滞后的那个玩家.人越多等得越久, 所以帧同步不适合mmo. 比较耗性能 : 因为帧同步技术的话, 在客户端中，每个对象都要执行所有的物理之类的运算; 而状态同步可以只同步当前玩家周围对象的状态, 不需要同步所有对象 . . . 原文 原文出处 原文标题 : Deterministic Lockstep (Keeping simulations in sync by sending only inputs) Introduction Hi, I&rsquo;m Glenn Fiedler and welcome to Networked Physics. In the previous article we explored the physics simulation we&rsquo;re going to network in this article series. In this article specifically, we&rsquo;re going to network this physics simulation using deterministic lockstep. Deterministic lockstep is a method of networking a system from one computer to another by sending only the inputs that control that system, rather than the state of that system. In the context of networking a physics simulation, this means we send across a small amount of input, while avoiding sending state like position, orientation, linear velocity and angular velocity per-object. The benefit is that bandwidth is proportional to the size of the input, not the number of objects in the simulation. Yes, with deterministic lockstep you can network a physics simulation of one million objects with the same bandwidth as just one. While this sounds great in theory, in practice it&rsquo;s difficult to implement deterministic lockstep because most physics simulations are not deterministic. Differences in floating point behavior between compilers, OS&rsquo;s and even instruction sets make it almost impossible to guarantee determinism for floating point calculations. Determinism Determinism means that given the same initial condition and the same set of inputs your simulation gives exactly the same result. And I do mean exactly the same result. Not close. Not near enough. Exactly the same. Exact down to the bit-level. So exact, you could take a checksum of your entire physics state at the end of each frame and it would be identical. Above you can see a simulation that is almost deterministic. The simulation on the left is controlled by the player. The simulation on the right has exactly the same inputs applied with a two second delay starting from the same initial condition. Both simulations step forward with the same delta time (a necessary precondition to ensure exactly the same result) and both simulations apply the same inputs. Notice how after the smallest divergence the simulation gets further and further out of sync. This simulation is non-deterministic. What&rsquo;s going on is that the physics engine I&rsquo;m using (Open Dynamics Engine) uses a random number generator inside its solver to randomize the order of constraint processing to improve stability. It&rsquo;s open source. Take a look and see! Unfortunately this breaks determinism because the simulation on the left processes constraints in a different order to the simulation on the right, leading to slightly different results. Luckily all that is required to make ODE deterministic on the same machine, with the same complied binary and on the same OS (is that enough qualifications?) is to set its internal random seed to the current frame number before running the simulation via dSetRandomSeed. Once this is done ODE gives exactly the same result and the left and right simulations stay in sync. And now a word of warning. Even though the simulation above is deterministic on the same machine, that does not necessarily mean it would also be deterministic across different compilers, a different OS or different machine architectures (eg. PowerPC vs. Intel). In fact, it&rsquo;s probably not even deterministic between debug and release builds due to floating point optimizations. Floating point determinism is a complicated subject and there&rsquo;s no silver bullet. For more information please refer to this article. Networking Inputs Now let&rsquo;s get down to implementation. Our example physics simulation is driven by keyboard input: arrow keys apply forces to make the player cube move, holding space lifts the cube up and blows other cubes around, and holding &lsquo;z&rsquo; enables katamari mode. How can we network these inputs? Must we send the entire state of the keyboard? No. It&rsquo;s not necessary to send the entire keyboard state, only the state of the keys that affect the simulation. What about key press and release events then? No. This is also not a good strategy. We need to ensure that exactly the same input is applied on the right side, at exactly the same time, so we can&rsquo;t just send &lsquo;key pressed&rsquo;, and &lsquo;key released&rsquo; events over TCP. What we do instead is represent the input with a struct and at the beginning of each simulation frame on the left side, sample this struct from the keyboard: struct Input { bool left; bool right; bool up; bool down; bool space; bool z; }; Next we send that input from the left simulation to the right simulation in a way that the simulation on the right side knows that the input belongs to frame n. And here&rsquo;s the key part: the simulation on the right can only simulate frame n when it has the input for that frame. If it doesn&rsquo;t have the input, it has to wait. For example, if you were sending across using TCP you could simply send the inputs and nothing else, and on the other side you could read the packets coming in, and each input received corresponds to one frame for the simulation to step forward. If no input arrives for a given render frame, the right side can&rsquo;t advance forward, it has to wait for the next input to arrive. So let&rsquo;s move forward with TCP, you&rsquo;ve disabled Nagle&rsquo;s Algorithm, and you&rsquo;re sending inputs from the left to the right simulation once per-frame (60 times per-second). Here it gets a little complicated. Since we can&rsquo;t simulate forward unless we have the input for the next frame, it&rsquo;s not enough to just take whatever inputs arrive over the network and then run the simulation on inputs as they arrive because the result would be very jittery. Data sent across the network at 60HZ doesn&rsquo;t typically arrive nicely spaced, 1/60th of a second between each packet. If you want this sort of behavior, you have to implement it yourself. Playout Delay Buffer Such a device is called a playout delay buffer. Unfortunately, the subject of playout delay buffers is a patent minefield. I would not advise searching for &ldquo;playout delay buffer&rdquo; or &ldquo;adaptive playout delay&rdquo; while at work. But in short, what you want to do is buffer packets for a short amount of time so they appear to be arriving at a steady rate even though in reality they arrive somewhat jittered. What you&rsquo;re doing here is similar to what Netflix does when you stream a video. You pause a little bit initially so you have a buffer in case some packets arrive late and then once the delay has elapsed video frames are presented spaced the correct time apart. If your buffer isn&rsquo;t large enough then the video playback will be hitchy. With deterministic lockstep your simulation behaves exactly the same way: showing hitches when the buffer isn&rsquo;t large enough to smooth out the jitter. Of course, the cost of increasing the buffer size is additional latency, so you can&rsquo;t just buffer your way out of all problems. At some point the user says enough! That&rsquo;s too much latency added. No sir, I will not play your game with 1 second of extra delay :) My playout delay buffer implementation is really simple. You add inputs to it indexed by frame, and when the very first input is received, it stores the current local time on the receiver machine and from that point on delivers packets assuming they should play at that time + 100ms. You&rsquo;ll likely need to something more complex for a real world situation, perhaps something that handles clock drift, and detecting when the simulation should slightly speed up or slow down to maintain a nice amount of buffering safety (being &ldquo;adaptive&rdquo;) while minimizing overall latency, but this is reasonably complicated and probably worth an article in itself. The goal is that under average conditions the playout delay buffer provides a steady stream of inputs for frame n, n+1, n+2 and so on, nicely spaced 1/60th of a second apart with no drama. In the worst case the time arrives for frame n and the input hasn&rsquo;t arrived yet it returns null and the simulation is forced to wait. If packets get bunched up and delivered late, it&rsquo;s possibly to have multiple inputs ready to dequeue per-frame. In this case I limit to 4 simulated frames per-render frame so the simulation has a chance to catch up, but doesn&rsquo;t simulate for so long that it falls further behind, aka. the &ldquo;spiral of death&rdquo;. Is TCP good enough? Using this playout buffer strategy and sending inputs across TCP we ensure that all inputs arrive reliably and in-order. This is convenient, and after all, TCP is designed for exactly this situation: reliable-ordered data. In fact, It&rsquo;s a common thing out there on the Internet for pundits to say stuff like: If you need reliable-ordered, you can&rsquo;t do better than TCP! Your game doesn&rsquo;t need UDP (yet) But I&rsquo;m here to tell you this kind of thinking is dead wrong. Above you can see the simulation networked using deterministic lockstep over TCP at 100ms latency and 1% packet loss. If you look closely on the right side you can see hitches every few seconds. What&rsquo;s happening here is that each time a packet is lost, TCP has to wait RTT*2 while it is resent (actually it can be much worse, but I&rsquo;m being generous&hellip;). The hitches happen because with deterministic lockstep the right simulation can&rsquo;t simulate frame n without input n, so it has to pause to wait for input n to be resent! That&rsquo;s not all. It gets significantly worse as latency and packet loss increase. Here is the same simulation networked using deterministic lockstep over TCP at 250ms latency and 5% packet loss: Now I will concede that if you have no packet loss and/or a very small amount of latency then you very well may get acceptable results with TCP. But please be aware that if you use TCP it behaves terribly under bad network conditions. Can we do better than TCP? Can we beat TCP at its own game. Reliable-ordered delivery? The answer is an emphatic YES. But only if we change the rules of the game. Here&rsquo;s the trick. We need to ensure that all inputs arrive reliably and in order. But if we send inputs in UDP packets, some of those packets will be lost. What if, instead of detecting packet loss after the fact and resending lost packets, we redundantly include all inputs in each UDP packet until we know for sure the other side has received them? Inputs are very small (6 bits). Let&rsquo;s say we&rsquo;re sending 60 inputs per-second (60fps simulation) and round trip time we know is going the be somewhere in 30-250ms range. Let&rsquo;s say just for fun that it could be up to 2 seconds worst case and at this point we&rsquo;ll time out the connection (screw that guy). This means that on average we only need to include between 2-15 frames of input and worst case we&rsquo;ll need 120 inputs. Worst case is 120 x 6 = 720 bits. That&rsquo;s only 90 bytes of input! That&rsquo;s totally reasonable. We can do even better. It&rsquo;s not common for inputs to change every frame. What if when we send our packet instead we start with the sequence number of the most recent input, and the 6 bits of the first (oldest) input, and the number of un-acked inputs. Then as we iterate across these inputs to write them to the packet we can write a single bit (1) if the next input is different to the previous, and (0) if the input is the same. So if the input is different from the previous frame we write 7 bits (rare). If the input is identical we write just one (common). Where inputs change infrequently this is a big win and in the worst case this really isn&rsquo;t that bad. 120 bits of extra data sent. Just 15 bytes overhead worst case. Of course another packet is required from the right simulation to the left so the left side knows which inputs have been received. Each frame the right simulation reads input packets from the network before adding them to the playout delay buffer and keeps track of the most recent input it has received and sends this back to the left as an &ldquo;ack&rdquo; or acknowledgment for inputs. When the left side receives this ack it discards any inputs older than the most recent received input. This way we have only a small number of inputs in flight proportional to the round trip time between the two simulations. Flawless Victory We have beaten TCP by changing the rules of the game. Instead of &ldquo;implementing 95% of TCP on top of UDP&rdquo; we have implemented something totally different and better suited to our requirements. A protocol that redundantly sends inputs because we know they are small, so we never have to wait for retransmission. So exactly how much better is this approach than sending inputs over TCP? Let&rsquo;s take a look&hellip; The video above shows deterministic lockstep synchronized over UDP using this technique with 2 seconds of latency and 25% packet loss. Imagine how awful TCP would look under these conditions. So in conclusion, even where TCP should have the most advantage, in the only networking model that relies on reliable-ordered data, we can still easily whip its ass with a simple protocol built on top of UDP. 译文 译文出处 翻译：张乾光（星际迷航） 审校：陈敬凤(nunu) 介绍 大家好，我是格伦·菲德勒。欢迎大家阅读系列教程《网络物理仿真》，这个系列教程的目的是将物理仿真的状态通过网络进行广播。在之前的文章中，我们讨论了物理仿真需要在网络上进行广播的各种属性。在这篇文章中，我们将使用具有确定性的帧同步技术来将物理仿真通过网络进行传递和广播。具有确定性的帧同步是一种用来在一台电脑和其他电脑之间进行同步的方法，这种方法发送的是控制仿真状态变化的输入，而不是像其他方法那样发送的是仿真过程中物体的状态变化。这种方法的背后思想是给定一个初始状态，不妨设为S(n)，我们通过使用输入信息I(n)来运行仿真就能得到S(n+1)这个状态。然后我们可以通过S(n+1)这个状态和输入信息I(n+1)来运行仿真就能得到S(n+2)这个状态，我们可以一直重复这个过程得到S(n+3)、S(n+4)以及其后的各个状态。这看上去有点像是数学归纳法，我们可以只通过输入信息和之前的仿真状态就能得到后面的仿真状态-而且得到的仿真状态是高度一致，并且也不需要发送任何状态方面的同步。这个网络模型的主要优点是所需的带宽仅仅用来传递输入信息，而输入信息所占的带宽其实是与仿真中物体的数目是完全无关的。你可以通过网络来对一百万个物体进行物理仿真，它所需的带宽会跟只对一个物体进行物理仿真所需的带宽完全相同。可以很容易的看到物理物体的状态通常是包含位置、方向、线性速度和角速度（如果是未压缩的话，这些状态一共需要52字节，在这里面假设方向使用的是四元数而其他所有的变量都是用vec3来表示），所以当你有大量的物体需要进行物理仿真的时候，这是一个非常具有吸引力的方案。 确定性 如果要采用具有确定性的帧同步这个方案来将物理仿真网络化，首先要做的第一件事就是要确保你的仿真具有确定性。在这个上下文中，确定性其实和自由意志之类的没有关系。它只是意味着给定相同的初始条件和相同的一组输入，仿真能够给出完全相同的结果。而且我在这里要着重强调下是完全相同的结果。而不是说的什么在在浮点数容忍度内足够接近。这种精确是精确到比特位的。所以这种精确性使得你可以在每帧的末尾对整个物理状态做一个校验和，不同机器上面同一帧得到的校验和是完全一致的。 从上面的图中可以看到，这里面的仿真几乎是具有确定性的，但是不完全具有确定性。左边的仿真由玩家进行控制，而右边的仿真有完全一致的初始状态，输入信息也和左边完全相同，但是要有2秒钟的延迟。这两个仿真使用相同的间隔时间进行更新（使用相同的间隔时间进行更新也是确保得到完全一致结果的一个必要前提条件），并且在每一帧前对相同的输入信息进行相应。你可以注意到随着仿真的进行，那些一开始很微小的差异是如何一点点被扩大，最后导致两个仿真完全不同步。所以说这个仿真其实不具有确定性。上面到底发生了什么?最后会导致两个仿真的结果差的这么大？这是因为我使用的物理引擎（ODE）在它的内部使用了一个随机数生成器来对约束处理的顺序进行随机化来提高稳定性。这个物理引擎是完全开源的，所以可以看看它的内部实现！不幸的是，由于左边的仿真处理约束的顺序和右边的仿真处理约束的顺序不同，这导致有一些轻微不同的结果。幸运的是我们还是能找到让ODE这个物理引擎具有确定性的条件：要在同一台机器上、使用同一个编译好的二进制文件、并且在完全相同的操作系统上运行（这是必要的限制条件么？），还有就是在运行仿真之前通过dSetRandomSeed把随机数的种子设为当前帧的帧数。一旦满足这些条件的话，ODE这个物理引擎能够给出完全相同的结果，并且左边和右边的仿真能够保持高度一致的同步。 现在让我们针对上面这个情况给出一个警告。即使ODE这个物理引擎能够在相同的机器上得到确定性的结果，但是这并不一定意味着在不同编译器、不同的操作系统甚至不同的机器架构上（比如说在PowerPC架构上和在Intel架构上）它能够得到确定性的结果。事实上，由于浮点数的优化，在程序的debug版本和release版本之间可能都没有办法得到确定性的结果。浮点数的确定性是一个非常复杂的问题，而且这个问题没有银弹（意味着这个问题没有什么简单可行的解决办法）。要了解更多这方面的信息，请参考这篇文章。 网络输入 Inputs 让我们讨论下具有确定性的帧同步的具体实现方法。你可能想知道在我们这个示例仿真中输入信息到底是啥，以及我们该如何吧这些输入信息进行网络化。我们这个示例仿真是由键盘输入进行驱动的：方向键会给代表玩家的立方体施加一个力让他进行移动、按下空格键会把代表玩家的立方体提起来并把碰到的立方体四处滚落、按下‘z’键会启动katamari模式。但是我们该如何对这些输入信息进行网络化呢？我们需要把整个键盘的状态在网络上进行传输么？在这些键被按下和释放的时候我们要发送这些事件么？不，整个键盘的状态不需要在网络上进行传输，我们只需要传输那些会影响仿真的按键。那么被按下和释放的键的事件需要在网络上进行传输么？不，这也不是一个好的策略。我们需要确保的是在仿真第n帧的时候右边的仿真能够应用完全相同的输入信息，所以我们不能仅仅是通过TCP来发送“按键按下”和“按键释放”的事件，因为这些事件到达网络的另外一侧的时间如果早于或者晚于第n帧的时候都会给仿真造成偏差。相反我们做的事情是用一个结构来表示整个输入信息，并且在左边一侧仿真开始的时候，通过键盘的访问来填充这个结构并把填充好的结构放到一个滑动窗口中，我们在后面可以根据帧号来对这个输入进行访问。?123456789struct Input{ bool left; bool right; bool up; bool down; bool space; bool z;};现在我们就可以通过上面的方法来把左边仿真的输入信息发送到右边仿真中去，这样右边的仿真就知道属于第n帧的输入信息到底是怎么样的。举个简单的例子来说，如果你在通过TCP进行发送的话，你可以简单的只发送输入信息而不发送其他的内容，而发送的输入信息的顺序隐含着帧号N。而在网络的另外一侧，你可以读取传送过来的数据包，并且对输入信息进行处理并把输入信息应用到仿真中去。我不推荐这种方法，但我们可以从这里开始，然后再向你展示如何把这种方法变得更好。在进一步对这个方法进行优化之前，让我们先统一下使用的网络环境，让我们假设下我们是通过TCP进行数据传输，已经禁止了Nagle算法并且每帧都会从左边的仿真向右边的仿真发送一次输入信息（频率是每秒60次）。这里面有一个问题会变得比较复杂。把左边仿真发生的输入信息通过网络进行传输，然后右边仿真并没有足够的时间来从网络上收到输入信息并利用这些到达的输入信息来模拟仿真，因为这个过程需要一定的时间。你不能按照某个频率在网络上发送信息并且期望它们能够按照完全相同一致的频率到达网络的另外一侧(比如说，每六十分之一秒到达一个数据包)。互联网并不是按照这个方式工作的。根本就没有这样的保证。 播放延迟缓冲区 如果你想要做到这一点的话，你必须实现一个叫做播放延迟缓冲区的东西。不幸的是，播放延迟缓冲区收到了专利保护，也就是一个专利雷区。我不建议读者在实际使用确定性的帧同步模型的时候搜索“播放延迟缓冲区”或者是“自适应性延迟缓冲区”。但简而言之，你所需要做的事情是缓存收到的数据包一小段时间以便让这些数据包表现的像是以一个稳定的速度到达那样，即使实际上它们的到达时间是充满抖动的。你现在所做的事情就跟你在看一个视频流的时候，Netflix所做的事情是很类似的。你在最初开始的时候停顿了一下以便你可以拥有一个缓冲区，这样即使一些数据包的到达时间有点晚，但是这种延迟不会对视频帧按正确时间间距的表现有什么影响，视频帧仍然会按照正确的时间间隔一帧帧的播放。当然如果你的缓冲区没有足够大的话，那么这些视频帧的播放可能还是会充满一些抖动。有了确定性的帧同步机制，你的模拟仿真将会以完全相同的方式执行。我建议在播放的时候最好在一开始有100毫秒-250毫秒的延迟。在下面的例子中，我使用的是100毫秒的延迟，这是因为我让延迟最小化来增强响应性。我的播放延迟缓冲区的实现非常的简单。是将输入信息按照帧序号进行添加，当收到第一个输入信息的时候，它保存了接收方机器上的当前本地时间，并且从那一个时刻起假设所有到达的数据包都会带上100毫秒的延迟。你可能需要一些更加复杂的机制来适应真实世界的情况，比如说可能需要处理时钟漂移、检测在什么时候应该适当的加速或者减慢模拟的速度来让缓冲区的大小在能够保证整体延迟最小的情况下保持在一个适度的情形（这就是所谓的“自适应”），但是这些内容可能会相当的复杂并且可能需要一整篇文章来专门对这些情况进行专门论述。而且如前所述，这些内容还涉及到了专利保护方面的内容，所以这些内容我就不详细展开了，把如何处理这些东西全部托付给你自己实现。在平均情况下，播放延迟缓冲区给帧n、n+1、n+2以及后续的帧提供了一个稳定的输入信息流，非常完美的以六十分之一秒的间隔依次到达。在最坏的情况下，就是已经该执行第N帧的模拟仿真了，但是这一帧的输入信息还没有到达，那么它就会返回一个空指针，这样整个模拟仿真就必须在那里进行等待了。如果数据包被集中起来发送并且到达接收方的时候已经比预期时间延迟了，这可能会导致多个帧的输入信息同时准备好等待出列进行计算。如果是这种情况的话，我会限制在一个渲染帧的时间最多只能进行4次模拟仿真，这样给模拟仿真一个追上来的机会。如果你把这个值设置的更高的话，那么可能会引起更多其他的问题，比如卡顿，因为你可能需要超过六十分之一秒的时间来运行这些帧（这可能会造成一个非常不好的反馈体验）。总而言之，重要的是确保你的模拟仿真在使用确定性的帧同步这个方案的时候性能不是在中央处理器这一端受限的，否则的话，你在运行更多的模拟帧来追上正常的模拟速度的时候会遇到很多麻烦。 TCP足够好了吗 通过使用这种延迟缓冲区的策略以及通过TCP协议来发送输入信息，我们可以很轻松的确保所有的输入信息会有序的到达并且传输是可信赖的。这就是一开始TCP协议在设计的时候希望达到的目标。实际上，下面这些东西就是互联网的专家常说的一些东西：· 如果你需要一个可以信赖的有序的发送信息的方法，你不可能找到一种比通过TCP协议进行传输更好的方法！· 你的游戏根本就不会需要UDP协议。我在这里将告诉你上面这些想法都是大错特错的。 在上面的视频中，你可以看到如果网络同步模型使用基于TCP协议的确定性的帧同步模型的话，模拟仿真的网络延迟大概是100毫秒，并且有百分之一的丢包率。如果你仔细看右边的话，你可以每隔几秒就会出现一些抖动。如果你在两边都出现这种的情况，那么很抱歉这意味着你的电脑的性能对于播放这些视频而言可能有些艰难。如果是这种情况的话，我建议下载这个视频然后离线观看。无论如何，这里所发生的事情是当一个数据包丢失的时候，TCP协议需要等待至少２个往返时延才会重新发送这个数据包（实际上这里面的等待时间可能会更糟，但是我很慷慨的设定了一个非常理想的情况。。。）。所以上面发生的抖动原因是确定性的帧同步模型要求右边的模拟仿真在没有第n帧的输入信息的时候不能执行第N帧的模拟仿真计算，所以整个模拟仿真就停下来等待对应帧的输入信息的到达!这还不是全部！随着延迟时间的增大和丢包率的增加，整个情况会变得更加的糟糕。这是在250毫秒延迟和百分之五丢包率的情况下，使用基于TCP协议的确定性的帧同步模型进行相同的仿真模拟运算导致的结果： 现在我要承认一个事情，如果延迟时间设置的非常低的话同时不存在丢包的情况下，那么使用TCP协议进行输入信息的传输会是一个非常可以接受的结果。但是请注意，如果你使用TCP协议来发送时间敏感的数据的话，随着延迟时间的增大和丢包率的增加，整个结果会急剧恶化。 我们能比TCP做得更好吗 我们可以做得更好吗?我们能在自己的游戏里面找到一种比使用TCP协议更好的办法。同时还能实现可信赖的有序传递？答案是肯定的。但前提是我们需要改变游戏的规则。下面将具体描述下我们将使用的技巧。我们需要确保所有的输入信息能够可靠地按顺序到达。但是如果我们只发送UDP数据包输入。但是如果我们只使用UDP数据包来发送输入信息的话，这里面的一些数据包会丢失。那么如果我们不采用事后检测的方法来判断哪些数据包丢失并发送这些丢失的数据包的话，我们采用另外一种方法，只是把我们有的所有输入信息都冗余的发送直到我们知道这些输入信息成功的到达另外一侧怎么样？输入信息都非常非常的小（只有6比特这么大）。让我们假设下我们在每秒需要发送60个输入信息（因为模拟仿真的频率是60fps ），而且我们知道一个往返的时间大概是在30毫秒到250毫秒之间。纯粹为了好玩，让我们假设下载最糟糕的情况下，一个往返的时间可以高达2秒，如果出现这种情况的话，那么整个连接就会超时。这意味着在平均情况我们只需要包括大概2到15帧的输入信息，而在最坏情况下，我们大概需要120帧的输入信息。那么最坏情况下，输入信息的大小是120 x 6 = 720比特。这只是90字节的输入信息!这是安全合理的。我们还能做的更好。在每一帧中都出现输入信息的变化是非常不常见的。我们可以用最近的那个input的序列号和第一个input的6比特还有所有未被确认的输入信息的数目来做一些事。然后，当我们对这些输入信息进行遍历将它们写入数据包的时候，如果发现这一帧的输入信息如果和之前帧的输入信息不同的话，我们可以写入一个单独的比特位（1），如果发现这一帧的输入信息如果和之前帧的输入信息相同的话，我们可以写入一个单独的比特位（0）。所以这一帧的输入信息如果和之前帧的输入信息不同的话（这种情况比较少），我们需要写入7个比特位，这一帧的输入信息如果和之前帧的输入信息相同的话（这种情况其实非常常见），我们只需要写入1个比特位。在输入信息很少发生变化的情况，这是一个重大的胜利，而在最坏的情况下出现的情况也不会非常糟糕。只需要发送额外120个比特的数据，也就是说在最坏情况下，也只有15字节的额外开销.当然在这种情况下，需要从右边的模拟仿真中发送一个数据包到左边的模拟仿真中去，这样左边的模拟仿真才知道哪些输入信息被成功收到了。在每一帧，右边的模拟仿真都会从网络中读取输入的数据包，然后才会把这些数据包添加到延迟播放缓冲区，并且通过帧号记录它已经收到的最近那一帧的输入信息，或者如果你想容易一点处理这个问题的话，那么使用一个16比特的序列号就能很好的包装这个信息。在所有的输入数据包都被处理以后，如果右边的模拟仿真收到任何帧的输入信息以后都会回复一个数据包给左边的模拟仿真，告诉它最近收到的最新序列号是多少，这基本就是一个“ack”包，也就是确认包。当左边的模拟仿真收到这个“ack”包，也就是确认包以后，它会滑动输入信息窗口并且丢弃比已经确认的序列号还老的输入信息包。已经没有必要再发送这些输入信息包给右边的模拟仿真了，因为已经知道右边的模拟仿真成功的接受到了这些输入信息包。通过这种方式，我们通常只有少量的输入信息正在传输过程中，而且这个数量还是与数据包的往返时间成正比的。 完美胜利 我们通过改变游戏的规则成功了找到了一种比TCP协议更好的办法。我们并不是通过在UDP协议纸上构建了实现TCP协议百分之九十五的功能的新协议，而是实现了一种完全不同的方法，而且更加适合我们的要求：数据对时间非常敏感。我们开发了一个自定义的协议，可以冗余的发送input，因为我们知道这些input非常小, 所以我们不必去等待重传它们。所以这种方法到底比通过TCP协议来发送数据好多少呢？让我们通过一个例子来看一下。 上面的视频是基于UDP协议来使用具有确定性的帧同步模型，延迟时间是２秒，并且有百分之二十五的丢包率。想象下如果我们是使用基于TCP协议的具有确定性的帧同步模型，我们该看到多么可怕的场景！所以最后我们能得到这么一个结论：即使这是一个TCP协议最具有优势的情况下，这是唯一一个依赖可靠性、有序性数据传输的网络模型，我们还是可以很容易的通过一个自定义的协议基于UDP来发送我们的数据包，并且得到的效果更好。【版权声明】原文作者未做权利声明，视为共享知识产权进入公共领域，自动获得授权。]]></content>
      <categories>
        <category>GS</category>
      </categories>
      <tags>
        <tag>GafferOnGames</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络物理模拟二之网络物理部分的视频演示]]></title>
    <url>%2Fblog%2F2017%2F01%2F21%2F%E7%BD%91%E7%BB%9C%E7%89%A9%E7%90%86%E6%A8%A1%E6%8B%9F%E4%BA%8C%E4%B9%8B%E7%BD%91%E7%BB%9C%E7%89%A9%E7%90%86%E9%83%A8%E5%88%86%E7%9A%84%E8%A7%86%E9%A2%91%E6%BC%94%E7%A4%BA%2F</url>
    <content type="text"><![CDATA[原文出处 Introduction to Networked Physics Introduction Hi, I&rsquo;m Glenn Fiedler and welcome to the first article in Networked Physics. In this article series we&rsquo;re going to network a physics simulation three different ways: deterministic lockstep, snapshot interpolation and state synchronization. But before we get to this, let&rsquo;s spend some time exploring the physics simulation we’re going to network in this article series: Your browser does not support the video tag. Here I’ve setup a simple simulation of a cube in the open source physics engine ODE. The player moves around by applying forces at its center of mass. The physics simulation takes this linear motion and calculates friction as the cube collides with the ground, inducing a rolling and tumbling motion. This is why I chose a cube instead a sphere. I want this complex, unpredictable motion because rigid bodies in general move in interesting ways according to their shape. An Interactive World Networked physics get interesting when the player interacts with other physically simulated objects, especially when those objects push back and affect the motion of the player. So let&rsquo;s add some more cubes to the simulation: Your browser does not support the video tag. When the player interacts with a cube it turns red. When that cube comes to rest it turns back to grey (non-interacting). While it’s cool to roll around and interact with other cubes, what I really wanted was a way to push lots of cubes around. What I came up with is this: Your browser does not support the video tag. As you can see, interactions aren’t just direct. Red cubes pushed around by the player turn other cubes they touch red as well. This way, interactions fan out to cover all affected objects. A Complicated Case I also wanted a very complex coupled motion between the player and non-player cubes such they become one system: a group of rigid bodies joined together by constraints. To implement this I thought it would be cool if the player could roll around and create a ball of cubes, like in one of my favorite games Katamari Damacy. Your browser does not support the video tag. Cubes within a certain distance of the player have a force applied towards the center of the cube. These cubes remain physically simulated while in the katamari ball, they are not just “stuck” to the player like in the original game. This is a very difficult situation for networked physics!]]></content>
      <categories>
        <category>GS</category>
      </categories>
      <tags>
        <tag>GafferOnGames</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络物理模拟一之网络物理部分的简介]]></title>
    <url>%2Fblog%2F2017%2F01%2F20%2F%E7%BD%91%E7%BB%9C%E7%89%A9%E7%90%86%E6%A8%A1%E6%8B%9F%E4%B8%80%E4%B9%8B%E7%BD%91%E7%BB%9C%E7%89%A9%E7%90%86%E9%83%A8%E5%88%86%E7%9A%84%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[译者：陈敬凤（nunu） 审校：崔国军（飞扬971）大家好，我是格伦·菲德勒，是一位来自洛杉矶的职业游戏开发者。我作为一名职业游戏开发者已经有15年了。而在这其中的十年时光里，我都是专门做网络编程的。在这期间我大部分的业余时间一直致力于研究网络物理模拟方面的问题。在这个系列文章中我的目标是分享我已经知道的一切有关网络物理模拟方面的知识。写这些文章是一个工作量很大的工作，而且我完全是在我的业余时间里面来做这些事情的。如果有了你们的支持，我可以找时间来继续写这个系列的文章。如果可以的话，请通过 patreon 支持我的工作。网络物理部分的入门是相当困难的。你可能想知道你的物理模拟是否需要确定性以便可以进行网络传输和通信？你应该通过网络来发送物体的物理状态么？或者你应该通过网络来发送一些诸如碰撞时间或者物体相互作用力这些东西么？你到底是应该通过UDP还是TCP来传送数据？你应该使用客户端/服务器通信模型还是点对点的通信模型？ 你是否需要一个专门的服务器？如何隐藏播放动作的时候的延迟？如何不让玩家作弊？人们经常问我这些问题。大多数情况下这些问题的正确答案依赖于他们所选用的网络模型。你可能甚至不知道你正在使用或计划使用的网络模型是什么，但事实是你必须选择一个网络模型并且你选择的这个网络模型会使某些事情变得容易，并使得另外一些事情变得困难。在这个系列文章中我们将使用三种不同的方式来构建一个网络物理模拟的例子，并通过这些例子来对不同的网络模型展开一个讨论。我把这些基本技术称为“同步策略”。他们是：《确定性的帧同步》、《快照插值》和《状态同步》。带宽是网络物理部分的另外一个重要方面。我该怎么做才能对所有这些对象进行同步状态？因此，我们打算花一整篇文章来进行介绍一个带宽优化的案例，向你展示如何将快照插值技术的带宽从每秒18m减少到每秒256k。在选择了合适的同步策略以及合理的优化完带宽以后，我们现在准备讨论客户机/服务器与点对点对等网络之间的优劣性了。除了讨论利弊以外，我还将分享在艰苦的发布使用客户机/服务器网络模型的游戏和点对点对等网络模型的游戏的过程中学到的那些经验和教训。这篇文章会有你在其他地方找不到的非常具体、务实的信息。最后，我们讨论构建在基本同步策略上的不同网络模型，这些网络模型的细节各有不同。在这里。你会发现你的网络物理模拟可以有许多不同的选择：《确定性帧同步网络模型》、《分布式权威网络模型》和《带有客户端预测的服务器仲裁网络模型》。所以系上安全带准备出发了。我们还有一大堆材料需要学习！接下来的这篇文章是：《物理模拟》为什么格伦·菲德勒在Patreon上寻求自助？嘿，大家好，我是格伦·菲德勒，网站gafferongames.com的作者。在过去的10年，我给大家分享了许多游戏开发方面的文章：《如何解决你的时间戳》、《整合基础》、《UDP与TCP》、《每个程序员都需要了解游戏开发网络方面的知识》以及其他一些文章。我还分享了《物理编程技巧》、《网络游戏编程的基础知识》和我的个人项目虚拟围棋。我想继续新的系列文章《网络物理模拟》以及《构建一个游戏网络协议》，但是我需要你的帮助！托管代码和文章需要费用，写这些文章需要花费大量的时间去研究和写作。如果你喜欢gafferongames.com上面的文章，请向我展现出你对我的支持并鼓励我写更多的文章并开放更多的源代码！【版权声明】原文作者未做权利声明，视为共享知识产权进入公共领域，自动获得授权。如果你觉得这篇文章有价值，请在 patreon 上支持原作者。]]></content>
      <categories>
        <category>GS</category>
      </categories>
      <tags>
        <tag>GafferOnGames</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[5分钟上手boost.asio]]></title>
    <url>%2Fblog%2F2017%2F01%2F12%2F5%E5%88%86%E9%92%9F%E4%B8%8A%E6%89%8Bboost.asio%2F</url>
    <content type="text"><![CDATA[Boost.Asio入门首先，让我们先来了解一下什么是Boost.Asio？怎么编译它？ linux下直接 : sudo apt-get install libboost-all-dev 什么是Boost.Asio简单来说，Boost.Asio是一个跨平台的、主要用于网络和其他一些底层输入/输出编程的C++库。 . . . 异步VS同步 首先，异步编程和同步编程是非常不同的。 在同步编程中，所有的操作都是顺序执行的，比如从socket中读取（请求），然后写入（回应）到socket中。 每一个操作都是阻塞的。 因为操作是阻塞的，所以为了不影响主程序，当在socket上读写时，通常会创建一个或多个线程来处理socket的输入/输出。 因此，同步的服务端/客户端通常是多线程的。 相反的，异步编程是事件驱动的。 虽然启动了一个操作，但是你不知道它何时会结束；它只是提供一个回调给你，当操作结束时，它会调用这个API，并返回操作结果。 对于有着丰富经验的QT（诺基亚用来创建跨平台图形用户界面应用程序的库）程序员来说，这就是他们的第二天性。 因此，在异步编程中，你只需要一个线程。 因为中途做改变会非常困难而且容易出错，所以你在项目初期（最好是一开始）就得决定用同步还是异步的方式实现网络通信。 不仅API有极大的不同，你程序的语意也会完全改变（异步网络通信通常比同步网络通信更加难以测试和调试）。 你需要考虑是采用阻塞调用和多线程的方式（同步，通常比较简单），或者是更少的线程和事件驱动（异步，通常更复杂）。 同步例子同步客户端下面是一个基础的同步客户端例子： 12345using boost::asio;io_service service;ip::tcp::endpoint ep( ip::address::from_string("127.0.0.1"), 2001);ip::tcp::socket sock(service);sock.connect(ep); 首先，你的程序至少需要一个io_service实例。 Boost.Asio使用io_service同操作系统的输入/输出服务进行交互。 通常一个io_service的实例就足够了。 然后，创建你想要连接的地址和端口，再建立socket。 把socket连接到你创建的地址和端口。 同步服务端下面是一个简单的同步Boost.Asio的服务端： 1234567891011121314151617typedef boost::shared_ptr&lt;ip::tcp::socket&gt; socket_ptr;io_service service;ip::tcp::endpoint ep( ip::tcp::v4(), 2001)); // listen on 2001ip::tcp::acceptor acc(service, ep);while ( true) &#123; socket_ptr sock(new ip::tcp::socket(service)); acc.accept(*sock); boost::thread( boost::bind(client_session, sock));&#125;void client_session(socket_ptr sock) &#123; while ( true) &#123; char data[512]; size_t len = sock-&gt;read_some(buffer(data)); if ( len &gt; 0) write(*sock, buffer("ok", 2)); &#125;&#125; 首先，同样是至少需要一个io_service实例。 然后你指定你想要监听的端口，再创建一个接收器——一个用来接收客户端连接的对象。 在接下来的循环中，你创建一个虚拟的socket来等待客户端的连接。 然后当一个连接被建立时，你创建一个线程来处理这个连接。 在client_session线程中来读取一个客户端的请求，进行解析，然后返回结果。 异步例子异步客户端而创建一个异步的客户端，你需要做如下的事情： 123456789using boost::asio;io_service service;ip::tcp::endpoint ep( ip::address::from_string("127.0.0.1"), 2001);ip::tcp::socket sock(service);sock.async_connect(ep, connect_handler);service.run();void connect_handler(const boost::system::error_code &amp; ec) &#123; // 如果ec返回成功我们就可以知道连接成功了&#125; 在程序中你需要创建至少一个io_service实例。 你需要指定连接的地址以及创建socket。 当连接完成时（其完成处理程序）你就异步地连接到了指定的地址和端口，也就是说，connect_handler被调用了。 当connect_handler被调用时，检查错误代码（ec），如果成功，你就可以向服务端进行异步的写入。 注意：只要还有待处理的异步操作，servece.run()循环就会一直运行。 在上述例子中，只执行了一个这样的操作，就是socket的async_connect。 在这之后，service.run()就退出了。 每一个异步操作都有一个完成处理程序——一个操作完成之后被调用的函数。 异步服务端 下面的代码是一个基本的异步服务端 123456789101112131415161718using boost::asio;typedef boost::shared_ptr&lt;ip::tcp::socket&gt; socket_ptr;io_service service;ip::tcp::endpoint ep( ip::tcp::v4(), 2001)); // 监听端口2001ip::tcp::acceptor acc(service, ep);socket_ptr sock(new ip::tcp::socket(service));start_accept(sock);service.run();void start_accept(socket_ptr sock) &#123; acc.async_accept(*sock, boost::bind( handle_accept, sock, _1) );&#125;void handle_accept(socket_ptr sock, const boost::system::error_code &amp;err) &#123; if ( err) return; // 从这里开始, 你可以从socket读取或者写入 socket_ptr sock(new ip::tcp::socket(service)); start_accept(sock);&#125; 在上述代码片段中，首先，你创建一个io_service实例，指定监听的端口。 然后，你创建接收器acc——一个接受客户端连接，创建虚拟的socket，异步等待客户端连接的对象。 最后，运行异步service.run()循环。 当接收到客户端连接时，handle_accept被调用（调用async_accept的完成处理程序）。 如果没有错误，这个socket就可以用来做读写操作。 在使用这个socket之后，你创建了一个新的socket，然后再次调用start_accept()，用来创建另外一个“等待客户端连接”的异步操作，从而使service.run()循环一直保持忙碌状态。]]></content>
      <categories>
        <category>NP</category>
      </categories>
      <tags>
        <tag>Boost</tag>
        <tag>CPP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式具体实现重要组件之RPC]]></title>
    <url>%2Fblog%2F2017%2F01%2F10%2F%E5%88%86%E5%B8%83%E5%BC%8F%E5%85%B7%E4%BD%93%E5%AE%9E%E7%8E%B0%E9%87%8D%E8%A6%81%E7%BB%84%E4%BB%B6%E4%B9%8BRPC%2F</url>
    <content type="text"><![CDATA[RPC 是什么？RPC 的全称是 Remote Procedure Call 是一种进程间通信方式。它允许程序调用另一个地址空间（通常是共享网络的另一台机器上）的过程或函数，而不用程序员显式编码这个远程调用的细节。即程序员无论是调用本地的还是远程的，本质上编写的调用代码基本相同。 像腾讯的phxrpc框架是使用Protobuf作为IDL用于描述RPC接口以及通信数据结构 c++ RPC的实现 1、一套完善的序列化框架；在不同的进程间传输数据，序列化是第一步，如何可靠且方便地将对象转化为二进制（或者其他格式），在对端则是如何正确且安全地将其从二进制恢复为对象。 2、完善的底层通信协议；其需要提供合适的语义抽象：服务端支持怎样的并发，是单客户单访问，还是多访问；而客户端的并发模型由服务端决定。当然，还需要健壮且足够的接口抽象，毕竟分布式环境，“一切皆有可能”，需要应对各种问题。 3、一个可用的反射系统。是的，需要在C++环境下建立一个反射系统。这一步是最为关键的，其由C++11支持。因为，我们需要注册一个类的各种信息，以供RPC调用。]]></content>
      <categories>
        <category>NP</category>
      </categories>
      <tags>
        <tag>Distributed</tag>
        <tag>RPC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tcp拥塞控制之慢启动和拥塞避免]]></title>
    <url>%2Fblog%2F2016%2F12%2F31%2Ftcp%E6%8B%A5%E5%A1%9E%E6%8E%A7%E5%88%B6%E4%B9%8B%E6%85%A2%E5%90%AF%E5%8A%A8%E5%92%8C%E6%8B%A5%E5%A1%9E%E9%81%BF%E5%85%8D%2F</url>
    <content type="text"><![CDATA[TCP拥塞控制概览TCP的拥塞控制算法被设计用来防止快速的发送者压垮整个网络。如果一个发送TCP发送包的速度要快于一个中间路由器转发的速度，那么该路由器就会开始丢弃包。这将会导致较高的包丢失率，其结果是如果TCP保持以相同的速度发送这些被丢弃的分段的话就会极大地降低性能。TCP的拥塞控制算法在下列两个场景中是比较重要的。 在连接建立之后：此时（或当传输在一个已经空闲了一段时间的连接上恢复时），发送者可以立即向网络中注入尽可能多的分段，只要接收者公告的窗口大小允许即可。（事实上，这就是早期的TCP实现的做法。）这里的问题在于如果网络无法处理这种分段洪泛，那么发送者会存在立即压垮整个网络的风险。 当拥塞被检测到时：如果发送TCP检测到发生了拥塞，那么它就必须要降低其传输速率。TCP是根据分段丢失来检测是否发牛了拥塞，因为传输错误率是非常低的，即如果一个包丢失了，那么就认为发生了拥塞。 TCP的拥塞控制策略组合采用了两种算法： 慢启动 拥塞避免。 慢启动算法会使发送TCP在开始的时候以低速传输分段，但同时允许它以指数级的速度提高其速率，只要这些分段都得到接收TCP的确认。慢启动能够防血一个快速的TCP发送者压垮整个网络。但如果不加限制的话，慢启动在传输速率上的指数级增长意味着发送者在短时间内就会压垮整个网络。TCP的拥塞避免算法用来防止这种情况的发生，它为速率的增长安排了一个管理实体。 有了拥塞避免之后，在连接刚建立时，发送TCP会使用一个较小的拥塞窗口，它会限制所能传输的未确认的数据数量。当发送者从对等TCP处接收到确认时，拥塞窗口在一开始时会呈现指数级增长。但一旦拥塞窗口增长到一个被认为是接近网络传输容量的阈值时，其增长速度就会变成线性，而不是指数级的。（对刚络容量的估算是根据检测到拥塞时的传输速率来计算得出的或者在一开始建立连接时设定为一个固定值。）在任何时刻，发送TCP传输的数据数量还会受到接收TCP的通告窗口和本地的TCP发送缓冲器的大小的限制。 慢启动和拥塞避免算法组合起来使得发送者可以快速地将传输速度提升至网络的可用容量，并且不会超出该容量。这些算法的作用是允许数据传输快速地到达一个平衡状态，即发送者传输包的速率与它从接收者处接收确认的速率一致。 在该图中，假定当cwnd为32个报文段时就会发生拥塞。于是设置ssthresh为16个报文段，而cwnd为1个报文段。在时刻0发送了一个报文段，并假定在时刻1接收到它的ACK，此时cwnd增加为2。接着发送了2个报文段，并假定在时刻2接收到它们的ACK，于是cwnd增加为4（对每个ACK增加1次）。这种指数增加算法一直进行到在时刻3和4之间收到8个ACK后cwnd等于ssthresh时才停止，从该时刻起，cwnd以线性方式增加，在每个往返时间内最多增加1个报文段。 慢启动当一个新的TCP连接建立或检测到由重传超时(RTO)导致的丢包时,需要执行慢启动o TCP发送端长时间处于空闲状态也可能调用慢启动算法。慢启动的目的是,使TCP在用拥塞避免探寻更多可用带宽之前得到cwnd值,以及帮助TCP建立ACK时钟。通常,TCP在建立新连接时执行慢启动,直至有丢包时,执行拥塞避免算法(参见16.2.2节)进人稳定状态。下文引自[RFC5681]: 在传输初始阶段，由于未知网络传输能力，需要缓慢探测可用传输资源，防止短时间内大量数据注入导致拥塞。慢启动算法正是针对这一问题而设计。在数据传输之初或者重传计时器检测到丢包后，需要执行慢启动。 TCP以发送一定数目的数据段开始慢启动（在SYN交换之后），称为初始窗口(Initial Window，IW)。IW的值初始设为一个SMSS（发送方的最大段大小），但在[RFC5681]中设为一个稍大的值，计算公式如下： IW= 2* (SMSS)且小于等于2个数据段（当SMSS&gt; 2190字节） IW=3+(SMSS)且小于等于3个数据段（当2190≥SMSS&gt; 1095字节） IW= 4* (SMSS)且小于等于4个数据段（其他） 述IW的计算方式可能使得初始窗口为几个数据包大小（如3个或4个），为简单起见，我们只讨论IW=1 SMSS的情况。TCP连接初始的cwnd=1 SMSS，意味着初始可用窗口矽也为1 SMSS。注意到大部分情况下，SMSS为接收方的MSS（最大段大小）和路径MTU（最大传输单元）两者中较小值。 假设没有出现丢包情况且每个数据包都有相应的ACK，第一个数据段的ACK到达，说明可发送一个新的数据段。每接收到一个好的ACK响应，慢启动算法会以min (N, SMSS)来增加cwnd值。这里的．Ⅳ是指在未经确认的传输数据中能通过这一“好的ACK”确认的字节数。所谓的“好的ACK”是指新接收的ACK号大于之前收到的ACK。 因此，在接收到一个数据段的ACK后，通常cwnd值会增加到2，接着会发送两个数据段。如果成功收到相应的新的ACK，cwnd会由2变4，由4变8，以此类推。一般情况下，假设没有丢包且每个数据包都有相应ACK，在t轮后∥的值为矽=2k，即t= log2W，需要t个RTT时间操作窗口才能达到矽大小。这种增长看似很快（以指数函数增长），但若与一开始就允许以最大可用速率（即接收方通知窗口大小）发送相比，仍显缓慢。(矽不会超过awnd。) 如果假设某个TCP连接中接收方的通知窗口非常大（比如说，无穷大），这时cwnd就是影响发送速率的主要因素（设发送方有较大发送需求）。如前所述，cwnd会随着RTT呈指数增长。因此，最终cwnd（矿也如此）会增至很大，大量数据包的发送将导致网络瘫痪(TCP吞吐量与W/RTT成正比)。当发生上述情况时，cwnd将大幅度减小（减至原值一半）。这是TCP由慢启动阶段至拥塞避免阶段的转折点，与cwnd和慢启动闽值(slow start threshold，ssthresh)相关。 拥塞避免如上所述，在连接建立之初以及由超时判定丢包发生的情况下，需要执行慢启动操作。在慢启动阶段，cwnd会快速增长，帮助确立一个慢启动阈值。一旦达到阈值，就意味着可能有更多可用的传输资源。如果立即全部占用这些资源，将会使共享路由器队列的其他连接出现严重的丢包和重传情况，从而导致整个网络性能不稳定。 为了得到更多的传输资源而不致影响其他连接传输，TCP实现了拥塞避免算法。一旦确立慢启动阈值，TCP会进入拥塞避免阶段，cwnd每次的增长值近似于成功传输的数据段大小。这种随时间线性增长方式与慢启动的指数增长相比缓慢许多。 慢启动与拥塞避免的选择在通常操作中，某个TCP连接总是选择运行慢启动和拥塞避免中的一个，不会出现两者同时进行的情况。现在我们考虑，在任一给定时刻如何决定选用哪种算法。我们已经知道，慢启动是在连接建立之初以及超时发生时执行的。那么决定使用慢启动还是拥塞避免的关键因素是什么呢？ 前面我们已经提到过慢启动阈值。这个值和cwnd的关系是决定采用慢启动还是拥塞避免的界线。当cwnd&lt; ssthresh，使用慢启动算法；当cwnd&gt; ssthresh，需要执行拥塞避免；而当两者相等时，任何一种算法都可以使用。由上面描述可以得出，慢启动和拥塞避免之间最大的区别在于，当新的ACK到达时，cwnd怎样增长。有趣的是，慢启动阈值不是固定的，而是随时间改变的。它的主要目的是，在没有丢包发生的情况下，记住上一次“最好的”操作窗口估计值。换言之，它记录TCP最优窗口估计值的下界。 慢启动阈值的初始值可任意设定（如awnd或更大），这会使得TCP总是以慢启动状态开始传输。当有重传情况发生，无论是超时重传还是快速重传，ssthresh会按下式改变： ssthresh - max(在外数据值／2，2*SMSS) (16-1) 注意微软最近的（“下一代”）TCP/IP访议栈中，上述等式变为ssthresh=max (min (cwnd, awnd) /2, 2*SMSS) 我们已经知道，如果出现重传情况，TCP会认为操作窗口超出了网络传输能力范围。这时会将慢启动阈值( ssthresh)减小至当前窗口大小的一半（但不小于2*SMSS），从而减小最优窗口估计值。这样通常会导致ssthresh减小，但也有可能会使之增大。分析TCP拥塞避免的操作流程，如果整个窗口的数据都成功传输，那么cwnd值可以近似增大1 SMSS。因此，若cwnd在一段时间范围内已经增大，将ssthresh设为整个窗口大小的一半可能使其增大。这种情况发生在当TCP探测到更多可用带宽时。在慢启动和拥塞避免结合的情况下，ssthresh和cwnd的相互作用使得TCP拥塞处理行为显现其独有特性。下面我们探讨将两者结合的完整的算法。]]></content>
      <categories>
        <category>NP</category>
      </categories>
      <tags>
        <tag>TCP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[游戏网络开发五之每个游戏开发者都需要知道的游戏网络知识]]></title>
    <url>%2Fblog%2F2016%2F11%2F18%2F%E6%B8%B8%E6%88%8F%E7%BD%91%E7%BB%9C%E5%BC%80%E5%8F%91%E4%BA%94%E4%B9%8B%E6%AF%8F%E4%B8%AA%E6%B8%B8%E6%88%8F%E5%BC%80%E5%8F%91%E8%80%85%E9%83%BD%E9%9C%80%E8%A6%81%E7%9F%A5%E9%81%93%E7%9A%84%E6%B8%B8%E6%88%8F%E7%BD%91%E7%BB%9C%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[原文原文出处 Introduction Hi, I&rsquo;m Glenn Fiedler and welcome to Networking for Game Programmers. Have you ever wondered how multiplayer games work? From the outside it seems magical: two or more players sharing a consistent experience across the network like they actually exist together in the same virtual world. But as programmers we know the truth of what is actually going on underneath is quite different from what you see. It turns out it&rsquo;s all an illusion. A massive sleight-of-hand. What you perceive as a shared reality is only an approximation unique to your own point of view and place in time. Peer-to-Peer Lockstep In the beginning games were networked peer-to-peer, with each each computer exchanging information with each other in a fully connected mesh topology. You can still see this model alive today in RTS games, and interestingly for some reason, perhaps because it was the first way - it&rsquo;s still how most people think that game networking works. The basic idea is to abstract the game into a series of turns and a set of command messages when processed at the beginning of each turn direct the evolution of the game state. For example: move unit, attack unit, construct building. All that is needed to network this is to run exactly the same set of commands and turns on each player&rsquo;s machine starting from a common initial state. Of course this is an overly simplistic explanation and glosses over many subtle points, but it gets across the basic idea of how networking for RTS games work. You can read more about this networking model here: 1500 Archers on a 28.8: Network Programming in Age of Empires and Beyond. It seems so simple and elegant, but unfortunately there are several limitations. First, it&rsquo;s exceptionally difficult to ensure that a game is completely deterministic; that each turn plays out identically on each machine. For example, one unit could take slightly a different path on two machines, arriving sooner to a battle and saving the day on one machine, while arriving later on the other and erm. not saving the day. Like a butterfly flapping it&rsquo;s wings and causing a hurricane on the other side of the world, one tiny difference results in complete desynchronization over time. The next limitation is that in order to ensure that the game plays out identically on all machines it is necessary to wait until all player&rsquo;s commands for that turn are received before simulating that turn. This means that each player in the game has latency equal to the most lagged player. RTS games typically hide this by providing audio feedback immediately and/or playing cosmetic animation, but ultimately any truly game affecting action may occur only after this delay has passed. The final limitation occurs because of the way the game synchronizes by sending just the command messages which change the state. In order for this to work it is necessary for all players to start from the same initial state. Typically this means that each player must join up in a lobby before commencing play, although it is technically possible to support late join, this is not common due to the difficulty of capturing and transmitting a completely deterministic starting point in the middle of a live game. Despite these limitations this model naturally suits RTS games and it still lives on today in games like &ldquo;Command and Conquer&rdquo;, &ldquo;Age of Empires&rdquo; and &ldquo;Starcraft&rdquo;. The reason being that in RTS games the game state consists of many thousands of units and is simply too large to exchange between players. These games have no choice but to exchange the commands which drive the evolution of the game state. But for other genres, the state of the art has moved on. So that&rsquo;s it for the deterministic peer-to-peer lockstep networking model. Now lets look at the evolution of action games starting with Doom, Quake and Unreal. Client/Server In the era of action games, the limitations of peer-to-peer lockstep became apparent in Doom, which despite playing well over the LAN played terribly over the internet for typical users: Although it is possible to connect two DOOM machines together across the Internet using a modem link, the resulting game will be slow, ranging from the unplayable (e.g. a 14.4Kbps PPP connection) to the marginally playable (e.g. a 28.8Kbps modem running a Compressed SLIP driver). Since these sorts of connections are of only marginal utility, this document will focus only on direct net connections. The problem of course was that Doom was designed for networking over LAN only, and used the peer-to-peer lockstep model described previously for RTS games. Each turn player inputs (key presses etc.) were exchanged with other peers, and before any player could simulate a frame all other player&rsquo;s key presses needed to be received. In other words, before you could turn, move or shoot you had to wait for the inputs from the most lagged modem player. Just imagine the wailing and gnashing of teeth that this would have resulted in for the sort of folks with internet connections that were &ldquo;of only marginal utility&rdquo;. :) In order to move beyond the LAN and the well connected elite at university networks and large companies, it was necessary to change the model. And in 1996, that&rsquo;s exactly what John Carmack and his team did when he released Quake using client/server instead of peer-to-peer. Now instead of each player running the same game code and communicating directly with each other, each player was now a &ldquo;client&rdquo; and they all communicated with just one computer called the &ldquo;server&rdquo;. There was no longer any need for the game to be deterministic across all machines, because the game really only existed on the server. Each client effectively acted as a dumb terminal showing an approximation of the game as it played out on the server. In a pure client/server model you run no game code locally, instead sending your inputs such as key presses, mouse movement, clicks to the server. In response the server updates the state of your character in the world and replies with a packet containing the state of your character and other players near you. All the client has to do is interpolate between these updates to provide the illusion of smooth movement and BAM you have a networked game. This was a great step forward. The quality of the game experience now depended on the connection between the client and the server instead of the most lagged peer in the game. It also became possible for players to come and go in the middle of the game, and the number of players increased as client/server reduced the bandwidth required on average per-player. But there were still problems with the pure client/server model: While I can remember and justify all of my decisions about networking from DOOM through Quake, the bottom line is that I was working with the wrong basic assumptions for doing a good internet game. My original design was targeted at &lt; 200ms connection latencies. People that have a digital connection to the internet through a good provider get a pretty good game experience. Unfortunately, 99% of the world gets on with a slip or ppp connection over a modem, often through a crappy overcrowded ISP. This gives 300+ ms latencies, minimum. Client. User&#39;s modem. ISP&#39;s modem. Server. ISP&#39;s modem. User&#39;s modem. Client. God, that sucks. Ok, I made a bad call. I have a T1 to my house, so I just wasn&#39;t familliar with PPP life. I&#39;m addressing it now. The problem was of course latency. What happened next would change the industry forever. Client-Side Prediction In the original Quake you felt the latency between your computer and the server. Press forward and you&rsquo;d wait however long it took for packets to travel to the server and back to you before you&rsquo;d actually start moving. Press fire and you wait for that same delay before shooting. If you&rsquo;ve played any modern FPS like Call of Duty: Modern Warfare, you know this is no longer what happens. So how exactly do modern FPS games remove the latency on your own actions in multiplayer? When writing about his plans for the soon to be released QuakeWorld, John Carmack said: I am now allowing the client to guess at the results of the users movement until the authoritative response from the server comes through. This is a biiiig architectural change. The client now needs to know about solidity of objects, friction, gravity, etc. I am sad to see the elegant client-as-terminal setup go away, but I am practical above idealistic. So now in order to remove the latency, the client runs more code than it previously did. It is no longer a dumb terminal sending inputs to the server and interpolating between state sent back. Instead it is able to predict the movement of your character locally and immediately in response to your input, running a subset of the game code for your player character on the client machine. Now as soon as you press forward, there is no wait for a round trip between client and server - your character start moving forward right away. The difficulty of this approach is not in the prediction, for the prediction works just as normal game code does - evolving the state of the game character forward in time according to the player&rsquo;s input. The difficulty is in applying the correction back from the server to resolve cases when the client and server disagree about where the player character should be and what it is doing. Now at this point you might wonder. Hey, if you are running code on the client - why not just make the client authoritative over their player character? The client could run the simulation code for their own character and simply tell the server where they are each time they send a packet. The problem with this is that if each player were able to simply tell the server &ldquo;here is my current position&rdquo; it would be trivially easy to hack the client such that a cheater could instantly dodge the RPG about to hit them, or teleport instantly behind you to shoot you in the back. So in FPS games it is absolutely necessary that the server is the authoritative over the state of each player character, in-spite of the fact that each player is locally predicting the motion of their own character to hide latency. As Tim Sweeney writes in The Unreal Networking Architecture: &ldquo;The Server Is The Man&rdquo;. Here is where it gets interesting. If the client and the server disagree, the client must accept the update for the position from the server, but due to latency between the client and server this correction is necessarily in the past. For example, if it takes 100ms from client to server and 100ms back, then any server correction for the player character position will appear to be 200ms in the past, relative to the time up to which the client has predicted their own movement. If the client were to simply apply this server correction update verbatim, it would yank the client back in time, completely undoing any client-side prediction. How then to solve this while still allowing the client to predict ahead? The solution is to keep a circular buffer of past character state and input for the local player on the client, then when the client receives a correction from the server, it first discards any buffered state older than the corrected state from the server, and replays the state starting from the corrected state back to the present &ldquo;predicted&rdquo; time on the client using player inputs stored in the circular buffer. In effect the client invisibly &ldquo;rewinds and replays&rdquo; the last n frames of local player character movement while holding the rest of the world fixed. This way the player appears to control their own character without any latency, and provided that the client and server character simulation code is reasonable, giving roughly exactly the same result for the same inputs on the client and server, it is rarely corrected. It is as Tim Sweeney describes: ... the best of both worlds: In all cases, the server remains completely authoritative. Nearly all the time, the client movement simulation exactly mirrors the client movement carried out by the server, so the client's position is seldom corrected. Only in the rare case, such as a player getting hit by a rocket, or bumping into an enemy, will the client's location need to be corrected. In other words, only when the player&rsquo;s character is affected by something external to the local player&rsquo;s input, which cannot possibly be predicted on the client, will the player&rsquo;s position need to be corrected. That and of course, if the player is attempting to cheat :) 译文译文出处 翻译：黄威（横写、意气风发） 审校：艾涛（轻描一个世界）介绍作为一名程序员，你是否曾想过多人游戏是如何运作的呢？从表面来看这是非常奇妙：两个或者更多的玩家通过网络能够拥有相同的游戏体验，就像他们确实存在于同一个虚拟世界一样。但是作为程序员，我们知道底层运行的情况与你看到的完全不同。事实证明，这完全是一种错觉，是一个精妙的戏法。你能感受到游戏中的玩家都处于同一个世界中，但其实这只是在各个时间点，你自己独有的视角与位置和其他玩家的视角与位置相似。对等同步起初，网络游戏形成一个对等的网络，在这个网络中每台电脑在一个完全连接的网状拓扑结构中互相交换信息。如今在RTS游戏（即时战略游戏）中你仍能够看到这一模型，有趣的是，因为某种原因，可能因为它是第一种网络连接方式——大多数人仍认为游戏网络是这样运作的。基本思想就是在处理数据时将游戏抽象化成一系列的数据改变与一组命令消息，每一个数据改变都决定了游戏状态的演变。例如：移动单位、攻击单位、建造建筑。所有的这一切都要求网络让每一位玩家的机器都从相同的初始状态开始，并且运行完全相同的命令，数据的改变也完全相同。当然，这只是一个过于简单并且忽略掉了许多微妙细节的解释，但这个解释向我们解释了RTS游戏网络工作的基本原理。你可以点击这里了解更多关于这个网络模型的细节。这看起来是如此简单而又巧妙，但是不幸的是这个模型有几个限制因素。首先，要想保证游戏状态完全确定是非常困难的；即每台机器都进行着相同的变动。比如说，一个单位可以在两台机器上走略微不同的道路，在一台机器上玩家更早进入战斗并反败为胜，而在另一台机器上玩家到达的更晚，然后，嗯，没有取得胜利。就像一只蝴蝶扇动了翅膀，然后在世界的另一边导致了飓风的出现，随着时间的过去，一个微小的区别会导致两边完全的不同步。另一个限制因素就是为了保证游戏在所有机器上表现同步，就有必要在游戏操作在设备上模拟之前进行等待，直到设备接收到了所有玩家对于那个变动的指令。这就意味着游戏中的每一个玩家的延迟都等于延迟最高玩家的延迟。RTS游戏通常代表性地通过立即提供音频反馈与（或是）播放过渡动画来掩盖这段延迟，但是最终真正影响游戏的操作要在这段延迟过去之后才能进行。最后一个限制因素就在于游戏的同步方式是通过发送改变当前状态的命令消息。为了让其正常工作就有必要让所有的玩家由同一初始状态开始游戏。通常来说，这就意味着每个玩家都要在开始游戏之前进入房间准备游戏，尽管支持让玩家随后加入游戏从技术上来说是可行的，但是由于在一场进行中的游戏中间捕获与传输一个完全确定的起始点的难度很大，所以这种情况并不常见。尽管有这些限制，这个模型还是很适合RTS游戏的，并且在现代的游戏中它仍然存在，例如“命令与征服”、“帝国时代”与“星际争霸”等。原因就是在RTS游戏中，游戏状态包含了成千上万的单位，并且通常游戏状态太大而不能在玩家之间交换。这些游戏别无选择，只能交换这些驱动着游戏状态改变的指令。但是对于其他类别的游戏，美工的状态已经改变了。所以对于确定的对等网络同步模型就讲到这里。现在让我们从Doom（毁灭战士）、Quake（雷神之锤）以及Unreal（魔域幻境）中看看动作类游戏的演变。客户端/服务器（C/S结构）在动作游戏的时代，对等同步的限制因素在Doom中表现得更加明显，尽管它在局域网中表现很好，但是在面对互联网中的普通用户时表现得很糟糕：“虽然可以通过调制解调器将两个运行DOOM的设备在互联网上连接在一起，最终游戏将会变得缓慢，延迟的情况在完全不能进行游戏（例如一个14.4Kbps的P2P连接）到略微可玩（例如一个28.8Kbps的调制解调器运行一个压缩驱动程序）之间不等。因为这些类型的连接只有边际效用，本文将只关注于网络连接。（faqs.org）”这个问题显然就是Doom本来就是只为局域网设计的，并且使用了前面描述的为RTS游戏制作的对等同步模型。每一个玩家输入的行为（关键按键等等）与其他人进行信息交换，只有在所有其他玩家的关键按键都被接收到之后，玩家才能够进行游戏画面的模拟。换句话说，在你能够操作、移动或是射击之前，你必须等待延迟最高的玩家进行连入。想想这上述的所谓“这些连接只有边际效用”将会导致的令人咬牙切齿的情况。现在的游戏局限于局域网游戏以及拥有良好连接条件的大学网络或是大公司的精英之间的游戏，为了改变这种情况，是时候改变这个模型了。这就是John Carmack 1996年在发布雷神之锤时所做的事情——他使用客户端/服务器（C/S结构）代替了对等同步模型（P2P）。现在，玩家们不再运行相同的游戏代码，直接地互相交换数据，如今每个玩家都是一个客户端，他们都与一台叫做“服务器”的电脑进行数据交换。现在的游戏不再有任何对于所有机器都要进行确定的要求，因为游戏实际上只存在于服务器上。每个客户端实际上都是作为哑终端，用来显示出一个游戏的近似情况，因为游戏实际上只在服务器上发生。在一个纯粹的客户端/服务器模型中你没有在本地运行游戏代码，而是将你的操作例如按键、鼠标移动、点击等发送到服务器。服务器响应并更新了虚拟世界中你的角色状态，然后将一个包含着你与你周围角色状态的数据包传回。所有客户端要做的事情就是在这些数据更新之间插入自己的数据，然后给你一种流畅移动的假象，然后，boom！你就有了一个联网的客户端/服务器游戏了。这是一个伟大的进步。游戏体验的质量现在取决于客户端与服务器之间的连接，而不是取决于游戏中延迟最高的玩家。这同时让玩家在游戏进行中的加入变成了可能，并且随着客户端/服务器结构对于每位玩家需要的平均带宽减少，游戏玩家也在逐渐增长。但是对于纯粹的客户端/服务器模型仍然存在一些问题。“尽管我能记得并整理出我从DOOM到Quake做出的所有关于网络的决定，结果就是尽管我为了做出一个好的网络游戏而努力着，但这些努力都是基于一个错误的基础假设。我原先的设计目标就是使延迟低于200ms。这样的话通过一个好的供应商数字连接到网络的人，就能有一个很好的游戏体验。很不幸，世界上百分之九十就都是通过调制解调器进行SLIP连接或是PPP连接，它们通常是通过一个糟糕拥挤的ISP（网络服务提供者）进行连接的。这就导致了300ms以上的延迟，并且这只是最低值。客户端，使用者的调制解调器，ISP的调制解调器，服务器，再回到ISP的调制解调器，使用者的调制解调器，最后再回到客户端。天呐，这真是糟透了！好吧，我做了一件错误的事。我在家里都使用T1载体进行联网，所以我对使用P2P的生活并不了解，我现在就解决这个问题。”问题当然就是延迟。接下来John在他发布QuakeWorld（雷神世界）时做的事情将永久改变这个行业。客户端预测在最初的雷神之锤中，你可以明显感受到你的电脑与服务器之间的延迟。在你向前点击之后，你需要等待数据包发送至服务器然后再传回到你的电脑，然后你才能够开始移动。点击开火，然后你在射击之前同样需要等待上述延迟。如果你玩过任何像《使命召唤4：现代战争》之类的现代FPS游戏，你就会知道这种情况现在已经不会再出现了。那么现代FPS游戏到底是如何做到在多人游戏中看似消除了你自己行为的延迟呢？这个问题在历史上分两个部分来解决。第一部分就是JohnCarmack为雷神世界开发的客户端移动预测，它后来被合并作为Tim Sweeney的魔域幻境网络模型的一部分。第二部分就是维尔福公司的Yahn Bernier为反恐精英开发的延迟补偿。在本节中，我们将主要讨论第一部分——如何隐藏玩家移动的延迟。当谈到他对于即将发布的雷神世界的计划时，JohnCarmack说：“我们现在允许客户端预测使用者行动的结果，直到服务器传来命令式回复。这是一个非常非常大的架构变化。客户端现在需要知道物体的硬度、摩擦力、重力之类的数据。对于简洁的客户端作为终端计划我们已经不再采用了，我对此表示遗憾，但我是一个实用主义者而不是一个理想主义者。”所以为了消除延迟，客户端需要比之前运行更多的代码。它现在不再是一个向服务器发送输入内容并在状态发回之前进行数据插入的哑终端，它现在能够在本地预测你的角色移动，并且对你的输入迅速做出反应，在客户端设备上为你的游戏角色运行一部分游戏代码。现在只要你向前点击，不需要再等待客户端与服务器之间的信息往返——你的角色立即开始向前移动。这种方法的难点不在于预测，因为预测就像是普通游戏代码做的那样——根据玩家的操作随时间发展游戏角色状态。难点就在于，当客户端和服务器对于游戏角色所处的位置及所做的事情有分歧时，客户端如何以服务器传来的信息为基础进行修正。对于这一点，你可能会想，嘿，如果你在客户端运行游戏代码——为什么不以客户端的情况作为游戏角色的标准呢。客户端可以为自己的角色运行仿真代码，并在每次发送数据包时告诉服务器现在的情况。那么问题就是，如果每个玩家都可以简单地告诉服务器“这就是我现在的情况”，那就非常容易黑进客户端进行作弊，例如作弊者可以瞬间躲开将要射向他们的子弹，或者立即传送到你身后从后方射击你。所以在FPS游戏中，尽管每个玩家在本地预测自己角色的运动，从表面上隐藏了延迟，但以服务器状态作为每个玩家角色状态的标准是绝对有必要的。就像Tim Sweeney在UE网络架构里写到的：“服务器才是大哥！”这就是有趣的地方。如果客户端和服务器信息不一致，客户端就必须接受来自服务器的位置更新，但是由于客户端与服务器之间的延迟，这个对过去修正是必然的。举个例子，如果从客户端到服务器要消耗100ms，再经过100ms回来，那么任何服务器对于玩家角色位置的修正就会有200ms的延迟，这个时间是相对于客户端开始预测自己移动的时间。如果客户端连续接收服务器的修正更新，这就会及时拉回客户端，这就会导致客户端完全不能做任何客户端预测。怎么解决这个问题的同时仍然允许客户端进行超前预测呢？解决方法就是在客户端为过去的角色状态以及本地玩家的输入创建一个循环缓冲区，然后当客户端收到一个来自服务器的修正，（首先它丢弃比服务器的修正状态更早的缓冲状态）依据玩家储存在循环缓冲区的输入对由上一次的正确状态开始到现在预测时间的状态进行重放。实际上客户端在等待接下来的情况匹配完成之前悄悄地“倒放与重放”当地的玩家角色移动的最后几帧。这个方法可以让玩家看似无延迟地控制他们的角色，并且如果客户端与服务器的角色模拟代码一致的话——由于在客户端与服务器上相同的输入可以准确给出相同的结果——这就很少出现要修正的情况。这就像是Tim Sweeney描述的那样：“……最好的两个世界：在所有情况下，服务器都是绝对权威。在几乎任何时间内，客户端的移动模拟都与服务器计算出的客户端移动完全相同，所以客户端的情况很少需要修正。只有在极少的情况，例如玩家被一枚火箭击中，或是撞上一名敌人，客户端的情况将被修正。”换句话说，只有当玩家的角色被一些外部事情影响到了玩家的输入，并且这些不能被客户端所预测时，玩家的情况需要被修正。当然，如果玩家试图作弊时亦然。 【版权声明】原文作者未做权利声明，视为共享知识产权进入公共领域，自动获得授权。]]></content>
      <categories>
        <category>GS</category>
      </categories>
      <tags>
        <tag>GafferOnGames</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[游戏网络开发四之基于UDP的可靠性与排序和避免拥堵]]></title>
    <url>%2Fblog%2F2016%2F11%2F17%2F%E6%B8%B8%E6%88%8F%E7%BD%91%E7%BB%9C%E5%BC%80%E5%8F%91%E5%9B%9B%E4%B9%8B%E5%9F%BA%E4%BA%8EUDP%E7%9A%84%E5%8F%AF%E9%9D%A0%E6%80%A7%E4%B8%8E%E6%8E%92%E5%BA%8F%E5%92%8C%E9%81%BF%E5%85%8D%E6%8B%A5%E5%A0%B5%2F</url>
    <content type="text"><![CDATA[原文原文出处 Introduction Hi, I&rsquo;m Glenn Fiedler and welcome to Networking for Game Programmers. In the previous article, we added our own concept of virtual connection on top of UDP. In this article we’re going to add reliability, ordering and congestion avoidance to our virtual UDP connection. The Problem with TCP Those of you familiar with TCP know that it already has its own concept of connection, reliability-ordering and congestion avoidance, so why are we rewriting our own mini version of TCP on top of UDP? The issue is that multiplayer action games rely on a steady stream of packets sent at rates of 10 to 30 packets per second, and for the most part, the data contained is these packets is so time sensitive that only the most recent data is useful. This includes data such as player inputs, the position, orientation and velocity of each player character, and the state of physics objects in the world. The problem with TCP is that it abstracts data delivery as a reliable ordered stream. Because of this, if a packet is lost, TCP has to stop and wait for that packet to be resent. This interrupts the steady stream of packets because more recent packets must wait in a queue until the resent packet arrives, so packets are received in the same order they were sent. What we need is a different type of reliability. Instead of having all data treated as a reliable ordered stream, we want to send packets at a steady rate and get notified when packets are received by the other computer. This allows time sensitive data to get through without waiting for resent packets, while letting us make our own decision about how to handle packet loss at the application level. It is not possible to implement a reliability system with these properties using TCP, so we have no choice but to roll our own reliability on top of UDP. Sequence Numbers The goal of our reliability system is simple: we want to know which packets arrive at the other side of the connection. First we need a way to identify packets. What if we had added the concept of a &ldquo;packet id&rdquo;? Let&rsquo;s make it an integer value. We could start this at zero then with each packet we send, increase the number by one. The first packet we send would be packet 0, and the 100th packet sent is packet 99. This is actually quite a common technique. It&rsquo;s even used in TCP! These packet ids are called sequence numbers. While we’re not going to implement reliability exactly as TCP does, it makes sense to use the same terminology, so we’ll call them sequence numbers from now on. Since UDP does not guarantee the order of packets, the 100th packet received is not necessarily the 100th packet sent. It follows that we need to insert the sequence number somewhere in the packet, so that the computer at the other side of the connection knows which packet it is. We already have a simple packet header for the virtual connection from the previous article, so we&rsquo;ll just add the sequence number in the header like this: [uint protocol id] [uint sequence] (packet data...) Now when the other computer receives a packet it knows its sequence number according to the computer that sent it. Acks Now that we can identify packets using sequence numbers, the next step is to let the other side of the connection know which packets we receive. Logically this is quite simple, we just need to take note of the sequence number of each packet we receive, and send those sequence numbers back to the computer that sent them. Because we are sending packets continuously between both machines, we can just add the ack to the packet header, just like we did with the sequence number: [uint protocol id] [uint sequence] [uint ack] (packet data...) Our general approach is as follows: Each time we send a packet we increase the local sequence number When we receieve a packet, we check the sequence number of the packet against the sequence number of the most recently received packet, called the remote sequence number. If the packet is more recent, we update the remote sequence to be equal to the sequence number of the packet. When we compose packet headers, the local sequence becomes the sequence number of the packet, and the remote sequence becomes the ack. This simple ack system works provided that one packet comes in for each packet we send out. But what if packets clump up such that two packets arrive before we send a packet? We only have space for one ack per-packet, so what do we do? Now consider the case where one side of the connection is sending packets at a faster rate. If the client sends 30 packets per-second, and the server only sends 10 packets per-second, we need at least 3 acks included in each packet sent from the server. Let&rsquo;s make it even more complex! What if the packet containing the ack is lost? The computer that sent the packet would think the packet got lost but it was actually received! It seems like we need to make our reliability system&hellip; more reliable! Reliable Acks Here is where we diverge significantly from TCP. What TCP does is maintain a sliding window where the ack sent is the sequence number of the next packet it expects to receive, in order. If TCP does not receive an ack for a given packet, it stops and resends a packet with that sequence number again. This is exactly the behavior we want to avoid! In our reliability system, we never resend a packet with a given sequence number. We sequence n exactly once, then we send n+1, n+2 and so on. We never stop and resend packet n if it was lost, we leave it up to the application to compose a new packet containing the data that was lost, if necessary, and this packet gets sent with a new sequence number. Because we&rsquo;re doing things differently to TCP, its now possible to have holes in the set of packets we ack, so it is no longer sufficient to just state the sequence number of the most recent packet we have received. We need to include multiple acks per-packet. How many acks do we need? As mentioned previously we have the case where one side of the connection sends packets faster than the other. Let&rsquo;s assume that the worst case is one side sending no less than 10 packets per-second, while the other sends no more than 30. In this case, the average number of acks we&rsquo;ll need per-packet is 3, but if packets clump up a bit, we would need more. Let&rsquo;s say 6-10 worst case. What about acks that don&rsquo;t get through because the packet containing the ack is lost? To solve this, we&rsquo;re going to use a classic networking strategy of using redundancy to defeat packet loss! Let&rsquo;s include 33 acks per-packet, and this isn&rsquo;t just going to be up to 33, but always 33. So for any given ack we redundantly send it up to 32 additional times, just in case one packet with the ack doesn&rsquo;t get through! But how can we possibly fit 33 acks in a packet? At 4 bytes per-ack thats 132 bytes! The trick is to represent the 32 previous acks before &ldquo;ack&rdquo; using a bitfield: [uint protocol id] [uint sequence] [uint ack] [uint ack bitfield] &lt;em&gt;(packet data...)&lt;/em&gt; We define &ldquo;ack bitfield&rdquo; such that each bit corresponds to acks of the 32 sequence numbers before &ldquo;ack&rdquo;. So let&rsquo;s say &ldquo;ack&rdquo; is 100. If the first bit of &ldquo;ack bitfield&rdquo; is set, then the packet also includes an ack for packet 99. If the second bit is set, then packet 98 is acked. This goes all the way down to the 32nd bit for packet 68. Our adjusted algorithm looks like this: Each time we send a packet we increase the local sequence number When we receive a packet, we check the sequence number of the packet against the remote sequence number. If the packet sequence is more recent, we update the remote sequence number. When we compose packet headers, the local sequence becomes the sequence number of the packet, and the remote sequence becomes the ack. The ack bitfield is calculated by looking into a queue of up to 33 packets, containing sequence numbers in the range [remote sequence - 32, remote sequence]. We set bit n (in [1,32]) in ack bits to 1 if the sequence number remote sequence - n is in the received queue. Additionally, when a packet is received, ack bitfield is scanned and if bit n is set, then we acknowledge sequence number packet sequence - n, if it has not been acked already. With this improved algorithm, you would have to lose 100% of packets for more than a second to stop an ack getting through. And of course, it easily handles different send rates and clumped up packet receives. Detecting Lost Packets Now that we know what packets are received by the other side of the connection, how do we detect packet loss? The trick here is to flip it around and say that if you don&rsquo;t get an ack for a packet within a certain amount of time, then we consider that packet lost. Given that we are sending at no more than 30 packets per second, and we are redundantly sending acks roughly 30 times, if you don&rsquo;t get an ack for a packet within one second, it is very likely that packet was lost. So we are playing a bit of a trick here, while we can know 100% for sure which packets get through, but we can only be reasonably certain of the set of packets that didn&rsquo;t arrive. The implication of this is that any data which you resend using this reliability technique needs to have its own message id so that if you receive it multiple times, you can discard it. This can be done at the application level. Handling Sequence Number Wrap-Around No discussion of sequence numbers and acks would be complete without coverage of sequence number wrap around! Sequence numbers and acks are 32 bit unsigned integers, so they can represent numbers in the range [0,4294967295]. Thats a very high number! So high that if you sent 30 packets per-second, it would take over four and a half years for the sequence number to wrap back around to zero. But perhaps you want to save some bandwidth so you shorten your sequence numbers and acks to 16 bit integers. You save 4 bytes per-packet, but now they wrap around in only half an hour. So how do we handle this wrap around case? The trick is to realize that if the current sequence number is already very high, and the next sequence number that comes in is very low, then you must have wrapped around. So even though the new sequence number is numerically lower than the current sequence value, it actually represents a more recent packet. For example, let&rsquo;s say we encoded sequence numbers in one byte (not recommended btw. :)), then they would wrap around after 255 like this: ... 252, 253, 254, 255, 0, 1, 2, 3, ... To handle this case we need a new function that is aware of the fact that sequence numbers wrap around to zero after 255, so that 0, 1, 2, 3 are considered more recent than 255. Otherwise, our reliability system stops working after you receive packet 255. Here&rsquo;s a function for 16 bit sequence numbers: inline bool sequence_greater_than( uint16_t s1, uint16_t s2 ) { return ( ( s1 &gt; s2 ) &amp;&amp; ( s1 - s2 &lt;= 32768 ) ) || ( ( s1 &lt; s2 ) &amp;&amp; ( s2 - s1 &gt; 32768 ) ); } This function works by comparing the two numbers and their difference. If their difference is less than 1&frasl;2 the maximum sequence number value, then they must be close together - so we just check if one is greater than the other, as usual. However, if they are far apart, their difference will be greater than 1&frasl;2 the max sequence, then we paradoxically consider the sequence number more recent if it is less than the current sequence number. This last bit is what handles the wrap around of sequence numbers transparently, so 0,1,2 are considered more recent than 255. Make sure you include this in any sequence number processing you do. Congestion Avoidance While we have solved reliability, there is still the question of congestion avoidance. TCP provides congestion avoidance as part of the packet when you get TCP reliability, but UDP has no congestion avoidance whatsoever! If we just send packets without some sort of flow control, we risk flooding the connection and inducing severe latency (2 seconds plus!) as routers between us and the other computer become congested and buffer up packets. This happens because routers try very hard to deliver all the packets we send, and therefore tend to buffer up packets in a queue before they consider dropping them. While it would be nice if we could tell the routers that our packets are time sensitive and should be dropped instead of buffered if the router is overloaded, we can&rsquo;t really do this without rewriting the software for all routers in the world. Instead, we need to focus on what we can actually do which is to avoid flooding the connection in the first place. We try to avoid sending too much bandwidth in the first place, and then if we detect congestion, we attempt to back off and send even less. The way to do this is to implement our own basic congestion avoidance algorithm. And I stress basic! Just like reliability, we have no hope of coming up with something as general and robust as TCP&rsquo;s implementation on the first try, so let&rsquo;s keep it as simple as possible. Measuring Round Trip Time Since the whole point of congestion avoidance is to avoid flooding the connection and increasing round trip time (RTT), it makes sense that the most important metric as to whether or not we are flooding our connection is the RTT itself. We need a way to measure the RTT of our connection. Here is the basic technique: For each packet we send, we add an entry to a queue containing the sequence number of the packet and the time it was sent. Each time we receive an ack, we look up this entry and note the difference in local time between the time we receive the ack, and the time we sent the packet. This is the RTT time for that packet. Because the arrival of packets varies with network jitter, we need to smooth this value to provide something meaningful, so each time we obtain a new RTT we move a percentage of the distance between our current RTT and the packet RTT. 10% seems to work well for me in practice. This is called an exponentially smoothed moving average, and it has the effect of smoothing out noise in the RTT with a low pass filter. To ensure that the sent queue doesn&rsquo;t grow forever, we discard packets once they have exceeded some maximum expected RTT. As discussed in the previous section on reliability, it is exceptionally likely that any packet not acked within a second was lost, so one second is a good value for this maximum RTT. Now that we have RTT, we can use it as a metric to drive our congestion avoidance. If RTT gets too large, we send data less frequently, if its within acceptable ranges, we can try sending data more frequently. Simple Binary Congestion Avoidance As discussed before, let&rsquo;s not get greedy, we&rsquo;ll implement a very basic congestion avoidance. This congestion avoidance has two modes. Good and bad. I call it simple binary congestion avoidance. Let&rsquo;s assume you send packets of a certain size, say 256 bytes. You would like to send these packets 30 times a second, but if conditions are bad, you can drop down to 10 times a second. So 256 byte packets 30 times a second is around 64kbits/sec, and 10 times a second is roughly 20kbit/sec. There isn&rsquo;t a broadband network connection in the world that can&rsquo;t handle at least 20kbit/sec, so we&rsquo;ll move forward with this assumption. Unlike TCP which is entirely general for any device with any amount of send/recv bandwidth, we&rsquo;re going to assume a minimum supported bandwidth for devices involved in our connections. So the basic idea is this. When network conditions are &ldquo;good&rdquo; we send 30 packets per-second, and when network conditions are &ldquo;bad&rdquo; we drop to 10 packets per-second. Of course, you can define &ldquo;good&rdquo; and &ldquo;bad&rdquo; however you like, but I&rsquo;ve gotten good results considering only RTT. For example if RTT exceeds some threshold (say 250ms) then you know you are probably flooding the connection. Of course, this assumes that nobody would normally exceed 250ms under non-flooding conditions, which is reasonable given our broadband requirement. How do you switch between good and bad? The algorithm I like to use operates as follows: If you are currently in good mode, and conditions become bad, immediately drop to bad mode If you are in bad mode, and conditions have been good for a specific length of time &rsquo;t&rsquo;, then return to good mode To avoid rapid toggling between good and bad mode, if you drop from good mode to bad in under 10 seconds, double the amount of time &rsquo;t&rsquo; before bad mode goes back to good. Clamp this at some maximum, say 60 seconds. To avoid punishing good connections when they have short periods of bad behavior, for each 10 seconds the connection is in good mode, halve the time &rsquo;t&rsquo; before bad mode goes back to good. Clamp this at some minimum like 1 second. With this algorithm you will rapidly respond to bad conditions and drop your send rate to 10 packets per-second, avoiding flooding of the connection. You&rsquo;ll also conservatively try out good mode, and persist sending packets at a higher rate of 30 packets per-second, while network conditions are good. Of course, you can implement much more sophisticated algorithms. Packet loss % can be taken into account as a metric, even the amount of network jitter (time variance in packet acks), not just RTT. You can also get much more greedy with congestion avoidance, and attempt to discover when you can send data at a much higher bandwidth (eg. LAN), but you have to be very careful! With increased greediness comes more risk that you&rsquo;ll flood the connection. Conclusion Our new reliability system let&rsquo;s us send a steady stream of packets and notifies us which packets are received. From this we can infer lost packets, and resend data that didn&rsquo;t get through if necessary. We also have a simple congestion avoidance system that drops from 30 packets per-second to 10 times a second so we don&rsquo;t flood the connection. 译文译文出处 翻译：艾涛（轻描一个世界） 审校：黄威（横写丶意气风发）简介嗨，我是格伦-菲德勒，欢迎来到我的游戏程序员网络设计文章系列的第四篇。在之前的文章里，我们将我们的虚拟连接的概念加入到UDP之上。现在我们将要给我们的虚拟UDP连接增加可靠性，排序和避免拥堵。这是迄今为止底层游戏网络设计中最复杂的一面，因此这将是一篇极其热情的文章，跟上我启程出发！ TCP的问题熟悉TCP的你们知道它已经有了自己关于连接、可靠性、排序和避免拥堵的概念，那么为什么我们还要重写我们自己的迷你版本的基于UDP的TCP呢？问题是多人动作游戏依靠于一个稳定的每秒发送10到30包的数据包流，而且在大多数情况下，这些数据包中包含的数据对时间是如此敏感以至于只有最新的数据才是有用的。这包括玩家的输入，位置方向和每个玩家角色的速度以及游戏世界中物理对象的状态等数据。TCP的问题是它提取的是以可靠有序的数据流发送的数据。正因为如此，如果一个数据包丢失了，TCP不得不停止以等待那个数据包重新发送，这打断了这个稳定的数据包流因为更多的最新的数据包在重新发送的数据包到达之前必须在队列中等待，所以数据包必须有序地提供。我们需要的是一种不同类型的可靠性。我们想要以一个稳定的速度发送数据包而且当数据被其他电脑接收到时我们会得到通知，而不是让所有的数据用一个可靠有效的数据流处理。这样的方法使得那些对时间敏感的数据能够不用等待重新发送的数据包就通过，而让我们自己拿主意怎么在应用层级去处理丢包。具有TCP这些特性的系统是不可能实现可靠性的，因此我们别无选择只能在UDP的基础上自行努力。不幸的是，可靠性并不是唯一一个我们必须重写的东西，这是因为TCP也提供避免拥堵功能，这样它就能够动态地衡量数据发送速率以来适应网络连接的性能。例如TCP在28.8k的调制调解器上会比在T1线路上发送更少的数据，而且它在不用事先知道这是什么类型的网络连接的情况下就能这么做！ 序列号现在回到可靠性！我们可靠性系统的目标很简单：我们想要知道哪些数据包到了网络连接的另一端。首先我们得鉴别数据包。如果我们添加一个“数据包id”的概念会怎么样？让我们先给id赋一个整数值。我们能够从零开始，然后随着我们每发送一个数据包，增加一个数值。我们发送的第一个数据包就是“包0”，发送的第100个数据包就是“包99”。这实际上是一个相当普遍的技术。甚至于在TCP中也得到了应用！这些数据包id叫做序列号，然而我们并不打算像TCP那样去做来实现可靠性，使用相同的术语是有意义的，因此从现在起我们还将称之为序列号。因为UDP并不能保证数据包的顺序，所以第100个收到的数据包并不一定是第100个发出的数据包。接下来我们需要在数据包中插入序列号这样网络连接另一端电脑便能够知道是哪个数据包。我们在前一篇文章中已经有了一个简单的关于虚拟网络连接的数据头，因此我们将只需要像这样在数据头中插入序列号： [uint protocol id] [uint sequence](packet data…)现在当其他电脑收到一个数据包时通过发送数据包的电脑它就能知道数据包的序列号啦。 应答系统既然我们已经能够使用序列号来鉴别数据包，下一步就该是让网络连接的另一端知道我们收到了哪个包了。逻辑上来说这是非常简单的，我们只需要记录我们收到的每个包的序列号，然后把那些序列号发回发送他们的电脑即可。因为我们是在两个机器间相互发送数据包，我们只能在数据包头添加上确认字符，就像我们加上序列号一样： [uint protocol id] [uint sequence] [uint ack](packet data…)我们的一般方法如下：每次我们发送一个数据包我们就增加本地序列号。当我们接收一个数据包时，我们将这个数据包的序列号与最近收到的数据包的序列号(称之为远程序列号)进行核对。如果这个包时间更近，我们就更新远程序列号使之等于这个数据包的序列号。当我们编写数据包头时，本地序列号就变成了数据包的序列号，而远程序列号则变成确认字符。这个简单的应答系统工作条件是每当我们发出一个数据包就会接收到一个数据包。但如果数据包一起发送这样在我们发送一个数据包之前有两个数据包到达该怎么办呢？我们每个数据包只留了一个确认字符的位置，那我们该怎么处理呢?现在考虑网络连接中的一端用更快的速率发送数据包这种情况。如果客户端每秒发送30个数据包，而服务器每秒只发送10个数据包，这样从服务器发出的每个数据包我们至少需要3个确认字符。让我们想得更复杂点！如果数据包留下来了而确认字符丢失了会怎么样？这样发送这个数据包的电脑会认为这个数据包已经丢失了而实际上它已经被收到了！貌似我们需要让我们的可靠性系统……更加可靠一点！ 可靠的应答系统这就是我们偏离TCP的地方。TCP的做法是在确认字符发送的地方给下一个按顺序预期该收到的数据包序列号的位置维持一个移动窗口。如果TCP对于一个已经发出的数据包没有收到确认字符，它将暂停并重新发送那个对应序列号的数据包。这正是我们想要避免的做法！因此在我们的可靠性系统里，我们从不为一个已经发出的序列号重新发送数据包，我们精确地只排序一次n，然后我们发送n+1，n+2，依次类推。如果数据包n丢失了我们也从不暂停重新发送它，而是把它留给应用程序来编写一个包含丢失数据的新的数据包，必要的话，这个包还会用一个新的序列号发送。因为我们工作的方式与TCP不同，它的做法现在可能在我们数据包的确定字符设置中有了个洞，因此现在仅仅陈述最近的数据包的序列号已经远远不够了。我们需要在每个数据包中包含多个确认字符。那我们需要多少确认字符呢?正如之前提到网络连接的一端发包速率比另一端快的情况，让我们假定最糟的情况是一端每秒钟发送不少于10个数据包，而另一端每秒钟发送不多于30个数据包。这种情况下，我们每个数据包需要的平均确认字符数是3个，但是如果数据包发送密集点，我们将需要更多。让我们说6-10个最差的情况。如果因为包含确认字符的数据包丢失而导致确认字符并没有到达怎么办?为了解决这个问题，我们将要使用一种经典的使用冗余码的网络设计策略来处理数据包丢失的情况！让我们在每个数据包中容纳33个确认字符，而且这不仅是他将要达到33个，而是一直是33个。因此对于每一个发出的确认字符我们多余地把它额外多发送了多达32次，仅仅是以防某个包含确认字符的数据包不能通过！但是我们怎么可能在一个数据包里配置33个确认字符呢？每个确认字符4字节那就是132字节了！窍门是在“相应确认字符”之前使用一段位域来代表32个之前的确认字符，就像这样： [uint protocol id] [uint sequence] [uint ack] [uint ack bitfield](packet data…)我们这样规定“位域”中每一位对应“相应确认字符”之前的32个确认字符。因此让我们说“相应确认字符”是100。如果位域的第一位设置好了，那么这个数据包也包含包99的一个确认字符。如果第二位设置好了，那么它也包含包98的一个确认字符。这样一路下来就到了包68的第32位。我们调整过的算法看起来就像这样:每次我们发送一个数据包我们就增加本地序列号。当我们接收一个数据包时，我们将这个数据包的序列号与最近收到的数据包的序列号(称之为远程序列号)进行核对。如果这个包是更新的，我们就更新远程序列号使之等于数据包的序列号。当我们编写数据包头时，本地序列号就变成了数据包的序列号，而远程序列号则变成确认字符。 计算确认字符位域是通过寻找一个多达33个数据包的队列，其中包括在[远程序列号-32，远程序列号]范围内的序列号。如果序列号“远程序列号-n”正在接收队列中那就把确认字符位域中的位n（在[1，32]范围内）设置为位1。此外，当一个数据包被接收了，确认字符位域也被扫描了，如果位n设置好了，那么即使它还没有被应答，我们也认可序列号“远程序列号-n”。利用这个改善过的算法，你将可能不得不在不止一秒内丢掉100%的数据包而不是让一个数据包停止通过。当然，它能够轻松地处理不同的发包速率和接受一起发送的数据包。 检测丢包既然我们知道网络连接另一端接受的是哪些数据包，那么我们该怎么检测数据包的丢失呢?这次的窍门是反过来想，如果你在一定时间内还没有收到某个数据包的应答，那么我们可以考虑说那个数据包已经丢失了。考虑到我们正在以每秒不超过30包的速率发送数据包，而且我们正在多余地发送数据包大概三十次。如果你在一秒内没有收到某个数据包的确认字符，那很有可能就是这个数据包已经丢失了。因此我们在这儿用了一些小窍门，尽管我们能100%确定哪个数据包通过了，但是我们只能适度地确定那些没有到达的数据包。这种情况的复杂性在于任何你重新发送的使用了这种可靠性方法的数据需要有它自己的信息id，这样的话在你多次收到它的时候你可以放弃它。这在应用层级是能够做到的。应对环绕式处理的序列号如果序列号没有环绕式处理覆盖，那么对于序列号和确认字符的讨论是不完整的！序列号和确认字符都是32比特的无符号整数，因此它们能够代表在范围[0，4294967295]内的数字。那是一个非常大的数字！那么大以至于如果你每秒发送三十个数据包也将要花费四年半来把这个序列号环绕式处理回零。但是可能你想要节省一些带宽这样你将你的序列号和确认字符缩减到到16比特整数。你每个数据包节省了4个字节，但现在他们只需要在仅仅半个小时内即可完成环绕式处理！所以我们该怎么应对这种环绕式处理的情况呢?诀窍是要认识到如果当前序列号已经非常高了，而且下一个到达的序列号很低，那么你就必须进行环绕式处理。那么即使新的序列号数值上比当前序列号值更低它也能实际代表一个更新的数据包。举个例子，让我们假设我们用一个字节编码序列号（顺便说一下并不推荐这样做）。 :))， 之后他们就会在255后面进行环绕式处理，就像这样: … 252, 253, 254, 255, 0, 1, 2, 3,…为了解决这种情况我们需要一个能够意识到在255之后需要环绕式处理回零这样一个事实的新功能，这样0，1，2，3就会被认为比255更新。否则，我们的可靠性系统就会在你收到包255后停止工作。这就是那个新功能：boolsequence_more_recent( unsigned int s1, unsigned int s2, unsigned int max ){ return ( s1 &gt; s2 ) &amp;&amp; ( s1 - s2 &lt;= max/2 ) || ( s2 &gt; s1 ) &amp;&amp; ( s2 - s1 &gt; max/2 );}这个功能通过比较两个数字和他们的不同来工作。如果它们之间的差异少于1/2的最大序列号值，那么它们必须靠在一起– 因此我们只需要照常检查某个序列号是否比另一个大。然而，如果它们相差很多，它们之间的差异将会比1/2的最大序列号值大，那么如果它比当前序列号小我们反而认为这个序列号是更新的。这最后一点是显然需要环绕式处理序列号的地方，那么0，1，2就会被认为比255更新。多么简洁而巧妙！一定要确保你在你所做的任何序列号处理当中包含了这一步！ 避免拥堵当你已经解决了可靠性的问题的时候，还有避免拥堵的问题。当你获得TCP的可靠性的时候TCP已经提供了避免拥堵的功能作为数据包的一部分，但是UDP无论怎样都不会有避免拥堵！如果我们仅仅发送数据包而没有某种流量控制，我们正在冒险占满网络连接而且会引起严重的延迟（2秒以上！），正如我们和另外一台电脑之间的路由器会超负荷而缓冲数据包。这个发生是因为路由器很努力地想要尝试传送我们发送的所有数据包，因此在它们考虑丢弃数据包之前会在队列中缓冲数据包。然而如果我们能告诉路由器我们的数据包是时间敏感的而且如果路由器超载的话这些数据包应该丢弃而不是缓冲这样会很棒的，但只有我们重写世界上所有路由器的软件才能做到这一点！那么我们反而需要把重点放在我们实际上能做的是避免占满首位网络连接。做到这个的方法是实施我们自己的基础避免拥堵算法。我强调基础！就像可靠性，我们并不寄希望于像TCP第一次尝试应用那样普通而粗暴地想出某些东西，那么让我们让它尽可能简单吧。 衡量往返时间因为所有避免拥堵的要点就是避免占满网络连接和避免增加往返时间（RTT），关于我们是不是占满网络的最重要的衡量标准是RTT它本身的观点是有道理的。我们需要一种方法来衡量我们网络连接的RTT。这是基础的技巧：对我们发送的每个数据包，我们对数据包队列中包含的序列号和他们发送的时间添加一个登记。当我们收到一个应答时，我们找到这个登记, 然后记录我们收到这个应答的时间t1与我们发送数据包的时间的t2的差值(都基于本地时间来计算)。这就是是这个数据包的RTT时间。因为数据包的到达因网络波动而不同，我们需要缓和这个值来提供某些有意义的东西，这样每次我们获得一个新的RTT我们就移动一个我们当前的RTT和数据包的RTT之间距离的百分比。10%在实践中看起来效果很好。这就叫做一个指数级平滑移动平均值，而且它在用一个低通滤波器的情况下能有效地平滑RTT中的杂音。为了确保发送队列永不增长，一旦超过某些最大预期RTT值我们就丢弃数据包。正如上一节关于可靠性讨论过的，任何在一秒内未应答的数据包都极有可能丢失了，那么对于最大RTT来说，一秒是个很棒的值。既然我们有RTT，我们能把它作为一个衡量标准来推动我们的避免拥堵功能。如果RTT变得太大了，我们更缓慢地发送数据，如果它的值低于可接受范围，我们能努力更频繁地发送数据。 简单的好坏机制避免拥堵正如之前讨论的，我们不要那么贪心，我们将要执行一个非常基础的避免拥堵机制。这个避免拥堵机制有两种模式。好和坏。我把它叫做简单的二进制避免拥堵。让我们假设你在发送一个确定大小的数据包，就假设256字节吧。你想要每秒发送这些数据包30次，但是如果网络条件差，你可以削减为每秒10次。那么30次256字节的数据包的速率大概是64kbits/sec，每秒10次的话大概20kbits/sec。世界上没有一个宽带连接不能处理至少20kbits/sec的速率，所以我们在这样的假定下继续前进。不像TCP这样对有任何数量的发送/接受带宽的任何设备都完全通用，我们将假设一个设备的最小支持带宽来参与我们的网络连接。所以基础想法就是这样了。当网络条件好的时候我们每秒发送30个数据包，当网络条件差的时候我们降至每秒10个数据包。当然，你能随你喜爱定义好和坏，但是仅考虑RTT的时候我已经得到了好的成效。举个例子，如果RTT超过某些极限值（假设250ms）那你就知道你可能已经正占满了网络连接。当然，这里假设一般没人在非占满网络条件下超过250ms，考虑到我们的宽带要求这是合理的。。好和坏之间你会怎么转换？我喜欢用下列操作的算法:如果你当前在好模式下，而网络条件突然变坏，立即降至坏模式。如果你正在坏模式下，而且网络条件已经好了一段特定时长”t”，那么回到好模式。为了避免好模式和坏模式之间的快速切换，如果你从好模式降至坏模式持续10秒钟以内，从坏模式回到好模式之前的时间是”t”的两倍。在某些最大值中固定这个时间值，假设60秒。为了避免打击良好的网络连接，当它们有一小段时期的差连接时，每过10秒连接就处于好模式，把坏模式回到好模式之前的时间“t”减半。在某些最小值中固定这个时间值，例如1秒。利用这个算法，你将对差网络连接迅速反应然后降低你的发送速率至每秒10个数据包，避免占满网络。在网络条件好时，你也将谨慎地尝试好模式，坚持以更高的每秒发送30个数据包的速率发送数据包。当然，你也能实施复杂得多的算法，丢包率百分比甚至是网络波动（数据包确认字符的时间差异）都可以考虑作为一个衡量标准，而不仅仅是RTT。对于避免拥堵你还可以更贪心点，并尝试发现什么时候你能以一个更高的带宽（例如LAN）发送数据，但是你必须非常小心！随着贪婪心的增加你占满网络连接的风险也在增大！ 结语我们全新的可靠性系统让我们稳定流畅发送数据包，而且能通知我们收到了什么数据包。从这我们能推断出丢失的数据包，必要的话重新发送没有通过的数据。基于此我们有了能够取决于网络条件在每秒10次和每秒30次发包速率间轮流切换的一个简单的避免拥堵系统，因此我们不会占满网络连接。还有很多实施细节因为太具体而不能在这篇文章一一提到，所以务必确保你检查示例源代码来看是否它都被实施了。这就是关于可靠性，排序和避免拥堵的一切了，或许是低层次网络设计中最复杂的一面了。 【版权声明】原文作者未做权利声明，视为共享知识产权进入公共领域，自动获得授权； 源码下载因 Gaffer On Games 的源码原下载地址失效, 所以特地补上. 请点击]]></content>
      <categories>
        <category>GS</category>
      </categories>
      <tags>
        <tag>GafferOnGames</tag>
        <tag>UDP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[游戏网络开发三之基于UDP的虚拟连接]]></title>
    <url>%2Fblog%2F2016%2F11%2F16%2Fvirtual_connection_over_udp%2F</url>
    <content type="text"><![CDATA[原文原文出处 Introduction Hi, I&rsquo;m Glenn Fiedler and welcome to Networking for Game Programmers. In the previous article we sent and received packets over UDP. Since UDP is connectionless, one UDP socket can be used to exchange packets with any number of different computers. In multiplayer games however, we usually only want to exchange packets between a small set of connected computers. As the first step towards a general connection system, we&rsquo;ll start with the simplest case possible: creating a virtual connection between two computers on top of UDP. But first, we&rsquo;re going to dig in a bit deeper about how the Internet really works! The Internet NOT a series of tubes In 2006, Senator Ted Stevens made internet history with his famous speech on the net neutrality act: "The internet is not something that you just dump something on. It's not a big truck. It's a series of tubes" When I first started using the Internet, I was just like Ted. Sitting in the computer lab in University of Sydney in 1995, I was &ldquo;surfing the web&rdquo; with this new thing called Netscape Navigator, and I had absolutely no idea what was going on. You see, I thought each time you connected to a website there was some actual connection going on, like a telephone line. I wondered, how much does it cost each time I connect to a new website? 30 cents? A dollar? Was somebody from the university going to tap me on the shoulder and ask me to pay the long distance charges? :) Of course, this all seems silly now. There is no switchboard somewhere that directly connects you via a physical phone line to the other computer you want to talk to, let alone a series of pneumatic tubes like Sen. Stevens would have you believe. No Direct Connections Instead your data is sent over Internet Protocol (IP) via packets that hop from computer to computer. A packet may pass through several computers before it reaches its destination. You cannot know the exact set of computers in advance, as it changes dynamically depending on how the network decides to route packets. You could even send two packets A and B to the same address, and they may take different routes. On unix-like systems can inspect the route that packets take by calling &ldquo;traceroute&rdquo; and passing in a destination hostname or IP address. On windows, replace &ldquo;traceroute&rdquo; with &ldquo;tracert&rdquo; to get it to work. Try it with a few websites like this: traceroute slashdot.org traceroute amazon.com traceroute google.com traceroute bbc.co.uk traceroute news.com.au Take a look and you should be able to convince yourself pretty quickly that there is no direct connection. How Packets Get Delivered In the first article, I presented a simple analogy for packet delivery, describing it as somewhat like a note being passed from person to person across a crowded room. While this analogy gets the basic idea across, it is much too simple. The Internet is not a flat network of computers, it is a network of networks. And of course, we don&rsquo;t just need to pass letters around a small room, we need to be able to send them anywhere in the world. It should be pretty clear then that the best analogy is the postal service! When you want to send a letter to somebody you put your letter in the mailbox and you trust that it will be delivered correctly. It&rsquo;s not really relevant to you how it gets there, as long as it does. Somebody has to physically deliver your letter to its destination of course, so how is this done? Well first off, the postman sure as hell doesn&rsquo;t take your letter and deliver it personally! It seems that the postal service is not a series of tubes either. Instead, the postman takes your letter to the local post office for processing. If the letter is addressed locally then the post office just sends it back out, and another postman delivers it directly. But, if the address is is non-local then it gets interesting! The local post office is not able to deliver the letter directly, so it passes it &ldquo;up&rdquo; to the next level of hierarchy, perhaps to a regional post office which services cities nearby, or maybe to a mail center at an airport, if the address is far away. Ideally, the actual transport of the letter would be done using a big truck. Lets be complicated and assume the letter is sent from Los Angeles to Sydney, Australia. The local post office receives the letter and given that it is addressed internationally, sends it directly to a mail center at LAX. The letter is processed again according to address, and gets routed on the next flight to Sydney. The plane lands at Sydney airport where an entirely different postal system takes over. Now the whole process starts operating in reverse. The letter travels &ldquo;down&rdquo; the hierarchy, from the general, to the specific. From the mail hub at Sydney Airport it gets sent out to a regional center, the regional center delivers it to the local post office, and eventually the letter is hand delivered by a mailman with a funny accent. Crikey! :) Just like post offices determine how to deliver letters via their address, networks deliver packets according to their IP address. The low-level details of this delivery and the actual routing of packets from network to network is actually quite complex, but the basic idea is that each router is just another computer, with a routing table describing where packets matching sets of addresses should go, as well as a default gateway address describing where to pass packets for which there is no matching entry in the table. It is routing tables, and the physical connections they represent that define the network of networks that is the Internet. The job of configuring these routing tables is up to network administrators, not programmers like us. But if you want to read more about it, then this article from ars technica provides some fascinating insight into how networks exchange packets between each other via peering and transit relationships. You can also read more details about routing tables in this linux faq, and about the border gateway protocol on wikipedia, which automatically discovers how to route packets between networks, making the internet a truly distributed system capable of dynamically routing around broken connectivity. Virtual Connections Now back to connections. If you have used TCP sockets then you know that they sure look like a connection, but since TCP is implemented on top of IP, and IP is just packets hopping from computer to computer, it follows that TCP&rsquo;s concept of connection must be a virtual connection. If TCP can create a virtual connection over IP, it follows that we can do the same over UDP. Lets define our virtual connection as two computers exchanging UDP packets at some fixed rate like 10 packets per-second. As long as the packets are flowing, we consider the two computers to be virtually connected. Our connection has two sides: One computer sits there and listens for another computer to connect to it. We'll call this computer the server. Another computer connects to a server by specifying an IP address and port. We'll call this computer the client. In our case, we only allow one client to connect to the server at any time. We&rsquo;ll generalize our connection system to support multiple simultaneous connections in a later article. Also, we assume that the IP address of the server is on a fixed IP address that the client may directly connect to. Protocol ID Since UDP is connectionless our UDP socket can receive packets sent from any computer. We&rsquo;d like to narrow this down so that the server only receives packets sent from the client, and the client only receives packets sent from the server. We can&rsquo;t just filter out packets by address, because the server doesn&rsquo;t know the address of the client in advance. So instead, we prefix each UDP packet with small header containing a 32 bit protocol id as follows: [uint protocol id] (packet data...) The protocol id is just some unique number representing our game protocol. Any packet that arrives from our UDP socket first has its first four bytes inspected. If they don&rsquo;t match our protocol id, then the packet is ignored. If the protocol id does match, we strip out the first four bytes of the packet and deliver the rest as payload. You just choose some number that is reasonably unique, perhaps a hash of the name of your game and the protocol version number. But really you can use anything. The whole point is that from the point of view of our connection based protocol, packets with different protocol ids are ignored. Detecting Connection Now we need a way to detect connection. Sure we could do some complex handshaking involving multiple UDP packets sent back and forth. Perhaps a client &ldquo;request connection&rdquo; packet is sent to the server, to which the server responds with a &ldquo;connection accepted&rdquo; sent back to the client, or maybe an &ldquo;i&rsquo;m busy&rdquo; packet if a client tries to connect to server which already has a connected client. Or&hellip; we could just setup our server to take the first packet it receives with the correct protocol id, and consider a connection to be established. The client just starts sending packets to the server assuming connection, when the server receives the first packet from the client, it takes note of the IP address and port of the client, and starts sending packets back. The client already knows the address and port of the server, since it was specified on connect. So when the client receives packets, it filters out any that don&rsquo;t come from the server address. Similarly, once the server receives the first packet from the client, it gets the address and port of the client from &ldquo;recvfrom&rdquo;, so it is able to ignore any packets that don&rsquo;t come from the client address. We can get away with this shortcut because we only have two computers involved in the connection. In later articles, we&rsquo;ll extend our connection system to support more than two computers in a client/server or peer-to-peer topology, and at this point we&rsquo;ll upgrade our connection negotiation to something more robust. But for now, why make things more complicated than they need to be? Detecting Disconnection How do we detect disconnection? Well if a connection is defined as receiving packets, we can define disconnection as not receiving packets. To detect when we are not receiving packets, we keep track of the number of seconds since we last received a packet from the other side of the connection. We do this on both sides. Each time we receive a packet from the other side, we reset our accumulator to 0.0, each update we increase the accumulator by the amount of time that has passed. If this accumulator exceeds some value like 10 seconds, the connection &ldquo;times out&rdquo; and we disconnect. This also gracefully handles the case of a second client trying to connect to a server that has already made a connection with another client. Since the server is already connected it ignores packets coming from any address other than the connected client, so the second client receives no packets in response to the packets it sends, so the second client times out and disconnects. Conclusion And that&rsquo;s all it takes to setup a virtual connection: some way to establish connection, filtering for packets not involved in the connection, and timeouts to detect disconnection. Our connection is as real as any TCP connection, and the steady stream of UDP packets it provides is a suitable starting point for a multiplayer action game. Now that you have your virtual connection over UDP, you can easily setup a client/server relationship for a two player multiplayer game without TCP. 译文译文出处 译者：张华栋(wcby) 审校：崔国军（飞扬971） 序言 大家好，我是Glenn Fiedler，欢迎阅读《针对游戏程序员的网络知识》系列教程的第三篇文章。 在之前的文章中，我向你展示了如何使用UDP协议来发送和接收数据包。 由于UDP协议是无连接的传输层协议，一个UDP套接字可以用来与任意数目的不同电脑进行数据包交换。但是在多人在线网络游戏中，我们通常只需要在一小部分互相连接的计算机之间交换数据包。 作为实现通用连接系统的第一步，我们将从最简单的可能情况开始：创建两台电脑之间构建于UDP协议之上的虚拟连接。 但是首先，我们将对互联网到底是如何工作的进行一点深度挖掘！ 互联网不是一连串的管子 在2006年，参议院特德·史蒂文斯(Ted Stevens) 用他关于互联网中立（netneutrality）法案的著名演讲创造了互联网的历史： ”互联网不是那种你随便丢点什么东西进去就能运行的东西。它不是一个大卡车。它是一连串的管子“ 当我第一次开始使用互联网的时候，我也像Ted一样无知。那是1995年，我坐在悉尼大学的计算机实验室里，在用一种叫做Netscape的网络浏览器（最早最热门的网页浏览工具）“在网上冲浪（surfing the web）“，那个时候我对发生了什么根本一无所知。 你看那个时候，我觉得每次连到一个网站上就一定有某个真实存在的连接在帮我们传递信息，就像电话线一样。那时候我在想，当我每次连到一个新的网站上需要花费多少钱? 30美分吗?一美元吗? 会有大学里的某个人过来拍拍我的肩膀让我付长途通信的费用么？当然，现在回头看那时候一切的想法都非常的愚蠢。 并没有在某个地方存在一个物理交换机用物理电话线将你和你希望通话的某个电脑直接连起来。更不用说像参议院史蒂文斯想让你相信的那样存在一串气压输送管。 没有直接的连接 相反你的数据是基于IP协议(InternetProtocol)通过在电脑到电脑之间发送数据包来传递信息的。 一个数据包可能在到达它的目的地之前要经过几个电脑。你没有办法提前知道数据包会经过具体哪些电脑，因为它会依赖当前网络的情况对数据包进行路由来动态的改变路径。甚至有可能给同一个地址发送A和B两个数据包，这两个数据包都采用不同的路由。这就是为什么UDP协议不能保证数据包的到达顺序。（其实这么说稍微容易有点引起误解，TCP协议是能保证数据包的到达顺序的，但是他也是基于IP协议进行数据包的发送，并且往同一个地址发送的两个数据包也有可能采用完全不同的路由，这主要是因为TCP在自己这一层做了一些控制而UDP没有，所以导致TCP协议可以保证数据包的有序性，而UDP协议不能，当然这种保证需要付出性能方面的代价）。在类unix的系统中可以通过调用“traceroute”函数并传递一个目的地主机名或IP地址来检查数据包的路由。 在Windows系统中，可以用“tracert”代替“traceroute”，其他不变，就能检查数据包的路由了。 像下面这样用一些网址来尝试下这种方法： traceroute slashdot.org traceroute amazon.com traceroute google.com traceroute bbc.co.uk traceroute news.com.au 运行下看下输出结果，你应该很快就能说服你自己确实连接到了网站上，但是并没有一个直接的连接。 数据包是如何传递到目的地的？ 在第一篇文章中，我对数据包传递到目的地这个事情做了一个简单的类比，把这个过程描述的有点像在一个拥挤的房间内一个人接着一个人的把便条传递下去。 虽然这个类比的基本思想还是表达出来了，但是它有点过于简单了。互联网并不是电脑组成的一个平面的网络，实际上它是网络的网络。当然，我们不只是要在一个小房间里面传递信件，我们要做的事能够把信息传递到全世界。 这就应该很清楚了，数据包传递到目的地的最好的类比是邮政服务! 当你想给某人写信的时候，你会把你的信件放到邮箱里并且你相信它将正确的传递到目的地。这封信件具体是怎么到达目的地的和你并不是十分相关，尽管它是否正确到达会对你有影响。当然会有某个人在物理上帮你把信件传递到目的地，所以这是怎么做的呢? 首先，邮递员肯定不需要自己去把你的信件送到目的地！看起来邮政服务也不是一串管子。相反，邮递员是把你的信件带到当地的邮政部门进行处理。 如果这封信件是发送给本地的，那么邮政部门就会把这封信件发送回来，另外一个邮递员会直接投递这封信件。但是，如果这封信件不是发送给本地的，那么这个处理过程就有意思了！当地的邮政部门不能直接投递这封信件，所以这封信件会被向上传递到层次结构的上一层，这个上一层也许是地区级的邮政部门它会负责服务附近的几个城市，如果要投递的地址非常远的话，这个上一层也许是位于机场的一个邮件中心。理想情况下，信件的实际运输将通过一个大卡车来完成。 让我们通过一个例子来把上面说的过程具体的走一遍，假设有一封信件要从洛杉矶发送到澳大利亚的悉尼。当地的邮政部门收到信件以后考虑到这封信件是一封跨国投递的信件，所以会直接把它发送到位于洛杉矶机场的邮件中心。在那里，这封信件会再次根据它的地址进行处理，并被安排通过下一个到悉尼的航班投递到悉尼去。 当飞机降落到悉尼机场以后，一个完全不同的邮政系统会负责接管这封信件。现在整个过程开始逆向操作。这封信件会沿着层次结构向下传递，从大的管理部门到具体的投递区域。这封信件会从悉尼机场的邮件中心被送往一个地区级的中心，然后地区级的中心会把这封信件投递到当地的邮政部门，最终这封信件会是由一个操着有趣的本地口音的邮政人员用手投递到真正的目的地的。哎呀! ! 就像邮局是通过信件的地址来决定这些信件是该如何投递的一样，网络也是根据这些数据包的IP地址来决定它们是该如何传递的。投递机制的底层细节以及数据包从网络到网络的实际路由其实都是相当复杂的，但是基本的想法都是一样的，就是每个路由器都只是另外一台计算机，它会携带一张路由表用来描述如果数据包的IP地址匹配了这张表上的某个地址集，那么这个数据包该如何传递，这张表还会记载着默认的网关地址，如果数据包的IP地址和这张路由表上的一个地址都匹配不上，那么这个数据包该传递到默认的网关地址那里。其实是路由表以及它们代表的物理连接定义了网络的网络，也就是互联网（互联网也被称为万维网）。 因特网于1969年诞生于美国。最初名为“阿帕网”（ARPAnet）是一个军用研究系统，后来又成为连接大学及高等院校计算机的学术系统，则已发展成为一个覆盖五大洲150多个国家的开放型全球计算机网络系统，拥有许多服务商。普通电脑用户只需要一台个人计算机用电话线通过调制解调器和因特网服务商连接，便可进入因特网。但因特网并不是全球唯一的互联网络。例如在欧洲，跨国的互联网络就有“欧盟网”（Euronet），“欧洲学术与研究网”（EARN），“欧洲信息网”（EIN），在美国还有“国际学术网”（BITNET），世界范围的还有“飞多网”（全球性的BBS系统）等。但这些网络其实根本就不需要知道，感谢IP协议的帮助，只要知道他们是可以互联互通的就可以。 这些路由表的配置工作是由网络管理员完成的，而不是由像我们这样的程序员来做。但是如果你想要了解这方面的更多内容， 那么来自ars technica的这篇文章将提供网络是如何在端与端之间互联来交换数据包以及传输关系方面一些非常有趣的见解。你还可以通过linux常见问题中路由表（routing tables）方面的文章以及维基百科上面的边界网关协议（border gateway protocol ）的解释来获得更多的细节。边界网关协议是用来自动发现如何在网络之间路由数据包的协议，有了它才真正的让互联网成为一个分布式系统，能够在不稳定的连接里面进行动态的路由。 边界网关协议（BGP）是运行于 TCP 上的一种自治系统的路由协议。 BGP 是唯一一个用来处理像因特网大小的网络的协议，也是唯一能够妥善处理好不相关路由域间的多路连接的协议。 BGP 构建在 EGP 的经验之上。 BGP 系统的主要功能是和其他的 BGP 系统交换网络可达信息。网络可达信息包括列出的自治系统（AS）的信息。这些信息有效地构造了 AS 互联的拓朴图并由此清除了路由环路，同时在 AS 级别上可实施策略决策。 虚拟的连接 现在让我们回到连接本身。 如果你已经使用过TCP套接字，那么你会知道它们看起来真的像是一个连接，但是由于TCP协议是在IP协议之上实现的，而IP协议是通过在计算机之间进行跳转来传递数据包的，所以TCP的连接仍然是一个虚拟连接。 如果TCP协议可以基于IP协议建立虚拟连接，那么我们在UDP协议上所做的一切都可以应用于TCP协议上。 让我们给虚拟连接下个定义：两个计算机之间以某个固定频率比如说每秒10个数据包来交换UDP的数据包。只要数据包仍然在传输，我们就认为这两台计算机之间存在一个虚拟连接。 我们的连接有两侧： 一个计算机坐在那儿侦听是否有另一台计算机连接到它。我们称负责监听的这台计算机为服务器（server）。 另一台计算机会通过一个指定的IP地址和端口连接到一个服务器。我们称主动连接的这台电脑为客户端（client）。 在我们的场景里，我们只允许一个客户端在任意的时候连接到服务器。我们将在下一篇文章里面拓展我们的连接系统以支持多个客户端的同时连接。此外，我们假定服务器的IP地址是一个固定的IP地址，客户端可以随时直接连接上来。我们将在后面的文章里面介绍匹配（matchmaking）和NAT打穿（NATpunch-through）。 协议ID 由于UDP协议是无连接的传输层协议，所以我们的UDP套接字可以接受来自任何电脑的数据包。 我们想要缩小接收数据包的范围，以便我们的服务器只接收那些从我们的客户端发送出来的数据包，并且我们的客户端只接收那些从我们的服务端发送出来的数据包。我们不能只通过地址来过滤我们的数据包，因为服务器没有办法提前知道客户端的地址。所以，我们会在每一个UDP数据包前面加上一个包含32位协议id的头,如下所示: [uint protocol id] (packet data...) 协议ID只是一些独特的代表我们的游戏协议的数字。我们的UDP套接字收到的任意数据包首先都要检查数据包的首四位。如果它们和我们的协议ID不匹配的话，这个数据包就会被忽略。如果它们和我们的协议ID匹配的话，我们会剔除数据包的第一个四个字节并把剩下的部分发给我们的系统进行处理。 你只要选择一些非常独特的数字就可以了，这些数字可以是你的游戏名字和协议版本号的散列值。不过说真的，你可以使用任何东西。这种做法的重点是把我们的连接视为基于协议进行通信的连接，如果协议ID不同，那么这样的数据包将被丢弃掉。 检测连接 现在我们需要一个方法来检测连接。 当然我们可以实现一些复杂的握手协议，牵扯到多个UDP数据包来回传递。比如说客户端发送一个”请求连接（request connection）“的数据包给服务器，当服务器收到这个数据包的时候会回应一个”连接接受（connection accepted）“的数据包给客户端，或者如果这个服务器已经有超过一个连接的客户端以后，会回复一个“我很忙（i’m busy）”的数据包给客户端。 或者。。我们可以设置我们的服务器，让它以它收到的第一个数据包的协议ID作为正确的协议ID，并在收到第一个数据包的时候就认为连接已经建立起来了。 客户端只是开始给服务器发送数据包，当服务器收到客户端发过来的第一个数据包的时候，它会记录下客户端的IP地址和端口号，然后开始给客户端回包。 客户端已经知道了服务器的地址和端口，因为这些信息是在连接的时候指定的。所以当客户端收到数据包的时候，它会过滤掉任何不是来自于服务器地址的数据包。同样的，一旦服务器收到客户端的第一个数据包，它就会从“recvfrom”函数里面得到客户端的地址和端口号，所以它也可以忽略任何不是发自客户端地址的数据包。 我们可以通过一个捷径来避开这个问题，因为我们的系统只有两台计算机会建立连接。在后面的文章里，我们将拓展我们的连接系统来支持超过两台计算机参与客户端/服务器或者端对端（peer-to-peer，p2p）网络模型，并且在那个时候我们会升级我们的连接协议方式来让它变得更加健壮。 但是现在，为什么我们要让事情变得超出需求的复杂度呢？（作者的意思是因为我们现在不需要解决这个问题，因为我们的场景是面对只有两台计算机的情况，所以我们可以先放过这个问题。） 检测断线的情况 我们该如何检测断线（disconnection）的情况？ 那么，如果一个连接被定义为接收数据包，我们可以定义断线为收不到数据包。 为了检测什么时候开始我们收不到数据包，我们要记录上一次我们从连接的另外一侧收到数据包到现在过去了多少秒，我们在连接的两侧都做了这个事情。 每次我们从连接的另外一端收到数据包的时候，我们都会重置我们的计数器为0.0，每一次更新的时候我们都会把这次更新到上一次更新逝去的时间量加到计数器上。 如果计数器的值超过某一个值，比如说10秒，那么我们就认定这个连接“超时”了并且我们会断开连接。 这也可以很优雅的处理当服务器已经与一个客户端建立连接以后，有第二个客户端试图与服务器建立连接的情况。因为服务器已经建立了连接，它会忽略掉不是来自连接的客户端地址发出来的数据包，所以第二个客户端在发出了数据包以后得不到任何回应，这样它就会判断连接超时并断开连接。 总结 而这一切都需要设置一个虚拟连接：用某种方法建立一个连接，过滤掉那些不是来自这个连接的数据包，并且如果发现连接超时就断开连接。 我们的连接就跟任何TCP连接一样真实，并且UDP数据包构成的稳定数据流为多人在线动作网络游戏提供了一个很好的起点。 我们还获得了一些互联网是如何路由数据包的见解。举个例子来说，我们现在知道UDP数据包有时候会在到达的时候是乱序的原因是因为它们在IP层传输的时候采用不同的路由！看下互联网的地图，你会不会对你的数据包能够到达正确的目的点感到非常的神奇？如果你想对这个问题进行更加深入的了解，维基百科上的这篇文章(Internet backbone)是一个很好的起点。 现在，既然你已经有了一个基于UDP协议的虚拟连接，你可以轻松的在两个玩家的多人在线游戏里面设置一个客户端/服务器关系而不需要使用TCP协议。 你可以在这篇文章的示例源代码（examplesource code ）找到一个具体实现。 这是一个简单的客户端/服务器程序，每秒交换30个数据包。你可以在任意你喜欢的机器上运行这个服务器，只要给它提供一个公共的IP地址就可以了，需要公共IP地址的原因是我们目前还不支持NAT打穿（NAT punch-through ）。 NAT穿越（NATtraversal）涉及TCP/IP网络中的一个常见问题，即在处于使用了NAT设备的私有TCP/IP网络中的主机之间建立连接的问题。 像这样来运行客户端： ./Client 205.10.40.50 它会尝试连接到你在命令行输入的地址。如果你不输入地址的话，默认情况下它会连接到127.0.0.1。 当一个客户端已经与服务器建立连接的时候，你可以尝试用另外一个客户端来连接这个服务器，你会注意到这次连接的尝试失败了。这么设计是故意的。因为到目前为止，一次只允许一个客户端连接上服务器。 你也可以在客户端和服务器连接的状态下尝试停止客户端或者服务器，你会注意到10秒以后连接的另外一侧会判断连接超时并断开连接。当客户端超时的时候它会退到shell窗口，但是服务器会退到监听状态为下一次的连接做好准备。 预告下接下来的一篇文章的题目:《基于UDP的可靠、有序和拥塞避免的传输》，欢迎继续阅读。 如果你喜欢这篇文章的话，请考虑对我做一个小小的捐赠。捐款会鼓励我写更多的文章!（原文作者在原文的地址上提供了一个捐赠网址，有兴趣的读者可以在文章开始的地方找到原文地址进行捐赠） 【版权声明】 原文作者未做权利声明，视为共享知识产权进入公共领域，自动获得授权。 源码下载因 Gaffer On Games 的源码原下载地址失效, 所以特地补上. 请点击]]></content>
      <categories>
        <category>GS</category>
      </categories>
      <tags>
        <tag>GafferOnGames</tag>
        <tag>UDP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[游戏网络开发二之数据的发送与接收]]></title>
    <url>%2Fblog%2F2016%2F11%2F15%2F%E6%B8%B8%E6%88%8F%E7%BD%91%E7%BB%9C%E5%BC%80%E5%8F%91%E4%BA%8C%E4%B9%8B%E6%95%B0%E6%8D%AE%E7%9A%84%E5%8F%91%E9%80%81%E4%B8%8E%E6%8E%A5%E6%94%B6%2F</url>
    <content type="text"><![CDATA[原文原文出处 Introduction Hi, I&rsquo;m Glenn Fiedler and welcome to Networking for Game Programmers. In the previous article we discussed options for sending data between computers and decided to use UDP instead of TCP for time critical data. In this article I am going to show you how to send and receive UDP packets. BSD sockets For most modern platforms you have some sort of basic socket layer available based on BSD sockets. BSD sockets are manipulated using simple functions like &ldquo;socket&rdquo;, &ldquo;bind&rdquo;, &ldquo;sendto&rdquo; and &ldquo;recvfrom&rdquo;. You can of course work directly with these functions if you wish, but it becomes difficult to keep your code platform independent because each platform is slightly different. So although I will first show you BSD socket example code to demonstrate basic socket usage, we won&rsquo;t be using BSD sockets directly for long. Once we&rsquo;ve covered all basic socket functionality we&rsquo;ll abstract everything away into a set of classes, making it easy to you to write platform independent socket code. Platform specifics First let&rsquo;s setup a define so we can detect what our current platform is and handle the slight differences in sockets from one platform to another: // platform detection #define PLATFORM_WINDOWS 1 #define PLATFORM_MAC 2 #define PLATFORM_UNIX 3 #if defined(_WIN32) #define PLATFORM PLATFORM_WINDOWS #elif defined(__APPLE__) #define PLATFORM PLATFORM_MAC #else #define PLATFORM PLATFORM_UNIX #endif Now let&rsquo;s include the appropriate headers for sockets. Since the header files are platform specific, we&rsquo;ll use the platform #define to include different sets of files depending on the platform: #if PLATFORM == PLATFORM_WINDOWS #include &amp;lt;winsock2.h&amp;gt; #elif PLATFORM == PLATFORM_MAC || PLATFORM == PLATFORM_UNIX #include &amp;lt;sys/socket.h&amp;gt; #include &amp;lt;netinet/in.h&amp;gt; #include &amp;lt;fcntl.h&amp;gt; #endif Sockets are built in to the standard system libraries on unix-based platforms so we don&rsquo;t have to link to any additonal libraries. However, on Windows we need to link to the winsock library to get socket functionality. Here is a simple trick to do this without having to change your project or makefile: #if PLATFORM == PLATFORM_WINDOWS #pragma comment( lib, &quot;wsock32.lib&quot; ) #endif I like this trick because I&rsquo;m super lazy. You can always link from your project or makefile if you wish. Initializing the socket layer Most unix-like platforms (including macosx) don&rsquo;t require any specific steps to initialize the sockets layer, however Windows requires that you jump through some hoops to get your socket code working. You must call &ldquo;WSAStartup&rdquo; to initialize the sockets layer before you call any socket functions, and &ldquo;WSACleanup&rdquo; to shutdown when you are done. Let&rsquo;s add two new functions: bool InitializeSockets() { #if PLATFORM == PLATFORM_WINDOWS WSADATA WsaData; return WSAStartup( MAKEWORD(2,2), &amp;WsaData ) == NO_ERROR; #else return true; #endif } void ShutdownSockets() { #if PLATFORM == PLATFORM_WINDOWS WSACleanup(); #endif } Now we have a platform independent way to initialize the socket layer. Creating a socket It&rsquo;s time to create a UDP socket, here&rsquo;s how to do it: int handle = socket( AF_INET, SOCK_DGRAM, IPPROTO_UDP ); if ( handle &amp;lt;= 0 ) { printf( &amp;quot;failed to create socket\n&amp;quot; ); return false; } Next we bind the UDP socket to a port number (eg. 30000). Each socket must be bound to a unique port, because when a packet arrives the port number determines which socket to deliver to. Don&rsquo;t use ports lower than 1024 because they are reserved for the system. Also try to avoid using ports above 50000 because they used when dynamically assigning ports. Special case: if you don&rsquo;t care what port your socket gets bound to just pass in &ldquo;0&rdquo; as your port, and the system will select a free port for you. sockaddr_in address; address.sin_family = AF_INET; address.sin_addr.s_addr = INADDR_ANY; address.sin_port = htons( (unsigned short) port ); if ( bind( handle, (const sockaddr*) &amp;amp;address, sizeof(sockaddr_in) ) &amp;lt; 0 ) { printf( &amp;quot;failed to bind socket\n&amp;quot; ); return false; } Now the socket is ready to send and receive packets. But what is this mysterious call to &ldquo;htons&rdquo; in the code above? This is just a helper function that converts a 16 bit integer value from host byte order (little or big-endian) to network byte order (big-endian). This is required whenever you directly set integer members in socket structures. You&rsquo;ll see &ldquo;htons&rdquo; (host to network short) and its 32 bit integer sized cousin &ldquo;htonl&rdquo; (host to network long) used several times throughout this article, so keep an eye out, and you&rsquo;ll know what is going on. Setting the socket as non-blocking By default sockets are set in what is called &ldquo;blocking mode&rdquo;. This means that if you try to read a packet using &ldquo;recvfrom&rdquo;, the function will not return until a packet is available to read. This is not at all suitable for our purposes. Video games are realtime programs that simulate at 30 or 60 frames per second, they can&rsquo;t just sit there waiting for a packet to arrive! The solution is to flip your sockets into &ldquo;non-blocking mode&rdquo; after you create them. Once this is done, the &ldquo;recvfrom&rdquo; function returns immediately when no packets are available to read, with a return value indicating that you should try to read packets again later. Here&rsquo;s how put a socket in non-blocking mode: #if PLATFORM == PLATFORM_MAC || PLATFORM == PLATFORM_UNIX int nonBlocking = 1; if ( fcntl( handle, F_SETFL, O_NONBLOCK, nonBlocking ) == -1 ) { printf( &amp;quot;failed to set non-blocking\n&amp;quot; ); return false; } #elif PLATFORM == PLATFORM_WINDOWS DWORD nonBlocking = 1; if ( ioctlsocket( handle, FIONBIO, &amp;amp;nonBlocking ) != 0 ) { printf( &amp;quot;failed to set non-blocking\n&amp;quot; ); return false; } #endif Windows does not provide the &ldquo;fcntl&rdquo; function, so we use the &ldquo;ioctlsocket&rdquo; function instead. Sending packets UDP is a connectionless protocol, so each time you send a packet you must specify the destination address. This means you can use one UDP socket to send packets to any number of different IP addresses, there&rsquo;s no single computer at the other end of your UDP socket that you are connected to. Here&rsquo;s how to send a packet to a specific address: int sent_bytes = sendto( handle, (const char*)packet_data, packet_size, 0, (sockaddr*)&amp;address, sizeof(sockaddr_in) ); if ( sent_bytes != packet_size ) { printf( &amp;quot;failed to send packet\n&amp;quot; ); return false; } Important! The return value from &ldquo;sendto&rdquo; only indicates if the packet was successfully sent from the local computer. It does not tell you whether or not the packet was received by the destination computer. UDP has no way of knowing whether or not the the packet arrived at its destination! In the code above we pass a &ldquo;sockaddr_in&rdquo; structure as the destination address. How do we setup one of these structures? Let&rsquo;s say we want to send to the address 207.45.186.98:30000 Starting with our address in this form: unsigned int a = 207; unsigned int b = 45; unsigned int c = 186; unsigned int d = 98; unsigned short port = 30000; We have a bit of work to do to get it in the form required by &ldquo;sendto&rdquo;: unsigned int address = ( a &lt;&lt; 24 ) | ( b &lt;&lt; 16 ) | ( c &lt;&lt; 8 ) | d; sockaddr_in addr; addr.sin_family = AF_INET; addr.sin_addr.s_addr = htonl( address ); addr.sin_port = htons( port ); As you can see, we first combine the a,b,c,d values in range [0,255] into a single unsigned integer, with each byte of the integer now corresponding to the input values. We then initialize a &ldquo;sockaddr_in&rdquo; structure with the integer address and port, making sure to convert our integer address and port values from host byte order to network byte order using &ldquo;htonl&rdquo; and &ldquo;htons&rdquo;. Special case: if you want to send a packet to yourself, there&rsquo;s no need to query the IP address of your own machine, just pass in the loopback address 127.0.0.1 and the packet will be sent to your local machine. Receiving packets Once you have a UDP socket bound to a port, any UDP packets sent to your sockets IP address and port are placed in a queue. To receive packets just loop and call &ldquo;recvfrom&rdquo; until it fails with EWOULDBLOCK indicating there are no more packets to receive. Since UDP is connectionless, packets may arrive from any number of different computers. Each time you receive a packet &ldquo;recvfrom&rdquo; gives you the IP address and port of the sender, so you know where the packet came from. Here&rsquo;s how to loop and receive all incoming packets: while ( true ) { unsigned char packet_data[256]; unsigned int max_packet_size = sizeof( packet_data ); #if PLATFORM == PLATFORM_WINDOWS typedef int socklen_t; #endif sockaddr_in from; socklen_t fromLength = sizeof( from ); int bytes = recvfrom( socket, (char*)packet_data, max_packet_size, 0, (sockaddr*)&amp;amp;from, &amp;amp;fromLength ); if ( bytes &amp;lt;= 0 ) break; unsigned int from_address = ntohl( from.sin_addr.s_addr ); unsigned int from_port = ntohs( from.sin_port ); // process received packet } Any packets in the queue larger than your receive buffer will be silently discarded. So if you have a 256 byte buffer to receive packets like the code above, and somebody sends you a 300 byte packet, the 300 byte packet will be dropped. You will not receive just the first 256 bytes of the 300 byte packet. Since you are writing your own game network protocol, this is no problem at all in practice, just make sure your receive buffer is big enough to receive the largest packet your code could possibly send. Destroying a socket On most unix-like platforms, sockets are file handles so you use the standard file &ldquo;close&rdquo; function to clean up sockets once you are finished with them. However, Windows likes to be a little bit different, so we have to use &ldquo;closesocket&rdquo; instead: #if PLATFORM == PLATFORM_MAC || PLATFORM == PLATFORM_UNIX close( socket ); #elif PLATFORM == PLATFORM_WINDOWS closesocket( socket ); #endif Hooray windows. Socket class So we&rsquo;ve covered all the basic operations: creating a socket, binding it to a port, setting it to non-blocking, sending and receiving packets, and destroying the socket. But you&rsquo;ll notice most of these operations are slightly platform dependent, and it&rsquo;s pretty annoying to have to remember to #ifdef and do platform specifics each time you want to perform socket operations. We&rsquo;re going to solve this by wrapping all our socket functionality up into a &ldquo;Socket&rdquo; class. While we&rsquo;re at it, we&rsquo;ll add an &ldquo;Address&rdquo; class to make it easier to specify internet addresses. This avoids having to manually encode or decode a &ldquo;sockaddr_in&rdquo; structure each time we send or receive packets. So let&rsquo;s add a socket class: class Socket { public: Socket(); ~Socket(); bool Open( unsigned short port ); void Close(); bool IsOpen() const; bool Send( const Address &amp;amp; destination, const void * data, int size ); int Receive( Address &amp;amp; sender, void * data, int size ); private: int handle; }; and an address class: class Address { public: Address(); Address( unsigned char a, unsigned char b, unsigned char c, unsigned char d, unsigned short port ); Address( unsigned int address, unsigned short port ); unsigned int GetAddress() const; unsigned char GetA() const; unsigned char GetB() const; unsigned char GetC() const; unsigned char GetD() const; unsigned short GetPort() const; private: unsigned int address; unsigned short port; }; Here&rsquo;s how to to send and receive packets with these classes: // create socket const int port = 30000; Socket socket; if ( !socket.Open( port ) ) { printf( &amp;quot;failed to create socket!\n&amp;quot; ); return false; } // send a packet const char data[] = &amp;quot;hello world!&amp;quot;; socket.Send( Address(127,0,0,1,port), data, sizeof( data ) ); // receive packets while ( true ) { Address sender; unsigned char buffer[256]; int bytes_read = socket.Receive( sender, buffer, sizeof( buffer ) ); if ( !bytes_read ) break; // process packet } As you can see it&rsquo;s much simpler than using BSD sockets directly. As an added bonus the code is the same on all platforms because everything platform specific is handled inside the socket and address classes. Conclusion You now have a platform independent way to send and receive packets. Enjoy :) 译文译文出处 因译文很多地方均有疏漏, 本文已经对部分疏漏做了修正. 翻译：杨嘉鑫（矫情到死的仓鼠君，）审校：赵菁菁（轩语轩缘）$hhd$1>序言 大家好，我是Glenn Fiedler，欢迎阅读《针对游戏程序员的网络知识》系列教程的第二篇文章。 在前面的文章中我们讨论了在不同计算机之间发送数据的方法，并决定使用用户数据报协议（UDP）而非传输控制协议（TCP）。我们之所以使用用户数据报协议（UDP），是因为它能够使数据在不等待重发包而造成数据聚集的情况下按时被送达。现在我将要告诉各位如何使用用户数据报协议（UDP）发送和接收数据包。$hhd$1>伯克利套接字 （BSD socket）对于大多数现代的平台来说你都可以找到建立在伯克利套接字上的sockets。伯克利套接字主要通过“socket”,“bind”, “sendto” and “recvfrom”几个简单函数进行控制。如果你愿意的话你当然可以直接对这几个函数进行调用，但是由于每个平台之间有细微差别，保持代码平台的独立性将会变得有些困难。因此，尽管我将先给各位介绍伯克利套接字的示例代码用以说明它的基本使用功能，我们也不会大量的直接使用伯克利套接字。所以当我们掌握了所有基础socket 功能后，我们将会把所有内容汇总到一个系列的课中，以便你可以轻松地编写代码。$hhd$1>平台的特殊性首先 我们先建立一个“define”程序用来测试我们现有的平台是什么，这样我们就可以发现不同平台间间各个socket里的细微差别。?123456789101112131415161718192021// platform detection #define PLATFORM_WINDOWS 1 #define PLATFORM_MAC 2 #define PLATFORM_UNIX 3 #if defined(_WIN32) #define PLATFORM PLATFORM_WINDOWS #elif defined(__APPLE__) #define PLATFORM PLATFORM_MAC #else #define PLATFORM PLATFORM_UNIX #endif接下来我们为sockets写入适当的标头，由于头文件具有平台的特殊性所以我们将使用“#define”来根据不同的平台引用不同的文件。?123456789101112131415#if PLATFORM == PLATFORM_WINDOWS #include &lt;winsock2.h&gt; #elif PLATFORM == PLATFORM_MAC || PLATFORM == PLATFORM_UNIX #include &lt;sys socket.h=""&gt; #include &lt;netinet in.h=""&gt; #include &lt;fcntl.h&gt; #endif&lt;/fcntl.h&gt;&lt;/netinet&gt;&lt;/sys&gt;&lt;/winsock2.h&gt;如果sockets是建立在unix平台上，我们就不需要任何其他多余的连接，若它是建立在windows系统里，为了确保socket正常使用我们就需要连接到“winsock”库内。以下是一个简单的技巧，它可以在不改变已有项目或生成文件的前提下完成上述工作。?12345#if PLATFORM == PLATFORM_WINDOWS #pragma comment( lib, "wsock32.lib" ) #endif我之所以非常喜欢这个小技巧是因为我太懒了~当然啦，如果你愿意每次都进行项目链接或生成文件也未尝不可。$hhd$1>socket层的初始化大多数“unix-like”的平台 (包括macosx) 是不需要任何特殊的步骤去初始化socket层的。但是Windows需要进行一些特殊设置来确保你的sockets代码正常工作。在你使用其他任何sockets功能前你必须先调用 “WSAStartup” 来初始化它们，在你的程序段结束时你也必须使用 “WSACleanup”来结束。下面让我们来添加以上两个新功能：?123456789101112131415161718192021222324bool InitializeSockets(){ #if PLATFORM == PLATFORM_WINDOWS WSADATA WsaData; return WSAStartup( MAKEWORD(2,2), &amp;WsaData ) == NO_ERROR; #else return true; #endif} void ShutdownSockets(){ #if PLATFORM == PLATFORM_WINDOWS WSACleanup(); #endif}这样我们就得到了一个初始化socket层的方法。对于那些不需要socket初始化的平台来说这些功能可以忽略不计。$hhd$1>建立一个socket现在是时候来建立一个基于用户数据报协议（UDP）的socket了，下面是实施的方法：?12345678int handle = socket( AF_INET, SOCK_DGRAM,IPPROTO_UDP ); if ( handle &lt;= 0 ){ printf( "failed to create socket\n" ); return false;}接下来我们把用户数据报协议（UDP）的socket对应到一个端口上（比如30000这个端口）。每一个socket都必须对应到一个独一无二的端口上。这么做的原因是端口号决定了每个数据包发送到的位置。不要使用1024以下的端口，因为这是为系统调用所预留的。有一种特殊情况，如果你不在乎socket指定到哪个端口上，你就可以输入“0”，这样系统将会自动为你选择一个闲置的端口。?123456789101112sockaddr_in address;address.sin_family = AF_INET;address.sin_addr.s_addr = INADDR_ANY;address.sin_port = htons( (unsigned short) port ); if ( bind( handle, (const sockaddr*) &amp;address, sizeof(sockaddr_in) ) &lt; 0 ){ printf( "failed to bind socket\n" ); return false;}这样我们的socket已经准备就绪并可以发送和接收包了。那么上面提到的“htons”是起什么作用呢？这是一个辅助功能，它将一个16位整数的值由主机字节序列（小端或大端）转换成网络字节序列（大端）。这就要求你在任何时候都直接在socket结构里设置整数数字。你会看到“htons”（主机到网络短字节）及其32位整数大小的表兄妹"htonl"（主机到网络长字节）这在这篇文章中被多次使用，你留意了以后你在下文中再次遇到就会明白。$hhd$1>将socket设置为非阻塞形式 默认情况下，socket是被设置在 “阻塞模式”的状态下。这意味着，如果你想使用“recvfrom”功能读一个包，在一个数据包被读取前该函数值将不能被返回。这与我们的目标完全不符。视频游戏是拟态在30或60帧每秒实时的程序，他们不能只是坐在那里等待数据包的到达！解决方案是你将socket转换成以“非阻塞模式”后再创建他们。一旦做到这一点，当没有包可供阅读时，“recvfrom”函数就可以立即返回，返回值显示你应该稍后再尝试读取包。 下面是如何将socket设置为非阻塞模式的方法：?1234567891011121314151617181920#if PLATFORM == PLATFORM_MAC || PLATFORM == PLATFORM_UNIX int nonBlocking = 1;if ( fcntl( handle, F_SETFL, O_NONBLOCK, nonBlocking ) == -1 ){ printf( "failed to set non-blocking\n" ); return false;} #elif PLATFORM == PLATFORM_WINDOWSDWORD nonBlocking = 1;if ( ioctlsocket( handle, FIONBIO, &amp;nonBlocking ) != 0 ){ printf( "failed to set non-blocking\n" ); return false;} #endif从上面的程序我们可以发现，Windows本身并不提供“框架”的功能，所以我们使用“ioctlsocket”功能来实现。$hhd$1>发送数据包用户数据报协议（UDP）是一种无连接协议，所以每次你发送一个数据包前都要指定一个目的地址。你可以使用一个用户数据报协议（UDP）发送数据包到任意数量的不同的IP地址，而在你用户数据报协议（UDP） socket的另一端并没有连接某一台计算机。下面是如何发送一个数据包到一个特定的地址方法：?1234567891011121314int sent_bytes = sendto( handle, (const char*)packet_data, packet_size, 0, (sockaddr*)&amp;address, sizeof(sockaddr_in) ); if ( sent_bytes != packet_size ){ printf( "failed to send packet\n" ); return false;}很重要的一点！“sendto”的返回值只是表明数据包是否被成功地从本地计算机发送，它并不能表明目标计算机是否成功接收到你的数据包！用户数据报协议（UDP）没有办法知道数据包是否能到达目的地。 上面的代码中，我们通过“sockaddr_in”结构为目的地址。那么我们如何设置这些结构呢？现在让我们以发送到207.45.186.98:30000 这个地址为例我们从以下这个程序开始：?12345678910111213141516unsigned int a = 207;unsigned int b = 45;unsigned int c = 186;unsigned int d = 98;unsigned short port = 30000; 我们还要在形式上进行设置从而符合“sendto”的要求：unsigned int address = ( a &lt;&lt; 24 ) | ( b &lt;&lt; 16 ) | ( c &lt;&lt; 8 ) | d; sockaddr_in addr;addr.sin_family = AF_INET;addr.sin_addr.s_addr = htonl( address );addr.sin_port = htons( port );正如您所看到的，我们首先将A、B、C、D值在范围[ 0, 255 ]内的值转化为一个单一的无符号整数，从而使这个整数的每个字节对应输入值。然后以整数地址和端口来初始化一个“sockaddr_in”结构，这样就确保使用“htonl” 和“htons”来将整型地址和端口值从主机字节序列转换为为网络字节序列。一种特殊情况：如果你想给自己发送一个数据包，不需要查询自己机器的IP地址，在回送地址127.0.0.1中数据包就将被发送到你的本地机器。$hhd$1>接收数据包 一旦你将一个用户数据报协议（UDP）套接字绑定到一个端口，任何发送到您scoket IP地址和端口的用户数据报协议（UDP）数据包都将放在一个队列里。接收数据包的话, 只需要循环调用 “recvfrom”函数直到他失败并返回"EWOULDBLOCK"，这就意味着队列里有没有留下其他的数据包了。由于用户数据报协议（UDP）是无连接性的，数据包可以到达许多不同的计算机。每当你收到一个数据包，“recvfrom”都会给你发送者的IP地址和端口以便你知道这是来自哪里的数据包。下面是如何进行循环接收传入的数据包的方法：?12345678910111213141516171819202122232425262728while ( true ){ unsigned char packet_data[256]; unsigned int max_packet_size = sizeof( packet_data ); #if PLATFORM == PLATFORM_WINDOWS typedef int socklen_t; #endif sockaddr_in from; socklen_t fromLength = sizeof( from ); int bytes = recvfrom( socket, (char*)packet_data, max_packet_size, 0, (sockaddr*)&amp;from, &amp;fromLength ); if ( bytes &lt;= 0 ) break; unsigned int from_address = ntohl( from.sin_addr.s_addr ); unsigned int from_port = ntohs( from.sin_port ); // process received packet }在队列中，数据包一旦大于你接收缓冲区的范围，他们都会被系统悄悄舍弃。因此，如果你有一个256字节的缓冲区用来接收数据包，有人给你一个发送300字节的数据包，300字节的数据包都将被删除。您将不会接收到300字节数据包的前256个字节。因为您正在编写自己的游戏网络协议，以上这些操作这是没有什么影响的。在实践中您就要确保您的接收缓冲区足够大，以接收最大的数据包。$hhd$1>关闭一个socket在大多数Unix平台，一旦你完成了自己所需的程序后，在socket文件中只要使用标准的文件“close”函数来清理即可。然而，在Windows系统中以上情形会有点不同，我们要用“closesocket”函数来操作：?123456789#if PLATFORM == PLATFORM_MAC || PLATFORM == PLATFORM_UNIX close( socket ); #elif PLATFORM == PLATFORM_WINDOWS closesocket( socket ); #endif $hhd$1>Socket class现在，我们已经完成了所有的基本操作：创建一个socket，将他绑定到一个端口并设置为非阻塞，发送和接收数据包，清除socket。但是你会发现以上这些操作中多多少少都是依赖于平台的，在每一次你想执行socket操作时，你不得不记住“# ifdef”指令和针对不同平台的各种细节，这些繁琐的操作是很令人抓狂的。为了解决这个问题，我们可以将所有的socket功能封装成一个“socket class‘’。当我们在使用它的时候，我们将添加一个“Address class‘’，这样使它更容易指定互联网地址。这避免了我们每次发送或接收数据包时进行手动编码或解码“sockaddr_in”结构。下面是“socket class‘’的程序：?123456789101112131415161718192021class Socket{public: Socket(); ~Socket(); bool Open( unsigned short port ); void Close(); bool IsOpen() const; bool Send( const Address &amp; destination, const void * data, int size ); int Receive( Address &amp; sender, void * data, int size ); private: int handle; };下面是“address class”的程序：?12345678910111213141516171819202122232425class Address{public: Address(); Address( unsigned char a, unsigned char b, unsigned char c, unsigned char d, unsigned short port ); Address( unsigned int address, unsigned short port ); unsigned int GetAddress() const; unsigned char GetA() const; unsigned char GetB() const; unsigned char GetC() const; unsigned char GetD() const; unsigned short GetPort() const; private: unsigned int address; unsigned short port; };下面是这些class如何接收和发送数据包的程序：?12345678910111213141516171819202122232425262728293031// create socketconst int port = 30000;Socket socket;if ( !socket.Open( port ) ){ printf( "failed to create socket!\n" ); return false;} // send a packetconst char data[] = "hello world!";socket.Send( Address(127,0,0,1,port), data, sizeof( data ) ); // receive packetswhile ( true ){ Address sender; unsigned char buffer[256]; int bytes_read = socket.Receive( sender, buffer, sizeof( buffer ) ); if ( !bytes_read ) break; // process packet }$hhd$1>结论我们现在有了一种不限平台的方法来发送和接收用户数据报协议（UDP）的数据包。用户数据报协议（UDP）是无连接性的，因此我编写了一个简单的示例程序，它可以从文本文件中读取IP地址，并能够每秒向这些地址发送一个数据包。每当这个程序接收到一个数据包时，它就会告诉你它们来自哪个机器，以及接收到的数据包的大小。您可以很容易地设置它，然后你就拥有了一系列在本地机器上互相发送数据包的节点。这样你就可以利用以下程序通过不同的端口，进入不同的应用程序： &gt; Node30000 &gt; Node 30001 &gt; Node 30002 etc...然后每个节点都将尝试发送数据包到每个其他节点，它的工作原理就像一个小型的“peer-to-peer”设置。我是在MacOSX系统中开发的这个程序，但我想你应该能够轻松地在任何Unix系统或Windows上对他进行编译。如果你有任何应用在其他不同机器上的兼容性补丁，也非常欢迎您与我取得联系。【版权声明】 原文作者未做权利声明，视为共享知识产权进入公共领域，自动获得授权。 源码下载因 Gaffer On Games 的源码原下载地址失效, 所以特地补上. 请点击]]></content>
      <categories>
        <category>GS</category>
      </categories>
      <tags>
        <tag>TCP</tag>
        <tag>GafferOnGames</tag>
        <tag>UDP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[游戏网络开发一之TCPvsUDP]]></title>
    <url>%2Fblog%2F2016%2F11%2F14%2F%E6%B8%B8%E6%88%8F%E7%BD%91%E7%BB%9C%E5%BC%80%E5%8F%91%E4%B8%80%E4%B9%8BTCPvsUDP%2F</url>
    <content type="text"><![CDATA[原文原文出处 Introduction Hi, I&rsquo;m Glenn Fiedler and welcome to Networking for Game Programmers. In this article we start with the most basic aspect of network programming: sending and receiving data over the network. This is perhaps the simplest and most basic part of what network programmers do, but still it is quite intricate and non-obvious as to what the best course of action is. You have most likely heard of sockets, and are probably aware that there are two main types: TCP and UDP. When writing a network game, we first need to choose what type of socket to use. Do we use TCP sockets, UDP sockets or a mixture of both? Take care because if you get this wrong it will have terrible effects on your multiplayer game! The choice you make depends entirely on what sort of game you want to network. So from this point on and for the rest of this article series, I assume you want to network an action game. You know, games like Halo, Battlefield 1942, Quake, Unreal, CounterStrike and Team Fortress. In light of the fact that we want to network an action game, we&rsquo;ll take a very close look at the properties of each protocol, and dig a bit into how the internet actually works. Once we have all this information, the correct choice is clear. TCP/IP TCP stands for &ldquo;transmission control protocol&rdquo;. IP stands for &ldquo;internet protocol&rdquo;. Together they form the backbone for almost everything you do online, from web browsing to IRC to email, it&rsquo;s all built on top of TCP/IP. If you have ever used a TCP socket, then you know it&rsquo;s a reliable connection based protocol. This means you create a connection between two machines, then you exchange data much like you&rsquo;re writing to a file on one side, and reading from a file on the other. TCP connections are reliable and ordered. All data you send is guaranteed to arrive at the other side and in the order you wrote it. It&rsquo;s also a stream protocol, so TCP automatically splits your data into packets and sends them over the network for you. IP The simplicity of TCP is in stark contrast to what actually goes on underneath TCP at the IP or &ldquo;internet protocol&rdquo; level. Here there is no concept of connection, packets are simply passed from one computer to the next. You can visualize this process being somewhat like a hand-written note passed from one person to the next across a crowded room, eventually, reaching the person it&rsquo;s addressed to, but only after passing through many hands. There is also no guarantee that this note will actually reach the person it is intended for. The sender just passes the note along and hopes for the best, never knowing whether or not the note was received, unless the other person decides to write back! Of course IP is in reality a little more complicated than this, since no one computer knows the exact sequence of computers to pass the packet along to so that it reaches its destination quickly. Sometimes IP passes along multiple copies of the same packet and these packets make their way to the destination via different paths, causing packets to arrive out of order and in duplicate. This is because the internet is designed to be self-organizing and self-repairing, able to route around connectivity problems rather than relying on direct connections between computers. It&rsquo;s actually quite cool if you think about what&rsquo;s really going on at the low level. You can read all about this in the classic book TCP/IP Illustrated. UDP Instead of treating communications between computers like writing to files, what if we want to send and receive packets directly? We can do this using UDP. UDP stands for &ldquo;user datagram protocol&rdquo; and it&rsquo;s another protocol built on top of IP, but unlike TCP, instead of adding lots of features and complexity, UDP is a very thin layer over IP. With UDP we can send a packet to a destination IP address (eg. 112.140.20.10) and port (say 52423), and it gets passed from computer to computer until it arrives at the destination or is lost along the way. On the receiver side, we just sit there listening on a specific port (eg. 52423) and when a packet arrives from any computer (remember there are no connections!), we get notified of the address and port of the computer that sent the packet, the size of the packet, and can read the packet data. Like IP, UDP is an unreliable protocol. In practice however, most packets that are sent will get through, but you&rsquo;ll usually have around 1-5% packet loss, and occasionally you&rsquo;ll get periods where no packets get through at all (remember there are lots of computers between you and your destination where things can go wrong&hellip;) There is also no guarantee of ordering of packets with UDP. You could send 5 packets in order 1,2,3,4,5 and they could arrive completely out of order like 3,1,2,5,4. In practice, packets tend to arrive in order most of the time, but you cannot rely on this! UDP also provides a 16 bit checksum, which in theory is meant to protect you from receiving invalid or truncated data, but you can&rsquo;t even trust this, since 16 bits is just not enough protection when you are sending UDP packets rapidly over a long period of time. Statistically, you can&rsquo;t even rely on this checksum and must add your own. So in short, when you use UDP you&rsquo;re pretty much on your own! TCP vs. UDP We have a decision to make here, do we use TCP sockets or UDP sockets? Lets look at the properties of each: TCP: Connection based Guaranteed reliable and ordered Automatically breaks up your data into packets for you Makes sure it doesn't send data too fast for the internet connection to handle (flow control) Easy to use, you just read and write data like its a file UDP: No concept of connection, you have to code this yourself No guarantee of reliability or ordering of packets, they may arrive out of order, be duplicated, or not arrive at all! You have to manually break your data up into packets and send them You have to make sure you don't send data too fast for your internet connection to handle If a packet is lost, you need to devise some way to detect this, and resend that data if necessary You can't even rely on the UDP checksum so you must add your own The decision seems pretty clear then, TCP does everything we want and its super easy to use, while UDP is a huge pain in the ass and we have to code everything ourselves from scratch. So obviously we just use TCP right? Wrong! Using TCP is the worst possible mistake you can make when developing a multiplayer game! To understand why, you need to see what TCP is actually doing above IP to make everything look so simple. How TCP really works TCP and UDP are both built on top of IP, but they are radically different. UDP behaves very much like the IP protocol underneath it, while TCP abstracts everything so it looks like you are reading and writing to a file, hiding all complexities of packets and unreliability from you. So how does it do this? Firstly, TCP is a stream protocol, so you just write bytes to a stream, and TCP makes sure that they get across to the other side. Since IP is built on packets, and TCP is built on top of IP, TCP must therefore break your stream of data up into packets. So, some internal TCP code queues up the data you send, then when enough data is pending the queue, it sends a packet to the other machine. This can be a problem for multiplayer games if you are sending very small packets. What can happen here is that TCP may decide it&rsquo;s not going to send data until you have buffered up enough data to make a reasonably sized packet to send over the network. This is a problem because you want your client player input to get to the server as quickly as possible, if it is delayed or &ldquo;clumped up&rdquo; like TCP can do with small packets, the client&rsquo;s user experience of the multiplayer game will be very poor. Game network updates will arrive late and infrequently, instead of on-time and frequently like we want. TCP has an option to fix this behavior called TCP_NODELAY. This option instructs TCP not to wait around until enough data is queued up, but to flush any data you write to it immediately. This is referred to as disabling Nagle&rsquo;s algorithm. Unfortunately, even if you set this option TCP still has serious problems for multiplayer games and it all stems from how TCP handles lost and out of order packets to present you with the &ldquo;illusion&rdquo; of a reliable, ordered stream of data. How TCP implements reliability Fundamentally TCP breaks down a stream of data into packets, sends these packets over unreliable IP, then takes the packets received on the other side and reconstructs the stream. But what happens when a packet is lost? What happens when packets arrive out of order or are duplicated? Without going too much into the details of how TCP works because its super-complicated (please refer to TCP/IP Illustrated) in essence TCP sends out a packet, waits a while until it detects that packet was lost because it didn&rsquo;t receive an ack (or acknowledgement), then resends the lost packet to the other machine. Duplicate packets are discarded on the receiver side, and out of order packets are resequenced so everything is reliable and in order. The problem is that if we were to send our time critical game data over TCP, whenever a packet is dropped it has to stop and wait for that data to be resent. Yes, even if more recent data arrives, that new data gets put in a queue, and you cannot access it until that lost packet has been retransmitted. How long does it take to resend the packet? Well, it&rsquo;s going to take at least round trip latency for TCP to work out that data needs to be resent, but commonly it takes 2*RTT, and another one way trip from the sender to the receiver for the resent packet to get there. So if you have a 125ms ping, you&rsquo;ll be waiting roughly 1/5th of a second for the packet data to be resent at best, and in worst case conditions you could be waiting up to half a second or more (consider what happens if the attempt to resend the packet fails to get through?). What happens if TCP decides the packet loss indicates network congestion and it backs off? Yes it actually does this. Fun times! Never use TCP for time critical data The problem with using TCP for realtime games like FPS is that unlike web browsers, or email or most other applications, these multiplayer games have a real time requirement on packet delivery. What this means is that for many parts of a game, for example player input and character positions, it really doesn&rsquo;t matter what happened a second ago, the game only cares about the most recent data. TCP was simply not designed with this in mind. Consider a very simple example of a multiplayer game, some sort of action game like a shooter. You want to network this in a very simple way. Every frame you send the input from the client to the server (eg. keypresses, mouse input controller input), and each frame the server processes the input from each player, updates the simulation, then sends the current position of game objects back to the client for rendering. So in our simple multiplayer game, whenever a packet is lost, everything has to stop and wait for that packet to be resent. On the client game objects stop receiving updates so they appear to be standing still, and on the server input stops getting through from the client, so the players cannot move or shoot. When the resent packet finally arrives, you receive this stale, out of date information that you don&rsquo;t even care about! Plus, there are packets backed up in queue waiting for the resend which arrive at same time, so you have to process all of these packets in one frame. Everything is clumped up! Unfortunately, there is nothing you can do to fix this behavior, it&rsquo;s just the fundamental nature of TCP. This is just what it takes to make the unreliable, packet-based internet look like a reliable-ordered stream. Thing is we don&rsquo;t want a reliable ordered stream. We want our data to get as quickly as possible from client to server without having to wait for lost data to be resent. This is why you should never use TCP when networking time-critical data! Wait? Why can&rsquo;t I use both UDP and TCP? For realtime game data like player input and state, only the most recent data is relevant, but for other types of data, say perhaps a sequence of commands sent from one machine to another, reliability and ordering can be very important. The temptation then is to use UDP for player input and state, and TCP for the reliable ordered data. If you&rsquo;re sharp you&rsquo;ve probably even worked out that you may have multiple &ldquo;streams&rdquo; of reliable ordered commands, maybe one about level loading, and another about AI. Perhaps you think to yourself, &ldquo;Well, I&rsquo;d really not want AI commands to stall out if a packet is lost containing a level loading command - they are completely unrelated!&rdquo;. You are right, so you may be tempted to create one TCP socket for each stream of commands. On the surface, this seems like a great idea. The problem is that since TCP and UDP are both built on top of IP, the underlying packets sent by each protocol will affect each other. Exactly how they affect each other is quite complicated and relates to how TCP performs reliability and flow control, but fundamentally you should remember that TCP tends to induce packet loss in UDP packets. For more information, read this paper on the subject. Also, it&rsquo;s pretty complicated to mix UDP and TCP. If you mix UDP and TCP you lose a certain amount of control. Maybe you can implement reliability in a more efficient way that TCP does, better suited to your needs? Even if you need reliable-ordered data, it&rsquo;s possible, provided that data is small relative to the available bandwidth to get that data across faster and more reliably that it would if you sent it over TCP. Plus, if you have to do NAT to enable home internet connections to talk to each other, having to do this NAT once for UDP and once for TCP (not even sure if this is possible&hellip;) is kind of painful. Conclusion My recommendation is not only that you use UDP, but that you only use UDP for your game protocol. Don&rsquo;t mix TCP and UDP! Instead, learn how to implement the specific features of TCP that you need inside your own custom UDP based protocol. Of course, it is no problem to use HTTP to talk to some RESTful services while your game is running. I&rsquo;m not saying you can&rsquo;t do that. A few TCP connections running while your game is running isn&rsquo;t going to bring everything down. The point is, don&rsquo;t split your game protocol across UDP and TCP. Keep your game protocol running over UDP so you are fully in control of the data you send and receive and how reliability, ordering and congestion avoidance are implemented. The rest of this article series show you how to do this, from creating your own virtual connection on top of UDP, to creating your own reliability, flow control and congestion avoidance. 译文译文出处 $hhd$2 style="margin: 11.25pt 0cm 1.5pt; background-image: initial; background-attachment: initial; background-size: initial; background-origin: initial; background-clip: initial; background-position: initial; background-repeat: initial;">翻译：削微寒 审校：削微寒$hhd$2 style="margin: 11.25pt 0cm 1.5pt; background-image: initial; background-attachment: initial; background-size: initial; background-origin: initial; background-clip: initial; background-position: initial; background-repeat: initial;">介绍你一定听说过sokcet(初探socket)，它分为两种常用类型：TCP和UDP。当要写一个网络游戏，我们首先要选择使用哪种类型的socket。是用TCP、UDP还是两者都用？选择哪种类型，完全取决于你要写的游戏的类型。后面的文章，我都将假设你要写一个‘动作’网游。就像：光环系列，战地1942，雷神之锤，这些游戏。我们将非常仔细的分析这两种socket类型的优劣，并且深入到底层，弄清楚互联网是如何工作的什么。当我们弄清楚这些信息后，就很容易做出正确的选择了。 $hhd$2 style="margin: 11.25pt 0cm 1.5pt; background-image: initial; background-attachment: initial; background-size: initial; background-origin: initial; background-clip: initial; background-position: initial; background-repeat: initial;">TCP/IPTCP代表“传输控制协议”，IP代表：“互联网协议”，你在互联网上做任何事情，都是建立在这两者的基础上，比如：浏览网页、收发邮件等等。 $hhd$4 style="margin: 11.25pt 0cm 1.5pt; background-image: initial; background-attachment: initial; background-size: initial; background-origin: initial; background-clip: initial; background-position: initial; background-repeat: initial;">TCP如果你曾经用过TCP socket，你肯定知道它是可靠连接的协议，面向连接的传输协议。简单的说：两台机器先建立起连接，然后两台机器相互发送数据，就像你在一台计算机上写文件，在另外一个台读文件一样。（我是这么理解的：TCP socket就像建立起连接的计算机，之间共享的一个'文件'对象，两者通过读写这个'文件'实现数据的传输）这个连接是可靠的、有序的，代表着：发送的所有的数据，保证到达传输的另一端的时候。另一端得到的数据，和发送数据一摸一样（可靠，有序。例如：A发送数据‘abc’，通过TCPsocket传输数据到B，B得到数据一定是：‘abc’。而不是‘bca’或者‘xueweihan’之类的鬼！）。传输的数据是‘数据流’的形式(数据流：用于操作数据集合的最小的有序单位，与操作本地文件中的stream一样。所以TCP socket和文件对象很像)，也就是说：TCP把你的数据拆分后，包装成数据包，然后通过网络发送出去。注意：就像读写文件那样，这样比较好理解。 $hhd$4 style="margin: 11.25pt 0cm 1.5pt; background-image: initial; background-attachment: initial; background-size: initial; background-origin: initial; background-clip: initial; background-position: initial; background-repeat: initial;">IP“IP”协议是在TCP协议的下面（这个牵扯到七层互联网协议栈，我就简单的贴个图不做详细的介绍）“IP”协议是没有连接的概念，它做的只是把上一层(传输层)的数据包从一个计算传递到下一个计算机。你可以理解成：这个过程就像一堆人手递手传递纸条一样，传递了很多次，最终到达纸条上标记的xxx手里（纸条上写着‘xxx亲启，偷看者3cm’）。在传递的过程中，不保证这个纸条(信件)能能够准确的送到收信人的手上。发信人发送信件，但是永远不知道信件是否可以准确到达收件人的手上，除非收件人回信告诉他（发信人）：“兄弟我收到信了！”（IP层只是用于传递信息，并不做信息的校验等其它操作）当然，传递信息的这个过程还是还是很复杂的。因为，不知道具体的传递次序，也就是说，因为不知道最优的传递路线（能够让数据包快速的到达目的地的最优路径）所以，有些时候“IP”协议就传递多份一样的数据，这些数据通过不同的路线到达目的地，从而发现最优的传递路线。这就是互联网设计中的：自动优化和自动修复，解决了连接的问题。这真的是一个很酷的设计，如果你想知道更多的底层实现，可以阅读关于TCP/IP的书。（推荐上野宣的图解系列) $hhd$4 style="margin: 11.25pt 0cm 1.5pt; background-image: initial; background-attachment: initial; background-size: initial; background-origin: initial; background-clip: initial; background-position: initial; background-repeat: initial;">UDP如果我们想要直接发送和接受数据包，那么就要使用另一种socket。我们叫它UDP。UDP代表“用户数据包协议”，它是另外一种建立在IP协议之上的协议，就像TCP一样，但是没有TCP那么多功能（例如：建立连接，信息的校验，数据流的拆分合并等）使用UDP我们能够向目标IP和端口（例如80），发送数据包。数据包会达到目标计算机或者丢失。收件人（目标计算机），我们只需要监听具体的端口（例如：80），当从任意一台计算机（注意：UDP是不建立连接的）接受到数据包后，我们会得知发送数据包的计算机地址（IP地址）和端口、数据包的大小、内容。UDP是不可靠协议。现实使用的过程中，发送的大多数的数据包都会被接收到，但是通常会丢失1-5%，偶尔，有的时候还可能啥都接收不到（数据包全部丢失一个都没接收到，传递数据的计算机之间的计算机的数量越多，出错的概率越大）。UDP协议中的数据包也是没有顺序的。比如：你发送5个包，顺序是1，2，3，4，5。但是，即接收到的顺序可能是3，1，4，2，5。现实使用的过程中，大多时候，接收到的数据的顺序是正确的，但是并不是每次都是这样。最后，尽管UDP并没有比“IP”协议高级多少，而且不可靠。但是你发送的数据，要么全部到达，要么全部丢失。比如：你发送一个大小为256 byte的数据包给另外一台计算机，这台计算机不会只接收到100 byte的数据包，它只可能接收到256 byte的数据包，或者什么都没接收到。这是UDP唯一可以保证的事情，其它所有的事情都需要你来决定（我的理解，UDP协议只是个简单的传输协议，只保证数据包的完整性，注意是数据包而不是信息。其他的事情需要自己去做，完善这个协议，达到自己使用的需求。） $hhd$2 style="margin: 11.25pt 0cm 1.5pt; background-image: initial; background-attachment: initial; background-size: initial; background-origin: initial; background-clip: initial; background-position: initial; background-repeat: initial;">TCP vs. UDP我们如何选择是使用TCP socket还是UDPsocket呢？我们先看看两者的特征吧：TCP：· 面向连接· 可靠、有序· 自动把数据拆分成数据包· 确保数据的发送一直在控制中（流量控制）· 使用简单，就像读写文件一样UDP：· 没有连接的概念，你需要自己通过代码实现（这个我也没自己实现过，应该还会讲）· 不可靠，数据包无序，数据包可能无序，重复，或者丢失· 你需要手动地把数据拆分成数据包，然后发送数据包· 你需要自己做流量控制· 如果数据包太多，你需要设计重发和统计机制通过上面的描述，不难发现：TCP做了所有我们想做的事情，而且使用十分简单。反观UDP就十分难用了，我们需要自己编写设计一切。很显然，我们只要用TCP就好了！不，你想的简单了（原来，是我太年轻了！）当你开发一个像上面说过的FPS（动作网游）的时候使用TCP协议，会是一个错误的决定，这个TCP协议就不好用了！为什么这么说？那么你就需要知道TCP到底做了什么，使得一起看起来十分简单。（让我们继续往下看，这是我最好奇的地方！！！有没有兴奋起来？）$hhd$4 style="margin: 11.25pt 0cm 1.5pt; background-image: initial; background-attachment: initial; background-size: initial; background-origin: initial; background-clip: initial; background-position: initial; background-repeat: initial;">TCP内部的工作原理TCP和UDP都是建立在“IP”协议上的，但是它俩完全不同。UDP和“IP”协议很像，然而TCP隐藏了数据包的所有的复杂和不可靠的部分，抽象成了类似文件的对象。那么TCP是如何做到这一点呢？首先，TCP是一个数据流的协议，所以你只需要把输入的内容变成数据流，然后TCP协议就会确保数据会到达发送的目的地。因为“IP”协议是通过数据包传递信息，TCP是建立在“IP”协议之上，所以TCP必须把用户输入的数据流分成数据包的形式。TCP协议会对需要发送的数据进行排队，然后当有足够的排除数据的时候，就发送数据包到目标计算机。 当在多人在线的网络游戏中发送非常小的数据包的时候，这样做就有一个问题。这个时候会发生什么？如果数据没有达到缓冲区设定的数值，数据包是不会发送的。这就会出现个问题：因为客户端的用户输入请求后，需要尽快的从服务器得到响应，如果像上面TCP 等待缓冲区满后才发送的话，就会出现延时，那么客户端的用户体验就会非常差！网络游戏几乎不能出现延时，我们希望看到的是“实时”和流畅。TCP有一个选项可以修复，上面说的那种等待缓冲区满才发送的情况，就是TCP_NODELAY。这个选项使得TCP socket不需要等待缓冲区满才发送，而是输入数据后就立即发送。然而，即使你已经设置了TCP_NODELAY选项，在多人网游中还是会有一系列的问题。这一切的源头都由于TCP处理丢包和乱序包的方式。使得你产生有序和可靠的“错觉”。$hhd$4 style="margin: 11.25pt 0cm 1.5pt; background-image: initial; background-attachment: initial; background-size: initial; background-origin: initial; background-clip: initial; background-position: initial; background-repeat: initial;">TCP如何保证数据的可靠性本质上TCP做的事情，分解数据流，成为数据包，使用在不可靠的“IP”协议，发送这些数据包。然后使得数据包到达目标计算机，然后重组成数据流。但是，如何处理当丢包？如何处理重复的数据包和乱序数据包？这里不会介绍TCP处理这些事情的细节，因为这些都是非常复杂的（想弄清楚的同学可以看我上面推荐的书单），大体上：TCP发送一个数据包，等待一段时间，直到检测到数据包丢失了，因为没有接收到它的ACK（一种传输类控制符号，用于确认接收无误），接下来就重新发送丢失的数据包到目标计算机。重复的数据包将被丢弃在接收端，乱序的数据包将被重新排序。所以保证了数据包的可靠性和有序性。如果我们用TCP实现数据的实时传输，就会出现一个问题：TCP无论什么情况，只要数据包出错，就必须等待数据包的重发。也就是说，即使最新的数据已经到达，但还是不能访问这些数据包，新到的数据会被放在一个队列中，需要等待丢失的包重新发过来之后，所有数据没有丢失才可以访问。需要等待多长时间才能重新发送数据包？举个例子：如果的延时是125ms，那么需要最好的情况下重发数据包需要250ms，但是如果遇到糟糕的情况，将会等待500ms以上，比如：网络堵塞等情况。那就没救了。。。 $hhd$4 style="margin: 11.25pt 0cm 1.5pt; background-image: initial; background-attachment: initial; background-size: initial; background-origin: initial; background-clip: initial; background-position: initial; background-repeat: initial;">为什么TCP不应该用于对网络延时要求极高的条件下如果FPS（第一人称射击）这类的网络游戏使用TCP就出现问题，但是web浏览器、邮箱、大多数应用就没问题，因为多人网络游戏有实时性的要求。比如：玩家输入角色的位置，重要的不是前一秒发生了什么，而是最新的情况！TCP并没有考虑这类需求，它并不是为这种需求而设计的。这里举一个简单的多人网游的例子，比如射击的游戏。对网络的要求很简单。玩家通过客户端发送给服务器的每个场景（用鼠标和键盘输入的行走的位置），服务器处理每个用户发送过来的所有场景，处理完再返回给客户端，客户端解析响应，渲染最新的场景展示给玩家。在上面说的哪个多人游戏的例子中，如果出现一个数据包丢失，所有事情都需要停下来等待这个数据包重发。客户端会出现等待接收数据，所以玩家操作的任务就会出现站着不动的情况（卡！卡！卡！），不能射击也不能移动。当重发的数据包到达后，你接收到这个过时的数据包，然而玩家并不关心过期的数据（激战中，卡了1秒，等能动了，都已经死了）不幸的是，没有办法修复TCP的这个问题，这是它本质的东西，没办法修复。这就是TCP如何做到让不可靠，无序的数据包，看起来像有序，可靠的数据流。我并不需要可靠，有序的数据流，我们希望的是客户端和服务端之间的延时越低越好，不需要等待重发丢失的包。所以，这就是为什么在对数据的实时性要求的下，我们不用TCP。 $hhd$4 style="margin: 11.25pt 0cm 1.5pt; background-image: initial; background-attachment: initial; background-size: initial; background-origin: initial; background-clip: initial; background-position: initial; background-repeat: initial;">那为什么不UDP和TCP一起用呢？像玩家输入实时游戏数据和状态的变更，只和最新的数据有关（这些数据强调实时性）。但是另外的一些数据，例如，从一台计算机发送给另外一个台计算机的一些列指令（交易请求，聊天？），可靠、有序的传输还是非常重要的！那么，用户输入和状态用UDP，TCP用于可靠、有序的数据传输，看起来是个不错的点子。但是，问题在于TCP和UDP都是建立“IP”协议之上，所以协议之间都是发送数据包，从而相互通信。协议之间的互相影响是相当复杂的，涉及到TCP性能、可靠性和流量控制。简而言之，TCP会导致UDP丢包，请参考这篇论文此外，UDP和TCP混合使用是非常复杂的，而且实现起来是非常痛苦的。（这段我就不翻译了，总而言之：不要混用UDP和TCP，容易失去对传输数据的控制） $hhd$2 style="margin: 11.25pt 0cm 1.5pt; background-image: initial; background-attachment: initial; background-size: initial; background-origin: initial; background-clip: initial; background-position: initial; background-repeat: initial;">总结我的建议并不是就一定要使用UDP，但是UDP协议应该用于游戏。请不要混合使用TCP和UDP，你应该学习TCP中一些地方是如何实现的技巧，然后可以把这些技巧用在UDP上，从而实现适合你的需求的协议（借鉴TCP中的实现，在UDP上，完善功能，从而达到你的需求）。 这个系列，接下来会讲到：如何在UDP上创建一个虚拟的连接（因为UDP本身，是没有连接的概念的）、如何使得UDP实现可靠性，流量控制，非阻塞。 $hhd$2 style="margin: 11.25pt 0cm 1.5pt; background-image: initial; background-attachment: initial; background-size: initial; background-origin: initial; background-clip: initial; background-position: initial; background-repeat: initial;">参考· MBA lib数据流· WiKi TCP/IP协议族· W3SchoolTCP/IP 协议· UDP和TCP的区]]></content>
      <categories>
        <category>GS</category>
      </categories>
      <tags>
        <tag>TCP</tag>
        <tag>GafferOnGames</tag>
        <tag>UDP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[快速编译技巧]]></title>
    <url>%2Fblog%2F2016%2F11%2F01%2F%E5%BF%AB%E9%80%9F%E7%BC%96%E8%AF%91%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[项目越来越大，每次需要重新编译整个项目都是一件很浪费时间的事情。Research了一下，找到以下可以帮助提高速度的方法，总结一下。 tmpfs 有人说在Windows下用了RAMDisk把一个项目编译时间从4. 5小时减少到了5分钟，也许这个数字是有点夸张了，不过粗想想，把文件放到内存上做编译应该是比在磁盘 上快多了吧，尤其如果编译器需要生成很多临时文件的话。 这个做法的实现成本最低，在Linux中，直接mount一个tmpfs就可以了。而且对所编译的工程没有任何要求，也不用改动编译环境。 mount -t tmpfs tmpfs ~/build -o size=1G 用2.6.32.2的Linux Kernel来测试一下编译速度： 用物理磁盘：40分16秒 用tmpfs：39分56秒 呃……没什么变化。看来编译慢很大程度上瓶颈并不在IO上面。但对于一个实际项目来说， 编译过程中可能还会有打包等IO密集的操作，所以只要可能，用tmpfs是有 益无害的。 当然对于大项目来说，你需要有足够的内存才能负担得起这个tmpfs的开销。 make -j 既然IO不是瓶颈，那CPU就应该是一个影响编译速度的重要因素了。 用make -j带一个参数，可以把项目在进行并行编译，比如在一台双核的机器上，完全可以用make -j4，让make最多允许4个编译命令同时执行，这样可以更有效的利用CPU资源。 还是用Kernel来测试： 用make： 40分16秒 用make -j4：23分16秒 用make -j8：22分59秒 由此看来，在多核CPU上，适当的进行并行编译还是可以明显提高编译速度的。但并行的任务不宜太多，一般是以CPU的核心数目的两倍为宜。 不过这个方案不是完全没有cost的，如果项目的Makefile不规范，没有正确的设置好依赖关系，并行编译的结果就是编译不能正常进行。如果依赖关系设置过于保守，则可能本身编译的可并行度就下降了，也不能取得最佳的效果。 ccache ccache用于把编译的中间结果进行缓存，以便在再次编译的时候可以节省时间。这对于玩Kernel来说实在是再好不过了，因为经常需要修改一些Kernel的代码，然后再重新编译，而这两次编译大部分东西可能都没有发生变化。对于平时开发项目来说，也是一样。为什么不是直接用make所支持的增量编译呢？还是因为现实中，因为Makefile的不规范，很可能这种“聪明”的方案根本不能正常工作，只有每次make clean再make才行。 安装完ccache后，可以在/usr/local/bin下建立gcc，g++，c++，cc的symboliclink，链到/usr/bin/ccache上。总之确认系统在调用gcc等命令时会调用到ccache就可以了（通常情况下/usr/local/bin会在PATH中排在/usr/bin前面）。 继续测试： 用ccache的第一次编译(make -j4)：23分38秒 用ccache的第二次编译(make -j4)：8分48秒 用ccache的第三次编译(修改若干配置，make -j4)：23分48秒 看来修改配置（我改了CPU类型…）对ccache的影响是很大的，因为基本头文件发生变化后，就导致所有缓存数据都无效了，必须重头来做。但如果只是修改一些.c文件的代码，ccache的效果还是相当明显的。而且使用ccache对项目没有特别的依赖，布署成本很低，这在日常工作中很实用。 可以用ccache -s来查看cache的使用和命中情况： cache directory /home/lifanxi/.ccache cache hit 7165 cache miss 14283 called for link 71 not a C/C++ file 120 no input file 3045 files in cache 28566 cache size 81.7 Mbytes max cache size 976.6 Mbytes 可以看到，显然只有第二编次译时cache命中了，cache miss是第一次和第三次编译带来的。两次cache占用了81.7M的磁盘，还是完全可以接受的。 distcc 一台机器的能力有限，可以联合多台电脑一起来编译。这在公司的日常开发中也是可行的，因为可能每个开发人员都有自己的开发编译环境，它们的编译器版本一般是一致的，公司的网络也通常具有较好的性能。这时就是distcc大显身手的时候了。 使用distcc，并不像想象中那样要求每台电脑都具有完全一致的环境，它只要求源代码可以用make -j并行编译，并且参与分布式编译的电脑系统中具有相同的编译器。因为它的原理只是把预处理好的源文件分发到多台计算机上，预处理、编译后的目标文件的链接和其它除编译以外的工作仍然是在发起编译的主控电脑上完成，所以只要求发起编译的那台机器具备一套完整的编译环境就可以了。 distcc安装后，可以启动一下它的服务： /usr/bin/distccd –daemon –allow 10.64.0.0/16 默认的3632端口允许来自同一个网络的distcc连接。 然后设置一下DISTCC_HOSTS环境变量，设置可以参与编译的机器列表。 通常localhost也参与编译，但如果可以参与编译的机器很多，则可以把localhost从这个列表 中去掉，这样本机就完全只是进行预处理、分发和链接了，编译都在别的机器上完成。 因为机器很多时，localhost的处理负担很重，所以它就不再“兼职”编译了。 export DISTCC_HOSTS=&quot;localhost 10.64.25.1 10.64.25.2 10.64.25.3&quot; 然后与ccache类似把g++，gcc等常用的命令链接到/usr/bin/distcc上就可以了。 在make的时候，也必须用-j参数，一般是参数可以用所有参用编译的计算机CPU内核总数的两倍做为并行的任务数。 同样测试一下： 一台双核计算机，make -j4：23分16秒 两台双核计算机，make -j4：16分40秒 两台双核计算机，make -j8：15分49秒 跟最开始用一台双核时的23分钟相比，还是快了不少的。如果有更多的计算机加入，也可以得到更好的效果。 在编译过程中可以用distccmon-text来查看编译任务的分配情况。distcc也可以与ccache同时使用，通过设置一个环境变量就可以做到，非常方便。 总结 tmpfs： 解决IO瓶颈，充分利用本机内存资源 make -j： 充分利用本机计算资源 distcc： 利用多台计算机资源 ccache： 减少重复编译相同代码的时间 这些工具的好处都在于布署的成本相对较低，综合利用这些工具，就可以轻轻松松的节省相当可观的时间。 上面介绍的都是这些工具最基本的用法，更多的用法可以参考它们各自的man page。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Compile</tag>
        <tag>Make</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一个基于虚幻4群聚鱼群AI插件]]></title>
    <url>%2Fblog%2F2016%2F10%2F18%2Fa_fish_flock_ai_plugin_for_ue4%2F</url>
    <content type="text"><![CDATA[A fish flock AI Plugin for Unreal Engine 4一个基于虚幻4的鱼群 AI 插件 this Plugin version can Run 2000+ fishes at the same time这个插件版本可以同时运行 2000+ 条鱼儿 源码已放到fish Video Preview 视频演示 DownloadMyFish.exe (Win64) . . . This is packaged by an unoptimized version( check out branch old_demo)下载一个打包好的试玩看看, 这个包是没有经过优化过的版本打包出来的( 是用old_demo分支的版本打包的 ) How to play VR : (My Device is HTC Vive) Motion Controller FaceButton1 =&gt; Move forward手柄圆盘上键 =&gt; 往前移动 PC’s KeyBoard Arrow UP and Down =&gt; Move faster or slower电脑键盘的上下箭头键 =&gt; 调整移动速度 Hold Motion Controller Trigger Down =&gt; Attract fishes按住手柄扳机键 =&gt; 吸引鱼群 PC : EQ =&gt; Up &amp; DownEQ 键 =&gt; 上下 WASD =&gt; Basic movementWASD 键 =&gt; 基本的移动指令(前后左右) Hold Left Mouse Button Down =&gt; Attract fishes按住鼠标左键 =&gt; 吸引鱼群 Arrow UP and Down =&gt; Move faster or slower上下箭头键 =&gt; 调整移动速度 How to useplace Plugins folder in your project root directory, then just like把Plugins文件夹放在你项目的根目录, 接下来如图 About This Unreal Engine Version 4.15 Read Craig Reynolds’s thesis查看 Craig Reynolds的论文 This project implements a new flocking Ai algorithm, with 3 components : 算法简要 Separation : every fish will try to steer away from their neighbors分离性 ：每条鱼都会与周围的鱼保持距离 Following the leader : every fish will try to follow its leader跟随一个领头者 ： 每条鱼都会跟随一个领头者 Avoiding enemies.躲避敌人]]></content>
      <categories>
        <category>GitHub</category>
      </categories>
      <tags>
        <tag>CPP</tag>
        <tag>GitHub</tag>
        <tag>AI</tag>
        <tag>UE4</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[XXTEA的python实现]]></title>
    <url>%2Fblog%2F2016%2F09%2F13%2FXXTEA%E7%9A%84python%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[在数据的加解密领域，算法分为对称密钥与非对称密钥两种。 对称密钥与非对称密钥由于各自的特点，所应用的领域是不尽相同的。 对称密钥加密算法由于其速度快，一般用于整体数据的加密，而非对称密钥加密算法的安全性能佳，在数字签名领域得到广泛的应用。 微型加密算法（TEA）及其相关变种（XTEA，Block TEA，XXTEA） 都是分组加密算法，它们很容易被描述，实现也很简单（典型的几行代码）。 TEA是Tiny Encryption Algorithm的缩写，以加密解密速度快，实现简单著称。 TEA 算法最初是由剑桥计算机实验室的 David Wheeler 和 Roger Needham 在 1994 年设计的。 该算法使用 128 位的密钥为 64 位的信息块进行加密，它需要进行 64 轮迭代，尽管作者认为 32 轮已经足够了。 该算法使用了一个神秘常数δ作为倍数，它来源于黄金比率，以保证每一轮加密都不相同。 但δ的精确值似乎并不重要，这里 TEA 把它定义为 δ=「(√5 - 1)231」（也就是程序中的 0×9E3779B9）。 之后 TEA 算法被发现存在缺陷，作为回应，设计者提出了一个 TEA 的升级版本——XTEA（有时也被称为“tean”）。 XTEA 跟 TEA 使用了相同的简单运算，但它采用了截然不同的顺序，为了阻止密钥表攻击，四个子密钥（在加密过程中，原 128 位的密钥被拆分为 4 个 32 位的子密钥）采用了一种不太正规的方式进行混合，但速度更慢了。 在跟描述 XTEA 算法的同一份报告中，还介绍了另外一种被称为 Block TEA 算法的变种，它可以对 32 位大小任意倍数的变量块进行操作。 该算法将 XTEA 轮循函数依次应用于块中的每个字，并且将它附加于它的邻字。 该操作重复多少轮依赖于块的大小，但至少需要 6 轮。 该方法的优势在于它无需操作模式（CBC，OFB，CFB 等），密钥可直接用于信息。 对于长的信息它可能比 XTEA 更有效率。 在 1998 年，Markku-Juhani Saarinen 给出了一个可有效攻击 Block TEA 算法的代码，但之后很快 David J. Wheeler 和 Roger M. Needham 就给出了 Block TEA 算法的修订版，这个算法被称为 XXTEA。 XXTEA 使用跟 Block TEA 相似的结构，但在处理块中每个字时利用了相邻字。 它利用一个更复杂的 MX 函数代替了 XTEA 轮循函数，MX 使用 2 个输入量。 XXTEA 算法很安全，而且非常快速，非常适合应用于 Web 开发中。 TEA算法是由剑桥大学计算机实验室的David Wheeler和Roger Needham于1994年发明， TEA是Tiny Encryption Algorithm的缩写，以加密解密速度快，实现简单著称。 TEA算法每一次可以操作64bit(8byte)，采用128bit(16byte)作为key，算法采用迭代的形式，推荐的迭代轮数是64轮，最少32轮。 为解决TEA算法密钥表攻击的问题，TEA算法先后经历了几次改进，从XTEA到BLOCK TEA，直至最新的XXTEA。 XTEA也称做TEAN，它使用与TEA相同的简单运算，但四个子密钥采取不正规的方式进行混合以阻止密钥表攻击。 Block TEA算法可以对32位的任意整数倍长度的变量块进行加解密的操作，该算法将XTEA轮循函数依次应用于块中的每个字，并且将它附加于被应用字的邻字。 XXTEA使用跟Block TEA相似的结构，但在处理块中每个字时利用了相邻字，且用拥有两个输入量的MX函数代替了XTEA轮循函数。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667import struct _DELTA = 0x9E3779B9 def _long2str(v, w): n = (len(v) - 1) &lt;&lt; 2 if w: m = v[-1] if (m &lt; n - 3) or (m &gt; n): return '' n = m s = struct.pack('&lt;%iL' % len(v), *v) return s[0:n] if w else s def _str2long(s, w): n = len(s) m = (4 - (n &amp; 3) &amp; 3) + n s = s.ljust(m, "\0") v = list(struct.unpack('&lt;%iL' % (m &gt;&gt; 2), s)) if w: v.append(n) return v def encrypt(str, key): if str == '': return str v = _str2long(str, True) k = _str2long(key.ljust(16, "\0"), False) n = len(v) - 1 z = v[n] y = v[0] sum = 0 q = 6 + 52 // (n + 1) while q &gt; 0: sum = (sum + _DELTA) &amp; 0xffffffff e = sum &gt;&gt; 2 &amp; 3 for p in xrange(n): y = v[p + 1] v[p] = (v[p] + ((z &gt;&gt; 5 ^ y &lt;&lt; 2) + (y &gt;&gt; 3 ^ z &lt;&lt; 4) ^ (sum ^ y) + (k[p &amp; 3 ^ e] ^ z))) &amp; 0xffffffff z = v[p] y = v[0] v[n] = (v[n] + ((z &gt;&gt; 5 ^ y &lt;&lt; 2) + (y &gt;&gt; 3 ^ z &lt;&lt; 4) ^ (sum ^ y) + (k[n &amp; 3 ^ e] ^ z))) &amp; 0xffffffff z = v[n] q -= 1 return _long2str(v, False) def decrypt(str, key): if str == '': return str v = _str2long(str, False) k = _str2long(key.ljust(16, "\0"), False) n = len(v) - 1 z = v[n] y = v[0] q = 6 + 52 // (n + 1) sum = (q * _DELTA) &amp; 0xffffffff while (sum != 0): e = sum &gt;&gt; 2 &amp; 3 for p in xrange(n, 0, -1): z = v[p - 1] v[p] = (v[p] - ((z &gt;&gt; 5 ^ y &lt;&lt; 2) + (y &gt;&gt; 3 ^ z &lt;&lt; 4) ^ (sum ^ y) + (k[p &amp; 3 ^ e] ^ z))) &amp; 0xffffffff y = v[p] z = v[n] v[0] = (v[0] - ((z &gt;&gt; 5 ^ y &lt;&lt; 2) + (y &gt;&gt; 3 ^ z &lt;&lt; 4) ^ (sum ^ y) + (k[0 &amp; 3 ^ e] ^ z))) &amp; 0xffffffff y = v[0] sum = (sum - _DELTA) &amp; 0xffffffff return _long2str(v, True) if __name__ == "__main__": print decrypt(encrypt('Hello XXTEA!', '16bytelongstring'), '16bytelongstring')]]></content>
      <categories>
        <category>Script</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>XXTEA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[内零头和外零头]]></title>
    <url>%2Fblog%2F2016%2F09%2F12%2F%E5%86%85%E9%9B%B6%E5%A4%B4%E5%92%8C%E5%A4%96%E9%9B%B6%E5%A4%B4%2F</url>
    <content type="text"><![CDATA[问题:在内存管理中，“内零头”和“外零头”个指的是什么？在固定式分区分配、可变式分区分配、页式虚拟存储系统、段式虚拟存储系统中，各会存在何种零头？为什么？ 解答： 在存储管理中， 内零头是指分配给作业的存储空间中未被利用的部分， 外零头是指系统中无法利用的小存储块。 在固定式分区分配中，为将一个用户作业装入内存，内存分配程序从系统分区表中找出一个能满足作业要求的空闲分区分配给作业，由于一个作业的大小并不一定与分区大小相等，因此，分区中有一部分存储空间浪费掉了。 由此可知，固定式分区分配中存在内零头。 在可变式分区分配中，为把一个作业装入内存，应按照一定的分配算法从系统中找出一个能满足作业需求的空闲分区分配给作业，如果这个空闲分区的容量比作业申 请的空间容量要大，则将该分区一分为二，一部分分配给作业，剩下的部分仍然留作系统的空闲分区。 由此可知，可变式分区分配中存在外零头。 在页式虚拟存储系统中，用户作业的地址空间被划分成若干大小相等的页面，存储空间也分成也页大小相等的物理块，但一般情况下，作业的大小不可能都是物理块大小的整数倍，因此作业的最后一页中仍有部分空间被浪费掉了。 由此可知，页式虚拟存储系统中存在内零头。 在段式虚拟存储系统中，作业的地址空间由若干个逻辑分段组成，每段分配一个连续的内存区，但各段之间不要求连续，其内存的分配方式类似于动态分区分配。 由此可知，段式虚拟存储系统中存在外零头。 详细解释 操作系统在分配内存时，有时候会产生一些空闲但是无法被正常使用的内存区域，这些就是内存碎片，或者称为内存零头，这些内存零头一共分为两类：内零头和外零头。 内零头是指进程在向操作系统请求内存分配时，系统满足了进程所需要的内存需求后，还额外还多分了一些内存给该进程，也就是说额外多出来的这部分内存归该进程所有，其他进程是无法访问的。 外零头是指内存中存在着一些空闲的内存区域，这些内存区域虽然不归任何进程所有，但是因为内存区域太小，无法满足其他进程所申请的内存大小而形成的内存零头。 页式存储管理的情况页式存储管理是以页为单位（页面的大小由系统确定，且大小是固定的）向进程分配内存的， 例如：假设内存总共有100K,分为10页，每页大小为10K。现在进程A提出申请56K内存，因为页式存储管理是以页为单位进程内存分配的，所以系统会向进程A提供6个页面，也就是60K的内存空间，那么在最后一页中进程只使用了6K，从而多出了4K的内存碎片，但是这4K的内存碎片系统已经分配给进程A了，其他进程是无法再访问这些内存区域的， 这种内存碎片就是内零头。 段式存储管理的情况段式存储管理是段（段的大小是程序逻辑确定，且大小不是固定的）为单位向进程进行内存分配的，进程申请多少内存，系统就给进程分配多少内存，这样就不会产生内零头，但是段式分配会产生外零头。 例如：假设内存总的大小为100K，现在进程A向系统申请60K的内存，系统在满足了进程A的内存申请要求后，还剩下40K的空闲内存区域；这时如果进程B向系统申请50K的内存区域，而系统只剩下了40K的内存区域，虽然这40K的内存区域不归任何进程所有，但是因为大小无法满足进程B的要求，所以也无法分配给进程B，这样就产生了外零头。 请求段式存储管理是在段式存储管理的基础上增加了请求调段功能和段置换功能。所以段式和请求段式存储管理会产生外零头 练习题下面的内存管理模式中，会产生外零头的是(正确答案B, D) A、页式B、段式C、请求页式D、请求段式]]></content>
      <categories>
        <category>Misc</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[PHP的魔术]]></title>
    <url>%2Fblog%2F2016%2F09%2F01%2FPHP%E7%9A%84%E9%AD%94%E6%9C%AF%2F</url>
    <content type="text"><![CDATA[PHP 将所有以 __（两个下划线）开头的类方法保留为魔术方法。所以在定义类方法时，除了上述魔术方法，建议不要以 __ 为前缀。 . . . PHP魔幻（术）变量1234567- __LINE__ 文件中的当前行号。- __FILE__ 文件的完整路径和文件名。如果用在被包含文件中，则返回被包含的文件名。- __DIR__ 文件所在的目录。如果用在被包括文件中，则返回被包括的文件所在的目录。 它等价于 dirname(__FILE__)。- __FUNCTION__ 本常量返回该函数被定义时的名字（区分大小写）- __CLASS__ 本常量返回该类被定义时的名字（区分大小写） PHP魔幻（术）方法 __construct() 实例化类时自动调用。 __destruct() 类对象使用结束时自动调用。 __set() 在给未定义的属性赋值的时候调用。 __get() 调用未定义的属性时候调用。 __isset() 使用isset()或empty()函数时候会调用。 __unset() 使用unset()时候会调用。 __sleep() 使用serialize序列化时候调用。 __wakeup() 使用unserialize反序列化的时候调用。 __call() 调用一个不存在的方法的时候调用。 __callStatic()调用一个不存在的静态方法是调用。 __toString() 把对象转换成字符串的时候会调用。比如 echo。 __invoke() 当尝试把对象当方法调用时调用。 __set_state() 当使用var_export()函数时候调用。接受一个数组参数。 __clone() 当使用clone复制一个对象时候调用。]]></content>
      <categories>
        <category>Script</category>
      </categories>
      <tags>
        <tag>PHP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[boost之program_options]]></title>
    <url>%2Fblog%2F2016%2F08%2F15%2Fboost%E4%B9%8Bprogram_options%2F</url>
    <content type="text"><![CDATA[介绍命令行接口是普遍,基础的人机交互接口，从命令行提取程序的运行时选项的方法有很多。 你可以自己编写相对应的完整的解析函数，或许你有丰富的C语言编程经验，熟知getopt()函数的用法，又或许使用Python的你已经在使用optparse库来简化这一工作。 大家在平时不断地谈及到“不要重复造轮子”，那就需要掌握一些顺手的库，这里介绍一种C++方式来解析命令行选项的方法，就是使用Boost.Program_options库。 program_options提供程序员一种方便的命令行和配置文件进行程序选项设置的方法。 使用program_options库而不是你自己动手写相关的解析代码，因为它更简单，声明程序选项的语法简洁，并且库自身也非常小。 将选项值转换为适合的类型值的工作也都能自动完成。 库有着完备的错误检查机制，如果自己手写解析代码时，就可能会错过对一些出错情况的检查了。 最后，选项值不仅能从命令行获取，也能从配置文件，甚至于环境变量中提取，而这些选择不会增加明显的工作量。 示例说明以下面简单的hello程序进行说明，默认打印hello world,如果传入-p选项，就会打印出人的姓名，另外通过传入-h选项，可以打印出帮助选项。 略微看一眼代码文件和相应的屏幕输入输出，然后我们再一起来看看这些是如何发生的。 123456789101112131415161718192021222324252627//hello.cpp #include &lt;iostream&gt;#include &lt;string&gt;#include &lt;boost/program_options.hpp&gt;using namespace std;int main(int argc, char* argv[])&#123; using namespace boost::program_options; //声明需要的选项 options_description desc(&quot;Allowed options&quot;); desc.add_options() (&quot;help,h&quot;, &quot;produce help message&quot;) (&quot;person,p&quot;, value&lt;string&gt;()-&gt;default_value(&quot;world&quot;), &quot;who&quot;) ; variables_map vm; store(parse_command_line(argc, argv, desc), vm); notify(vm); if (vm.count(&quot;help&quot;)) &#123; cout &lt;&lt; desc; return 0; &#125; cout &lt;&lt; &quot;Hello &quot; &lt;&lt; vm[&quot;person&quot;].as&lt;string&gt;() &lt;&lt; endl; return 0;&#125; 下面是在Windows命令提示符窗口上的输入输出结果，其中”&gt;”表示提示符。 1234567891011&gt;hello Hello world&gt;hello -hAllowed options: -h [ --help ] produce help message -p [ --person ] arg (=world) who&gt;hello --person lenHello len 首先通过options_description类声明了需要的选项，add_options返回了定义了operator()的特殊的代理对象。 这个调用看起来有点奇怪，其参数依次为选项名，选项值，以及选项的描述。 注意到示例中的选项名为”help,h”，是因为声明了具有短选项名和长选项名的选项，这跟gnu程序的命令行具有一致性。 当然你可以省略短选项名，但是这样就不能用命令选项简写了。 第二个选项的声明，定义了选项值为string类型，其默认值为world. 接下来,声明了variables_map类的对象，它主要用来存储选项值，并且能储存任意类型的值。 然后，store,parse_command_line和notify函数使vm能存储在命令行中发现的选项。 最后我们就自由地使用这些选项了，variables_map类的使用就像使用std::map一样，除了它必须用as方法去获取值。 如果as方法调用的指定类型与实际存储的类型不同，就会有异常抛出。 具有编程的你可能有这样的经验，使用cl或gcc对源文件进行编译时，可直接将源文件名放置在命令行中，而无需什么选项字母，如gcc a.c之类的。 prgram_options也能处理这种情况，在库中被称为”positional options”(位置选项),但这需要程序员的一点儿帮助才能完成。 看下面的经过对应修改的代码，我们无需传入”-p”选项，就能可指定”person”选项值 123positional_options_description p;p.add(&quot;person&quot;, -1);store(command_line_parser(argc, argv).options(desc).positional(p).run(), vm); 12&gt;hello lenHello len 前面新增的两行是为了说明所有的位置选项都应被解释成”person”选项，这里还采用了command_line_parser类来解析命令行，而不是用parse_command_line函数。 后者只是对前者类的简单封装，但是现在我们需要传入一些额外的信息，所以要使用类本身。 选项复合来源一般来说，在命令行上指定所有选项，对用户来说是非常烦人的。 如果有些选项要应用于每次运行，那该怎么办呢。 我们当然希望能创建出带有些常用设置的选项文件，跟命令行一起应用于程序中。 当然这一切需要将命令行与配置文件中的值结合起来。 比如，在命令行中指定的某些选项值应该能覆盖配置文件中的对应值，或者将这些值组合起来。 下面的代码段将选项通过文件读取，这文件是文本格式，可用”#”表示注释，格式如命令行中的参数一样，选项=值 123ifstream ifs(&quot;config.cfg&quot;);store(parse_config_file(ifs,config),vm);notify(vm); 参考Boost.prgram_options库文档]]></content>
      <categories>
        <category>CPP</category>
      </categories>
      <tags>
        <tag>Boost</tag>
        <tag>CPP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于UNP与APUE与TLPI三本大部头的阅读建议(着重讲UNP)]]></title>
    <url>%2Fblog%2F2016%2F08%2F11%2F%E5%85%B3%E4%BA%8EUNP%E4%B8%8EAPUE%E4%B8%8ETLPI%E4%B8%89%E6%9C%AC%E5%A4%A7%E9%83%A8%E5%A4%B4%E7%9A%84%E9%98%85%E8%AF%BB%E5%BB%BA%E8%AE%AE(%E7%9D%80%E9%87%8D%E8%AE%B2UNP)%2F</url>
    <content type="text"><![CDATA[这本书不能一次性所有都想看完。 要有目的性的看，因为这本书类似于百科全书所有都讲， 不分轻重， 如果都看，硬啃，只会迷失了自己，反而不知道看了什么 这本书不能单独看。 这本书必须配合TCP/IP详解和UNIX环境高级编程（简称APUE）以及The Linux Programming Interface（不知道这本书的译名是什么， 简称TLPI）来看 个人看的是卷一第三版，因当前工作经验范围和阅历受限，对于IT码农，IPv6的和SCTP的章节暂且略过，所以目前大概划出的必看章节（其他可挑选）是 1 2 3 4 5 6 7 8 11 13 16 22 26 30 . . . 书中的源码在linux环境下不一定能一次性编译过。 有些地方得自己修改 建议阅读电子版。 使用可以搜索书签的pdf阅读器（比较推荐福昕）， 并且多开几个此书的副本，因为经常会源码和源码讲解对照着看，如果有双屏效率会极大的提高]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>UNP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[服务器模型总结]]></title>
    <url>%2Fblog%2F2016%2F08%2F08%2Fserver_model_summary%2F</url>
    <content type="text"><![CDATA[关于Reactor模式讲解请转 此文 服务器模型总结 其中“互通”指的是如果开发chat服务，多个客户连接之间是否能方便地交换数据（chat也是三大TCP网络编程案例之一）。 对于echo/httpd/Sudoku这类“连接相互独立”的服务程序，这个功能无足轻重，但是对于chat类服务却至关重要。 “顺序性”指的是在httpd/Sudoku这类请求响应服务中，如果客户连接顺序发送多个请求，那么计算得到的多个响应是否按相同的顺序发还给客户（这里指的是在自然条件下，不含刻意同步）。 方案0方案0 这其实不是并发服务器，而是iterative 服务器，因为它一次只能服务一个客户。代码见[UNP]中的Figure 1.9，[UNP]以此为对比其他方案的基准点。这个方案不适合长连接，倒是很适合daytime这种write-only短连接服务。 *. . . * 方案1方案1 这是传统的Unix并发网络编程方案，[UNP]称之为child-per-client或fork()-per-client，另外也俗称process-per-connection。这种方案适合并发连接数不大的情况。至今仍有一些网络服务程序用这种方式实现，比如PostgreSQL和Perforce的服务端。这种方案适合“计算响应的工作量远大于fork()的开销”这种情况，比如数据库服务器。这种方案适合长连接，但不太适合短连接，因为fork()开销大于求解Sudoku的用时。 方案2方案2 这是传统的Java网络编程方案thread-per-connection，在Java 1.4引入NIO之前，Java网络服务多采用这种方案。它的初始化开销比方案1要小很多，但与求解Sudoku的用时差不多，仍然不适合短连接服务。这种方案的伸缩性受到线程数的限制，一两百个还行，几千个的话对操作系统的scheduler恐怕是个不小的负担。 方案3方案3 这是针对方案1的优化，[UNP]详细分析了几种变化，包括对accept(2)“惊群”问题（thundering herd）的考虑。 方案4方案4 这是对方案2的优化，[UNP]详细分析了它的几种变化。方案3和方案4这两个方案都是Apache httpd长期使用的方案。 方案5 - 单线程Reactor以上几种方案都是阻塞式网络编程，程序流程（thread of control）通常阻塞在read()上，等待数据到达。但是TCP是个全双工协议，同时支持read()和write()操作，当一个线程/进程阻塞在read()上，但程序又想给这个TCP连接发数据，那该怎么办？比如说echo client，既要从stdin读，又要从网络读，当程序正在阻塞地读网络的时候，如何处理键盘输入？ 又比如proxy，既要把连接a收到的数据发给连接b，又要把从b收到的数据发给a，那么到底读哪个？（proxy是附录A讲的三大TCP网络编程案例之一。） Reactor出现的原因一种方法是用两个线程/进程，一个负责读，一个负责写。[UNP]也在实现echoclient时介绍了这种方案。 另一种方法是使用IO multiplexing，也就是select/poll/epoll/kqueue这一系列的“多路选择器”，让一个thread of control 能处理多个连接。 “IO复用”其实复用的不是IO连接，而是复用线程。使用select/poll几乎肯定要配合non-blockingIO，而使用non-blocking IO肯定要使用应用层buffer。这就不是一件轻松的事儿了，如果每个程序都去搞一套自己的IO multiplexing机制（本质是event-driven事件驱动），这是一种很大的浪费。 感谢Doug Schmidt为我们总结出了Reactor模式，让event-driven网络编程有章可循。继而出现了一些通用的Reactor框架/库，比如libevent、muduo、Netty、twisted、POE等等。有了这些库，我想基本不用去编写阻塞式的网络程序了（特殊情况除外，比如proxy流量限制）。 Reactor的意义Doug Schmidt指出，其实网络编程中有很多是事务性（routine）的工作，可以提取为公用的框架或库，而用户只需要填上关键的业务逻辑代码，并将回调注册到框架中，就可以实现完整的网络服务，这正是Reactor模式的主要思想。 而Reactor的意义在于将消息（IO事件）分发到用户提供的处理函数，并保持网络部分的通用代码不变，独立于用户的业务逻辑。 Reactor的具体模型单线程Reactor的程序执行顺序下图中的左图所示。在没有事件的时候，线程等待在select/poll/epoll_wait等函数上。事件到达后由网络库处理IO，再把消息通知（回调）客户端代码。Reactor事件循环所在的线程通常叫IO线程。通常由网络库负责读写socket，用户代码负载解码、计算、编码。 注意由于只有一个线程，因此事件是顺序处理的，一个线程同时只能做一件事情。在这种协作式多任务中，事件的优先级得不到保证，因为从“poll返回之后”到“下一次调用poll进入等待之前”这段时间内，线程不会被其他连接上的数据或事件抢占（也就是说, 不会发生下图中的右图这种情况）。如果我们想要延迟计算（把compute()推迟100ms），那么也不能用sleep()之类的阻塞调用，而应该注册超时回调，以避免阻塞当前IO线程。 这种方案的优点是由网络库搞定数据收发，程序只关心业务逻辑；缺点在前面已经谈了：适合IO密集的应用，不太适合CPU密集的应用，因为较难发挥多核的威力。 另外，与方案2相比，方案5处理网络消息的延迟可能要略大一些，因为方案2直接一次read(2)系统调用就能拿到请求数据，而方案5要先poll(2)再read(2)，多了一次系统调用。 方案6方案6 这是一个过渡方案，收到Sudoku请求之后，不在Reactor线程计算，而是创建一个新线程去计算，以充分利用多核CPU。这是非常初级的多线程应用，因为它为每个请求（而不是每个连接）创建了一个新线程。这个开销可以用线程池来避免，即方案8。这个方案还有一个特点是out-of-order，即同时创建多个线程去计算同一个连接上收到的多个请求，那么算出结果的次序是不确定的，可能第2个Sudoku比较简单，比第1个先算出结果。这也是我们在一开始设计协议的时候使用了id 的原因，以便客户端区分response对应的是哪个request。 方案7方案7 为了让返回结果的顺序确定，我们可以为每个连接创建一个计算线程，每个连接上的请求固定发给同一个线程去算，先到先得。这也是一个过渡方案，因为并发连接数受限于线程数目，这个方案或许还不如直接使用阻塞IO 的thread-per-connection 方案2。 方案7与方案6的另外一个区别是单个client的最大CPU占用率。在方案6中，一个TCP连接上发来的一长串突发请求（burst requests）可以占满全部8个core；而在方案7中，由于每个连接上的请求固定由同一个线程处理，那么它最多占用12.5%的CPU资源。这两种方案各有优劣，取决于应用场景的需要（到底是公平性重要还是突发性能重要）。这个区别在方案8和方案9中同样存在，需要根据应用来取舍。 方案8 - Reactor+ThreadPool 方案8 : 为了弥补方案6中为每个请求创建线程的缺陷，我们使用固定大小线程池。全部的IO工作都在一个Reactor线程完成，而计算任务交给thread pool。如果计算任务彼此独立，而且IO的压力不大，那么这种方案是非常适用的。 线程池的另外一个作用是执行阻塞操作。比如有的数据库的客户端只提供同步访问，那么可以把数据库查询放到线程池中，可以避免阻塞IO线程，不会影响其他客户连接。另外也可以用线程池来调用一些阻塞的IO函数，例如fsync(2)/fdatasync(2)，这两个函数没有非阻塞的版本。 如果IO的压力比较大，一个Reactor处理不过来，可以试试方案9，它采用多个Reactor来分担负载。 方案9 - Reactors In Threads 方案9 : 这是muduo内置的多线程方案，也是Netty内置的多线程方案。这种方案的特点是one loop per thread，有一个main Reactor负责accept(2)连接，然后把连接挂在某个sub Reactor中（muduo采用round-robin的方式来选择sub Reactor），这样该连接的所有操作都在那个sub Reactor所处的线程中完成。 多个连接可能被分派到多个线程中，以充分利用CPU。 muduo采用的是固定大小的Reactor pool，池子的大小通常根据CPU数目确定，也就是说线程数是固定的，这样程序的总体处理能力不会随连接数增加而下降。 另外，由于一个连接完全由一个线程管理，那么请求的顺序性有保证，突发请求也不会占满全部8个核（如果需要优化突发请求，可以考虑方案11）。这种方案把IO分派给多个线程，防止出现一个Reactor的处理能力饱和。 与方案8的线程池相比，方案9减少了进出thread pool的两次上下文切换，在把多个连接分散到多个Reactor线程之后，小规模计算可以在当前IO线程完成并发回结果，从而降低响应的延迟。 这是一个适应性很强的多线程IO模型，因此陈硕把它作为muduo的默认线程模型 方案10方案10 这是Nginx的内置方案。如果连接之间无交互，这种方案也是很好的选择。工作进程之间相互独立，可以热升级。 方案11 - Reactors+Thread Pool 方案11 把方案8和方案9混合，既使用多个Reactor来处理IO，又使用线程池来处理计算。这种方案适合既有突发IO（利用多线程处理多个连接上的IO），又有突发计算的应用（利用线程池把一个连接上的计算任务分配给多个线程去做） 哪些是实用的模型 上表中的N表示并发连接数目，C1和C2是与连接数无关、与CPU数目有关的常数。 我再用银行柜台办理业务为比喻，简述各种模型的特点。银行有旋转门，办理业务的客户人员从旋转门进出（IO）；银行也有柜台，客户在柜台办理业务（计算）。要想办理业务，客户要先通过旋转门进入银行；办理完之后，客户要再次通过旋转门离开银行。一个客户可以办理多次业务，每次都必须从旋转门进出（TCP长连接）。另外，旋转门一次只允许一个客户通过（无论进出），因为read()/write()只能同时调用其中一个。 方案5这间小银行有一个旋转门、一个柜台，每次只允许一名客户办理业务。而且当有人在办理业务时，旋转门是锁住的（计算和IO在同一线程）。为了维持工作效率，银行要求客户应该尽快办理业务，最好不要在取款的时候打电话去问家里人密码，也不要在通过旋转门的时候停下来系鞋带，这都会阻塞其他堵在门外的客户。如果客户很少，这是很经济且高效的方案；但是如果场地较大（多核），则这种布局就浪费了不少资源，只能并发（concurrent）不能并行（parallel）。如果确实一次办不完，应该离开柜台，到门外等着，等银行通知再来继续办理（分阶段回调）。 方案8这间银行有一个旋转门，一个或多个柜台。银行进门之后有一个队列，客户在这里排队到柜台（线程池）办理业务。即在单线程Reactor后面接了一个线程池用于计算，可以利用多核。旋转门基本是不锁的，随时都可以进出。但是排队会消耗一点时间，相比之下，方案5中客户一进门就能立刻办理业务。另外一种做法是线程池里的每个线程有自己的任务队列，而不是整个线程池共用一个任务队列。这样的好处是避免全局队列的锁争用，坏处是计算资源有可能分配不平均，降低并行度。 方案9这间大银行相当于包含方案5中的多家小银行，每个客户进大门的时候就被固定分配到某一间小银行中，他的业务只能由这间小银行办理，他每次都要进出小银行的旋转门。但总体来看，大银行可以同时服务多个客户。这时同样要求办理业务时不能空等（阻塞），否则会影响分到同一间小银行的其他客户。而且必要的时候可以为VIP客户单独开一间或几间小银行，优先办理VIP业务。这跟方案5不同，当普通客户在办理业务的时候，VIP客户也只能在门外等着（见图6-11的右图）。这是一种适应性很强的方案，也是muduo原生的多线程IO模型。 方案11这间大银行有多个旋转门，多个柜台。旋转门和柜台之间没有一一对应关系，客户进大门的时候就被固定分配到某一旋转门中（奇怪的安排，易于实现线程安全的IO，见§4.6），进入旋转门之后，有一个队列，客户在此排队到柜台办理业务。这种方案的资源利用率可能比方案9更高，一个客户不会被同一小银行的其他客户阻塞，但延迟也比方案9略大。 应该使用几个event loop一个程序到底是使用一个event loop还是使用多个event loops呢？ ZeroMQ的手册给出的建议是，按照每千兆比特每秒的吞吐量配一个event loop的比例来设置event loop的数目，即muduo::TcpServer::setThreadNum()的参数。 依据这条经验规则，在编写运行于千兆以太网上的网络程序时，用一个event loop就足以应付网络IO。如果程序本身没有多少计算量，而主要瓶颈在网络带宽，那么可以按这条规则来办，只用一个event loop。另一方面，如果程序的IO带宽较小，计算量较大，而且对延迟不敏感，那么可以把计算放到thread pool中，也可以只用一个event loop。 值得指出的是，以上假定了TCP连接是同质的，没有优先级之分，我们看重的是服务程序的总吞吐量。但是如果TCP连接有优先级之分，那么单个event loop可能不适合，正确的做法是把高优先级的连接用单独的event loop来处理。]]></content>
      <categories>
        <category>NP</category>
      </categories>
      <tags>
        <tag>DesignPattern</tag>
        <tag>NP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Reactor模式讲解]]></title>
    <url>%2Fblog%2F2016%2F08%2F07%2Freactor_intro%2F</url>
    <content type="text"><![CDATA[关于服务器模型总结请转 此文, 文中也有Reactor的讲解. 对于 IO 来说，我们听得比较多的是: BIO: 阻塞 IO NIO: 非阻塞 IO 同步 IO 异步 IO 以及其组合: 同步阻塞 IO 同步非阻塞 IO 异步阻塞 IO 异步非阻塞 IO . . . 那么什么是阻塞 IO、非阻塞 IO、同步 IO、异步 IO 呢？ 一个 IO 操作其实分成了两个步骤：发起 IO 请求和实际的 IO 操作 阻塞 IO 和非阻塞 IO 的区别在于第一步：发起 IO 请求是否会被阻塞，如果阻塞直到完成那么就是传统的阻塞 IO; 如果不阻塞，那么就是非阻塞 IO 同步 IO 和异步 IO 的区别就在于第二个步骤是否阻塞，如果实际的 IO 读写阻塞请求进程，那么就是同步 IO，因此阻塞 IO、非阻塞 IO、IO 复用、信号驱动 IO 都是同步 IO; 如果不阻塞，而是操作系统帮你做完 IO 操作再将结果返回给你，那么就是异步 IO 举个不太恰当的例子 ：比如你家网络断了，你打电话去中国电信报修！ 你拨号 — 客户端连接服务器 电话通了 — 连接建立 你说：“我家网断了, 帮我修下”— 发送消息 说完你就在那里等，那么就是阻塞 IO 如果正好你有事，你放下带电话，然后处理其他事情了，过一会你来问下，修好了没 — 那就是非阻塞 IO 如果客服说：“马上帮你处理，你稍等”— 同步 IO 如果客服说：“马上帮你处理，好了通知你”，然后挂了电话 — 异步 IO 本文只讨论 BIO 和 NIO,AIO 使用度没有前两者普及，暂不讨论！ 下面从代码层面看看 BIO 与 NIO 的流程! BIO模型图如下所示： BIO 优缺点 优点 模型简单 编码简单 缺点 性能瓶颈低 优缺点很明显。这里主要说下缺点：主要瓶颈在线程上。每个连接都会建立一个线程。虽然线程消耗比进程小，但是一台机器实际上能建立的有效线程有限，以 Java 来说，1.5 以后，一个线程大致消耗 1M 内存！且随着线程数量的增加，CPU 切换线程上下文的消耗也随之增加，在高过某个阀值后，继续增加线程，性能不增反降！而同样因为一个连接就新建一个线程，所以编码模型很简单！ 就性能瓶颈这一点，就确定了 BIO 并不适合进行高性能服务器的开发！像 Tomcat 这样的 Web 服务器，从 7 开始就从 BIO 改成了 NIO，来提高服务器性能！ NIONIO 模型示例如下： Acceptor 注册 Selector，监听 accept 事件 当客户端连接后，触发 accept 事件 服务器构建对应的 Channel，并在其上注册 Selector，监听读写事件 当发生读写事件后，进行相应的读写处理 NIO 优缺点 优点 性能瓶颈高 缺点 模型复杂 编码复杂 需处理半包问题 NIO 的优缺点和 BIO 就完全相反了! 性能高，不用一个连接就建一个线程，可以一个线程处理所有的连接！相应的，编码就复杂很多，从上面的代码就可以明显体会到了。还有一个问题，由于是非阻塞的，应用无法知道什么时候消息读完了，就存在了半包问题！ 半包问题简单看一下下面的图就能理解半包问题了！ 我们知道 TCP/IP 在发送消息的时候，可能会拆包 (如上图 1)！这就导致接收端无法知道什么时候收到的数据是一个完整的数据。例如: 发送端分别发送了 ABC,DEF,GHI 三条信息，发送时被拆成了 AB,CDRFG,H,I 这四个包进行发送，接受端如何将其进行还原呢？在 BIO 模型中，当读不到数据后会阻塞，而 NIO 中不会! 所以需要自行进行处理! 例如，以换行符作为判断依据，或者定长消息发生，或者自定义协议！ NIO 虽然性能高，但是编码复杂，且需要处理半包问题！为了方便的进行 NIO 开发，就有了 Reactor 模型! Reactor 模型 AWT Events Reactor 模型和 AWT 事件模型很像，就是将消息放到了一个队列中，通过异步线程池对其进行消费！ Reactor 中的组件 Reactor:Reactor 是 IO 事件的派发者。 Acceptor:Acceptor 接受 client 连接，建立对应 client 的 Handler，并向 Reactor 注册此 Handler。 Handler: 和一个 client 通讯的实体，按这样的过程实现业务的处理。一般在基本的 Handler 基础上还会有更进一步的层次划分， 用来抽象诸如 decode，process 和 encoder 这些过程。比如对 Web Server 而言，decode 通常是 HTTP 请求的解析， process 的过程会进一步涉及到 Listener 和 Servlet 的调用。业务逻辑的处理在 Reactor 模式里被分散的 IO 事件所打破， 所以 Handler 需要有适当的机制在所需的信息还不全（读到一半）的时候保存上下文，并在下一次 IO 事件到来的时候（另一半可读了）能继续中断的处理。为了简化设计，Handler 通常被设计成状态机，按 GoF 的 state pattern 来实现。 对应上面的 NIO 代码来看: Reactor：相当于有分发功能的 Selector Acceptor：NIO 中建立连接的那个判断分支 Handler：消息读写处理等操作类 Reactor 从线程池和 Reactor 的选择上可以细分为如下几种： Reactor 单线程模型 如果上图表达得不够明白, 还可以看看下图 如果上图还是表达得不够明白, 还可以看看下图 这个模型和上面的 NIO 流程很类似，只是将消息相关处理独立到了 Handler 中去了！ 虽然上面说到 NIO 一个线程就可以支持所有的 IO 处理。但是瓶颈也是显而易见的！我们看一个客户端的情况，如果这个客户端多次进行请求，如果在 Handler 中的处理速度较慢，那么后续的客户端请求都会被积压，导致响应变慢！所以引入了 Reactor 多线程模型! Reactor 多线程模型 如果上图表达得不够明白, 还可以看看下图 如果上图还是表达得不够明白, 还可以看看下图 Reactor 多线程模型就是将 Handler 中的 IO 操作和非 IO 操作分开，操作 IO 的线程称为 IO 线程，非 IO 操作的线程称为工作线程! 这样的话，客户端的请求会直接被丢到线程池中，客户端发送请求就不会堵塞！ 但是当用户进一步增加的时候，Reactor 会出现瓶颈！因为 Reactor 既要处理 IO 操作请求，又要响应连接请求！为了分担 Reactor 的负担，所以引入了主从 Reactor 模型! 主从 Reactor 模型 如果上图表达得不够明白, 还可以看看下图 如果上图还是表达得不够明白, 还可以看看下图 主 Reactor 用于响应连接请求，从 Reactor 用于处理 IO 操作请求！]]></content>
      <categories>
        <category>NP</category>
      </categories>
      <tags>
        <tag>DesignPattern</tag>
        <tag>NP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[重新看unix网络编程的一些心得]]></title>
    <url>%2Fblog%2F2016%2F08%2F02%2F%E9%87%8D%E6%96%B0%E7%9C%8Bunix%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E7%9A%84%E4%B8%80%E4%BA%9B%E5%BF%83%E5%BE%97%2F</url>
    <content type="text"><![CDATA[老书新看, 有了许多不同的见解, 也准备拿出以前自己私人的老笔记做修正放到博客里, 加深理解. 在这个浮躁人人都能写书的时代基本要看一本书需要挑很久, 谁写的, 写得怎么样, 是否是业界经典, 都要需要一一斟酌各种查证方可, 不然看一本烂书事半功倍, 浪费生命,影响效率, 被误导跑偏, 能让人静下心来的书籍不多, 能让人看好几遍仍回味无穷的经典就更少了 陈硕说过, 学东西不要只是从网上看点大牛的博客总结就行了, 总要完整地看些相关领域的经典著作的, 系统的知识结构打下坚实的基础才能继续看有关优化效率方面的书籍, 深以为然 孟岩说, 最近不止一次听到当一个人拥有相关领域的知识基础之后就可以找一本effective*的书来研读了, 颇有道理 看书的顺序忌胶柱鼓瑟, 并不是他的目录结构怎么编排你就该怎么看的, 大多数经典书籍基本都是面面俱到, 如果直接跟着这种面向知识体系本身的编排思路去看, 容易迷失乏味而无法继续, 应该找到属于自己看书顺序 个人总结的看书顺序是先翻阅自己感兴趣的,接着仔细观察他的目录来确定他的编排思路, 然后花一到两天的时间通览全书以获得大体知识体系构架, 之后找到实战性最强的章节来实操并逐个突破实战过程中的各个知识点]]></content>
      <categories>
        <category>NP</category>
      </categories>
      <tags>
        <tag>UNP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[快速完成一个简易SLG游戏思路二]]></title>
    <url>%2Fblog%2F2016%2F07%2F30%2Fhow_to_implement_a_slg_two%2F</url>
    <content type="text"><![CDATA[ChatServer从登录成功就开始连接,注册一个Chat_ID,Player_ID 和 Chat_ID相互对应,会注册相应的房间频道,并为每位 Player 存了一份黑名单,在客户端做了本地黑名单,聊天服务器也做了黑名单二次验证处理. 世界频道 : 则用MsgServer的非实时推送思路 私密聊天 : 则选择 WorkerMan 的TCP, MsgServer 实时推送 : WorkerMan 的 TCP 非实时推送 : 客户端定时15秒轮询一下服务器，如果有消息就取下来，如果没消息可以逐步放长轮询时间，比如30秒；如果有消息，就缩短轮询时间到10秒，5秒， . . . DeployToolCapistrano是一个开源的部署工具, 用Ruby来写, 语法超简洁的 优点 实时则走 WorkerMan 非实时则跑 yii 访问一下，请求一下关卡数据，玩完了又提交一下，验算一下是否合法，获得什么奖励，数据库用单台 MySQL或者 MongoDB即可，后端的 Redis做缓存此类服务器用来实现一款三国类策略或者卡牌及酷跑的游戏已经绰绰有余，这类游戏因为逻辑简单，玩家之间交互不强，使用HTTP来开发的话，开发速度快，调试只需要一个浏览器就可以把逻辑调试清楚了]]></content>
      <categories>
        <category>GS</category>
      </categories>
      <tags>
        <tag>SLG</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[快速完成一个简易SLG游戏思路一]]></title>
    <url>%2Fblog%2F2016%2F07%2F30%2Fhow_to_implement_a_slg_one%2F</url>
    <content type="text"><![CDATA[LoginServer(Gate)当完成渠道SDK回调验证之后,验证完玩家信息,用了Nginx的负载均衡给每位玩家分配一台不繁忙的游戏服务器,在Redis中存了一份玩家在线有效时间key,这个key也可以用来完成封号操作 MainServer因为弱交互/短连接的关系,大多数情况玩家和玩家之间不需要实时面对面PK，打一下对方的离线数据，计算下排行榜，排行榜是实时计算的,存在Redis里, 做了分页处理买卖下道具即可. 所以 MainServer 选择了: yii Redis MySQL Nginx . . . BattleServer而 BattleServer 则选择 WorkerMan 的TCP模式,实时交互数据,MainServer 和 BattleServer 之间通信并接同一个数据库. 其实这里可以不选择用TCP, 而自撸一个可靠UDP来提高效率]]></content>
      <categories>
        <category>GS</category>
      </categories>
      <tags>
        <tag>SLG</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多人快节奏游戏四之延迟补偿实现爆头]]></title>
    <url>%2Fblog%2F2016%2F07%2F19%2F%E5%A4%9A%E4%BA%BA%E5%BF%AB%E8%8A%82%E5%A5%8F%E6%B8%B8%E6%88%8F%E5%9B%9B%E4%B9%8B%E5%BB%B6%E8%BF%9F%E8%A1%A5%E5%81%BF%E5%AE%9E%E7%8E%B0%E7%88%86%E5%A4%B4%2F</url>
    <content type="text"><![CDATA[原文出处 Fast-Paced Multiplayer (Part IV): Lag Compensation Introduction The previous three articles explained a client-server game architecture which can be summarized as follows: Server gets inputs from all the clients, with timestamps Server processes inputs and updates world status Server sends regular world snapshots to all clients Client sends input and simulates their effects locally Client get world updates and Syncs predicted state to authoritative state Interpolates known past states for other entities From a player’s point of view, this has two important consequences: Player sees himself in the present Player sees other entities in the past This situation is generally fine, but it’s quite problematic for very time- and space-sensitive events; for example, shooting your enemy in the head! Lag Compensation So you’re aiming perfectly at the target’s head with your sniper rifle. You shoot - it’s a shot you can’t miss. But you miss. Why does this happen? Because of the client-server architecture explained before, you were aiming at where the enemy’s head was 100ms before you shot - not when you shot! In a way, it’s like playing in an universe where the speed of light is really, really slow; you’re aiming at the past position of your enemy, but he’s long gone by the time you squeeze the trigger. Fortunately, there’s a relatively simple solution for this, which is also pleasant for most players most of the time (with the one exception discussed below). Here’s how it works: When you shoot, client sends this event to the server with full information: the exact timestamp of your shot, and the exact aim of the weapon. Here’s the crucial step. Since the server gets all the input with timestamps, it can authoritatively reconstruct the world at any instant in the past. In particular, it can reconstruct the world exactly as it looked like to any client at any point in time. This means the server can know exactly what was on your weapon’s sights the instant you shot. It was the past position of your enemy’s head, but the server knows it was the position of his head in your present. The server processes the shot at that point in time, and updates the clients. And everyone is happy! The server is happy because he’s the server. He’s always happy. You’re happy because you were aiming at your enemy’s head, shot, and got a rewarding headshot! The enemy may be the only one not entirely happy. If he was standing still when he got shot, it’s his fault, right? If he was moving… wow, you’re a really awesome sniper. But what if he was in an open position, got behind a wall, and then got shot, a fraction of a second later, when he thought he was safe? Well, that can happen. That’s the tradeoff you make. Because you shoot at him in the past, he may still be shot up to a few milliseconds after he took cover. It is somewhat unfair, but it’s the most agreeable solution for everyone involved. It would be much worse to miss an unmissable shot! Conclusion This ends my series on Fast-paced Multiplayer. This kind of thing is clearly tricky to get right, but with a clear conceptual understanding about what’s going on, it’s not exceedingly difficult. Although the audience of these articles were game developers, it found another group of interested readers: gamers! From a gamer point of view it’s also interesting to understand why some things happen the way they happen. Further Reading As clever as these techniques are, I can’t claim any credit for them; these articles are just an easy to understand guide to some concepts I’ve learned from other sources, including articles and source code, and some experimentation.]]></content>
      <categories>
        <category>GS</category>
      </categories>
      <tags>
        <tag>GabrielGambetta</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多人快节奏游戏三之实体插值]]></title>
    <url>%2Fblog%2F2016%2F07%2F18%2F%E5%A4%9A%E4%BA%BA%E5%BF%AB%E8%8A%82%E5%A5%8F%E6%B8%B8%E6%88%8F%E4%B8%89%E4%B9%8B%E5%AE%9E%E4%BD%93%E6%8F%92%E5%80%BC%2F</url>
    <content type="text"><![CDATA[原文出处 Fast-Paced Multiplayer (Part III): Entity Interpolation Introduction In the first article of the series, we introduced the concept of an authoritative server and its usefulness to prevent client cheats. However, using this technique naively can lead to potentially showstopper issues regarding playability and responsiveness. In the second article, we proposed client-side prediction as a way to overcome these limitations. The net result of these two articles is a set of concepts and techniques that allow a player to control an in-game character in a way that feels exactly like a single-player game, even when connected to an authoritative server through an internet connection with transmission delays. In this article, we’ll explore the consequences of having other player-controled characters connected to the same server. Server time step In the previous article, the behavior of the server we described was pretty simple – it read client inputs, updated the game state, and sent it back to the client. When more than one client is connected, though, the main server loop is somewhat different. In this scenario, several clients may be sending inputs simultaneously, and at a fast pace (as fast as the player can issue commands, be it pressing arrow keys, moving the mouse or clicking the screen). Updating the game world every time inputs are received from each client and then broadcasting the game state would consume too much CPU and bandwidth. A better approach is to queue the client inputs as they are received, without any processing. Instead, the game world is updated periodically at low frequency, for example 10 times per second. The delay between every update, 100ms in this case, is called thetime step. In every update loop iteration, all the unprocessed client input is applied (possibly in smaller time increments than the time step, to make physics more predictable), and the new game state is broadcast to the clients. In summary, the game world updates independent of the presence and amount of client input, at a predictable rate. Dealing with low-frequency updates From the point of view of a client, this approach works as smoothly as before – client-side prediction works independently of the update delay, so it clearly also works under predictable, if relatively infrequent, state updates. However, since the game state is broadcast at a low frequency (continuing with the example, every 100ms), the client has very sparse information about the other entities that may be moving throughout the world. A first implementation would update the position of other characters when it receives a state update; this immediately leads to very choppy movement, that is, discrete jumps every 100ms instead of smooth movement. Client 1 as seen by Client 2. Depending on the type of game you’re developing there are many ways to deal with this; in general, the more predictable your game entities are, the easier it is to get it right. Dead reckoning Suppose you’re making a car racing game. A car that goes really fast is pretty predictable – for example, if it’s running at 100 meters per second, a second later it will be roughly 100 meters ahead of where it started. Why “roughly”? During that second the car could have accelerated or decelerated a bit, or turned to the right or to the left a bit – the key word here is “a bit”. The maneuverability of a car is such that at high speeds its position at any point in time is highly dependent on its previous position, speed and direction, regardless of what the player actually does. In other words, a racing car can’t do a 180º turn instantly. How does this work with a server that sends updates every 100 ms? The client receives authoritative speed and heading for every competing car; for the next 100 ms it won’t receive any new information, but it still needs to show them running. The simplest thing to do is to assume the car’s heading and acceleration will remain constant during that 100 ms, and run the car physics locally with that parameters. Then, 100 ms later, when the server update arrives, the car’s position is corrected. The correction can be big or relatively small depending on a lot of factors. If the player does keep the car on a straight line and doesn’t change the car speed, the predicted position will be exactly like the corrected position. On the other hand, if the player crashes against something, the predicted position will be extremely wrong. Note that dead reckoning can be applied to low-speed situations – battleships, for example. In fact, the term “dead reckoning” has its origins in marine navigation. Entity interpolation There are some situations where dead reckoning can’t be applied at all – in particular, all scenarios where the player’s direction and speed can change instantly. For example, in a 3D shooter, players usually run, stop, and turn corners at very high speeds, making dead reckoning essentially useless, as positions and speeds can no longer be predicted from previous data. You can’t just update player positions when the server sends authoritative data; you’d get players who teleport short distances every 100 ms, making the game unplayable. What you do have is authoritative position data every 100 ms; the trick is how to show the player what happens inbetween. The key to the solution is to show the other players in the past relative to the user’s player. Say you receive position data at t = 1000. You already had received data at t = 900, so you know where the player was at t = 900 and t = 1000. So, from t = 1000 and t = 1100, you show what the other player did from t = 900 to t = 1000. This way you’re always showing the user actual movement data, except you’re showing it 100 ms “late”. Client 2 renders Client 1 “in the past”, interpolating last known positions. The position data you use to interpolate from t = 900 to t = 1000 depends on the game. Interpolation usually works well enough. If it doesn’t, you can have the server send more detailed movement data with each update – for example, a sequence of straight segments followed by the player, or positions sampled every 10 ms which look better when interpolated (you don’t need to send 10 times more data – since you’re sending deltas for small movements, the format on the wire can be heavily optimized for this particular case). Note that using this technique, every player sees a slightly different rendering of the game world, because each player sees itself in the present but sees the other entities in the past. Even for a fast paced game, however, seeing other entities with a 100 ms isn’t generally noticeable. There are exceptions – when you need a lot of spatial and temporal accuracy, such as when the player shoots at another player. Since the other players are seen in the past, you’re aiming with a 100 ms delay – that is, you’re shooting where your target was 100 ms ago! We’ll deal with this in the next article. Summary In a client-server environment with an authoritative server, infrequent updates and network delay, you must still give players the illusion of continuity and smooth movement. In part 2 of the series we explored a way to show the user controlled player’s movement in real time using client-side prediction and server reconciliation; this ensures user input has an immediate effect on the local player, removing a delay that would render the game unplayable. Other entities are still a problem, however. In this article we explored two ways of dealing with them. The first one, dead reckoning, applies to certain kinds of simulations where entity position can be acceptably estimated from previous entity data such as position, speed and acceleration. This approach fails when these conditions aren’t met. The second one, entity interpolation, doesn’t predict future positions at all – it uses only real entity data provided by the server, thus showing the other entities slightly delayed in time. The net effect is that the user’s player is seen in the present and the other entities are seen in the past. This usually creates an incredibly seamless experience. However, if nothing else is done, the illusion breaks down when an event needs high spatial and temporal accuracy, such as shooting at a moving target: the position where Client 2 renders Client 1 doesn’t match the server’s nor Client 1′s position, so headshots become impossible! Since no game is complete without headshots, we’ll deal with this issue in the next article.]]></content>
      <categories>
        <category>GS</category>
      </categories>
      <tags>
        <tag>GabrielGambetta</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多人快节奏游戏二之客户端预测与服务器修正]]></title>
    <url>%2Fblog%2F2016%2F07%2F17%2F%E5%A4%9A%E4%BA%BA%E5%BF%AB%E8%8A%82%E5%A5%8F%E6%B8%B8%E6%88%8F%E4%BA%8C%E4%B9%8B%E5%AE%A2%E6%88%B7%E7%AB%AF%E9%A2%84%E6%B5%8B%E4%B8%8E%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%BF%AE%E6%AD%A3%2F</url>
    <content type="text"><![CDATA[原文出处 Fast-Paced Multiplayer (Part II): Client-Side Prediction and Server Reconciliation Introduction In the first article of this series, we explored a client-server model with an authoritative server and dumb clients that just send inputs to the server and then render the updated game state when the server sends it. A naive implementation of this scheme leads to a delay between user commands and changes on the screen; for example, the player presses the right arrow key, and the character takes half a second before it starts moving. This is because the client input must first travel to the server, the server must process the input and calculate a new game state, and the updated game state must reach the client again. Effect of network delays. In a networked environment such as the internet, where delays can be in the orders of tenths of a second, a game may feel unresponsive at best, or in the worst case, be rendered unplayable. In this article, we’ll find ways to minimize or even eliminate that problem. Client-side prediction Even though there are some cheating players, most of the time the game server is processing valid requests (from non-cheating clients and from cheating clients who aren’t cheating at that particular time). This means most of the input received will be valid and will update the game state as expected; that is, if your character is at (10, 10) and the right arrow key is pressed, it will end up at (11, 10). We can use this to our advantage. If the game world is deterministic enough (that is, given a game state and a set of inputs, the result is completely predictable), Let’s suppose we have a 100 ms lag, and the animation of the character moving from one square to the next takes 100 ms. Using the naive implementation, the whole action would take 200 ms: Network delay + animation. Since the world is deterministic, we can assume the inputs we send to the server will be executed successfully. Under this assumption, the client can predict the state of the game world after the inputs are processed, and most of the time this will be correct. Instead of sending the inputs and waiting for the new game state to start rendering it, we can send the input and start rendering the outcome of that inputs as if they had succeded, while we wait for the server to send the “true” game state – which more often than not, will match the state calculated locally : Animation plays while the server confirms the action. Now there’s absolutely no delay between the player’s actions and the results on the screen, while the server is still authoritative (if a hacked client would send invalid inputs, it could render whatever it wanted on the screen, but it wouldn’t affect the state of the server, which is what the other players see). Synchronization issues In the example above, I chose the numbers carefully to make everything work fine. However, consider a slightly modified scenario: let’s say we have a 250 ms lag to the server, and moving from a square to the next takes 100 ms. Let’s also say the player presses the right key 2 times in a row, trying to move 2 squares to the right. Using the techniques so far, this is what would happen: Predicted state and authoritative state mismatch. We run into an interesting problem at t = 250 ms, when the new game state arrives. The predicted state at the client is x = 12, but the server says the new game state is x = 11. Because the server is authoritative, the client must move the character back to x = 11. But then, a new server state arrives at t = 350, which says x = 12, so the character jumps again, forward this time. From the point of view of the player, he pressed the right arrow key twice; the character moved two squares to the right, stood there for 50 ms, jumped one square to the left, stood there for 100 ms, and jumped one square to the right. This, of course, is unacceptable. Server reconciliation The key to fix this problem is to realize that the client sees the game world in present time, but because of lag, the updates it gets from the server are actually the state of the game in the past. By the time the server sent the updated game state, it hadn’t processed all the commands sent by the client. This isn’t terribly difficult to work around, though. First, the client adds a sequence number to each request; in our example, the first key press is request #1, and the second key press is request #2. Then, when the server replies, it includes the sequence number of the last input it processed: Client-side prediction + server reconciliation. 或许这幅图更加解释得清楚些 Now, at t = 250, the server says “based on what I’ve seen up to your request #1, your position is x = 11”. Because the server is authoritative, it sets the character position at x = 11. Now let’s assume the client keeps a copy of the requests it sends to the server. Based on the new game state, it knows the server has already processed request #1, so it can discard that copy. But it also knows the server still has to send back the result of processing request #2. So applying client-side prediction again, the client can calculate the “present” state of the game based on the last authoritative state sent by the server, plus the inputs the server hasn’t processed yet. So, at t = 250, the client gets “x = 11, last processed request = #1”. It discards its copies of sent input up to #1 – but it retains a copy of #2, which hasn’t been acknowledged by the server. It updates it internal game state with what the server sent, x = 11, and then applies all the input still not seen by the server – in this case, input #2, “move to the right”. The end result is x = 12, which is correct. Continuing with our example, at t = 350 a new game state arrives from the server; this time it says “x = 12, last processed request = #2”. At this point, the client discards all input up to #2, and updates the state with x = 12. There’s no unprocessed input to replay, so processing ends there, with the correct result. Odds and ends The example discussed above implies movement, but the same principle can be applied to almost anything else. For example, in a turn-based combat game, when the player attacks another character, you can show blood and a number representing the damage done, but you shouldn’t actually update the health of the character until the server says so. Because of the complexities of game state, which isn’t always easily reversible, you may want to avoid killing a character until the server says so, even if its health dropped below zero in the client’s game state (what if the other character used a first-aid kit just before receiving your deadly attack, but the server hasn’t told you yet?) This brings us to an interesting point – even if the world is completely deterministic and no clients cheat at all, it’s still possible that the state predicted by the client and the state sent by the server don’t match after a reconciliation. The scenario is impossible as described above with a single player, but it’s easy to run into when several players are connected to the server at once. This will be the topic of the next article. Summary When using an authoritative server, you need to give the player the illusion of responsiveness, while you wait for the server to actually process your inputs. To do this, the client simulates the results of the inputs. When the updated server state arrives, the predicted client state is recomputed from the updated state and the inputs the client sent but the server hasn’t acknowledged yet.]]></content>
      <categories>
        <category>GS</category>
      </categories>
      <tags>
        <tag>GabrielGambetta</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多人快节奏游戏一之C/S游戏架构]]></title>
    <url>%2Fblog%2F2016%2F07%2F16%2F%E5%A4%9A%E4%BA%BA%E5%BF%AB%E8%8A%82%E5%A5%8F%E6%B8%B8%E6%88%8F%E4%B8%80%E4%B9%8BCS%E6%B8%B8%E6%88%8F%E6%9E%B6%E6%9E%84%2F</url>
    <content type="text"><![CDATA[原文出处 Fast-Paced Multiplayer (Part I): Client-Server Game Architecture Introduction This is the first in a series of articles exploring the techniques and algorithms that make fast-paced multiplayer games possible. If you’re familiar with the concepts behind multiplayer games, you can safely skip to the next article – what follows is an introductory discussion. Developing any kind of game is itself challenging; multiplayer games, however, add a completely new set of problems to be dealt with. Interestingly enough, the core problems are human nature and physics! The problem of cheating It all starts with cheating. As a game developer, you usually don’t care whether a player cheats in your single-player game – his actions don’t affect anyone but him. A cheating player may not experience the game exactly as you planned, but since it’s their game, they have the right to play it in any way they please. Multiplayer games are different, though. In any competitive game, a cheating player isn’t just making the experience better for himself, he’s also making the experience worse for the other players. As the developer, you probably want to avoid that, since it tends to drive players away from your game. There are many things that can be done to prevent cheating, but the most important one (and probably the only really meaningful one) is simple : don’t trust the player. Always assume the worst – that players will try to cheat. Authoritative servers and dumb clients This leads to a seemingly simple solution – you make everything in your game happen in a central server under your control, and make the clients just privileged spectators of the game. In other words, your game client sends inputs (key presses, commands) to the server, the server runs the game, and you send the results back to the clients. This is usually called using an authoritative server, because the one and only authority regarding everything that happens in the world is the server. Of course, your server can be exploited for vulnerabilities, but that’s out of the scope of this series of articles. Using an authoritative server does prevent a wide range of hacks, though. For example, you don’t trust the client with the health of the player; a hacked client can modify its local copy of that value and tell the player it has 10000% health, but the server knows it only has 10% – when the player is attacked it will die, regardless of what a hacked client may think. You also don’t trust the player with its position in the world. If you did, a hacked client would tell the server “I’m at (10,10)” and a second later “I’m at (20,10)”, possibly going through a wall or moving faster than the other players. Instead, the server knows the player is at (10,10), the client tells the server “I want to move one square to the right”, the server updates its internal state with the new player position at (11,10), and then replies to the player “You’re at (11, 10)”: Effect of network delays. In summary: the game state is managed by the server alone. Clients send their actions to the server. The server updates the game state periodically, and then sends the new game state back to clients, who just render it on the screen. Dealing with networks The dumb client scheme works fine for slow turn based games, for example strategy games or poker. It would also work in a LAN setting, where communications are, for all practical purposes, instantaneous. But this breaks down when used for a fast-paced game over a network such as the internet. Let’s talk physics. Suppose you’re in San Francisco, connected to a server in the NY. That’s approximately 4,000 km, or 2,500 miles (that’s roughly the distance between Lisbon and Moscow). Nothing can travel faster than light, not even bytes on the Internet (which at the lower level are pulses of light, electrons in a cable, or electromagnetic waves). Light travels at approximately 300,000 km/s, so it takes 13 ms to travel 4,000 km. This may sound quite fast, but it’s actually a very optimistic setup – it assumes data travels at the speed of light in a straight path, with is most likely not the case. In real life, data goes through a series of jumps (called hops in networking terminology) from router to router, most of which aren’t done at lightspeed; routers themselve introduce a bit of delay, since packets must be copied, inspected, and rerouted. For the sake of the argument, let’s assume data takes 50 ms from client to server. This is close to a best-case scenario – what happens if you’re in NY connected to a server in Tokyo? What if there’s network congestion for some reason? Delays of 100, 200, even 500 ms are not unheard of. Back to our example, your client sends some input to the server (“I pressed the right arrow”). The server gets it 50 ms later. Let’s say the server processes the request and sends back the updated state immediately. Your client gets the new game state (“You’re now at (1, 0)”) 50 ms later. From your point of view, what happened is that you pressed the right arrow but nothing happened for a tenth of a second; then your character finally moved one square to the right. This perceived lag between your inputs and its consequences may not sound like much, but it’s noticeable – and of course, a lag of half a second isn’t just noticeable, it actually makes the game unplayable. Summary Networked multiplayer games are incredibly fun, but introduce a whole new class of challenges. The authoritative server architecture is pretty good at stopping most cheats, but a straightforward implementation may make games quite unresponsive to the player. In the following articles, we’ll explore how can we build a system based on an authoritative server, while minimizing the delay experienced by the players, to the point of making it almost indistinguishable from local or single player games.]]></content>
      <categories>
        <category>GS</category>
      </categories>
      <tags>
        <tag>GabrielGambetta</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[游戏服务端常用架构三]]></title>
    <url>%2Fblog%2F2016%2F07%2F11%2F%E6%B8%B8%E6%88%8F%E6%9C%8D%E5%8A%A1%E7%AB%AF%E5%B8%B8%E7%94%A8%E6%9E%B6%E6%9E%84%E4%B8%89%2F</url>
    <content type="text"><![CDATA[休闲游戏服务器休闲游戏同战网服务器类似，都是全区架构，不同的是有房间服务器，还有具体的游戏服务器，游戏主体不再以玩家 P2P进行，而是连接到专门的游戏服务器处理： 和战网一样的全区架构，用户数据不能象分区的 RPG那样一次性load到内存，然后在内存里面直接修改。全区架构下，为了应对一个用户同时玩几个游戏，用户数据需要区分基本数据和不同的游戏数据，而游戏数据又需要区分积分数据、和文档数据。胜平负之类的积分可以直接提交增量修改，而更为普遍的文档类数据则需要提供读写令牌，写令牌只有一块，读令牌有很多块。同帐号同一个游戏同时在两台电脑上玩时，最先开始的那个游戏获得写令牌，可以操作任意的用户数据。而后开始的那个游戏除了可以提交胜平负积分的增量改变外，对用户数据采用只读的方式，保证游戏能运行下去，但是会提示用户，游戏数据锁定。 . . . 现代动作类网游从早期的韩国动作游戏开始，传统的战网动作类游戏和 RPG游戏开始尝试融合。单纯的动作游戏玩家容易疲倦，留存也没有 RPG那么高；而单纯 RPG战斗却又慢节奏的乏味，无法满足很多玩家激烈对抗的期望，于是二者开始融合成为新一代的：动作 + 城镇模式。玩家在城镇中聚集，然后以开副本的方式几个人出去以动作游戏的玩法来完成各种 RPG任务。本质就是一套 RPG服务端+副本服务端。由于每次副本时人物可以控制在8人以内，因此可以获得更为实时的游戏体验，让玩家玩的更加爽快。 说了那么多的游戏服务器类型，其实也差不多了，剩下的类型大家拼凑一下其实也就是这个样子而已。游戏服务端经历了那么多结构上的变迁，内部开发模式是否依然不变？究竟是继续延续传统的开发方式？还是有了更多突破性的方法？经历那么多次架构变迁，后面是否有共通的逻辑？未来的发展还会存在哪些困难？游戏服务端开发如何达到最终的彼岸？]]></content>
      <categories>
        <category>GS</category>
      </categories>
      <tags>
        <tag>DesignPattern</tag>
        <tag>GS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[游戏服务端常用架构二]]></title>
    <url>%2Fblog%2F2016%2F07%2F11%2F%E6%B8%B8%E6%88%8F%E6%9C%8D%E5%8A%A1%E7%AB%AF%E5%B8%B8%E7%94%A8%E6%9E%B6%E6%9E%84%E4%BA%8C%2F</url>
    <content type="text"><![CDATA[第三代游戏服务器 2007从魔兽世界开始无缝世界地图已经深入人心，比较以往游戏玩家走个几步还需要切换场景，每次切换就要等待 LOADING个几十秒是一件十分破坏游戏体验的事情。于是对于 2005年以后的大型 MMORPG来说，无缝地图已成为一个标准配置。比较以往按照地图来切割游戏而言，无缝世界并不存在一块地图上面的人有且只由一台服务器处理了： 每台 Node服务器用来管理一块地图区域，由 NodeMaster（NM）来为他们提供总体管理。更高层次的 World则提供大陆级别的管理服务。这里省略若干细节服务器，比如传统数据库前端，登录服务器，日志和监控等，统统用 ADMIN概括。在这样的结构下，玩家从一块区域走向另外一块区域需要简单处理一下： 玩家1完全由节点A控制，玩家3完全由节点B控制。而处在两个节点边缘的2号玩家，则同时由A和B提供服务。玩家2从A移动到B的过程中，会同时向A请求左边的情况，并向B请求右边的情况。但是此时玩家2还是属于A管理。直到玩家2彻底离开AB边界很远，才彻底交由B管理。按照这样的逻辑将世界地图分割为一块一块的区域，交由不同的 Node去管理。 对于一个 Node所负责的区域，地理上没必要连接在一起，比如大陆的四周边缘部分和高山部分的区块人比较少，可以统一交给一个Node去管理，而这些区块在地理上并没有联系在一起的必要性。一个 Node到底管理哪些区块，可以根据游戏实时运行的负载情况，定时维护的时候进行更改 NodeMaster 上面的配置。 于是碰到第一个问题是很多 Node服务器需要和玩家进行通信，需要问管理服务器特定UID为多少的玩家到底在哪台 Gate上，以前按场景切割的服务器这个问题不大，问了一次以后就可以缓存起来了，但是现在服务器种类增加不少，玩家又会飘来飘去，按UID查找玩家比较麻烦；另外一方面 GATE需要动态根据坐标计算和哪些 Node通信，导致逻辑越来越厚，于是把：“用户对象”从负责连接管理的 GATE中切割出来势在必行于是有了下面的模型： . . . 网关服务器再次退回到精简的网络转发功能，而用户逻辑则由按照 UID划分的 OBJ服务器来承担，GATE是按照网络接入时的负载来分布，而 OBJ则是按照资源的编号（UID）来分布，这样和一个用户通信直接根据 UID计算出 OBJ服务器编号发送数据即可。而新独立出来的 OBJ则提供了更多高层次的服务： • 对象移动：管理具体玩家在不同的 Node所管辖的区域之间的移动，并同需要的 Node进行沟通。• 数据广播：Node可以给每个用户设置若干 TAG，然后通知 Object Master 按照TAG广播。• 对象消息：通用消息推送，给某个用户发送数据，直接告诉 OBJ，不需要直接和 GATE打交道。• 好友聊天：角色之间聊天直接走 OBJ/OBJ MASTER。整个服务器主体分为三层以后，NODE专注场景，OBJ专注玩家对象，GATE专注网络。这样的模型在无缝场景服务器中得到广泛的应用。但是随着时间的推移，负载问题也越来越明显，做个活动，远来不活跃的区域变得十分活跃，靠每周维护来调整还是比较笨重的，于是有了动态负载均衡。 动态负载均衡有两种方法，第一种是按照负载，由 Node Master 定时动态移动修改一下各个 Node的边界，而不同的玩家对象按照先前的方法从一台 Node上迁移到另外一台 Node上： 图11 动态负载均衡 这样 Node Master定时查找地图上的热点区域，计算新的场景切割方式，然后告诉其他服务器开始调整，具体处理方式还是和上面对象跨越边界移动的方法一样。 但是上面这种方式实现相对复杂一些，于是人们设计出了更为简单直接的一种新方法： 图12 基于网格的动态负载均衡 还是将地图按照标准尺寸均匀切割成静态的网格，每个格子由一个具体的Node负责，但是根据负载情况，能够实时的迁移到其他 Node上。在迁移分为三个阶段：准备，切换，完成。三个状态由Node Master负责维护。准备阶段新的 Node开始同步老 Node上面该网格的数据，完成后告诉NM；NM确认OK后同时通知新旧 Node完成切换。完成切换后，如果 Obj服务器还在和老的 Node进行通信，老的 Node将会对它进行纠正，得到纠正的 OBJ将修正自己的状态，和新的 Node进行通信。 很多无缝动态负载均衡的服务端宣称自己支持无限的人数，但不意味着 MMORPG游戏的人数上限真的可以无限扩充，因为这样的体系会受制于网络带宽和客户端性能。带宽决定了同一个区域最大广播上限，而客户端性能决定了同一个屏幕到底可以绘制多少个角色。 从无缝地图引入了分布式对象模型开始，已经完全脱离 MUDOS体系，成为一种新的服务端模型。又由于动态负载均衡的引入，让无缝服务器如虎添翼，容纳着超过上一代游戏服务器数倍的人数上限，并提供了更好的游戏体验，我们称其为第三代游戏服务端架构。网游以大型多人角色扮演为开端，RPG网游在相当长的时间里一度占据90%以上，使得基于 MMORPG的服务端架构得到了蓬勃的发展，然而随着玩家对RPG的疲惫，各种非MMORPG游戏如雨后春笋般的出现在人们眼前，受到市场的欢迎。 战网游戏服务器经典战网服务端和 RPG游戏有两个区别：RPG是分区分服的，北京区的用户和广州区的用户老死不相往来。而战网，虽然每局游戏一般都是 8人以内，但全国只有一套服务器，所有的玩家都可以在一起游戏，而玩家和玩家之使用 P2P的方式连接在一起，组成一局游戏： 玩家通过 Match Making 服务器使用：创建、加入、自动匹配、邀请等方式组成一局游戏。服务器会选择一个人做 Host，其他人 P2P连接到做主的玩家上来。STUN是帮助玩家之间建立 P2P的牵引服务器，而由于 P2P联通情况大概只有 75%，实在联不通的玩家会通过 Forward进行转发。 大量的连接对战，体育竞技游戏采用类似的结构。P2P有网状模型（所有玩家互相连接），和星状模型（所有玩家连接一个主玩家）。复杂的游戏状态在网状模型下难以形成一致，因此星状P2P模型经受住了历史的考验。除去游戏数据，支持语音的战网系统也会将所有人的语音数据发送到做主的那个玩家机器上，通过混音去重再编码的方式返回给所有用户。 战网类游戏，以竞技、体育、动作等类型的游戏为主，较慢节奏的 RPG（包括ARPG）有本质上的区别，而激烈的游戏过程必然带来到较 RPG复杂的多的同步策略，这样的同步机制往往带来的是很多游戏结果由客户端直接计算得出，那在到处都是破解的今天，如何保证游戏结果的公正呢？ 主要方法就是投票法，所有客户端都会独立计算，然后传递给服务器。如果结果相同就更新记录，如果结果不一致，会采取类似投票的方式确定最终结果。同时记录本剧游戏的所有输入，在可能的情况下，找另外闲散的游戏客户端验算整局游戏是否为该结果。并且记录经常有作弊嫌疑的用户，供运营人员封号时参考。]]></content>
      <categories>
        <category>GS</category>
      </categories>
      <tags>
        <tag>DesignPattern</tag>
        <tag>GS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[游戏服务端常用架构一]]></title>
    <url>%2Fblog%2F2016%2F07%2F11%2F%E6%B8%B8%E6%88%8F%E6%9C%8D%E5%8A%A1%E7%AB%AF%E5%B8%B8%E7%94%A8%E6%9E%B6%E6%9E%84%E4%B8%80%2F</url>
    <content type="text"><![CDATA[卡牌、跑酷等弱交互服务端卡牌跑酷类因为交互弱，玩家和玩家之间不需要实时面对面PK，打一下对方的离线数据，计算下排行榜，买卖下道具即可，所以实现往往使用简单的 HTTP服务器： 登录时可以使用非对称加密（RSA, DH），服务器根据客户端uid，当前时间戳还有服务端私钥，计算哈希得到的加密 key 并发送给客户端。之后双方都用 HTTP通信，并用那个key进行RC4加密。客户端收到key和时间戳后保存在内存，用于之后通信，服务端不需要保存 key，因为每次都可以根据客户端传上来的 uid 和时间戳以及服务端自己的私钥计算得到。用模仿 TLS的行为，来保证多次 HTTP请求间的客户端身份，并通过时间戳保证同一人两次登录密钥不同。 每局开始时，访问一下，请求一下关卡数据，玩完了又提交一下，验算一下是否合法，获得什么奖励，数据库用单台 MySQL或者 MongoDB即可，后端的 Redis做缓存（可选）。如果要实现通知，那么让客户端定时15秒轮询一下服务器，如果有消息就取下来，如果没消息可以逐步放长轮询时间，比如30秒；如果有消息，就缩短轮询时间到10秒，5秒，即便两人聊天，延迟也能自适应。 此类服务器用来实现一款三国类策略或者卡牌及酷跑的游戏已经绰绰有余，这类游戏因为逻辑简单，玩家之间交互不强，使用 HTTP来开发的话，开发速度快，调试只需要一个浏览器就可以把逻辑调试清楚了。 第一代游戏服务器 19781978年，英国著名的财经学校University of Essex的学生 Roy Trubshaw编写了世界上第一个MUD程序《MUD1》，在University of Essex于1980年接入 ARPANET之后加入了不少外部的玩家，甚至包括国外的玩家。《MUD1》程序的源代码在 ARPANET共享之后出现了众多的改编版本，至此MUD才在全世界广泛流行起来。不断完善的 MUD1的基础上产生了开源的 MudOS（1991），成为众多网游的鼻祖： MUDOS采用 C语言开发，因为玩家和玩家之间有比较强的交互（聊天，交易，PK），MUDOS使用单线程无阻塞套接字来服务所有玩家，所有玩家的请求都发到同一个线程去处理，主线程每隔1秒钟更新一次所有对象（网络收发，更新对象状态机，处理超时，刷新地图，刷新NPC）。 . . . 游戏世界采用房间的形式组织起来，每个房间有东南西北四个方向可以移动到下一个房间，由于欧美最早的网游都是地牢迷宫形式的，因此场景的基本单位被成为 “房间”。MUDOS使用一门称为LPC的脚本语言来描述整个世界（包括房间拓扑，配置，NPC，以及各种剧情）。游戏里面的高级玩家（巫师），可以不断的通过修改脚本来为游戏添加房间以及增加剧情。早年 MUD1上线时只有17个房间，Roy Trubshaw毕业以后交给他的师弟 Richard Battle，在 Richard Battle手上，不断的添加各种玩法到一百多个房间，终于让 MUD发扬光大。 用户使用 Telnet之类的客户端用 Tcp协议连接到 MUDOS上，使用纯文字进行游戏，每条指令用回车进行分割。比如 1995年国内第一款 MUD游戏《侠客行》，你敲入：”go east”，游戏就会提示你：“后花园 - 这里是归云庄的后花园，种满了花草，几个庄丁正在浇花。此地乃是含羞草生长之地。这里唯一的出口是 north。这里有：花待阿牧（A mu），还有二位庄丁（Zhuang Ding）”，然后你继续用文字操作，查看阿牧的信息：“look a mu”，系统提示：“花待阿牧（A mu）他是陆乘风的弟子，受命在此看管含羞草。他看起来三十多岁，生得眉清目秀，端正大方，一表人才。他的武艺看上去【不是很高】，出手似乎【极轻】”。然后你可以选择击败他获得含羞草，但是你吃了含羞草却又可能会中毒死亡。在早期网上资源贫乏的时候，这样的游戏有很强的代入感。 用户数据保存在文件中，每个用户登录时，从文本文件里把用户的数据全部加载进来，操作全部在内存里面进行，无需马上刷回磁盘。用户退出了，或者每隔5分钟检查到数据改动了，都会保存会磁盘。这样的系统在当时每台服务器承载个4000人同时游戏，不是特别大的问题。从1991年的 MUDOS发布后，全球各地都在为他改进，扩充，退出新版本，随着 Windows图形机能的增强。1997游戏《UO》在 MUDOS的基础上为角色增加的x,y坐标，为每个房间增加了地图，并且为每个角色增加了动画，形成了第一代的图形网络游戏。 因为游戏内容基本可以通过 LPC脚本进行定制，所以MUDOS也成为名副其实的第一款服务端引擎，引擎一次性开发出来，然后制作不同游戏内容。后续国内的《万王之王》等游戏，很多都是跟《UO》一样，直接在 MUDOS上进行二次开发，加入房间的地图还有角色的坐标等要素，该架构一直为国内的第一代 MMORPG提供了稳固的支持，直到 2003年，还有游戏基于 MUDOS开发。 虽然后面图形化增加了很多东西，但是这些MMORPG后端的本质还是 MUDOS。随着游戏内容的越来越复杂，架构变得越来越吃不消了，各种负载问题慢慢浮上水面，于是有了我们的第二代游戏服务器。 类型3：第二代游戏服务器 2003 2000年后，网游已经脱离最初的文字MUD，进入全面图形化年代。最先承受不住的其实是很多小文件，用户上下线，频繁的读取写入用户数据，导致负载越来越大。随着在线人数的增加和游戏数据的增加，服务器变得不抗重负。同时早期 EXT磁盘分区比较脆弱，稍微停电，容易发生大面积数据丢失。因此第一步就是拆分文件存储到数据库去。 此时游戏服务端已经脱离陈旧的 MUDOS体系，各个公司在参考 MUDOS结构的情况下，开始自己用 C在重新开发自己的游戏服务端。并且脚本也抛弃了 LPC，采用扩展性更好的 Python或者 Lua来代替。由于主逻辑使用单线程模型，随着游戏内容的增加，传统单服务器的结构进一步成为瓶颈。于是有人开始拆分游戏世界，变为下面的模型： 游戏服务器压力拆分后得意缓解，但是两台游戏服务器同时访问数据库，大量重复访问，大量数据交换，使得数据库成为下一个瓶颈。于是形成了数据库前端代理（DB Proxy），游戏服务器不直接访问数据库而是访问代理，再有代理访问数据库，同时提供内存级别的cache。早年 MySQL4之前没有提供存储过程，这个前端代理一般和 MySQL跑在同一台上，它转化游戏服务器发过来的高级数据操作指令，拆分成具体的数据库操作，一定程度上代替了存储过程： 但是这样的结构并没有持续太长时间，因为玩家切换场景经常要切换连接，中间的状态容易错乱。而且游戏服务器多了以后，相互之间数据交互又会变得比较麻烦，于是人们拆分了网络功能，独立出一个网关服务 Gate（有的地方叫 Session，有的地方叫 LinkSvr之类的，名字不同而已）： 把网络功能单独提取出来，让用户统一去连接一个网关服务器，再有网关服务器转发数据到后端游戏服务器。而游戏服务器之间数据交换也统一连接到网管进行交换。这样类型的服务器基本能稳定的为玩家提供游戏服务，一台网关服务1-2万人，后面的游戏服务器每台服务5k-1w，依游戏类型和复杂度不同而已，图中隐藏了很多不重要的服务器，如登录和管理。这是目前应用最广的一个模型，到今天任然很多新项目会才用这样的结构来搭建。 人都是有惯性的，按照先前的经验，似乎把 MUDOS拆分的越开性能越好。于是大家继续想，网关可以拆分呀，基础服务如聊天交易，可以拆分呀，还可以提供web接口，数据库可以拆分呀，于是有了下面的模型： 这样的模型好用么？确实有成功游戏使用类似这样的架构，并且发挥了它的性能优势，比如一些大型 MMORPG。但是有两个挑战：每增加一级服务器，状态机复杂度可能会翻倍，导致研发和找bug的成本上升；并且对开发组挑战比较大，一旦项目时间吃紧，开发人员经验不足，很容易弄挂。 比如我见过某上海一线游戏公司的一个 RPG上来就要上这样的架构，我看了下他们团队成员的经验，问了下他们的上线日期，劝他们用前面稍微简单一点的模型。人家自信得很，认为有成功项目是这么做的，他们也要这么做，自己很想实现一套。于是他们义无反顾的开始编码，项目做了一年多，然后，就没有然后了。 现今在游戏成功率不高的情况下，一开始上一套比较复杂的架构需要考虑投资回报率，比如你的游戏上线半年内 PCU会去到多少？如果一个 APRG游戏，每组服务器5千人都到不了的话，那么选择一套更为贴近实际情况的结构更为经济。即使后面你的项目真的超过5千人朝着1万人目标奔的话，相信那个时候你的项目已经挣大钱了，你数着钱加着班去逐步迭代，一次次拆分它，相信心里也是乐开花的。 上面这些类型基本都是从拆分 MUDOS开始，将 MUDOS中的各个部件从单机一步步拆成分布式。虽然今天任然很多新项目在用上面某一种类似的结构，或者自己又做了其他热点模块的拆分。因为他们本质上都是对 MUDOS的分解，故将他们归纳为第二代游戏服务器。]]></content>
      <categories>
        <category>GS</category>
      </categories>
      <tags>
        <tag>DesignPattern</tag>
        <tag>GS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[阅读开源服务器源码基础]]></title>
    <url>%2Fblog%2F2016%2F07%2F04%2F%E9%98%85%E8%AF%BB%E5%BC%80%E6%BA%90%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%BA%90%E7%A0%81%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[当阅读一些开源服务器源码的时候, 如果不知道以下知识, 就会有知识盲点, 导致不知所云.这篇博客会讲述一些相关的编程知识点, 把之前的笔记总结一下.还是那句老话, 带着问题阅读是最容易让人类迅速进入状态的. 进程的内存布局是什么样的?记忆口诀 : 文初堆栈 每个进程所分配的内存由很多部分组成，通常称之为“段( segment)”。如下所示。 文本段 包含了进程运行的程序机器语言指令。文本段具有只读属性，以防止进程通过错 误指针意外修改自身指令。因为多个进程可同时运行同一程序，所以又将文本段设为可 共享，这样，一份程序代码的拷贝可以映射到所有这些进程的虚拟地址空间中。 初始化数据段 包含显式初始化的全局变量和静态变量。当程序加载到内存时，从可执 行文件中读取这些变量的值。 未初始化数据段 包含了未进行显式初始化的全局变量和静态变量。程序启动之前，系统 将本段内所有内存初始化为0。出于历史原因，此段常被称为BSS段，这源于老版本的 汇编语言助记符“block started by symbol”o将经过初始化的全局变量和静态变量与未经 初始化的全局变量和静态变量分开存放，其主要原因在于程序在磁盘上存储时，没有必 要为未经初始化的变量分配存储空间。相反，可执行文件只需记录未初始化数据段的位 置及所需大小，直到运行时再由程序加载器来分配这一空间。 堆(heap) 是可在运行时（为变量）动态进行内存分配的一块区域盛顶端称作program break。 栈( stack) 是一个动态增长和收缩的段，由栈帧（stack frames）组成。系统会为每个 当前调用的函数分配一个栈帧。栈帧中存倍了函数的局部变量（所谓自动变量）、实 参和返回值。 线程的同步机制有哪些? 互斥量 条件变量 自旋锁 自旋锤与互斥量类似，但它不是通过休眠使进程阻塞，而是在获取镪之前一直处于忙等（自 旋）阻塞状态．自旋锁可用于“下情况锁被持有的时间短，而且线程并不希望在重新调度上花 费太多的成本。 读写锁(也叫做共享互斥锁) 读写锁也叫做共享互斥锁( shared-exclusive lock)。当读写锁是读模式锁住时，就可以说成是 以共享模式锁住的。当它是写模式锁住的时候，就可以说成是以互斥模式锁住的。 . . . 如何避免死锁如果线程试图对同一个互斥量加锁两次，那么它自身就会陷入死锁状态，但是使用互斥量时，还有其他不太明显的方式也能产牛死锁。例如，程序中使用一个以上的互斥量时，如果允许一个线程一直占有第一个互斥量，并且在试图锁住第二个互斥量时处于阻塞状态，但是拥有第二个互斥量的线程也在试图锁住第一个互斥量。因为两个线程都在相互请求另一个线程拥有的资源，所以这两个线程都无法向前运行，于是就产生死锁。 可以通过仔细控制互斥量加锁的顺序来避免死锁的发生。例如，假设需要对两个互斥量A和B同时加锁。如果所有线程总是在对互斥量B加锁之前锁住可：斥最A，那么使用这两个瓦斥量就小会产生死锁（当然在其他的资源上仍可能出现死锁）。 类似地，如果所有的线程总是在锁住互斥量A之前锁住互斥量B，那么也不会发生死锁。可能出现的死锁只会发生在一个线程试图锁住另一个线程以相反的顺序锁住的互斥量。 有时候，应用程序的结构使得对互斥量进行排序是很困难的。如果涉及了太多的锁和数据结构，可用的函数并不能把它转换成简单的层次，那么就需要采用另外的方法。 在这种情况下，可以先释放占有的锁，然后过一段时间再试。这种情况可以使用pthread_mutex_trylock接口避免死锁。如果已经占有某些锁而且pthread_mutextrylock接口返回成功，那么就可以前进。但是，如果不能获取锁，可以先释放已经占有的锁，做好浦理工作，然后过一段时间再重新试。 总结 顺序加锁 可以先释放占有的锁，然后过一段时间再试 进程间的同步机制(也就是进程间通信, 能通信就能同步了嘛)有哪些?博客中 有详细说明 管道 匿名管道(父子进程间使用) 命名管道(无亲缘关系进程间使用) FIFO 消息队列 信号量 信号 共享内存 套接字 linux的任务调度机制是什么？Linux 分实时进程和普通进程，实时进程应该先于普通进程而运行。而实时进程的调度机制为： FIFO(先入先出服务调度) RR（时间片轮转调度）。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>UNP</tag>
        <tag>TLPI</tag>
        <tag>APUE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB能取代MySQL或者Redis能取代memcached么]]></title>
    <url>%2Fblog%2F2016%2F06%2F30%2FMongoDB%E8%83%BD%E5%8F%96%E4%BB%A3MySQL%E6%88%96%E8%80%85Redis%E8%83%BD%E5%8F%96%E4%BB%A3memcached%E4%B9%88%2F</url>
    <content type="text"><![CDATA[mongodb和memcached不是一个范畴内的东西。 mongodb是文档型的非关系型数据库，其优势在于查询功能比较强大，能存储海量数据。 mongodb和memcached不存在谁替换谁的问题。和memcached更为接近的是redis。 它们都是内存型数据库，数据保存在内存中，通过tcp直接存取，优势是速度快，并发高，缺点是数据&gt; 类型有限，查询功能不强，一般用作缓存。 一般现在的项目中，用redis来替代memcached。 . . . Redis相比memcached： redis具有持久化机制，可以定期将内存中的数据持久化到硬盘上。 redis具备binlog功能，可以将所有操作写入日志，当redis出现故障，可依照binlog进行数据恢复。 redis支持virtual memory，可以限定内存使用大小，当数据超过阈值，则通过类似LRU的算法把内存中的最不常用数据保存到硬盘的页面文件中。 redis原生支持的数据类型更多，使用的想象空间更大。 mongodb 是文档数据库，用于方便懒人替代mysql等关系数据库的。不过mongodb在内存足够的情况下读写性能不错，大部分应用可以省去cache这一层了。 根据业务场景, 懒人可以使用MongoDB来取代MySQL+memcached,.]]></content>
      <categories>
        <category>DB</category>
      </categories>
      <tags>
        <tag>MongoDB</tag>
        <tag>Redis</tag>
        <tag>MySQL</tag>
        <tag>Memcached</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NAT穿越基础]]></title>
    <url>%2Fblog%2F2016%2F06%2F21%2FNAT%E7%A9%BF%E8%B6%8A%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[NAT类型 锥NAT 对称NAT NAT作用 穿透锥NAT 网络拓扑结构 使用UDP穿透NAT 使用TCP穿透NAT 穿透对称NAT [_同时开放TCP（ Simultaneous TCP open ）策略_](#同时开放TCP（ Simultaneous TCP open ）策略) UDP端口猜测策略 问题总结 参考 NAT类型注 : 我们本文主要讨论穿越锥NAT 锥NAT 全锥NAT ：全锥NAT 把所有来自相同内部IP 地址和端口的请求映射到相同的外部IP 地址和端口。任何一个外部主机均可通过该映射发送数据包到该内部主机。 限制性锥NAT ：限制性锥NAT 把所有来自相同内部IP 地址和端口的请求映射到相同的外部IP 地址和端口。但是, 和全锥NAT 不同的是：只有当内部主机先给外部主机发送数据包, 该外部主机才能向该内部主机发送数据包。 端口限制性锥NAT ：端口限制性锥NAT 与限制性锥NAT 类似, 只是多了端口号的限制, 即只有内部主机先向外部地址：端口号对发送数据包, 该外部主机才能使用特定的端口号向内部主机发送数据包。 对称NAT对称NAT 与上述3 种类型都不同, 不管是全锥NAT ，限制性锥NAT 还是端口限制性锥NAT ，它们都属于锥NAT （Cone NAT ）。当同一内部主机使用相同的端口与不同地址的外部主机进行通信时, 对称NAT 会重新建立一个Session ，为这个Session 分配不同的端口号，或许还会改变IP 地址。 . . . NAT作用NAT 不仅实现地址转换，同时还起到防火墙的作用，隐藏内部网络的拓扑结构，保护内部主机。 NAT 不仅完美地解决了 lP 地址不足的问题，而且还能够有效地避免来自网络外部的攻击，隐藏并保护网络内部的计算机。 这样对于外部主机来说，内部主机是不可见的。 但是，对于P2P 应用来说，却要求能够建立端到端的连接，所以如何穿透NAT 也是P2P 技术中的一个关键。 穿透锥NAT要让处于NAT 设备之后的拥有私有IP 地址的主机之间建立P2P 连接，就必须想办法穿透NAT ，现在常用的传输层协议主要有TCP 和UDP ，下面就是用这两种协议来介绍穿透NAT 的策略。 网络拓扑结构下面假设有如图1 所示网络拓扑结构图。 Server （129.208.12.38 ）是公网上的服务器，NAT-A 和NAT-B 是两个NAT 设备（可能是集成NAT 功能的路由器，防火墙等），它们具有若干个合法公网IP ，在NAT-A 阻隔的私有网络中有若干台主机【ClientA-1 ，ClientA-N 】，在NAT-B 阻隔的私有网络中也有若干台主机【ClientB-1 ，ClientB-N 】。 为了以后说明问题方便，只讨论主机ClientA-1 和ClientB-1 。 假设主机ClientA-1 和主机ClientB-1 都和服务器Server 建立了“连接”，如图2 所示。 由于NAT 的透明性，所以ClientA-1 和ClientB-1 不用关心和Server 通信的过程，它们只需要知道Server 开放服务的地址和端口号即可。 根据图1 ，假设在ClientA-1 中有进程使用socket （192.168.0.2 ：7000 ）和Server 通信，在ClientB-1 中有进程使用socket （192.168.1.12:8000 ）和Server 通信。 它们通过各自的NAT 转换后分别变成了socket （202.103.142.29 ：5000 ）和socket （221.10.145.84 ：6000 ）。 使用UDP穿透NAT通常情况下，当进程使用UDP 和外部主机通信时，NAT 会建立一个Session ，这个Session 能够保留多久并没有标准，或许几秒，几分钟，几个小时。 假设ClientA-1 在应用程序中看到了ClientB-1 在线，并且想和ClientB-1 通信，一种办法是Server 作为中间人，负责转发ClientA-1 和ClientB-1 之间的消息，但是这样服务器太累，会吃不消。 另一种方法就是让ClientA-1 何ClientB-1 建立端到端的连接，然后他们自己通信。 这也就是P2P 连接。 根据不同类型的NAT ，下面分别讲解。 全锥NAT ，穿透全锥型NAT 很容易，根本称不上穿透，因为全锥型NAT 将内部主机的映射到确定的地址，不会阻止从外部发送的连接请求，所以可以不用任何辅助手段就可以建立连接。 限制性锥NAT 和端口限制性锥NAT （简称限制性NAT ），穿透限制性锥NAT 会丢弃它未知的源地址发向内部主机的数据包。 所以如果现在ClientA-1 直接发送UDP 数据包到ClientB-1 ，那么数据包将会被NAT-B 无情的丢弃。 所以采用下面的方法来建立ClientA-1 和ClientB-1 之间的通信。 ClientA-1 （202.103.142.29:5000 ）发送数据包给Server ，请求和ClientB-1 （221.10.145.84:6000 ）通信。 Server 将ClientA-1 的地址和端口（202.103.142.29:5000 ）发送给ClientB-1 ，告诉ClientB-1 ，ClientA-1 想和它通信。 ClientB-1 向ClientA-1 （202.103.142.29:5000 ）发送UDP 数据包，当然这个包在到达NAT-A 的时候，还是会被丢弃，这并不是关键的，因为发送这个UDP 包只是为了让NAT-B 记住这次通信的目的地址：端口号，当下次以这个地址和端口为源的数据到达的时候就不会被NAT-B 丢弃，这样就在NAT-B 上打了一个从ClientB-1 到ClientA-1 的孔。 为了让ClientA-1 知道什么时候才可以向ClientB-1 发送数据，所以ClientB-1 在向ClientA-1 （202.103.142.29:5000 ）打孔之后还要向Server 发送一个消息，告诉Server 它已经准备好了。 Server 发送一个消息给ClientA-1 ，内容为：ClientB-1 已经准备好了，你可以向ClientB-1 发送消息了。 ClientA-1 向ClientB-1 发送UDP 数据包。 这个数据包不会被NAT-B 丢弃，以后ClientB-1 向ClientA-1 发送的数据包也不会被ClientA-1 丢弃，因为NAT-A 已经知道是ClientA-1 首先发起的通信。 至此，ClientA-1 和ClientB-1 就可以进行通信了。 使用TCP穿透NAT使用TCP 协议穿透NAT 的方式和使用UDP 协议穿透NAT 的方式几乎一样，没有什么本质上的区别，只是将无连接的UDP 变成了面向连接的TCP 。 值得注意是： ClientB-1 在向ClientA-1 打孔时，发送的SYN 数据包，而且同样会被NAT-A 丢弃。同时，ClientB-1 需要在原来的socket 上监听，由于重用socket ，所以需要将socket 属性设置为SO_REUSEADDR 。 ClientA-1 向ClientB-1 发送连接请求。同样，由于ClientB-1 到ClientA-1 方向的孔已经打好，所以连接会成功，经过3 次握手后，ClientA-1 到ClientB-1 之间的连接就建立起来了。 穿透对称NAT上面讨论的都是怎样穿透锥（Cone ）NAT ，对称NAT 和锥NAT 很不一样。 对于 对称NAT ，当一个私网内主机和外部多个不同主机通信时，对称NAT 并不会像锥（Cone ，全锥，限制性锥，端口限制性锥）NAT 那样分配同一个端口。 而是会新建立一个Session ，重新分配一个端口。 参考上面穿透限制性锥NAT 的过程，在步骤3 时：ClientB-1 （221.10.145.84: ？）向ClientA-1 打孔的时候，对称NAT 将给ClientB-1 重新分配一个端口号，而这个端口号对于Server 、ClientB-1 、ClientA-1 来说都是未知的。 同样， ClientA-1 根本不会收到这个消息，同时在步骤4 ，ClientB-1 发送给Server 的通知消息中，ClientB-1 的socket 依旧是（221.10.145.84:6000 ）。 而且，在步骤6 时：ClientA-1 向它所知道但错误的ClientB-1 发送数据包时，NAT-1 也会重新给ClientA-1 分配端口号。 所以，穿透对称NAT 的机会很小。 下面是两种有可能穿透对称NAT 的策略。 同时开放TCP（ Simultaneous TCP open ）策略如果一个 对称 NAT 接收到一个来自 本地 私有网 络 外面的 TCP SYN 包， 这 个包想 发 起一个 “ 引入” 的 TCP 连 接，一般来 说 ， NAT 会拒 绝这 个 连 接 请 求并扔掉 这 个 SYN 包，或者回送一个TCP RST （connection reset ，重建 连 接）包 给请 求方。 但是，有一 种 情况 却会接受这个“引入”连接。 RFC 规定：对于对称NAT ， 当 这 个接收到的 SYN 包中的源IP 地址 ： 端口、目 标 IP 地址 ： 端口都与NAT 登 记 的一个已 经 激活的 TCP 会 话 中的地址信息相符 时 ， NAT 将会放行 这 个 SYN 包。 需要 特 别 指出 的是：怎样才是一个已经激活的TCP 连接？除了真正已经建立完成的TCP 连接外，RFC 规范指出： 如果 NAT 恰好看到一个 刚刚发 送出去的一个 SYN 包和 随之 接收到的SYN 包中的地址 ：端口 信息相符合的 话 ，那 么 NAT 将会 认为这 个 TCP 连 接已 经 被激活，并将允 许这 个方向的 SYN 包 进 入 NAT 内部。 同时开放TCP 策略就是利用这个时机来建立连接的。 如果 Client A -1 和 Client B -1 能 够 彼此正确的 预 知 对 方的 NAT 将会 给 下一个 TCP 连 接分配的公网 TCP 端口，并且两个客 户 端能 够 同 时 地 发 起一 个面向对方的 “ 外出 ” 的 TCP 连 接 请求 ，并在 对 方的 SYN 包到达之前，自己 刚发 送出去的 SYN 包都能 顺 利的穿 过 自己的 NAT 的 话 ，一条端 对 端的 TCP 连 接就 能 成功地建立了 。 UDP端口猜测策略同时开放TCP 策略非常依赖于猜测对方的下一个端口，而且强烈依赖于发送连接请求的时机，而且还有网络的不确定性，所以能够建立的机会很小，即使Server 充当同步时钟的角色。 下面是一种通过UDP 穿透的方法，由于UDP 不需要建立连接，所以也就不需要考虑“同时开放”的问题。 为了介绍ClientB-1 的诡计，先介绍一下STUN 协议。 STUN （Simple Traversal of UDP Through NATs ）协议是一个轻量级协议，用来探测被NAT 映射后的地址：端口。 STUN 采用C/S 结构，需要探测自己被NAT 转换后的地址：端口的Client 向Server 发送请求，Server 返回Client 转换后的地址：端口。 参考4.2 节中穿透NAT 的步骤2 ，当ClientB-1 收到Server 发送给它的消息后，ClientB-1 即打开3 个socket 。 socket-0 向STUN Server 发送请求，收到回复后，假设得知它被转换后的地址：端口（ 221.10.145.84:600 5 ），socket-1 向ClientA-1 发送一个UDP 包，socket-2 再次向另一个STUN Server 发送请求，假设得到它被转换后的地址：端口（ 221.10.145.84:60 20 ）。 通常，对称NAT 分配端口有两种策略，一种是按顺序增加，一种是随机分配。 如果这里对称NAT 使用顺序增加策略，那么，ClientB-1 将两次收到的地址：端口发送给Server 后，Server 就可以通知ClientA-1 在这个端口范围内猜测刚才ClientB-1 发送给它的socket-1 中被NAT 映射后的地址：端口，ClientA-1 很有可能在孔有效期内成功猜测到端口号，从而和ClientB-1 成功通信。 问题总结从上面两种穿透对称NAT 的方法来看，都建立在了严格的假设条件下。 但是现实中多数的NAT 都是锥NAT ，因为资源毕竟很重要，反观对称NAT ，由于太不节约端口号所以相对来说成本较高。 所以，不管是穿透锥NAT ，还是对称NAT ，现实中都是可以办到的。 除非对称NAT 真的使用随机算法来分配可用的端口。 参考这篇博客]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>NAT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL进阶二]]></title>
    <url>%2Fblog%2F2016%2F06%2F16%2FMySQL%E8%BF%9B%E9%98%B6%E4%BA%8C%2F</url>
    <content type="text"><![CDATA[事务MySQL 事务主要用于处理操作量大，复杂度高的数据。比如说，在人员管理系统中，你删除一个人员，你即需要删除人员的基本资料，也要删除和该人员相关的信息，如信箱，文章等等，这样，这些数据库操作语句就构成一个事务！ 在 MySQL 中只有使用了 Innodb 数据库引擎的数据库或表才支持事务。 事务处理可以用来维护数据库的完整性，保证成批的 SQL 语句要么全部执行，要么全部不执行。 事务用来管理 insert,update,delete 语句 . . . ACID一般来说，事务是必须满足4个条件（ACID）： Atomicity（原子性）、Consistency（一致性或稳定性）、Isolation（隔离性）、Durability（持久性） A 事务的原子性：一组事务，要么成功, 要么撤回。 C 稳定性 ：有非法数据（外键约束之类），事务撤回。 I 隔离性：事务独立运行。一个事务处理后的结果，影响了其他事务，那么其他事务会撤回。事务的100%隔离，需要牺牲速度。 D 持久性：软、硬件崩溃后，InnoDB数据表驱动会利用日志文件重构修改。可靠性和高速度不可兼得. 在 MySQL 命令行的默认设置下，事务都是自动提交的，即执行 SQL 语句后就会马上执行 COMMIT 操作。因此要显式地开启一个事务务须使用命令 BEGIN 或 START TRANSACTION，或者执行命令 SET AUTOCOMMIT=0，用来禁止使用当前会话的自动提交。 事务控制语句： BEGIN或START TRANSACTION；显式地开启一个事务； COMMIT；也可以使用COMMIT WORK，不过二者是等价的。COMMIT会提交事务，并使已对数据库进行的所有修改称为永久性的； ROLLBACK；有可以使用ROLLBACK WORK，不过二者是等价的。回滚会结束用户的事务，并撤销正在进行的所有未提交的修改； SAVEPOINT identifier；SAVEPOINT允许在事务中创建一个保存点，一个事务中可以有多个SAVEPOINT； RELEASE SAVEPOINT identifier；删除一个事务的保存点，当没有指定的保存点时，执行该语句会抛出一个异常； ROLLBACK TO identifier；把事务回滚到标记点； SET TRANSACTION；用来设置事务的隔离级别。InnoDB存储引擎提供事务的隔离级别有READ UNCOMMITTED、READ COMMITTED、REPEATABLE READ和SERIALIZABLE。 MYSQL 事务处理主要有两种方法： 用 BEGIN, ROLLBACK, COMMIT来实现 BEGIN 开始一个事务 ROLLBACK 事务回滚 COMMIT 事务确认 直接用 SET 来改变 MySQL 的自动提交模式: SET AUTOCOMMIT=0 禁止自动提交 SET AUTOCOMMIT=1 开启自动提交 事务的4个隔离级别具体例子参考这篇博客 读未提交(Read Uncommitted)：允许脏读，也就是可能读取到其他会话中未提交事务修改的数据 读已提交(Read Committed)：只能读取到已经提交的数据。Oracle等多数数据库默认都是该级别 (不重复读) 可重复读(Repeated Read)：可重复读。在同一个事务内的查询都是事务开始时刻一致的，InnoDB默认级别。在SQL标准中，该隔离级别消除了不可重复读，但是还存在幻象读 串行读(Serializable)：完全串行化的读，每次读都需要获得表级共享锁，读写相互都会阻塞 相关术语 脏读 :脏读就是指当一个事务正在访问数据，并且对数据进行了修改，而这种修改还没有提交到数据库中，这时，另外一个事务也访问这个数据，然后使用了这个数据。 不可重复读 :是指在一个事务内，多次读同一数据。在这个事务还没有结束时，另外一个事务也访问该同一数据。那么，在第一个事务中的两次读数据之间，由于第二个事务的修改，那么第一个事务两次读到的的数据可能是不一样的。这样就发生了在一个事务内两次读到的数据是不一样的，因此称为是不可重复读。 幻读 :第一个事务对一个表中的数据进行了修改，这种修改涉及到表中的全部数据行。同时，第二个事务也修改这个表中的数据，这种修改是向表中插入一行新数据。那么，以后就会发生操作第一个事务的用户发现表中还有没有修改的数据行，就好象发生了幻觉一样。 临时表临时表在我们需要保存一些临时数据时是非常有用的。临时表只在当前连接可见，当关闭连接时，Mysql会自动删除表并释放所有空间。 防SQL注入防止SQL注入，我们需要注意以下几个要点： 永远不要信任用户的输入。对用户的输入进行校验，可以通过正则表达式，或限制长度；对单引号和 双-“进行转换等。 永远不要使用动态拼装SQL，可以使用参数化的SQL或者直接使用存储过程进行数据查询存取。 永远不要使用管理员权限的数据库连接，为每个应用使用单独的权限有限的数据库连接。 不要把机密信息直接存放，加密或者hash掉密码和敏感的信息。 应用的异常信息应该给出尽可能少的提示，最好使用自定义的错误信息对原始错误信息进行包装 SQL注入的检测方法一般采取辅助软件或网站平台来检测，软件一般采用SQL注入检测工具jsky，网站平台就有亿思网站安全平台检测工具。MDCSOFT SCAN等。采用MDCSOFT-IPS可以有效的防御SQL注入，XSS攻击等。 可以总结为 : 包装提示 (白) 连接权限 (莲) 校验输入 (教)]]></content>
      <categories>
        <category>DB</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL进阶一]]></title>
    <url>%2Fblog%2F2016%2F06%2F14%2FMySQL%E8%BF%9B%E9%98%B6%E4%B8%80%2F</url>
    <content type="text"><![CDATA[引擎MySQL是有多个引擎的, 不同的场景情况用不同的引擎以提升性能和灵活性.三大最常用的引擎 : InnoDB : 可靠的事务处理引擎 ,不支持全文搜索 MyISAM : 支持全文搜索, 不支持事务处理 MEMORY : 功能等同于MyISAM, 但数据存储在内存而不是磁盘, 所以速度非常快, 特别适用于临时表(temporary table) 索引索引是用来改善搜索性能的, 不要滥用索引, 比如如对表进行INSERT、UPDATE和DELETE的操作索引反而会降低更新表的速度。因为更新表时，MySQL不仅要保存数据，还要保存一下索引文件。 . . . 索引有三种 : 普通索引normal 唯一索引unique : 它与前面的普通索引类似，不同的就是：索引列的值必须唯一，但允许有空值。如果是组合索引，则列值的组合必须唯一。 全文索引fulltext : 表示 全文搜索的索引。 FULLTEXT 用于搜索很长一篇文章的时候，效果最好。用在比较短的文本，如果就一两行字的，普通的 INDEX 也可以。 建立索引需要遵守的规则 : 选择唯一性索引 为经常需要排序、分组和联合操作的字段建立索引 为常作为查询条件的字段建立索引 限制索引的数目 尽量使用数据量少的索引 尽量使用前缀来索引 删除不再使用或者很少使用的索引 一些优化建议 like和fulltext like没有fulltext快, 但需要大内容的全文本搜索的时候来一发fulltext吧 选择正确数据类型 比如 : 定长的数据类型比可变长度的数据类型性能要高 正确使用索引 勿滥用select *语句 关闭自动提交]]></content>
      <categories>
        <category>DB</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[UE4旋转笔记]]></title>
    <url>%2Fblog%2F2016%2F05%2F31%2FUE4%E6%97%8B%E8%BD%AC%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[最近想将一个vector转化为rotator，转而需要考虑UE4到底是怎么旋转的。下面我们做个实验： 我们先将两个staticMesh放入场景，并将它们的rotation调成一样，如上图。上面那个为renti_a_gear，下面那个为renti_a_gear2. . . . 第一种情况： 绕自身坐标系来旋转 如上图，两个staticMesh旋转之后rotation是一样的，可以证明，绕自身坐标系旋转的顺序是Z-&gt;Y-&gt;X 第二种情况： 绕世界坐标系来旋转 如上图，两个staticMesh旋转之后rotation是一样的，可以证明，绕世界坐标系旋转的顺序跟第一种情况刚好反过来， 是X-&gt;Y-&gt;Z]]></content>
      <categories>
        <category>UE4</category>
      </categories>
      <tags>
        <tag>UE4</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[http报文笔记整理]]></title>
    <url>%2Fblog%2F2016%2F05%2F24%2Fhttp%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86%E4%B9%8B%E6%8A%A5%E6%96%87%2F</url>
    <content type="text"><![CDATA[看了书和各种网上资料, 学东西嘛, 要做总结, 这些老笔记整理一下, 供以后方便查阅也加强印象和理解. 报文的组成 起始行(start line) 首部(header) 主体(body) 可细分为 : 方法 :如GET, HEAD, POST . . . *关于HTTP请求GET和POST的区别 : **1.提交方式的区别: * GET提交，请求的数据会附在URL之后（就是把数据放置在http起始行中），以?分割URL和传输数据，多个参数用&amp;连接;例如：login.action?name=hyddd&amp;password=idontknow&amp;verify=%E4%BD%A0 %E5%A5%BD。如果数据是英文字母/数字，原样发送，如果是空格，转换为+，如果是中文/其他字符，则直接把字符串用BASE64加密，得出如： %E4%BD%A0%E5%A5%BD，其中％XX中的XX为该符号以16进制表示的ASCII。 POST提交：把提交的数据放置在是HTTP主体中。 因此，GET提交的数据会在地址栏中显示出来，而POST提交，地址栏不会改变 2.传输数据的大小： 首先声明,HTTP协议没有对传输的数据大小进行限制，HTTP协议规范也没有对URL长度进行限制。 而在实际开发中存在的限制主要有： GET:特定浏览器和服务器对URL长度有限制，例如IE对URL长度的限制是2083字节(2K+35)。对于其他浏览器，如Netscape、FireFox等，理论上没有长度限制，其限制取决于操作系统的支持。 因此对于GET提交时，传输数据就会受到URL长度的限制。 POST:由于不是通过URL传值，理论上数据不受限。但实际各个WEB服务器会规定对post提交数据大小进行限制，Apache、IIS6都有各自的配置。 3.安全性： POST的安全性要比GET的安全性高。注意：这里所说的安全性和上面GET提到的“安全”不是同个概念。上面“安全”的含义仅仅是不作数据修改，而这里安全的含义是真正的Security的含义，比如：通过GET提交数据，用户名和密码将明文出现在URL上，因为 (1)登录页面有可能被浏览器缓存， (2)其他人查看浏览器的历史纪录，那么别人就可以拿到你的账号和密码了， *请求URL *URL是浏览器寻找信息时所需的资源位置 .URL分为三个部分 : URL文案 服务器位置 资源路径 版本号上图中的HTTP/1.0 200 OK, HTTP/1.0就是版本号 *状态码 : *如最著名的404, 302, 如上图中的HTTP/1.0 200 OK中, 状态码就是200 原因短语 如上图中的HTTP/1.0 200 OK中, OK就是原因短语 首部 主体主体部分是可选的, 主体是http报文要传输的内容, 可以承载很多类型的数字数据 : 图片, 视频, 软件应用程序, 电子邮件等]]></content>
      <categories>
        <category>NP</category>
      </categories>
      <tags>
        <tag>HTTP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[php与cgi]]></title>
    <url>%2Fblog%2F2016%2F05%2F22%2Fphp_cgi%2F</url>
    <content type="text"><![CDATA[首先，CGI是干嘛的？CGI是为了保证web server传递过来的数据是标准格式的，方便CGI程序的编写者。 web server（比如说nginx）只是内容的分发者。比如，如果请求/index.html，那么web server会去文件系统中找到这个文件，发送给浏览器，这里分发的是静态数据。好了，如果现在请求的是/index.php，根据配置文件，nginx知道这个不是静态文件，需要去找PHP解析器来处理，那么他会把这个请求简单处理后交给PHP解析器。Nginx会传哪些数据给PHP解析器呢？url要有吧，查询字符串也得有吧，POST数据也要有，HTTP header不能少吧，好的，CGI就是规定要传哪些数据、以什么样的格式传递给后方处理这个请求的协议。仔细想想，你在PHP代码中使用的用户从哪里来的。 当web server收到/index.php这个请求后，会启动对应的CGI程序，这里就是PHP的解析器。接下来PHP解析器会解析php.ini文件，初始化执行环境，然后处理请求，再以规定CGI规定的格式返回处理后的结果，退出进程。web server再把结果返回给浏览器。 好了，CGI是个协议，跟进程什么的没关系。那fastcgi又是什么呢？Fastcgi是用来提高CGI程序性能的。 . . . 提高性能，那么CGI程序的性能问题在哪呢？”PHP解析器会解析php.ini文件，初始化执行环境”，就是这里了。标准的CGI对每个请求都会执行这些步骤（不闲累啊！启动进程很累的说！），所以处理每个时间的时间会比较长。这明显不合理嘛！那么Fastcgi是怎么做的呢？首先，Fastcgi会先启一个master，解析配置文件，初始化执行环境，然后再启动多个worker。当请求过来时，master会传递给一个worker，然后立即可以接受下一个请求。这样就避免了重复的劳动，效率自然是高。而且当worker不够用时，master可以根据配置预先启动几个worker等着；当然空闲worker太多时，也会停掉一些，这样就提高了性能，也节约了资源。这就是fastcgi的对进程的管理。 那PHP-FPM又是什么呢？是一个实现了Fastcgi的程序，被PHP官方收了。 大家都知道，PHP的解释器是php-cgi。php-cgi只是个CGI程序，他自己本身只能解析请求，返回结果，不会进程管理（皇上，臣妾真的做不到啊！）所以就出现了一些能够调度php-cgi进程的程序，比如说由lighthttpd分离出来的spawn-fcgi。好了PHP-FPM也是这么个东东，在长时间的发展后，逐渐得到了大家的认可（要知道，前几年大家可是抱怨PHP-FPM稳定性太差的），也越来越流行。好了，最后来回来你的问题。 网上有的说，fastcgi是一个协议，php-fpm实现了这个协议 对。 有的说，php-fpm是fastcgi进程的管理器，用来管理fastcgi进程的 对。php-fpm的管理对象是php-cgi。但不能说php-fpm是fastcgi进程的管理器，因为前面说了fastcgi是个协议，似乎没有这么个进程存在，就算存在php-fpm也管理不了他（至少目前是）。 有的说，php-fpm是php内核的一个补丁 以前是对的。因为最开始的时候php-fpm没有包含在PHP内核里面，要使用这个功能，需要找到与源码版本相同的php-fpm对内核打补丁，然后再编译。后来PHP内核集成了PHP-FPM之后就方便多了，使用–enalbe-fpm这个编译参数即可。 有的说，修改了php.ini配置文件后，没办法平滑重启，所以就诞生了php-fpm 是的，修改php.ini之后，php-cgi进程的确是没办法平滑重启的。php-fpm对此的处理机制是新的worker用新的配置，已经存在的worker处理完手上的活就可以歇着了，通过这种机制来平滑过度。 还有的说PHP-CGI是PHP自带的FastCGI管理器，那这样的话干吗又弄个php-fpm出 不对。php-cgi只是解释PHP脚本的程序而已。 参考]]></content>
      <categories>
        <category>Script</category>
      </categories>
      <tags>
        <tag>PHP</tag>
        <tag>CGI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[也想做一个这样的博客吗?]]></title>
    <url>%2Fblog%2F2016%2F05%2F22%2Fhow_to_make_blog_like_this%2F</url>
    <content type="text"><![CDATA[这是我的博客源码 ,我修改了很多NexT的代码来对原版 NexT 做了优化, 如下 : 改了NexT的很多地方来优化移动端的表现, header的布局 移动端和PC端的侧边栏更加统一 移动端的文章目录列表现在可以滑动了 重做了本地搜索引擎 现在移动端不会经常无故弹不出键盘了 也不会列出加密文章的内容了 更优雅的过渡动画 添加了headroom支持, 现在有一个可以会自动隐藏的header了, 往下滚一下鼠标则隐藏, 往上则出现 升级到了fancybox3并完成适配, 3更流畅且拥有更多效果 添加了文章加密的支持 CSDN sucks 实在是被CSDN的广告恶心到了 最近CSDN的Markdown的无序列表每一列前面的小黑点都没有了 手机网页版本的CSDN排版全无 所有博客瞬间排版全部变成一坨, 实在是不能忍. 所以才做了这个私人博客.]]></content>
      <categories>
        <category>Misc</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[vector和string的内存分配与使用注意点]]></title>
    <url>%2Fblog%2F2016%2F05%2F17%2Fvector%E5%92%8Cstring%E7%9A%84%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E4%B8%8E%E4%BD%BF%E7%94%A8%E6%B3%A8%E6%84%8F%E7%82%B9%2F</url>
    <content type="text"><![CDATA[增长方式为了支持快速随机访问 ， vector 将元素连续存储一一每个元素紧挨着前一个元素存储 。 问题假定容器中元素是连续存储 的， 且容器的大小是可变的 ， 考虑 向 vector 或 string中添加元素会发生什么 :如果没有空间容纳新元素，容器不可能简单地将它添加到内存中其他位置一一因为元素必须连续存储。 容器必须分配新的内存空间来保存己有元素和新元素 ， 将已有元素从 旧位置移动到新空 间中， 然后添加新元素，释放旧存储空间 。 如果我们每添加一个新元素， vector 就执行一次这样的内存分配和释放操作 ，性能会慢到不可接受 。 . . . 解决方案为了避免这种代价，标准库实现者采用了可以减少容器空间重新分配次数的策略。当不得不在取新的内 存空间 时， vector 和 string 的实现通常会分配比新的空间需求更大的内存空间 。 容器预留这些空间作为备用 ， 可用来保存更多的新元素 。这样，就不需要每次添加新元素都重新分配容器的内存空间了 。这种分配策略比每次添加新元素时都重新分配容器内存空间的策略要高效得多 。 其实际性能也表现得足够好一一虽然 vector 在每次重新分配内存空间时都要移动所有元素，但使用 此策略后，其扩张操作通常比 list 和 deque 还要快 。 增长方式的具体实现STL提供了很多泛型容器，如vector，list和map。程序员在使用这些容器时只需关心何时往容器内塞对象，而不用关心如何管理内存，需要用多少内存，这些STL容器极大地方便了C++程序的编写。 例如可以通过以下语句创建一个vector，它实际上是一个按需增长的动态数组，其每个元素的类型为int整型： stl::vector&lt;int&gt; testVector; 拥有这样一个动态数组后，用户只需要调用push_back方法往里面添加对象，而不需要考虑需要多少内存： 12testVector.push_back(10); testVector.push_back(2); vector会根据需要自动增长内存，在testVector退出其作用域时也会自动销毁占有的内存，这些对于用户来说是透明的，stl容器巧妙的避开了繁琐且易出错的内存管理工作。 隐藏在这些容器后的内存管理工作是通过STL提供的一个默认的allocator实现的。当然，用户也可以定制自己的allocator，只要实现allocator模板所定义的接口方法即可，然后通过将自定义的allocator作为模板参数传递给STL容器，创建一个使用自定义allocator的STL容器对象，如： stl::vector&lt;int, UserDefinedAllocator&gt; testVector; 大多数情况下，STL默认的allocator就已经足够了。这个allocator是一个由两级分配器构成的内存管理器， 当申请的内存大小大于128byte时，就启动第一级分配器通过malloc直接向系统的堆空间分配， 如果申请的内存大小小于128byte时，就启动第二级分配器，从一个预先分配好的内存池中取一块内存交付给用户，这个内存池由16个不同大小（8的倍数，8~128byte）的空闲列表组成，allocator会根据申请内存的大小（将这个大小round up成8的倍数）从对应的空闲块列表取表头块给用户。 这种做法有两个优点： 小对象的快速分配。小对象是从内存池分配的，这个内存池是系统调用一次malloc分配一块足够大的区域给程序备用，当内存池耗尽时再向系统申请一块新的区域，整个过程类似于批发和零售，起先是由allocator向总经商批发一定量的货物，然后零售给用户，与每次都总经商要一个货物再零售给用户的过程相比，显然是快捷了。当然，这里的一个问题时，内存池会带来一些内存的浪费，比如当只需分配一个小对象时，为了这个小对象可能要申请一大块的内存池，但这个浪费还是值得的，况且这种情况在实际应用中也并不多见。 避免了内存碎片的生成。程序中的小对象的分配极易造成内存碎片，给操作系统的内存管理带来了很大压力，系统中碎片的增多不但会影响内存分配的速度，而且会极大地降低内存的利用率。以内存池组织小对象的内存，从系统的角度看，只是一大块内存池，看不到小对象内存的分配和释放。 vector的内存释放由于vector的内存占用空间只增不减，比如你首先分配了10,000个字节， 然后erase掉后面9,999个，留下一个有效元素，但是内存占用仍为10,000个。 所有内存空间是在vector析构时候才能被系统回收。empty()用来检测容器是否为空的， clear()可以清空所有元素。但是即使clear()， vector所占用的内存空间依然如故，无法保证内存的回收。 如果需要空间动态缩小，可以考虑使用 deque 。如果vector，可以用swap()来帮助你释放内存。具体方法如下： vector&lt;int&gt;().swap(tempVector); //或者 tempVector.swap(vector&lt;int&gt;()) vector的内存释放代码实例1123456789101112131415161718192021222324252627282930313233#include &lt;iostream&gt;#include &lt;vector&gt;int main()&#123; std::vector&lt;int&gt; foo; foo.push_back(1); foo.push_back(2); foo.push_back(3); foo.push_back(4); foo.push_back(5); std::vector&lt;int&gt; bar; bar.push_back(1); bar.push_back(2); std::cout &lt;&lt; "foo size:" &lt;&lt; foo.size() &lt;&lt; std::endl; std::cout &lt;&lt; "foo capacity:" &lt;&lt; foo.capacity() &lt;&lt; std::endl; std::cout &lt;&lt; "bar size:" &lt;&lt; bar.size() &lt;&lt; std::endl; std::cout &lt;&lt; "bar capacity:" &lt;&lt; bar.capacity() &lt;&lt; std::endl; foo.swap(bar); std::cout &lt;&lt; "after swap foo size:" &lt;&lt; foo.size() &lt;&lt; std::endl; std::cout &lt;&lt; "after swap foo capacity:" &lt;&lt; foo.capacity() &lt;&lt; std::endl; std::cout &lt;&lt; "after swap bar size:" &lt;&lt; bar.size() &lt;&lt; std::endl; std::cout &lt;&lt; "after swap bar capacity:" &lt;&lt; bar.capacity() &lt;&lt; std::endl; return 0;&#125; 输出： 12345678foo size:5foo capacity:6bar size:2bar capacity:2after swap foo size:2after swap foo capacity:2after swap bar size:5after swap bar capacity:6 看到了吗，swap之后，不仅仅是size变化了，capacity也是变化了。那么于是就把swap替代clear了： vector的内存释放代码实例21234567891011121314151617181920#include&lt;iostream&gt;#include&lt;vector&gt;using namespace std;int main()&#123; vector&lt;int&gt; v; v.push_back(1); v.push_back(2); v.push_back(3); v.push_back(4); v.push_back(5); cout &lt;&lt; "size:" &lt;&lt; v.size() &lt;&lt; endl; cout &lt;&lt; "capacity:" &lt;&lt; v.capacity() &lt;&lt; endl; vector&lt;int&gt;().swap(v); cout &lt;&lt; "after swap size:" &lt;&lt; v.size() &lt;&lt; endl; cout &lt;&lt; "after swap capacity:" &lt;&lt; v.capacity() &lt;&lt; endl; return 0;&#125; 输出： 1234size:5capacity:6after swap size:0after swap capacity:0 迭代器失效的问题看了上方的实现方式, 相信很容易能理解迭代器失效问题了啊 向容器中添加元素和从容器中删除元素 的操作可能(下面将会讨论为什么是可能)会使指向容器元素的指针、引用或法代器失效。 一个失效的指针、引用或法代器将不再表示任何元素 。 使用失效的指针、引用或迭代器是一种严重的程序设计错误，很可能引起与使用未初始化指针一样的问题 *所以, 向容器中添加元素和从容器中删除元素的操作都需要用以下方法重置一下迭代器 : * i = q.insert(i,22); i = q.erase(i); (以下三种情况都是在ubuntu g++环境测试的, 不同平台不同编译器会有不同表现, 比如同样的代码有可能会在vs下崩溃) 删除元素123456789101112131415161718192021//erase操作#include&lt;vector&gt;#include&lt;iostream&gt;using namespace std;int main()&#123; vector&lt;int&gt;q&#123;1,2,3,4,5,6,7,8,9,10&#125;; int cnt = 0; int flag = 0; for(vector&lt;int&gt;::iterator i = q.begin(); i != q.end(); ++i)&#123; ++cnt; if(cnt &gt; 15)&#123; cout&lt;&lt;"gg"&lt;&lt;endl; break; &#125; if(*i == 3) //删除第三个 i = q.erase(i); cout &lt;&lt; *i &lt;&lt; endl; cout &lt;&lt; &amp;(*i) &lt;&lt; endl; &#125; return 0;&#125; output: 12345678910111213141516171810xc7215820xc7215c40xc7216050xc7216460xc7216870xc7216c80xc7217090xc72174100xc72178 输出结果分析: 当删除第3个元素以后我们发现第四个元素是紧邻第二个元素的（刚好差一个int的内存）, 也就是说vector执行erase（i）后会将迭代器i之后的元素逐个向前移动一个type单位,所以其实这种c++实现迭代器没失效, 但是其他的c++实现, 有可能所有元素全部移到另外一块内存, 比如这段代码放到vs是会崩溃的 添加元素时若预分配的内存足够迭代器就不会失效123456789101112131415161718192021222324252627282930//insert操作//内存充足情况#include&lt;vector&gt;#include&lt;iostream&gt;using namespace std;int main()&#123; vector&lt;int&gt;q&#123;1,2,3,4,5,6,7,8,9,10&#125;; q.push_back(11); cout&lt;&lt;"初始vector分配的容量:"&lt;&lt;q.capacity()&lt;&lt;endl; int cnt = 0; int flag = 0; //flag保证只插入一次 for(vector&lt;int&gt;::iterator i = q.begin(); i != q.end(); ++i)&#123; ++cnt; if(cnt &gt; 15)&#123; cout&lt;&lt; "gg" &lt;&lt;endl; break; &#125; if(*i == 3&amp;&amp;!flag)&#123; flag = 1; i = q.insert(i,22); cout&lt;&lt;"插入元素后vector分配的容量:" &lt;&lt;q.capacity() &lt;&lt;endl; &#125; cout &lt;&lt; *i &lt;&lt; endl; cout &lt;&lt; &amp;(*i) &lt;&lt; endl; &#125; return 0;&#125; 输出为: 1234567891011121314151617181920212223242526初始vector分配的容量:20 1 0x1f2188 2 0x1f218c 插入元素后vector分配的容量:20 22 0x1f2190 3 0x1f2194 4 0x1f2198 5 0x1f219c 6 0x1f21a0 7 0x1f21a4 8 0x1f21a8 9 0x1f21ac 10 0x1f21b0 11 0x1f21b4 输出结果分析: 很显然当内存充足的情况下, 执行insert操作只会将迭代器i及i之后的的所有元素向后移动一个type单位.所以这种情况下即使没有使用返回值也不会发生迭代器失效 添加元素时若预分配的内存不足迭代器就会失效1234567891011121314151617181920212223242526272829303132333435//insert操作//内存不够情况#include&lt;vector&gt;#include&lt;iostream&gt;using namespace std;int main()&#123; vector&lt;int&gt;q&#123;1,2,3,4,5,6,7,8,9,10&#125;; // c++11列表初始化 vector&lt;int&gt;::iterator j = q.begin(); j++; cout&lt;&lt;"第二个元素:"&lt;&lt;*j&lt;&lt;endl; cout&lt;&lt;"第二个元素地址:"&lt;&lt;&amp;(*j)&lt;&lt;endl; cout&lt;&lt;"初始vector分配的容量:"&lt;&lt;q.capacity()&lt;&lt;endl; // 有多少元素即分配多少内存 int cnt = 0; int flag = 0; //flag保证只插入一次 for(vector&lt;int&gt;::iterator i = q.begin(); i != q.end(); ++i)&#123; ++cnt; if(cnt &gt; 15)&#123; cout&lt;&lt; "gg" &lt;&lt;endl; break; &#125; if(*i == 3&amp;&amp;!flag)&#123; flag = 1; i = q.insert(i,22); cout&lt;&lt;"\n插入后原第二个元素:"&lt;&lt;*j&lt;&lt;endl; cout&lt;&lt;"插入后原第二个元素地址:"&lt;&lt;&amp;(*j)&lt;&lt;endl; cout&lt;&lt;"插入元素后vector分配的容量:" &lt;&lt;q.capacity() &lt;&lt;endl; &#125; cout &lt;&lt; *i &lt;&lt; endl; cout &lt;&lt; &amp;(*i) &lt;&lt; endl; &#125; return 0;&#125; output: 1234567891011121314151617181920212223242526272829第二个元素:2 第二个元素地址:0xe5215c 初始vector分配的容量:10 1 0xe52158 2 0xe5215c 插入后第二个元素:15007936 插入后第二个元素地址:0xe5215c 插入元素后vector分配的容量:20 22 0xe52190 3 0xe52194 4 0xe52198 5 0xe5219c 6 0xe521a0 7 0xe521a4 8 0xe521a8 9 0xe521ac 10 0xe521b0 输出结果分析: vector内存分配策略为 二倍扩容 , 每次当内存不够的情况下vector会将容量扩展为当前的两倍. 那这些新分配的会在原内存的后面吗？ 根据输出结果显然不是的。 上例代码在插入元素22 后, 新的3号元素内存位置距离上一个元素不是4byte(1个int单位), 也就是说, 当vector扩容时, 会在另一个内存分配一段新的内存(原内存的二倍). 并把原内存中的元素全部拷贝到新内存中… 指向二号元素的迭代器在插入操作之后指向的值由2变成了15007936,也验证了上述结论. capacity &amp; size 理解 capacity 和 size 的区别非常重要。 容器的 size 是指它已经保存的元素的数目 ; 而 capacity 则是在不分配新的内存空间的前提下它最多可以保存多少元素。 函数 函数 表述 c.begin() 传回指向第一个元素的迭代器。 c.capacity() 返不分配新的内存空间的前提下它最多可以保存多少元素。 c.clear() 移除容器中所有数据。 c.empty() 判断容器是否为空。 c.end() 指向迭代器中的最后一个数据地址。 c.insert(iter,elem) 在pos位置插入一个elem拷贝，传回新数据位置。 c.pop_back() 删除最后一个数据。 c.push_back(elem) 在尾部加入一个数据。 c.size() 返回容器中实际数据的个数。 c1.swap(c2) 将c1和c2这两个容器中的元素互换。 swap(c1,c2) 同上操作。 c.empty() 判断容器是否为空。 c.erase(iter) 删除pos位置的数据，传回下一个数据的位置。 c.reserve() 保留(预先分配)适当的容量。 c.assign(beg,end) 将[beg; end)区间中的数据赋值给c。 c.assign(n,elem) 将n个elem的拷贝赋值给c。 c.at(idx) 传回索引idx所指的数据，如果idx越界，抛出out_of_range。 c.back() 传回指向最后一个元素的迭代器 c.erase(beg,end) 删除[beg,end)区间的数据，传回下一个数据的位置。 c.front() 传回第一个数据。 get_allocator 使用构造函数返回一个拷贝。 c.insert(pos,n,elem) 在pos位置插入n个elem数据。无返回值。 c.insert(pos,beg,end) 在pos位置插入在[beg,end)区间的数据。无返回值。 c.max_size() 返回容器中最大数据的数量。 c.rbegin() 传回一个逆向队列的第一个数据。 c.rend() 传回一个逆向队列的最后一个数据的下一个位置。 c.resize(num) 重新指定队列的长度。 构造 &amp; 销毁 写法 表述 vector c 创建一个空的vector。 vector c1(c2) 复制一个vector。 vector c(n) 创建一个vector，含有n个数据，数据均已缺省构造产生。 vector c(n, elem) 创建一个含有n个elem拷贝的vector。 vector c(beg,end) 创建一个以[beg;end)区间的vector。 c.~ vector () 销毁所有数据，释放内存。]]></content>
      <categories>
        <category>CPP</category>
      </categories>
      <tags>
        <tag>CPP</tag>
        <tag>STL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[stl关联容器的特性]]></title>
    <url>%2Fblog%2F2016%2F04%2F26%2Fstll_set_map_tutorial%2F</url>
    <content type="text"><![CDATA[概绍关联容器和顺序容器有着根本的不同 : 关联容器中的元素是按关键字来保存和访问的 。与之相对，顺序容器中的元素是按它们在容器中的位置来顺序保存和访问的 。 关联容器支持高效的关键字查找和访问 。两个主要的关联容器类型是 : map set map概绍map 中 的元素是一些关键字一值 ( key-value )对 : 关键字起到索 引 的作用，值则表示与索引相关联的数据 。字典则是一个很好的使用 map 的例子, 可以将单词作为关键字，将单词释义作为值 。 set概绍set 中每个元素只包含一个关键字 : set 支持高效的关键字检查一个给定关键字是否在 set 中 。例如，在某些文本处理过程中，可以用一个 set 来保存想要忽略的单词。 . . . map &amp; set 的实现因为需要快速定位到键值的关系, 以红黑树的结构实现，其自平衡特性可以让插入删除等操作都可以在O(log n)时间内完成 map的基本操作函数 函数 含义 begin() 返回指向map头部的迭代器 clear() 删除所有元素 insert() 插入元素 empty() 如果map为空则返回true end() 返回指向map末尾的迭代器 erase() 删除一个元素 find() 查找一个元素 lower_bound() 返回键值&gt;=给定元素的第一个元素的迭代器 upper_bound() 返回键值&gt;给定元素的第一个元素的迭代器 size() 返回map中元素的个数 count() 返回指定元素出现的次数 equal_range() 返回特殊条目的迭代器对 get_allocator() 返回map的配置器 key_comp() 返回比较元素key的函数 max_size() 返回可以容纳的最大元素个数 rbegin() 返回一个指向map尾部的逆向迭代器 rend() 返回一个指向map头部的逆向迭代器 swap() 交换两个map value_comp() 返回比较元素value的函数 迭代器失效123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354#include &lt;map&gt;#include &lt;string&gt;#include &lt;iostream&gt;using namespace std;int main()&#123; map&lt;int, string&gt; map_student; map_student.insert(pair&lt;int, string&gt;(1, "stu1")); map_student.insert(pair&lt;int, string&gt;(2, "stu2")); map_student.insert(pair&lt;int, string&gt;(3, "stu3")); map_student.insert(pair&lt;int, string&gt;(4, "stu4")); map&lt;int, string&gt;::iterator iter; if (map_student.find(2) != map_student.end()) &#123; cout &lt;&lt; "found" &lt;&lt; endl; &#125; for (iter = map_student.begin(); iter != map_student.end(); ++iter) &#123; if (iter-&gt;first == 2) &#123; map_student.erase(iter); // 移除元素会让迭代器失效, 所以上面这5行应改为: // for (iter = map_student.begin(); iter != map_student.end();) // 注意, 这里没有 `++iter` 了 // &#123; // if (iter-&gt;first == 2) // &#123; // iter = map_student.erase(iter); // 这里也可以用 `map_student.erase(iter++);`代替 // map_student.insert(pair&lt;int, string&gt;(5, "stu5")); // map增加元素并不会使迭代器失效, 因为map增加元素跟vector不一样, // vector要重新找一块内存把当前所有元素复制过去并释放原有元素所以会导致vector的迭代器失效, // 但是map只是直接在红黑树上增加一个结点而已, 并不会移动原有元素, 内存没动, // 自然map的迭代器不会失效了 &#125; cout &lt;&lt; iter-&gt;first &lt;&lt; " : " &lt;&lt; iter-&gt;second &lt;&lt; endl; &#125; // test lower_bound &amp; upper_bound set&lt;int&gt; s; s.insert(1); s.insert(3); s.insert(4); cout&lt;&lt;*s.lower_bound(2)&lt;&lt;endl; // output : 3 cout&lt;&lt;*s.lower_bound(3)&lt;&lt;endl; // output : 3 cout&lt;&lt;*s.upper_bound(3)&lt;&lt;endl; // output : 4 cout&lt;&lt;*s.upper_bound(1)&lt;&lt;endl; // output : 3 return 0;&#125;]]></content>
      <categories>
        <category>CPP</category>
      </categories>
      <tags>
        <tag>CPP</tag>
        <tag>STL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线程局部存储]]></title>
    <url>%2Fblog%2F2016%2F04%2F22%2Fthread_local_storage%2F</url>
    <content type="text"><![CDATA[使用全局变量或者静态变量是导致多线程编程中非线程安全的常见原因。在多线程程序中，保障非线程安全的常用手段之一是使用互斥锁来做保护，这种方法带来了并发性能下降，同时也只能有一个线程对数据进行读写。 如果程序中能避免使用全局变量或静态变量，那么这些程序就是线程安全的，性能也可以得到很大的提升。 如果有些数据只能有一个线程可以访问，那么这一类数据就可以使用线程局部存储机制来处理，虽然使用这种机制会给程序执行效率上带来一定的影响，但对于使用锁机制来说，这些性能影响将可以忽略。 还有一种大致相当的编程技术就是使用 线程特有数据(没 线程局部存储 易用, 也没 线程局部存储 高效) ，这将在 线程特有数据 中讨论。 线程局部存储介绍__thread 是GCC内置的线程局部存储设施，存取效率可以和全局变量相比。 __thread 变量每一个线程有一份独立实体，各个线程的值互不干扰。可以用来修饰那些带有全局性且值可能变，但是又不值得用全局变量保护的变量。 __thread 使用规则：只能修饰POD类型(类似整型指针的标量，不带自定义的构造、拷贝、赋值、析构的类型， 二进制内容可以任意复制memset, memcpy, 且内容可以复原， 不能修饰class类型，因为无法自动调用构造函数和析构函数，可以用于修饰全局变量，函数内的静态变量，不能修饰函数的局部变量或者class的普通成员变量，且 __thread 变量值只能初始化为编译器常量( 例如 : 值在编译器就可以确定const int i=5,运行期常量是运行初始化后不再改变const int i=rand() ). 一个简单例子123456789101112131415161718192021222324252627#include&lt;iostream&gt;#include&lt;pthread.h&gt;#include&lt;unistd.h&gt;using namespace std;const int i=5;__thread int var=i;//两种方式效果一样//__thread int var=5;//void* worker1(void* arg);void* worker2(void* arg);int main()&#123; pthread_t pid1,pid2; //__thread int temp=5; static __thread int temp=10;//修饰函数内的static变量 pthread_create(&amp;pid1,NULL,worker1,NULL); pthread_create(&amp;pid2,NULL,worker2,NULL); pthread_join(pid1,NULL); pthread_join(pid2,NULL); cout&lt;&lt;temp&lt;&lt;endl;//输出10 return 0;&#125;void* worker1(void* arg)&#123; cout&lt;&lt;++var&lt;&lt;endl;//输出 6&#125;void* worker2(void* arg)&#123; sleep(1);//等待线程1改变var值，验证是否影响线程2 cout&lt;&lt;++var&lt;&lt;endl;//输出6&#125; 程序输出 : 6 6 //可见__thread值线程间互不干扰 10如何使用线程局部存储技术来实现函数的线程安全我们先讨论一下非线程安全的 stderror() 的实现, 接着说明如何使用线程局部存储技术来实现该函数的线程安全. 非线程安全的stderror()An implementation of strerror() that is not thread-safe. 1234567891011121314151617181920212223242526272829303132333435363738/*************************************************************************\* Copyright (C) Michael Kerrisk, 2017. ** ** This program is free software. You may use, modify, and redistribute it ** under the terms of the GNU General Public License as published by the ** Free Software Foundation, either version 3 or (at your option) any ** later version. This program is distributed without any warranty. See ** the file COPYING.gpl-v3 for details. *\*************************************************************************//* Listing 31-1 *//* strerror.c An implementation of strerror() that is not thread-safe.*/#define _GNU_SOURCE /* Get '_sys_nerr' and '_sys_errlist' declarations from &lt;stdio.h&gt; */#include &lt;stdio.h&gt;#include &lt;string.h&gt; /* Get declaration of strerror() */#define MAX_ERROR_LEN 256 /* Maximum length of string returned by strerror() */static char buf[MAX_ERROR_LEN]; /* Statically allocated return buffer */char *strerror(int err)&#123; if (err &lt; 0 || err &gt;= _sys_nerr || _sys_errlist[err] == NULL) &#123; snprintf(buf, MAX_ERROR_LEN, "Unknown error %d", err); &#125; else &#123; strncpy(buf, _sys_errlist[err], MAX_ERROR_LEN - 1); buf[MAX_ERROR_LEN - 1] = '\0'; /* Ensure null termination */ &#125; return buf;&#125; 线程安全的stderror()这是使用线程局部存储技术实现的线程安全的stderror(). 如果对使用线程特有数据技术实现的线程安全的stderror()感兴趣,请转 线程特有数据 12345678910111213141516171819202122232425262728293031323334353637383940414243444546/*************************************************************************\* Copyright (C) Michael Kerrisk, 2017. ** ** This program is free software. You may use, modify, and redistribute it ** under the terms of the GNU General Public License as published by the ** Free Software Foundation, either version 3 or (at your option) any ** later version. This program is distributed without any warranty. See ** the file COPYING.gpl-v3 for details. *\*************************************************************************//* Listing 31-4 *//* strerror_tls.c An implementation of strerror() that is made thread-safe through the use of thread-local storage. See also strerror_tsd.c. Thread-local storage requires: Linux 2.6 or later, NPTL, and gcc 3.3 or later.*/#define _GNU_SOURCE /* Get '_sys_nerr' and '_sys_errlist' declarations from &lt;stdio.h&gt; */#include &lt;stdio.h&gt;#include &lt;string.h&gt; /* Get declaration of strerror() */#include &lt;pthread.h&gt;#define MAX_ERROR_LEN 256 /* Maximum length of string in per-thread buffer returned by strerror() */static __thread char buf[MAX_ERROR_LEN]; /* Thread-local return buffer */char *strerror(int err)&#123; if (err &lt; 0 || err &gt;= _sys_nerr || _sys_errlist[err] == NULL) &#123; snprintf(buf, MAX_ERROR_LEN, "Unknown error %d", err); &#125; else &#123; strncpy(buf, _sys_errlist[err], MAX_ERROR_LEN - 1); buf[MAX_ERROR_LEN - 1] = '\0'; /* Ensure null termination */ &#125; return buf;&#125;]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>CPP</tag>
        <tag>Linux</tag>
        <tag>MultiThread</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线程特有数据]]></title>
    <url>%2Fblog%2F2016%2F04%2F17%2Fthread_specific_data%2F</url>
    <content type="text"><![CDATA[在 Linux 系统中使用 C/C++ 进行多线程编程时，我们遇到最多的就是对同一变量的多线程读写问题，大多情况下遇到这类问题都是通过锁机制来处理，但这对程序的性能带来了很大的影响， 当然对于那些系统原生支持原子操作的数据类型来说，我们可以使用原子操作来处理，这能对程序的性能会得到一定的提高。那么对于那些系统不支持原子操作的自定义数据类型， 在不使用锁的情况下如何做到线程安全呢？本文将从线程特有数据方面，简单讲解处理这一类线程安全问题的方法。 如果有些数据只能有一个线程可以访问，那么这一类数据就可以使用线程特有数据机制来处理，虽然使用这种机制会给程序执行效率上带来一定的影响，但对于使用锁机制来说，这些性能影响将可以忽略。 还有一种大致相当的编程技术就是使用 __thread (比 线程特有数据 易用, 也比 线程特有数据 高效), 它是 GCC 内置的线程局部存储设施 ，这将在 线程局部存储 中讨论。 数据类型在 C/C++ 程序中常存在全局变量、函数内定义的静态变量以及局部变量，对于局部变量来说，其不存在线程安全问题，因此不在本文讨论的范围之内。 全局变量和函数内定义的静态变量，是同一进程中各个线程都可以访问的共享变量，因此它们存在多线程读写问题。 在一个线程中修改了变量中的内容，其他线程都能感知并且能读取已更改过的内容，这对数据交换来说是非常快捷的，但是由于多线程的存在，对于同一个变量可能存在两个或两个以上的线程同时修改变量所在的内存内容，同时又存在多个线程在变量在修改的时去读取该内存值，如果没有使用相应的同步机制来保护该内存的话，那么所读取到的数据将是不可预知的，甚至可能导致程序崩溃。 如果需要在一个线程内部的各个函数调用都能访问、但其它线程不能访问的变量，这就需要新的机制来实现，我们称之为 Static memory local to a thread (线程局部静态变量)，同时也可称之为线程特有数据（TSD: Thread-Specific Data） 这一类型的数据，在程序中每个线程都会分别维护一份变量的副本 (copy)，并且长期存在于该线程中，对此类变量的操作不影响其他线程。如下图： 一次性初始化在讲解线程特有数据之前，先让我们来了解一下一次性初始化。 多线程程序有时有这样的需求：不管创建多少个线程，有些数据的初始化只能发生一次。 列如：在 C++ 程序中某个类在整个进程的生命周期内只能存在一个实例对象，在多线程的情况下，为了能让该对象能够安全的初始化，一次性初始化机制就显得尤为重要了。 ——在设计模式中这种实现常常被称之为单例模式（Singleton）。Linux 中提供了如下函数来实现一次性初始化： 123456789101112#include &lt;pthread.h&gt;// Returns 0 on success, or a positive error number on errorint pthread_once (pthread_once_t *once_control, void (*init) (void));// 利用参数once_control的状态，函数pthread_once()可以确保无论有多少个线程调用多少次该函数，// 也只会执行一次由init所指向的由调用者定义的函数。// init所指向的函数没有任何参数，形式如下：void init (void)&#123; // some variables initializtion in here&#125; 另外，参数 once_control 必须是 pthread_once_t 类型变量的指针，指向初始化为 PTHRAD_ONCE_INIT 的静态变量。在 C++0x 以后提供了类似功能的函数 std::call_once ()，用法与该函数类似。使用实例请参考 :https://github.com/ApusApp/Swift/blob/master/swift/base/singleton.hpp实现。 线程特有数据API介绍在 Linux 中提供了如下函数来对线程特有数据进行操作 12345678910111213#include &lt;pthread.h&gt;// Returns 0 on success, or a positive error number on errorint pthread_key_create (pthread_key_t *key, void (*destructor)(void *));// Returns 0 on success, or a positive error number on errorint pthread_key_delete (pthread_key_t key);// Returns 0 on success, or a positive error number on errorint pthread_setspecific (pthread_key_t key, const void *value);// Returns pointer, or NULL if no thread-specific data is associated with keyvoid *pthread_getspecific (pthread_key_t key); pthread_key_create// Returns 0 on success, or a positive error number on error int pthread_key_create (pthread_key_t *key, void (*destructor)(void *));函数 pthread_key_create() 为线程特有数据创建一个新键，并通过 key 指向新创建的键缓冲区。 因为所有线程都可以使用返回的新键，所以参数 key 可以是一个全局变量（在 C++ 多线程编程中一般不使用全局变量，而是使用单独的类对线程特有数据进行封装，每个变量使用一个独立的 pthread_key_t）。 destructor 所指向的是一个自定义的函数，其格式如下： void Dest (void *value) { // Release storage pointed to by &apos;value&apos; }只要线程终止时与 key 关联的值不为 NULL，则 destructor 所指的函数将会自动被调用。 如果一个线程中有多个线程特有数据变量，那么对各个变量所对应的 destructor 函数的调用顺序是不确定的，因此，每个变量的 destructor 函数的设计应该相互独立。 pthread_key_delete// Returns 0 on success, or a positive error number on error int pthread_key_delete (pthread_key_t key);函数 pthread_key_delete() 并不检查当前是否有线程正在使用该线程特有数据变量，也不会调用清理函数 destructor，而只是将其释放以供下一次调用 pthread_key_create() 使用。 在 Linux 线程中，它还会将与之相关的线程数据项设置为 NULL。 由于系统对每个进程中 pthread_key_t 类型的个数是有限制的，所以进程中并不能创建无限个的 pthread_key_t 变量。Linux 中可以通过 PTHREAD_KEY_MAX（定义于 limits.h 文件中）或者系统调用 sysconf(_SC_THREAD_KEYS_MAX) 来确定当前系统最多支持多少个键。Linux 中默认是 1024 个键，这对于大多数程序来说已经足够了。如果一个线程中有多个线程特有数据变量，通常可以将这些变量封装到一个数据结构中，然后使封装后的数据结构与一个线程局部变量相关联，这样就能减少对键值的使用。 pthread_setspecific// Returns 0 on success, or a positive error number on error int pthread_setspecific (pthread_key_t key, const void *value);函数 pthread_setspecific() 用于将 value 的副本存储于一数据结构中，并将其与调用线程以及 key 相关联。 参数 value 通常指向由调用者分配的一块内存，当线程终止时，会将该指针作为参数传递给与 key 相关联的 destructor 函数。 pthread_getspecific// Returns pointer, or NULL if no thread-specific data is associated with key void *pthread_getspecific (pthread_key_t key);当线程被创建时，会将所有的线程特有数据变量初始化为 NULL，因此第一次使用此类变量前必须先调用 pthread_getspecific() 函数来确认是否已经于对应的 key 相关联，如果没有，那么 pthread_getspecific() 会分配一块内存并通过 pthread_setspecific() 函数保存指向该内存块的指针。 参数 value 的值也可以不是一个指向调用者分配的内存区域，而是任何可以强制转换为 void * 的变量值，在这种情况下，先前的 pthread_key_create() 函数应将参数_ _destructor 设置为 NULL 函数 pthread_getspecific() 正好与 pthread_setspecific() 相反，其是将 pthread_setspecific() 设置的 value 取出。在使用取出的值前最好是将 void * 转换成原始数据类型的指针。 使用线程特有数据API我们先讨论一下非线程安全的 stderror() 的实现, 接着说明如何使用线程特有数据来实现该函数的线程安全. 非线程安全的stderror()An implementation of strerror() that is not thread-safe. 1234567891011121314151617181920212223242526272829303132333435363738/*************************************************************************\* Copyright (C) Michael Kerrisk, 2017. ** ** This program is free software. You may use, modify, and redistribute it ** under the terms of the GNU General Public License as published by the ** Free Software Foundation, either version 3 or (at your option) any ** later version. This program is distributed without any warranty. See ** the file COPYING.gpl-v3 for details. *\*************************************************************************//* Listing 31-1 *//* strerror.c An implementation of strerror() that is not thread-safe.*/#define _GNU_SOURCE /* Get '_sys_nerr' and '_sys_errlist' declarations from &lt;stdio.h&gt; */#include &lt;stdio.h&gt;#include &lt;string.h&gt; /* Get declaration of strerror() */#define MAX_ERROR_LEN 256 /* Maximum length of string returned by strerror() */static char buf[MAX_ERROR_LEN]; /* Statically allocated return buffer */char *strerror(int err)&#123; if (err &lt; 0 || err &gt;= _sys_nerr || _sys_errlist[err] == NULL) &#123; snprintf(buf, MAX_ERROR_LEN, "Unknown error %d", err); &#125; else &#123; strncpy(buf, _sys_errlist[err], MAX_ERROR_LEN - 1); buf[MAX_ERROR_LEN - 1] = '\0'; /* Ensure null termination */ &#125; return buf;&#125; 线程安全的stderror()这是使用线程特有数据技术实现的线程安全的stderror(). 如果对使用线程局部存储技术实现的线程安全的stderror()感兴趣,请转 线程局部存储 An implementation of strerror() that is made thread-safe through the use of thread-specific data. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384/*************************************************************************\* Copyright (C) Michael Kerrisk, 2017. ** ** This program is free software. You may use, modify, and redistribute it ** under the terms of the GNU General Public License as published by the ** Free Software Foundation, either version 3 or (at your option) any ** later version. This program is distributed without any warranty. See ** the file COPYING.gpl-v3 for details. *\*************************************************************************//* Listing 31-3 *//* strerror_tsd.c An implementation of strerror() that is made thread-safe through the use of thread-specific data. See also strerror_tls.c.*/#define _GNU_SOURCE /* Get '_sys_nerr' and '_sys_errlist' declarations from &lt;stdio.h&gt; */#include &lt;stdio.h&gt;#include &lt;string.h&gt; /* Get declaration of strerror() */#include &lt;pthread.h&gt;#include "tlpi_hdr.h"static pthread_once_t once = PTHREAD_ONCE_INIT;static pthread_key_t strerrorKey;#define MAX_ERROR_LEN 256 /* Maximum length of string in per-thread buffer returned by strerror() */static void /* Free thread-specific data buffer */destructor(void *buf)&#123; free(buf);&#125;static void /* One-time key creation function */createKey(void)&#123; int s; /* Allocate a unique thread-specific data key and save the address of the destructor for thread-specific data buffers */ s = pthread_key_create(&amp;strerrorKey, destructor); if (s != 0) errExitEN(s, "pthread_key_create");&#125;char *strerror(int err)&#123; int s; char *buf; /* Make first caller allocate key for thread-specific data */ s = pthread_once(&amp;once, createKey); if (s != 0) errExitEN(s, "pthread_once"); buf = pthread_getspecific(strerrorKey); if (buf == NULL) &#123; /* If first call from this thread, allocate buffer for thread, and save its location */ buf = malloc(MAX_ERROR_LEN); if (buf == NULL) errExit("malloc"); s = pthread_setspecific(strerrorKey, buf); if (s != 0) errExitEN(s, "pthread_setspecific"); &#125; if (err &lt; 0 || err &gt;= _sys_nerr || _sys_errlist[err] == NULL) &#123; snprintf(buf, MAX_ERROR_LEN, "Unknown error %d", err); &#125; else &#123; strncpy(buf, _sys_errlist[err], MAX_ERROR_LEN - 1); buf[MAX_ERROR_LEN - 1] = '\0'; /* Ensure null termination */ &#125; return buf;&#125; 深入理解线程特有数据机制深入理解线程特有数据的实现有助于对其 API 的使用。 在典型的实现中包含以下数组： 一个全局（进程级别）的数组，用于存放线程特有数据的键值信息pthread_key_create() 返回的 pthread_key_t 类型值只是对全局数组的索引，该全局数组标记为 pthread_keys，其格式大概如下： 数组的每个元素都是一个包含两个字段的结构，第一个字段标记该数组元素是否在用，第二个字段用于存放针对此键、线程特有数据变的解构函数的一个副本，即 destructor 函数。 每个线程还包含一个数组，存有为每个线程分配的线程特有数据块的指针（通过调用 pthread_setspecific() 函数来存储的指针，即参数中的 value） 在常见的存储 pthread_setspecific()函数参数 value 的实现中，大多数都类似于下图的实现。 图中假设 pthread_keys[1]分配给 func1()函数，pthread API 为每个函数维护指向线程特有数据数据块的一个指针数组，其中每个数组元素都与图线程特有数据键的实现 (上图) 中的全局 pthread_keys 中元素一一对应。 参考 [1] Linux/UNIX 系统编程手册（上） [2] http://www.groad.net/bbs/thread-2182-1-1.html [3] http://baike.baidu.com/view/598128.htm]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>CPP</tag>
        <tag>Linux</tag>
        <tag>MultiThread</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[5分钟学会Git二]]></title>
    <url>%2Fblog%2F2016%2F04%2F13%2Fgit_tutorial_two%2F</url>
    <content type="text"><![CDATA[贮藏stash设想一个场景，假设我们正在一个新的分支做新的功能，这个时候突然有一个紧急的bug需要修复，而且修复完之后需要立即发布。当然你说我先把刚写的一点代码进行提交不就行了么？这样理论上当然是ok的，但是这会产品垃圾commit，原则上我们每次的commit都要有实际的意义，你的代码只是刚写了一半，还没有什么实际的意义是不建议就这样commit的，那么有没有一种比较好的办法，可以让我暂时切到别的分支，修复完bug再切回来，而且代码也能保留的呢？ 试试git stash吧 git stash : 把当前的文件改动贮藏起来 git stash list: 查看当前有哪些贮藏记录 git stash pop stash_list_id: 会帮你把代码还原，还自动帮你把这条 stash 记录删除 12345678910111213141516171819202122232425b@b-VirtualBox:~/git_test_link/Flock-AI-Fish-Unreal-VR$ git checkout old_demo error: Your local changes to the following files would be overwritten by checkout: README.mdPlease, commit your changes or stash them before you can switch branches.Abortingb@b-VirtualBox:~/git_test_link/Flock-AI-Fish-Unreal-VR$ git stash Saved working directory and index state WIP on new_test_branch: b3ad5d2 modify mdHEAD is now at b3ad5d2 modify mdb@b-VirtualBox:~/git_test_link/Flock-AI-Fish-Unreal-VR$ git checkout old_demo Switched to branch &apos;old_demo&apos;Your branch is up-to-date with &apos;origin/old_demo&apos;.b@b-VirtualBox:~/git_test_link/Flock-AI-Fish-Unreal-VR$ git checkout new_test_branch Switched to branch &apos;new_test_branch&apos;b@b-VirtualBox:~/git_test_link/Flock-AI-Fish-Unreal-VR$ git stash liststash@&#123;0&#125;: WIP on new_test_branch: b3ad5d2 modify mdb@b-VirtualBox:~/git_test_link/Flock-AI-Fish-Unreal-VR$ git stash pop stash@&#123;0&#125;On branch new_test_branchChanges not staged for commit: (use &quot;git add &lt;file&gt;...&quot; to update what will be committed) (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory) modified: README.mdno changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;)Dropped stash@&#123;0&#125; (88cd440c10c80bb6eaef9f4d86ab4a0be3d6dc00) . . . 处理冲突假设这样一个场景，A和B两位同学各自开了两个分支来开发不同的功能，大部分情况下都会尽量互不干扰的，但是有一个需求A需要改动一个基础库中的一个类的方法，不巧B这个时候由于业务需要也改动了基础库的这个方法，因为这种情况比较特殊，A和B都认为不会对地方造成影响，等两人各自把功能做完了，需要合并的到主分支 master 的时候，我们假设先合并A的分支，这个时候没问题的，之后再继续合并B的分支，这个时候想想也知道就有冲突了，因为A和B两个人同时更改了同一个地方，Git 本身他没法判断你们两个谁更改的对，但是这个时候他会智能的提示有 conflicts 就像下面这种情况 : 1234b@b-VirtualBox:~/git_test_link/Flock-AI-Fish-Unreal-VR$ git merge plugin Auto-merging README.mdCONFLICT (content): Merge conflict in README.mdAutomatic merge failed; fix conflicts and then commit the result. 打开READ.md文件一看发现冲突的地方如下: 12345&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEADmmmp=======nani&gt;&gt;&gt;&gt;&gt;&gt;plugin 冲突的地方由 ==== 分出了上下两个部分，上部分一个叫 HEAD 的字样代表是我当前所在分支的代码，下半部分是一个叫 plugin 分支的代码，可以看到 HEAD 是那里加了一行mmmp，而plugin分支则加了一句nani, 所以我们得跟团队的其他人商量一下看看要改成什么样，而且同时也要把那些 «&lt; HEAD、==== 以及 »»»plugin 这些标记符号也一并删除，最后进行一次 commit 就ok了。 标签tag主要介绍附注标签( annotated tag) 创建附注标签git tag -a v0.1.2 -m “0.1.2版本” 创建轻量标签不需要传递参数，直接指定标签名称即可。创建附注标签时，参数a即annotated的缩写，指定标签类型，后附标签名。参数m指定标签说明，说明信息会保存在标签对象中。 切换到标签与切换分支命令相同，用git checkout [tagname] 查看标签信息用git show命令可以查看标签的版本信息：git show v0.1.2 删除标签误打或需要修改标签时，需要先将标签删除，再打新标签。 删除本地标签git tag -d v0.1.2参数d即delete的缩写，意为删除其后指定的标签。 删除远程标签用法 : git push origin :refs/tags/标签名 或 git push origin --delete tag 标签名 比如 : git push origin :refs/tags/protobuf-2.5.0rc1 或 git push origin --delete tag protobuf-2.5.0rc1 给指定的commit打标签打标签不必要在head之上，也可在之前的版本上打，这需要你知道某个提交对象的校验和（通过git log获取）。git tag -a v0.1.1 9fbc3d0 标签发布通常的git push不会将标签对象提交到git服务器，我们需要进行显式的操作： git push origin v0.1.2 将v0.1.2标签提交到git服务器 git push origin -–tags 将本地所有标签一次性提交到git服务器 对比diff 比如查看当前还未git add的文件的不同 : git diff 比如查看当前已经add 没有commit 的改动 : git diff --cached 比如查看当前所有改动和HEAD的区别(当前还未git add的文件的改动和当前当前已经add 但没有commit 的改动), 也就是上面两条命令的合并 : git diff HEAD 比如查看 commit_id为a和commit_id为b的temp文件夹的差异 : git diff a b temp]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[5分钟学会Git一]]></title>
    <url>%2Fblog%2F2016%2F04%2F12%2Fgit_tutorial_one%2F</url>
    <content type="text"><![CDATA[之前有一份私人git笔记老长老长了, 今天得空, 把它浓缩成5分钟版本.感觉纯基础性的东西整理成博客差也差不多了, 还有很多凌乱的工作笔记慢慢在一点一点整理放上来吧,估计下面几篇博客就开始游戏服务器的开发心得之类的了. 本篇博客因为要5分钟撸完git, 所以语言尽量精简, 只说新人必须知道的, 如果要git进阶的, 后面再另写博客说明, 不该说的废话就不说了 安装sudo apt-get install git 通过SSH的key来push到GithubCreate a repo. Make sure there is at least one file in it (even just the README) Generate ssh key: ssh-keygen -t rsa -C &quot;your_email@example.com&quot; Copy the contents of the file ~/.ssh/id_rsa.pub to your SSH keys in your GitHub account settings. Test SSH key: ssh -T git@github.com clone the repo: git clone git://github.com/username/your-repository Now cd to your git clone folder and do: git remote set-url origin git@github.com:username/your-repository.git Now try editing a file (try the README) and then do: git add -A git commit -am &quot;my update msg&quot; git push Git概念图一般来说，日常使用只要记住下图6个命令，就可以了。 Workspace：工作区 Index / Stage：暂存区 Repository：仓库区（或本地仓库） Remote：远程仓库 查看状态 比如查看当前分支的状态 : git status, 这条命令也会给很多其他的git命令提示的喔 查看当前在哪个分支 : git branch12345b@b-VirtualBox:~/git_test_link/Flock-AI-Fish-Unreal-VR$ git branch master new_test_branch* old_demo plugin 标记为*的那个就是当前分支, 也就是old_demo分支 . . . 克隆比如从我的一个远端github项目克隆一份到本地 : git clone git@github.com:no5ix/Flock-AI-Fish-Unreal-VR.git这个地址是这样得来的, 如图 : 分支 比如创建一个新的分支test_branch :git branch test_branch 比如切换到分支test_branch :git checkout test_branch 比如把test_branch合并到主分支master上来 : 先切换到master上来git checkout master 然后 git merge test_branch git rebase test_branch : rebase 跟 merge 的区别你们可以理解成有两个书架，你需要把两个书架的书整理到一起去，第一种做法是 merge ，比较粗鲁暴力，就直接腾出一块地方把另一个书架的书全部放进去，虽然暴力，但是这种做法你可以知道哪些书是来自另一个书架的；第二种做法就是 rebase ，他会把两个书架的书先进行比较，按照购书的时间来给他重新排序，然后重新放置好，这样做的好处就是合并之后的书架看起来很有逻辑，但是你很难清晰的知道哪些书来自哪个书架的。各有好处的，不同的团队根据不同的需要以及不同的习惯来选择就好。 比如删除本地分支test_branch :有些时候可能会删除失败，比如如果a分支的代码还没有合并到master，你执行 git branch -d a 是删除不了的，它会智能的提示你a分支还有未合并的代码，但是如果你非要删除，那就执行 git branch -D a 就可以强制删除a分支。 git branch -d test_branch git branch -D test_branch 删除远程分支 : git push origin --delete 分支名 加到暂存区和提交 比如将修改之后的文件test_file加入到暂存区里 :git add test_file 比如将暂存区里的提交到本地仓库并加入提交信息”update test_file” :git commit -m &quot;update test_file&quot; 远程同步 下载远程仓库的所有变动git fetch [remote] 取回远程仓库的变化，并与本地分支合并git pull [remote] [branch] 上传本地指定分支到远程仓库git push [remote] [branch]将代码推到远端仓库 : 这之前的所有这些add, commit都是本地的操作, 比如我们把本地的master分支推到github的那个项目的master分支 :git push origin master 撤销和回退 比如只是撤销某个文件test_file的修改(还未被add的) :git checkout -- test_file 撤销刚刚的git add : git reset HEAD : 把add了的都从暂存区中移出 git reset HEAD test_file : 只把test_file从暂存区中移出 把git commit回退(在公司很少用, 因为把之前的commit都弄没了) git reset :git reset 是回到某次提交A，提交A及之前的commit都会被保留，A之后的commit都没有了, A之后的修改都会被退回到暂存区 只是把commit回退并且把文件从暂存区中移出, 但保留已有的文件更改 :通用命令为 git reset commit_id, 这个commit_id用git log命令来查看, 比如要恢复到刚刚提交的上一次提交的版本, 就用git reset HEAD^(这句命令的意思是说: 恢复到commit id 为HEAD^的版本, HEAD是指向最新的提交，上一次提交是HEAD^,上上次是HEAD^^,也可以写成HEAD～2 ,依次类推. ) 把commit回退且不保留已有的文件更改(慎用) :git reset --hard commit_id 把git commit撤销(抹除并覆盖) git revert :git revert 是生成一个新的提交来撤销(或者说是抹除并覆盖某次提交)，此次提交之前的commit都会被保留 git revert和git reset的区别git revert 和 git reset 的区别看一个例子 具体一个例子，假设有三个commit:commit3: add test3.ccommit2: add test2.ccommit1: add test1.c 当执行git revert HEAD~1时， commit2被撤销了git log可以看到： revert “commit2”:this reverts commit 5fe21s2… commit3: add test3.c commit2: add test2.c commit1: add test1.c而git status 没有任何变化 如果换做执行git reset HEAD~1后，运行git log commit2: add test2.c commit1: add test1.c运行git status， 则test3.c处于暂存区，准备提交。 如果换做执行git reset --hard HEAD~1后，显示：HEAD is now at commit2，运行git log commit2: add test2.c commit1: add test1.c运行git status， 没有任何变化 git reset回退之后不能push的问题假设一开始你的本地和远程都是：a -&gt; b -&gt; c你想把HEAD回退git reset到b，那么在本地就变成了：a -&gt; b这个时候，如果没有远程库，你就接着怎么操作都行，比如：a -&gt; b -&gt; d但是在有远程库的情况下，你push会失败，因为远程库是 a-&gt;b-&gt;c，你的是 a-&gt;b-&gt;d两种方案： push的时候用--force，强制把远程库变成a -&gt; b -&gt; d，大部分公司严禁这么干，会被别人揍一顿 做一个反向操作，把自己本地变成a -&gt; b -&gt; c -&gt; d，注意b和d文件快照内容一莫一样，但是commit id肯定不同，再push上去远程也会变成 a -&gt; b -&gt; c -&gt; d 综上所述, 一个是撤销(抹除并覆盖), 一个是回退 GitIgnore在git中如果想忽略掉某个文件，不让这个文件提交到版本库中，可以使用修改根目录中 .gitignore 文件的方法（如无，则需自己手工建立此文件）。这个文件每一行保存了一个匹配的规则例如： # 符号为注释 – #开头的那行的内容将被 Git 忽略 *.a # 忽略所有 .a 结尾的文件 !lib.a # 但 lib.a 除外 /TODO # 仅仅忽略项目根目录下的 TODO 文件，不包括 subdir/TODO build/ # 忽略 build/ 目录下的所有文件 doc/*.txt # 会忽略 doc/notes.txt 但不包括 doc/server/arch.txtgitignore不生效的问题规则很简单，不做过多解释，但是有时候在项目开发过程中，突然心血来潮想把某些目录或文件加入忽略规则，按照上述方法定义后发现并未生效，原因是.gitignore只能忽略那些原来没有被track的文件，如果某些文件已经被纳入了版本管理中，则修改.gitignore是无效的。那么解决方法就是先把本地缓存删除（改变成未track状态），然后再提交： ` git rm -r --cached . &amp;&amp; git add . &amp;&amp; git commit -m &apos;update .gitignore&apos; `]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C/S游戏架构中延迟补偿的设计和优化方法]]></title>
    <url>%2Fblog%2F2016%2F01%2F06%2Flatency_compensating_methods_in_client_server_in_game_protocol_design_and_optimization%2F</url>
    <content type="text"><![CDATA[自我总结注 : V社这篇文章相当有价值, 所以会有尽可能详细的注解以及对原译文各种翻译纰漏的修正. 广义的延迟补偿主要包括两个方面 : A 如何显示目标 a. 对于本玩家自己 客户端预表现(本文翻译为”客户端预测”) : 比如对于玩家移动的预测, 可以把服务器确认过的movement信息作为开始, 然后使用自己本地的movement input来进行预表现, 服务器跟客户端共享同一套move代码, 当服务器的确认信息过来之后就直接使用服务器发过来的from state进行修正并以from state为基础执行之后的预测. b. 对于其他玩家 i. 外推法 : 即航位推测法, 外推法把其它玩家/物体看作一个点，这个点开始的位置、方向、速度已知，沿着自己的弹道向前移动。因此，假设延时是100ms，最新的协议通知客户端这个玩家奔跑速度是500单位每秒，方向垂直于玩家视线，客户端就可以假设事实上这个玩家当前实际的位置已经向前移动了50个单位。客户端可以在这个外推的位置渲染这个玩家. 这个方法不适用于FPS游戏, 因为大部分FPS游戏采用非现实的玩家系统，玩家可以随时转弯，可以在任意角度作用不现实的加速度，因此外推法得到的结果经常是错误地 ii. 内推法 : 即影子跟随法, 这种方法是用延时来换取平滑, 客户端物体实际移动位置总是滞后一段时间。举个例子，如果服务器每秒同步10次世界信息，客户端渲染的时候会有100ms滞后。这样，每一帧渲染的时候，我们通过最新收到的位置信息和前100ms的位置信息（或者上一帧渲染位置）进行差值得到结果. 如果一个更新包没有收到，有2种处理方法 : 用上面介绍的外推法（有可能产生较大误差）； 保持玩家位于当前位置直到收到下一个更新包(会导致玩家移动顿挫) B. 延迟补偿, 步骤如下 : 为玩家计算一个相当精确的延迟时间 对每个玩家，从服务器历史信息中找一个已发送给这个玩家并且这个玩家已收到的的world update, 这个world update是在这个玩家将要执行这个movement command之前的world update 对于每一个玩家，将其从上述的world update处拉回到这个玩家生成此user command的更新时间中执行用户命令。这个回退时间需要考虑到命令执行的时候的网络延时和插值量 执行玩家命令（包括武器开火等。） 将所有移动的、错位的玩家移动到他们当前正确位置 原文原文出处 原文标题 : Latency Compensating Methods in Client/Server In-game Protocol Design and Optimization Contents 1 Overview 2 Basic Architecture of a Client / Server Game 3 Contents of the User Input messages 4 Client Side Prediction 5 Client-Side Prediction of Weapon Firing 6 Umm, This is a Lot of Work 7 Display of Targets 8 Lag Compensation 9 Game Design Implications of Lag Compensation 10 Conclusion 11 Footnotes Overview Dsigning first-person action games for Internet play is a challenging process. Having robust on-line gameplay in your action title, however, is becoming essential to the success and longevity of the title. In addition, the PC space is well known for requiring developers to support a wide variety of customer setups. Often, customers are running on less than state-of-the-art hardware. The same holds true for their network connections. While broadband has been held out as a panacea for all of the current woes of on-line gaming, broadband is not a simple solution allowing developers to ignore the implications of latency and other network factors in game designs. It will be some time before broadband truly becomes adopted in the United States, and much longer before it can be assumed to exist for your clients in the rest of the world. In addition, there are a lot of poor broadband solutions, where users may occasionally have high bandwidth, but more often than not also have significant latency and packet loss in their connections. Your game must behave well in this world. This discussion will give you a sense of some of the tradeoffs required to deliver a cutting-edge action experience on the Internet. The discussion will provide some background on how client / server architectures work in many on-line action games. In addition, the discussion will show how predictive modeling can be used to mask the effects of latency. Finally, the discussion will describe a specific mechanism, lag compensation, for allowing the game to compensate for connection quality. . . . Basic Architecture of a Client / Server Game Most action games played on the net today are modified client / server games. Games such as Half-Life, including its mods such as Counter-Strike and Team Fortress Classic, operate on such a system, as do games based on the Quake3 engine and the Unreal Tournament engine. In these games, there is a single, authoritative server that is responsible for running the main game logic. To this are connected one or more "dumb" clients. These clients, initially, were nothing more than a way for the user input to be sampled and forwarded to the server for execution. The server would execute the input commands, move around other objects, and then send back to the client a list of objects to render. Of course, the real world system has more components to it, but the simplified breakdown is useful for thinking about prediction and lag compensation. With this in mind, the typical client / server game engine architecture generally looks like this: For this discussion, all of the messaging and coordination needed to start up the connection between client and server is omitted. The client's frame loop looks something like the following: Sample clock to find start time Sample user input (mouse, keyboard, joystick) Package up and send movement command using simulation time Read any packets from the server from the network system Use packets to determine visible objects and their state Render Scene Sample clock to find end time End time minus start time is the simulation time for the next frame Each time the client makes a full pass through this loop, the "frametime" is used for determining how much simulation is needed on the next frame. If your framerate is totally constant then frametime will be a correct measure. Otherwise, the frametimes will be incorrect, but there isn't really a solution to this (unless you could deterministically figure out exactly how long it was going to take to run the next frame loop iteration before running it...). The server has a somewhat similar loop: Sample clock to find start time Read client user input messages from network Execute client user input messages Simulate server-controlled objects using simulation time from last full pass For each connected client, package up visible objects/world state and send to client Sample clock to find end time End time minus start time is the simulation time for the next frame In this model, non-player objects run purely on the server, while player objects drive their movements based on incoming packets. Of course, this is not the only possible way to accomplish this task, but it does make sense. Contents of the User Input messages In Half-Life engine games, the user input message format is quite simple and is encapsulated in a data structure containing just a few essential fields: typedef struct usercmd_s &#123; // Interpolation time on client short lerp_msec; // Duration in ms of command byte msec; // Command view angles. vec3_t viewangles; // intended velocities // Forward velocity. float forwardmove; // Sideways velocity. float sidemove; // Upward velocity. float upmove; // Attack buttons unsigned short buttons; // // Additional fields omitted... // &#125; usercmd_t; The critical fields here are the msec, viewangles, forward, side, and upmove, and buttons fields. The msec field corresponds to the number of milliseconds of simulation that the command corresponds to (it's the frametime). The viewangles field is a vector representing the direction the player was looking during the frame. The forward, side, and upmove fields are the impulses determined by examining the keyboard, mouse, and joystick to see if any movement keys were held down. Finally, the buttons field is just a bit field with one or more bits set for each button that is being held down. Using the above data structures and client / server architecture, the core of the simulation is as follows. First, the client creates and sends a user command to the server. The server then executes the user command and sends updated positions of everything back to client. Finally, the client renders the scene with all of these objects. This core, though quite simple, does not react well under real world situations, where users can experience significant amounts of latency in their Internet connections. The main problem is that the client truly is "dumb" and all it does is the simple task of sampling movement inputs and waiting for the server to tell it the results. If the client has 500 milliseconds of latency in its connection to the server, then it will take 500 milliseconds for any client actions to be acknowledged by the server and for the results to be perceptible on the client. While this round trip delay may be acceptable on a Local Area Network (LAN), it is not acceptable on the Internet. Client Side Prediction One method for ameliorating this problem is to perform the client's movement locally and just assume, temporarily, that the server will accept and acknowledge the client commands directly. This method is labeled as client-side prediction. Client-side prediction of movements requires us to let go of the "dumb" or minimal client principle. That's not to say that the client is fully in control of its simulation, as in a peer-to-peer game with no central server. There still is an authoritative server running the simulation just as noted above. Having an authoritative server means that even if the client simulates different results than the server, the server's results will eventually correct the client's incorrect simulation. Because of the latency in the connection, the correction might not occur until a full round trip's worth of time has passed. The downside is that this can cause a very perceptible shift in the player's position due to the fixing up of the prediction error that occurred in the past. To implement client-side prediction of movement, the following general procedure is used. As before, client inputs are sampled and a user command is generated. Also as before, this user command is sent off to the server. However, each user command (and the exact time it was generated) is stored on the client. The prediction algorithm uses these stored commands. For prediction, the last acknowledged movement from the server is used as a starting point. The acknowledgement indicates which user command was last acted upon by the server and also tells us the exact position (and other state data) of the player after that movement command was simulated on the server. The last acknowledged command will be somewhere in the past if there is any lag in the connection. For instance, if the client is running at 50 frames per second (fps) and has 100 milliseconds of latency (roundtrip), then the client will have stored up five user commands ahead of the last one acknowledged by the server. These five user commands are simulated on the client as a part of client-side prediction. Assuming full prediction1, the client will want to start with the latest data from the server, and then run the five user commands through "similar logic" to what the server uses for simulation of client movement. Running these commands should produce an accurate final state on the client (final player position is most important) that can be used to determine from what position to render the scene during the current frame. In Half-Life, minimizing discrepancies between client and server in the prediction logic is accomplished by sharing the identical movement code for players in both the server-side game code and the client-side game code. These are the routines in the pm_shared/ (which stands for "player movement shared") folder of the HL SDK. The input to the shared routines is encapsulated by the user command and a "from" player state. The output is the new player state after issuing the user command. The general algorithm on the client is as follows: "from state" &lt;- state after last user command acknowledged by the server; “command” &lt;- first command after last user command acknowledged by server; while (true){ run “command” on “from state” to generate “to state”; if (this was the most up to date “command”) break; &quot;from state&quot; = &quot;to state&quot;; &quot;command&quot; = next &quot;command&quot;;}; The origin and other state info in the final "to state" is the prediction result and is used for rendering the scene that frame. The portion where the command is run is simply the portion where all of the player state data is copied into the shared data structure, the user command is processed (by executing the common code in the pm_shared routines in Half-Life's case), and the resulting data is copied back out to the "to state". There are a few important caveats to this system. First, you'll notice that, depending upon the client's latency and how fast the client is generating user commands (i.e., the client's framerate), the client will most often end up running the same commands over and over again until they are finally acknowledged by the server and dropped from the list (a sliding window in Half-Life's case) of commands yet to be acknowledged. The first consideration is how to handle any sound effects and visual effects that are created in the shared code. Because commands can be run over and over again, it's important not to create footstep sounds, etc. multiple times as the old commands are re-run to update the predicted position. In addition, it's important for the server not to send the client effects that are already being predicted on the client. However, the client still must re-run the old commands or else there will be no way for the server to correct any erroneous prediction by the client. The solution to this problem is easy: the client just marks those commands which have not been predicted yet on the client and only plays effects if the user command is being run for the first time on the client. The other caveat is with respect to state data that exists solely on the client and is not part of the authoritative update data from the server. If you don't have any of this type of data, then you can simply use the last acknowledged state from the server as a starting point, and run the prediction user commands "in-place" on that data to arrive at a final state (which includes your position for rendering). In this case, you don't need to keep all of the intermediate results along the route for predicting from the last acknowledged state to the current time. However, if you are doing any logic totally client side (this logic could include functionality such as determining where the eye position is when you are in the process of crouching—and it's not really totally client side since the server still simulates this data also) that affects fields that are not replicated from the server to the client by the networking layer handling the player's state info, then you will need to store the intermediate results of prediction. This can be done with a sliding window, where the "from state" is at the start and then each time you run a user command through prediction, you fill in the next state in the window. When the server finally acknowledges receiving one or more commands that had been predicted, it is a simple matter of looking up which state the server is acknowledging and copying over the data that is totally client side to the new starting or "from state". So far, the above procedure describes how to accomplish client side prediction of movements. This system is similar to the system used in QuakeWorld2. Client-Side Prediction of Weapon Firing Layering prediction of the firing effects of weapons onto the above system is straightforward. Additional state information is needed for the local player on the client, of course, including which weapons are being held, which one is active, and how much ammo each of these weapons has remaining. With this information, the firing logic can be layered on top of the movement logic because, once again, the state of the firing buttons is included in the user command data structure that is shared between the client and the server. Of course, this can get complicated if the actual weapon logic is different between client and server. In Half-Life, we chose to avoid this complication by moving the implementation of a weapon's firing logic into "shared code" just like the player movement code. All of the variables that contribute to determining weapon state (e.g., ammo, when the next firing of the weapon can occur, what weapon animation is playing, etc.), are then part of the authoritative server state and are replicated to the client-side so there, they can be used for prediction of weapon state. Predicting weapon firing on the client will likely lead to the decision also to predict weapon switching, deployment, and holstering. In this fashion, the user feels that the game is 100% responsive to his or her movement and weapon activation activities. This goes a long way toward reducing the feeling of latency that many players have come to endure with today's Internet-enabled action experiences. Umm, This is a Lot of Work Replicating the necessary fields to the client and handling all of the intermediate state is a fair amount of work. At this point, you may be asking, why not eliminate all of the server stuff and just have the client report where s/he is after each movement? In other words, why not ditch the server stuff and just run the movement and weapons purely on the client-side? Then, the client would just send results to the server along the lines of, "I'm now at position x and, by the way, I just shot player 2 in the head." This is fine if you can trust the client. This is how a lot of the military simulation systems work (i.e., they are a closed system and they trust all of the clients). This is how peer-to-peer games generally work. For Half-Life, this mechanism is unworkable because of realistic concerns about cheating. If we encapsulated absolute state data in this fashion, we'd raise the motivation to hack the client even higher than it already is3. For our games, this risk is too high and we fall back to requiring an authoritative server. A system where movements and weapon effects are predicted client-side is a very workable system. For instance, this is the system that the Quake3 engine supports. One of the problems with this system is that you still have to have a feel for your latency to determine how to lead your targets (for instant hit weapons). In other words, although you get to hear the weapons firing immediately, and your position is totally up-to-date, the results of your shots are still subject to latency. For example, if you are aiming at a player running perpendicular to your view and you have 100 milliseconds of latency and the player is running at 500 units per second, then you'll need to aim 50 units in front of the target to hit the target with an instant hit weapon. The greater the latency, the greater the lead targeting needed. Getting a "feel" for your latency is difficult. Quake3 attempted to mitigate this by playing a brief tone whenever you received confirmation of your hits. That way, you could figure out how far to lead by firing your weapons in rapid succession and adjusting your leading amount until you started to hear a steady stream of tones. Obviously, with sufficient latency and an opponent who is actively dodging, it is quite difficult to get enough feedback to focus in on the opponent in a consistent fashion. If your latency is fluctuating, it can be even harder. Display of Targets Another important aspect influencing how a user perceives the responsiveness of the world is the mechanism for determining, on the client, where to render the other players. The two most basic mechanisms for determining where to display objects are extrapolation and interpolation4. For extrapolation, the other player/object is simulated forward in time from the last known spot, direction, and velocity in more or less a ballistic manner. Thus, if you are 100 milliseconds lagged, and the last update you received was that (as above) the other player was running 500 units per second perpendicular to your view, then the client could assume that in "real time" the player has moved 50 units straight ahead from that last known position. The client could then just draw the player at that extrapolated position and the local player could still more or less aim right at the other player. The biggest drawback of using extrapolation is that player's movements are not very ballistic, but instead are very non-deterministic and subject to high jerk5. Layer on top of this the unrealistic player physics models that most FPS games use, where player's can turn instantaneously and apply unrealistic forces to create huge accelerations at arbitrary angles and you'll see that the extrapolation is quite often incorrect. The developer can mitigate the error by limiting the extrapolation time to a reasonable value (QuakeWorld, for instance, limited extrapolation to 100 milliseconds). This limitation helps because, once the true player position is finally received, there will be a limited amount of corrective warping. In a world where most players still have greater than 150 milliseconds of latency, the player must still lead other players in order to hit them. If those players are "warping" to new spots because of extrapolation errors, then the gameplay suffers nonetheless. The other method for determining where to display objects and players is interpolation. Interpolation can be viewed as always moving objects somewhat in the past with respect to the last valid position received for the object. For instance, if the server is sending 10 updates per second (exactly) of the world state, then we might impose 100 milliseconds of interpolation delay in our rendering. Then, as we render frames, we interpolate the position of the object between the last updated position and the position one update before that (alternatively, the last render position) over that 100 milliseconds. As the object just gets to the last updated position, we receive a new update from the server (since 10 updates per second means that the updates come in every 100 milliseconds) we can start moving toward this new position over the next 100 milliseconds. If one of the update packets fails to arrive, then there are two choices: We can start extrapolating the player position as noted above (with the large potential errors noted) or we can simply have the player rest at the position in the last update until a new update arrives (causing the player's movement to stutter). The general algorithm for this type of interpolation is as follows: Each update contains the server time stamp for when it was generated6 From the current client time, the client computes a target time by subtracting the interpolation time delta (100 ms) If the target time is in between the timestamp of the last update and the one before that, then those timestamps determine what fraction of the time gap has passed. This fraction is used to interpolate any values (e.g., position and angles). In essence, you can think of interpolation, in the above example, as buffering an additional 100 milliseconds of data on the client. The other players, therefore, are drawn where they were at a point in the past that is equal to your exact latency plus the amount of time over which you are interpolating. To deal with the occasional dropped packet, we could set the interpolation time as 200 milliseconds instead of 100 milliseconds. This would (again assuming 10 updates per second from the server) allow us to entirely miss one update and still have the player interpolating toward a valid position, often moving through this interpolation without a hitch. Of course, interpolating for more time is a tradeoff, because it is trading additional latency (making the interpolated player harder to hit) for visual smoothness. In addition, the above type of interpolation (where the client tracks only the last two updates and is always moving directly toward the most recent update) requires a fixed time interval between server updates. The method also suffers from visual quality issues that are difficult to resolve. The visual quality issue is as follows. Imagine that the object being interpolated is a bouncing ball (which actually accurately describes some of our players). At the extremes, the ball is either high in the air or hitting the pavement. However, on average, the ball is somewhere in between. If we only interpolate to the last position, it is very likely that this position is not on the ground or at the high point. The bounciness of the ball is "flattened" out and it never seems to hit the ground. This is a classical sampling problem and can be alleviated by sampling the world state more frequently. However, we are still quite likely never actually to have an interpolation target state be at the ground or at the high point and this will still flatten out the positions. In addition, because different users have different connections, forcing updates to occur at a lockstep like 10 updates per second is forcing a lowest common denominator on users unnecessarily. In Half-Life, we allow the user to ask for as many updates per second as he or she wants (within limit). Thus, a user with a fast connection could receive 50 updates per second if the user wanted. By default, Half-Life sends 20 updates per second to each player the Half-Life client interpolates players (and many other objects) over a period of 100 milliseconds.7 To avoid the flattening of the bouncing ball problem, we employ a different algorithm for interpolation. In this method, we keep a more complete "position history" for each object that might be interpolated. The position history is the timestamp and origin and angles (and could include any other data we want to interpolate) for the object. Each update we receive from the server creates a new position history entry, including timestamp and origin/angles for that timestamp. To interpolate, we compute the target time as above, but then we search backward through the history of positions looking for a pair of updates that straddle the target time. We then use these to interpolate and compute the final position for that frame. This allows us to smoothly follow the curve that completely includes all of our sample points. If we are running at a higher framerate than the incoming update rate, we are almost assured of smoothly moving through the sample points, thereby minimizing (but not eliminating, of course, since the pure sampling rate of the world updates is the limiting factor) the flattening problem described above. The only consideration we have to layer on top of either interpolation scheme is some way to determine that an object has been forcibly teleported, rather than just moving really quickly. Otherwise we might "smoothly" move the object over great distances, causing the object to look like it's traveling way too fast. We can either set a flag in the update that says, "don't interpolate" or "clear out the position history," or we can determine if the distance between the origin and one update and another is too big, and thereby presumed to be a teleportation/warp. In that case, the solution is probably to just move the object to the latest know position and start interpolating from there. Lag Compensation Understanding interpolation is important in designing for lag compensation because interpolation is another type of latency in a user's experience. To the extent that a player is looking at other objects that have been interpolated, then the amount of interpolation must be taken into consideration in computing, on the server, whether the player's aim was true. Lag compensation is a method of normalizing server-side the state of the world for each player as that player's user commands are executed. You can think of lag compensation as taking a step back in time, on the server, and looking at the state of the world at the exact instant that the user performed some action. The algorithm works as follows: Before executing a player's current user command, the server: Computes a fairly accurate latency for the player Searches the server history (for the current player) for the world update that was sent to the player and received by the player just before the player would have issued the movement command From that update (and the one following it based on the exact target time being used), for each player in the update, move the other players backwards in time to exactly where they were when the current player's user command was created. This moving backwards must account for both connection latency and the interpolation amount8 the client was using that frame. Allow the user command to execute (including any weapon firing commands, etc., that will run ray casts against all of the other players in their "old" positions). Move all of the moved/time-warped players back to their correct/current positions Note that in the step where we move the player backwards in time, this might actually require forcing additional state info backwards, too (for instance, whether the player was alive or dead or whether the player was ducking). The end result of lag compensation is that each local client is able to directly aim at other players without having to worry about leading his or her target in order to score a hit. Of course, this behavior is a game design tradeoff. Game Design Implications of Lag Compensation The introduction of lag compensation allows for each player to run on his or her own clock with no apparent latency. In this respect, it is important to understand that certain paradoxes or inconsistencies can occur. Of course, the old system with the authoritative server and "dumb" or simple clients had it's own paradoxes. In the end, making this tradeoff is a game design decision. For Half-Life, we believe deciding in favor of lag compensation was a justified game design decision. The first problem of the old system was that you had to lead your target by some amount that was related to your latency to the server. Aiming directly at another player and pressing the fire button was almost assured to miss that player. The inconsistency here is that aiming is just not realistic and that the player controls have non-predictable responsiveness. With lag compensation, the inconsistencies are different. For most players, all they have to do is acquire some aiming skill and they can become proficient (you still have to be able to aim). Lag compensation allows the player to aim directly at his or her target and press the fire button (for instant hit weapons9). The inconsistencies that sometimes occur, however, are from the points of view of the players being fired upon. For instance, if a highly lagged player shoots at a less lagged player and scores a hit, it can appear to the less lagged player that the lagged player has somehow "shot around a corner"10. In this case, the lower lag player may have darted around a corner. But the lagged player is seeing everything in the past. To the lagged player, s/he has a direct line of sight to the other player. The player lines up the crosshairs and presses the fire button. In the meantime, the low lag player has run around a corner and maybe even crouched behind a crate. If the high lag player is sufficiently lagged, say 500 milliseconds or so, this scenario is quite possible. Then, when the lagged player's user command arrives at the server, the hiding player is transported backward in time and is hit. This is the extreme case, and in this case, the low ping player says that s/he was shot from around the corner. However, from the lagged player's point of view, they lined up their crosshairs on the other player and fired a direct hit. From a game design point of view, the decision for us was easy: let each individual player have completely responsive interaction with the world and his or her weapons. In addition, the inconsistency described above is much less pronounced in normal combat situations. For first-person shooters, there are two more typical cases. First, consider two players running straight at each other pressing the fire button. In this case, it's quite likely that lag compensation will just move the other player backwards along the same line as his or her movement. The person being shot will be looking straight at his attacker and no "bullets bending around corners" feeling will be present. The next example is two players, one aiming at the other while the other dashes in front perpendicular to the first player. In this case, the paradox is minimized for a wholly different reason. The player who is dashing across the line of sight of the shooter probably has (in first-person shooters at least) a field of view of 90 degrees or less. In essence, the runner can't see where the other player is aiming. Therefore, getting shot isn't going to be surprising or feel wrong (you get what you deserve for running around in the open like a maniac). Of course, if you have a tank game, or a game where the player can run one direction, and look another, then this scenario is less clear-cut, since you might see the other player aiming in a slightly incorrect direction. Conclusion Lag compensation is a tool to ameliorate the effects of latency on today's action games. The decision of whether to implement such a system rests with the game designer since the decision directly changes the feel of the game. For Half-Life, Team Fortress and Counter Strike, the benefits of lag compensation easily outweighed the inconsistencies noted above. Footnotes In the Half-Life engine, it is possible to ask the client-side prediction algorithm to account for some, but not all, of the latency in performing prediction. The user could control the amount of prediction by changing the value of the "pushlatency" console variable to the engine. This variable is a negative number indicating the maximum number of milliseconds of prediction to perform. If the number is greater (in the negative) than the user's current latency, then full prediction up to the current time occurs. In this case, the user feels zero latency in his or her movements. Based upon some erroneous superstition in the community, many users insisted that setting pushlatency to minus one-half of the current average latency was the proper setting. Of course, this would still leave the player's movements lagged (often described as if you are moving around on ice skates) by half of the user's latency. All of this confusion has brought us to the conclusion that full prediction should occur all of the time and that the pushlatency variable should be removed from the Half-Life engine. (Return) http://www.quakeforge.net/files/q1source.zip (Return) A discussion of cheating and what developers can do to deter it is beyond the scope of this paper. (Return) Though hybrids and corrective methods are also possible. (Return) "Jerk" is a measure of how fast accelerative forces are changing. (Return) It is assumed in this paper that the client clock is directly synchronized to the server clock modulo the latency of the connection. In other words, the server sends the client, in each update, the value of the server's clock and the client adopts that value as its clock. Thus, the server and client clocks will always be matched, with the client running the same timing somewhat in the past (the amount in the past is equal to the client's current latency). Smoothing out discrepancies in the client clock can be solved in various ways. (Return) The time spacing of these updates is not necessarily fixed. The reason why is that during high activity periods of the game (especially for users with lower bandwidth connections), it's quite possible that the game will want to send you more data than your connection can accommodate. If we were on a fixed update interval, then you might have to wait an entire additional interval before the next packet would be sent to the client. However, this doesn't match available bandwidth effectively. Instead, the server, after sending every packet to a player, determines when the next packet can be sent. This is a function of the user's bandwidth or "rate" setting and the number of updates requested per second. If the user asks for 20 updates per second, then it will be at least 50 milliseconds before the next update packet can be sent. If the bandwidth choke is active (and the server is sufficiently high framerate), it could be 61, etc., milliseconds before the next packet gets sent. Thus, Half-Life packets can be somewhat arbitrarily spaced. The simple move to latest goal interpolation schemes don't behave as well (think of the old anchor point for movement as being variable) under these conditions as the position history interpolation method (described below). (Return) Which Half-Life encodes in the lerp_msec field of the usercmd_t structure described previously. (Return) For weapons that fire projectiles, lag compensation is more problematic. For instance, if the projectile lives autonomously on the server, then what time space should the projectile live in? Does every other player need to be "moved backward" every time the projectile is ready to be simulated and moved by the server? If so, how far backward in time should the other players be moved? These are interesting questions to consider. In Half-Life, we avoided them; we simply don't lag compensate projectile objects (that's not to say that we don't predict the sound of you firing the projectile on the client, just that the actual projectile is not lag compensated in any way). (Return) This is the phrase our user community has adopted to describe this inconsistency. (Return) 中英对照 译文出处 Overview 综述 Designing first-person action games for Internet play is a challenging process. Having robust on-line gameplay in your action title, however, is becoming essential to the success and longevity of the title. In addition, the PC space is well known for requiring developers to support a wide variety of customer setups. Often, customers are running on less than state-of-the-art hardware. The same holds true for their network connections. 第一人称角色网络游戏的设计是一项很有挑战性的工作。网络环境下的健壮性，是动作游戏能否成功的一个重要因素。另外，PC上面的开发者需要考虑到玩家层次不齐的机器配置以及网络状况，很多用户的硬件配置跟网络跟当前最好的配置跟网络有一定差距。 While broadband has been held out as a panacea for all of the current woes of on-line gaming, broadband is not a simple solution allowing developers to ignore the implications of latency and other network factors in game designs. It will be some time before broadband truly becomes adopted the United States, and much longer before it can be assumed to exist for your clients in the rest of the world. In addition, there are a lot of poor broadband solutions, where users may occasionally have high bandwidth, but more often than not also have significant latency and packet loss in their connections. 宽带网络的出现有利于在线游戏开发，但是开发者还是需要考虑网络延迟和其它网络特性。而且宽带网络在美国被广泛采用还需要一段时间，在世界上其它国家可能需要更长的一段时间。另外，很多宽带网络质量很差，用户虽然偶尔能够享受到高带宽，但更多的时候他们不得不面对高延迟和高丢包率。 Your game must behave well in this world. This discussion will give you a sense of some of the tradeoffs required to deliver a cutting-edge action experience on the Internet. The discussion will provide some background on how client / server architectures work in many on-line action games. In addition, the discussion will show how predictive modeling can be used to mask the effects of latency. Finally, the discussion will describe a specific mechanism, lag compensation, for allowing the game to compensate for connection quality. 我们应该提供给玩家良好的游戏。本篇文章讨论了如何提供给玩家顶尖的操作体验；介绍了很多在线动作游戏中采用的C/S架构背景。此外，我们还讨论了如何通过一个预测模型来掩饰延迟带来的影响。文章的最后描述了一个叫做延迟补偿的机制，弥补了因为网络质量不好带来的负面影响 Basic Architecture of a Client / Server Game C/S游戏的基本架构 Most action games played on the net today are modified client / server games. Games such as Half-Life, including its mods such as Counter-Strike and Team Fortress Classic, operate on such a system, as do games based on the Quake3 engine and the Unreal Tournament engine. In these games, there is a single, authoritative server that is responsible for running the main game logic. To this are connected one or more &quot;dumb&quot; clients. These clients, initially, were nothing more than a way for the user input to be sampled and forwarded to the server for execution. The server would execute the input commands, move around other objects, and then send back to the client a list of objects to render. Of course, the real world system has more components to it, but the simplified breakdown is useful for thinking about prediction and lag compensation. 网络上可玩的大部分动作游戏都是C/S结构游戏基础上修改完成的，比如半条命以及其修改版反恐精英、军团要塞，以及一些基于quake3引擎和虚幻引擎的游戏。这类游戏都有一个用来执行游戏逻辑的服务器以及连接到这个服务器的多个客户端。客户端仅仅是用来接收玩家的操作并发给服务器，服务器对这些操作作出响应，移动玩家周围物体，并将游戏世界的信息发给客户端显示出来。当然世界的游戏系统有更多组件，我们这样简化有利于分析预测和延迟补偿。 With this in mind, the typical client / server game engine architecture generally looks like this: 基于这种考虑，典型的C/S游戏引擎通常看起来是这样的 For this discussion, all of the messaging and coordination needed to start up the connection between client and server is omitted. The client’s frame loop looks something like the following: 为了便于讨论，我们假定客户端跟服务器之间已经建立连接；客户端的每一帧循环如下： 1.Sample clock to find start time 1.获取帧开始时间 2.Sample user input (mouse, keyboard, joystick) 2.采集用户输入 3.Package up and send movement command using simulation time 3.根据模拟时间将移动命令打包发送给服务器 4.Read any packets from the server from the network system 4.获取处理服务器传过来的数据包 5.Use packets to determine visible objects and their state 5.根据服务器数据包的内容决定可见物体及其状态 6.Render Scene 6.渲染场景 7.Sample clock to find end time 7.获取帧结束时间 8.End time minus start time is the simulation time for the next frame 8.结束时间减去开始时间就是下一帧的模拟时间 Each time the client makes a full pass through this loop, the &quot;frametime&quot; is used for determining how much simulation is needed on the next frame. If your framerate is totally constant then frametime will be a correct measure. Otherwise, the frametimes will be incorrect, but there isn't really a solution to this (unless you could deterministically figure out exactly how long it was going to take to run the next frame loop iteration before running it...). 客户端每完成一个帧循环，就用“frametime”来决定下一帧需要多少时间，如果帧率恒定，“frametime”就是准确的，否则就没办法获得准确的“frametime”（因为在每一帧开始之前你不可能知道这一帧需要多长时间） The server has a somewhat similar loop: 服务器的循环大同小异： 1.Sample clock to find start time 1.获取帧开始时间 2.Read client user input messages from network 2.读取客户端发过来的操作信息 3.Execute client user input messages 3.根据客户端操作执行逻辑运算 4.Simulate server-controlled objects using simulation time from last full pass 4.采用上一个循环得到的模拟时间来模拟服务器控制的物体移动状态 5.For each connected client, package up visible objects/world state and send to client 5.对每一个连接的客户端，发送打包相应的物体/世界状态 6.Sample clock to find end time 6.获取帧结束时间 7.End time minus start time is the simulation time for the next frame 7.结束时间减去开始时间就是下一帧的模拟时间 In this model, non-player objects run purely on the server, while player objects drive their movements based on incoming packets. Of course, this is not the only possible way to accomplish this task, but it does make sense. 在这个模型中，非玩家物体完全由服务器控制其状态，每个玩家根据服务器发过来的数据包控制自己的移动。这是一种很自然的方法，当然还有其它的方法也可以完成这个功能。 Contents of the User Input messages 用户消息的内容 In Half-Life engine games, the user input message format is quite simple and is encapsulated in a data structure containing just a few essential fields: 基于half-life引擎的游戏用户消息都很简单，只需要封装在一个包含几个关键成员的结构中： typedef struct usercmd_s { // Interpolation time on client shortlerp_msec; &nbsp;&nbsp; // Duration in ms of command bytemsec; &nbsp; &nbsp; &nbsp; // Command view angles. vec3_tviewangles; &nbsp;&nbsp; // intended velocities // Forward velocity. floatforwardmove; &nbsp; // Sideways velocity. floatsidemove; &nbsp; &nbsp; // Upward velocity. floatupmove; &nbsp;&nbsp; // Attack buttons unsigned short buttons;&nbsp; // // Additional fields omitted... // } usercmd_t; The critical fields here are the msec, viewangles, forward, side, and upmove, and buttons fields. The msec field corresponds to the number of milliseconds of simulation that the command corresponds to (it's the frametime). The viewangles field is a vector representing the direction the player was looking during the frame. The forward, side, and upmove fields are the impulses determined by examining the keyboard, mouse, and joystick to see if any movement keys were held down. Finally, the buttons field is just a bit field with one or more bits set for each button that is being held down. 结构中最关键的变量时msec,viewangles,forward,side,upmove和buttons。msec表示这个命令执行对应的毫秒数（也就是上面提到的“frametime”）。viewangles是一个三维向量，表示玩家的朝向。forward,side和upmove表示玩家是否通过键盘、鼠标或控制杆控制移动。最后，buttons这个字段包含一个或多个比特，标志玩家是否按着某些按键。 Using the above data structures and client / server architecture, the core of the simulation is as follows. First, the client creates and sends a user command to the server. The server then executes the user command and sends updated positions of everything back to client. Finally, the client renders the scene with all of these objects. This core, though quite simple, does not react well under real world situations, where users can experience significant amounts of latency in their Internet connections. The main problem is that the client truly is &quot;dumb&quot; and all it does is the simple task of sampling movement inputs and waiting for the server to tell it the results. If the client has 500 milliseconds of latency in its connection to the server, then it will take 500 milliseconds for any client actions to be acknowledged by the server and for the results to be perceptible on the client. While this round trip delay may be acceptable on a Local Area Network (LAN), it is not acceptable on the Internet. 基于C/S架构的游戏采用以上数据结构运行如下：客户端创建命令并发送到服务器，服务器响应这些命令并把更新了的世界和物体位置信息发回客户端，客户端收到以后进行渲染。这种方式非常简单，但是在实际应用中效果差强人意，用户会感觉到网络连接带来的明显延迟。这主要是由于客户端完全没有逻辑操作，发出消息以后就等待服务器响应。如果客户端跟服务器有500ms的延迟，客户端执行了操作到看到操作的结果就需要500ms，这种延迟在局域网通常可以接受（因为通常延迟比较小），但在因特网上是没法接受的 Client Side Prediction 客户端预测 One method for ameliorating this problem is to perform the client's movement locally and just assume, temporarily, that the server will accept and acknowledge the client commands directly. This method is labeled as client-side prediction. 有一种方法可以改善这种情况：客户端本地即时执行移动操作，假定服务器即时通知客户端可以执行操作，这种方法可以称为客户端预测。 Client-side prediction of movements requires us to let go of the &quot;dumb&quot; or minimal client principle. That's not to say that the client is fully in control of its simulation, as in a peer-to-peer game with no central server. There still is an authoritative server running the simulation just as noted above. Having an authoritative server means that even if the client simulates different results than the server, the server's results will eventually correct the client's incorrect simulation. Because of the latency in the connection, the correction might not occur until a full round trip's worth of time has passed. The downside is that this can cause a very perceptible shift in the player's position due to the fixing up of the prediction error that occurred in the past. 采用客户端运动预测以后，客户端就不再是一个“小型客户端”，不再单单响应服务器命令；但也不是说客户端可以像没有中央服务器的p2p游戏完全自治。服务器仍然在运行并保证在客户端跟服务器运行结果不一致的情况下纠正客户端错误的模拟。由于网络延迟，修正在一个网络传输周期以后才会执行，这个时候纠正信息通常已经过期，这样会导致明显的位置漂移，因为客户端收到的修正信息是过去某个时间的。 To implement client-side prediction of movement, the following general procedure is used. As before, client inputs are sampled and a user command is generated. Also as before, this user command is sent off to the server. However, each user command (and the exact time it was generated) is stored on the client. The prediction algorithm uses these stored commands. 为了使客户端运动预测有效，我们采用以下方法：还是客户端采样并生成命令发送到服务器，但是每个包含生成时间的命令在客户端本地存起来并在预测算法中使用。 For prediction, the last acknowledged movement from the server is used as a starting point. The acknowledgement indicates which user command was last acted upon by the server and also tells us the exact position (and other state data) of the player after that movement command was simulated on the server. The last acknowledged command will be somewhere in the past if there is any lag in the connection. For instance, if the client is running at 50 frames per second (fps) and has 100 milliseconds of latency (roundtrip), then the client will have stored up five user commands ahead of the last one acknowledged by the server. These five user commands are simulated on the client as a part of client-side prediction. Assuming full prediction[1], the client will want to start with the latest data from the server, and then run the five user commands through &quot;similar logic&quot; to what the server uses for simulation of client movement. Running these commands should produce an accurate final state on the client (final player position is most important) that can be used to determine from what position to render the scene during the current frame. 预测的过程中，我们把服务器确认的移动信息作为开始，这样客户端就可以确定服务器执行上次命令以后游戏中玩家的准确信息（比如位置）。如果网络有延迟，这个确认命令也会有一定延迟。假设客户端运行帧率为50fps，网络延时为100ms，这样在客户端收到服务器的确认命令的时候，本地命令队列中已经有5条信息，这5条信息被用来执行客户端预测。假设执行完全预测【1】客户端在收到来自服务器的最新信息后，就开始按照与服务器相同的逻辑执行本地消息队列中的5个命令。这些命令执行以后得到当前状态（最重要的是玩家最后的位置），然后根据玩家的状态信息渲染当前帧。 In Half-Life, minimizing discrepancies between client and server in the prediction logic is accomplished by sharing the identical movement code for players in both the server-side game code and the client-side game code. These are the routines in the pm_shared/ (which stands for &quot;player movement shared&quot;) folder of the HL SDK. The input to the shared routines is encapsulated by the user command and a &quot;from&quot; player state. The output is the new player state after issuing the user command. The general algorithm on the client is as follows: 在半条命这个游戏中，客户端跟服务器采用相同的代码来计算移动，这样可以减小客户端预测跟服务器之间的误差。这些代码位于HLSDK中的pm_shared/（意思是“player movement shared”）。这段代码的输入是玩家操作和客户端的初始状态，输出是玩家操作以后的状态。客户端算法大致如下： 12345678910111213“from state” &lt;- state after last user command acknowledged by the server;“command” &lt;- first command after last user command acknowledged by server;while (true)&#123; run “command” on “from state” to generate “to state”; if (this was the most up to date “command”) break; “from state” = “to state”; “command” = next “command”;&#125;; 123456789101112“初始状态” &lt;- 上一条已被服务器确认过的玩家command之后的state"命令" &lt;- 上一条已被服务器确认过的玩家command之后的commandwhile (true)&#123; 以"from state" 为基础执行"command"得到 "to state"; if (这是最新的 "command") break; "from state" = "to state"; "command" = next "command";&#125; The origin and other state info in the final &quot;to state&quot; is the prediction result and is used for rendering the scene that frame. The portion where the command is run is simply the portion where all of the player state data is copied into the shared data structure, the user command is processed (by executing the common code in the pm_shared routines in Half-Life's case), and the resulting data is copied back out to the &quot;to state&quot;. 玩家的初始状态和预测结果用来渲染场景。命令的执行过程就是：将玩家状态复制到共享数据结构中，执行玩家操作（执行hlsdk中pm_shared中的共用代码），然后将结果复制到目标状态（to state） There are a few important caveats to this system. First, you'll notice that, depending upon the client's latency and how fast the client is generating user commands (i.e., the client's framerate), the client will most often end up running the same commands over and over again until they are finally acknowledged by the server and dropped from the list (a sliding window in Half-Life's case) of commands yet to be acknowledged. The first consideration is how to handle any sound effects and visual effects that are created in the shared code. Because commands can be run over and over again, it's important not to create footstep sounds, etc. multiple times as the old commands are re-run to update the predicted position. In addition, it's important for the server not to send the client effects that are already being predicted on the client. However, the client still must re-run the old commands or else there will be no way for the server to correct any erroneous prediction by the client. The solution to this problem is easy: the client just marks those commands which have not been predicted yet on the client and only plays effects if the user command is being run for the first time on the client. 这个系统中有几个需要注意的地方，首先，由于网络延迟，客户端又在不停地以一定速度（客户端帧率）生成命令，一个命令通常会被客户端反复执行，直到得到服务器的确认以后才将其从命令列表中删除（这就是半条命中的滑动窗口）。首先要考虑的是如何处理共享代码中生成的声效和动画效果。因为命令可能会被多次执行，预测位置的过程被多次执行的时候要注意避免重声等不正确的效果。另外，服务器也要避免客户端意见预测的效果。然而，客户端必须重新运行旧的命令，否则就没法根据服务器来纠正客户端的预测错误。解决方法很简单：客户端将没有执行的客户端命令进行标记，如果这些命令在客户端第一次执行，则播放相应的效果。 The other caveat is with respect to state data that exists solely on the client and is not part of the authoritative update data from the server. If you don't have any of this type of data, then you can simply use the last acknowledged state from the server as a starting point, and run the prediction user commands &quot;quot;in-place&quot; on that data to arrive at a final state (which includes your position for rendering). In this case, you don't need to keep all of the intermediate results along the route for predicting from the last acknowledged state to the current time. However, if you are doing any logic totally client side (this logic could include functionality such as determining where the eye position is when you are in the process of crouching—and it's not really totally client side since the server still simulates this data also) that affects fields that are not replicated from the server to the client by the networking layer handling the player's state info, then you will need to store the intermediate results of prediction. This can be done with a sliding window, where the &quot;from state&quot; is at the start and then each time you run a user command through prediction, you fill in the next state in the window. When the server finally acknowledges receiving one or more commands that had been predicted, it is a simple matter of looking up which state the server is acknowledging and copying over the data that is totally client side to the new starting or &quot;from state&quot;. 另外需要注意的是服务器不处理，只有客户端才有的一些数据；如果没有这种类型的数据，我们可以如上面所述，以服务器第一条消息作为起点进行预测得到下一帧状态（包括用来渲染的位置信息）。然而，如果有些逻辑是纯客户端的，服务器不会处理（比如玩家蹲下来&#30524;睛的位置-然而这也不是纯客户端信息，因为服务器也会处理这个数据），这种情况下我们需要将预测的中间结果存起来。可以用一个滑动窗口完成这项工作，其中“开始状态”是开始，以后每次执行一个玩家命令预测完成后，填写窗口中的下一个状态；当服务器通知某个命令被接受并执行以后，从窗口中查找服务器处理的是哪条命令并将相应的数据传到下一个帧的“起始状态” So far, the above procedure describes how to accomplish client side prediction of movements. This system is similar to the system used in QuakeWorld2. 到此为止，我们描述了客户端的运动预测。quakeworld2中采用了这种类型的预测 Client-Side Prediction of Weapon Firing 开火过程中的客户端预测 Layering prediction of the firing effects of weapons onto the above system is straightforward. Additional state information is needed for the local player on the client, of course, including which weapons are being held, which one is active, and how much ammo each of these weapons has remaining. With this information, the firing logic can be layered on top of the movement logic because, once again, the state of the firing buttons is included in the user command data structure that is shared between the client and the server. Of course, this can get complicated if the actual weapon logic is different between client and server. In Half-Life, we chose to avoid this complication by moving the implementation of a weapon's firing logic into &quot;shared code&quot; just like the player movement code. All of the variables that contribute to determining weapon state (e.g., ammo, when the next firing of the weapon can occur, what weapon animation is playing, etc.), are then part of the authoritative server state and are replicated to the client so that they can be used on the client for prediction of weapon state there. 上面描述的系统可以很自然地用于武器开火效果预测。客户端玩家需要记录一些状态，比如身上有哪些武器，正在使用的是哪一个，每把武器都还剩多少弹药。有了这些信息，开火逻辑可以建立在运动逻辑上面，只需要在客户端和服务器使用的命令里面加上玩家开火的按键信息。在半条命中，为了简单，武器开火逻辑代码也跟运动代码一样也作为“共享代码”。所有会影响到武器状态的变量，比如弹药、下次可开火时间、正在播放那个武器动画，都作为服务器的状态，这些状态会通知给客户端用来预测武器状态。 Predicting weapon firing on the client will likely lead to the decision also to predict weapon switching, deployment, and holstering. In this fashion, the user feels that the game is 100% responsive to his or her movement and weapon activation activities. This goes a long way toward reducing the feeling of latency that many players have come to endure with today's Internet-enabled action experiences. 客户端武器开火预测包括预测武器切换、部署、手枪皮套。这样，玩家会感觉游戏中的移动和武器状态100%受他控制。这在减小网络延迟给玩家带来的不爽上面迈出了一大步。 Umm, This is a Lot of Work 一些工作 Replicating the necessary fields to the client and handling all of the intermediate state is a fair amount of work. At this point, you may be asking, why not eliminate all of the server stuff and just have the client report where s/he is after each movement? In other words, why not ditch the server stuff and just run the movement and weapons purely on the client-side? Then, the client would just send results to the server along the lines of, &quot;I'm now at position x and, by the way, I just shot player 2 in the head.&quot; This is fine if you can trust the client. This is how a lot of the military simulation systems work (i.e., they are a closed system and they trust all of the clients). This is how peer-to-peer games generally work. For Half-Life, this mechanism is unworkable because of realistic concerns about cheating. If we encapsulated absolute state data in this fashion, we'd raise the motivation to hack the client even higher than it already is3. For our games, this risk is too high and we fall back to requiring an authoritative server. 服务器需要将必要的字段发给客户端，并且处理很多中间状态，有人可能有这样的疑问，为什么不把服务器逻辑取消，让客户端广播自己的位置，也就是将所有的移动、开火逻辑放在客户端。这样，客户端就会给服务器发送类&#20284;这样的结果报告：“我在X位置，我爆了玩家2的脑袋”。如果客户端可信的话，这样做是可以的，很多军方仿真系统就是这样做的（他们是一个封闭系统，所有客户端都可信）。点对点的游戏也是这么做的。对于半条命来说不可以这样做，因为客户端可能“欺骗”服务器。如果我们以这种方法封装状态数据，就会诱导玩家破解客户端【3】。对于我们的游戏来说这样做风险太大，我们还是选择采用服务器模式来做校验。 A system where movements and weapon effects are predicted client-side is a very workable system. For instance, this is the system that the Quake3 engine supports. One of the problems with this system is that you still have to have a feel for your latency to determine how to lead your targets (for instant hit weapons). In other words, although you get to hear the weapons firing immediately, and your position is totally up-to-date, the results of your shots are still subject to latency. For example, if you are aiming at a player running perpendicular to your view and you have 100 milliseconds of latency and the player is running at 500 units per second, then you'll need to aim 50 units in front of the target to hit the target with an instant hit weapon. The greater the latency, the greater the lead targeting needed. Getting a &quot;feel&quot; for your latency is difficult. Quake3 attempted to mitigate this by playing a brief tone whenever you received confirmation of your hits. That way, you could figure out how far to lead by firing your weapons in rapid succession and adjusting your leading amount until you started to hear a steady stream of tones. Obviously, with sufficient latency and an opponent who is actively dodging, it is quite difficult to get enough feedback to focus in on the opponent in a consistent fashion. If your latency is fluctuating, it can be even harder. 客户端进行运动和武器效果预测是非常可行的。例如quake3就支持这样的预测。这个系统需要注意一点，在判断目标的时候需要考虑到延迟（比如即时射击武器）。换句话说，虽然你看到自己用即时武器进行了射击，你自己的位置也是最新的，射击结果仍然跟延迟有关。例如，如果你射击一个玩家，这个玩家沿与你实现垂直的方向奔跑，假设你客户端延迟为100ms，玩家奔跑速度是500单位每秒，这样你需要瞄准玩家前方50单位才能准确击中。延迟越大，就需要更大的提前量。靠感觉弥补延迟太困难了。为了减轻这种效果，quake3对你的射击播放一个短音来进行确定。这样，玩家可以算出快速发射武器的时候需要多大的提前量，同时调整提前量直到听到稳定的音调串。如果延迟比较大，而你的对手又在不断躲避，就很难获得足够的反馈判断。如果延迟也不断变化，就更难了。 Display of Targets 目标的显示 Another important aspect influencing how a user perceives the responsiveness of the world is the mechanism for determining, on the client, where to render the other players. The two most basic mechanisms for determining where to display objects are extrapolation and interpolation[4]. 影响玩家游戏体验的另一个重要方面是客户端如何渲染其它玩家。两种基本的判断机制是：外推法和内插法【4】 For extrapolation, the other player/object is simulated forward in time from the last known spot, direction, and velocity in more or less a ballistic manner. Thus, if you are 100 milliseconds lagged, and the last update you received was that (as above) the other player was running 500 units per second perpendicular to your view, then the client could assume that in &quot;real time&quot; the player has moved 50 units straight ahead from that last known position. The client could then just draw the player at that extrapolated position and the local player could still more or less aim right at the other player. 外推法把其它玩家/物体看作一个点，这个点开始的位置、方向、速度已知，沿着自己的弹道向前移动。因此，假设延时是100ms，最新的协议通知客户端这个玩家奔跑速度是500单位每秒，方向垂直于玩家视线，客户端就可以假设事实上这个玩家当前实际的位置已经向前移动了50个单位。客户端可以在这个外推的位置渲染这个玩家，这样本地玩家就差不多可以正确瞄准。 The biggest drawback of using extrapolation is that player's movements are not very ballistic, but instead are very non-deterministic and subject to high jerk[5]. Layer on top of this the unrealistic player physics models that most FPS games use, where player's can turn instantaneously and apply unrealistic forces to create huge accelerations at arbitrary angles and you'll see that the extrapolation is quite often incorrect. The developer can mitigate the error by limiting the extrapolation time to a reasonable value (QuakeWorld, for instance, limited extrapolation to 100 milliseconds). This limitation helps because, once the true player position is finally received, there will be a limited amount of corrective warping. In a world where most players still have greater than 150 milliseconds of latency, the player must still lead other players in order to hit them. If those players are &quot;warping&quot; to new spots because of extrapolation errors, then the gameplay suffers nonetheless. 外推法的最大缺点是玩家的移动并不是完全弹道的，而是不确定的并且高&quot;jerk&quot;【5】。大部分FPS游戏采用非现实的玩家系统，玩家可以随时转弯，可以在任意角度作用不现实的加速度，因此外推法得到的结果经常是错误地。开发者可以通过限制外推时间来减轻外推误差（比如quake限制不能超过100ms）。这种限制使得在客户端收到玩家正确位置以后，纠错不至于太大。当前大部分玩家的网络延迟高于150ms，玩家必须对游戏中的其他玩家进行外推以便正确击中。如果别的玩家因为外推错误，被服务器拉回，游戏体验将非常差。 The other method for determining where to display objects and players is interpolation. Interpolation can be viewed as always moving objects somewhat in the past with respect to the last valid position received for the object. For instance, if the server is sending 10 updates per second (exactly) of the world state, then we might impose 100 milliseconds of interpolation delay in our rendering. Then, as we render frames, we interpolate the position of the object between the last updated position and the position one update before that (alternatively, the last render position) over that 100 milliseconds. As the object just gets to the last updated position, we receive a new update from the server (since 10 updates per second means that the updates come in every 100 milliseconds) we can start moving toward this new position over the next 100 milliseconds. 另一种方法叫插&#20540;法。插&#20540;法可以这样理解：客户端物体实际移动位置总是滞后一段时间。举个例子，如果服务器每秒同步10次世界信息，客户端渲染的时候会有100ms滞后。这样，每一帧渲染的时候，我们通过最新收到的位置信息和前100ms的位置信息（或者上一帧渲染位置）进行差&#20540;得到结果。我们每收到一个物体位置的更新信息，（每秒10个更新意味着每100ms收到一个更新）接下来的100ms我们就可以朝这个新的位置移动。 If one of the update packets fails to arrive, then there are two choices: We can start extrapolating the player position as noted above (with the large potential errors noted) or we can simply have the player rest at the position in the last update until a new update arrives (causing the player's movement to stutter). 如果一个更新包没有收到，有2种处理方法：第一、用上面介绍的外推法（有可能产生较大误差）；第二、保持玩家位于当前位置直到收到下一个更新包（会导致玩家移动顿挫） The general algorithm for this type of interpolation is as follows: 内插法的大致过程如下： 1.Each update contains the server time stamp for when it was generated[6] 1.每个更新包包含生成的服务器时间戳【6】 2.From the current client time, the client computes a target time by subtracting the interpolation time delta (100 ms) 2.根据客户端当前时间，客户端通过减去时间差（100ms）计算 一个目标时间 3.If the target time is in between the timestamp of the last update and the one before that, then those timestamps determine what fraction of the time gap has passed. 3.如果计算得到的目标时间在上一个更新时间和上上个更新时间之间，这些时间戳可以决定目标时间在过去的时间间隙中的情况 4.This fraction is used to interpolate any values (e.g., position and angles). 4.目标时间情况用来通过插&#20540;计算结果（如位置、角度） In essence, you can think of interpolation, in the above example, as buffering an additional 100 milliseconds of data on the client. The other players, therefore, are drawn where they were at a point in the past that is equal to your exact latency plus the amount of time over which you are interpolating. To deal with the occasional dropped packet, we could set the interpolation time as 200 milliseconds instead of 100 milliseconds. This would (again assuming 10 updates per second from the server) allow us to entirely miss one update and still have the player interpolating toward a valid position, often moving through this interpolation without a hitch. Of course, interpolating for more time is a tradeoff, because it is trading additional latency (making the interpolated player harder to hit) for visual smoothness. 上面提到的插&#20540;法，本质上是客户端缓存了接下来100ms的数据。对于每一个周围的玩家，他们都位于过去某个时间的位置，根据每一个具体的时间点进行插&#20540;。如果偶尔发生丢包，我们就将插&#20540;时间延长到200ms。这样我们就可以忽略一次更新（假设同步频率还是10次每秒），玩家还可以移动到合理的目标位置，这样进行插&#20540;通常不会有什么问题。当然，插&#20540;多少时间需要权衡，因为这种方法是用延时（玩家更难击中）来换取平滑。 In addition, the above type of interpolation (where the client tracks only the last two updates and is always moving directly toward the most recent update) requires a fixed time interval between server updates. The method also suffers from visual quality issues that are difficult to resolve. The visual quality issue is as follows. Imagine that the object being interpolated is a bouncing ball (which actually accurately describes some of our players). At the extremes, the ball is either high in the air or hitting the pavement. However, on average, the ball is somewhere in between. If we only interpolate to the last position, it is very likely that this position is not on the ground or at the high point. The bounciness of the ball is &quot;flattened&quot; out and it never seems to hit the ground. This is a classical sampling problem and can be alleviated by sampling the world state more frequently. However, we are still quite likely never actually to have an interpolation target state be at the ground or at the high point and this will still flatten out the positions. 另外，上述插&#20540;方法（客户端通过2个更新信息插&#20540;并且朝最新更新位置移动）需要服务器更新信息间隔固定。对于所谓的“视觉效果因素”，这种方式很难处理，“视觉效果因素”是这样的：假设我们插&#20540;的物体是弹球（这种模型可以准确描述某些玩家）。极端情况下，球或者在空中，或者正在碰地板。然而，通常情况下球在这两种状态之间。如果我们只插&#20540;上一个位置，这个位置可能既不在地面上，也不是最高点，这样，弹球弹的效果就被平滑掉了，好像永远没有弹到地面一样。这是一个经典问题，增加采样率可以减轻这种影响，但是仍然有可能我们采样不到球在地面的点跟最高点，这些点会给平滑掉。 In addition, because different users have different connections, forcing updates to occur at a lockstep like 10 updates per second is forcing a lowest common denominator on users unnecessarily. In Half-Life, we allow the user to ask for as many updates per second as he or she wants (within limit). Thus, a user with a fast connection could receive 50 updates per second if the user wanted. By default, Half-Life sends 20 updates per second to each player the Half-Life client interpolates players (and many other objects) over a period of 100 milliseconds.[7] 另外，不同用户网络状况不同，强迫每个用户都以固定速度更新（比如每秒10次）效果不是很好，在半条命中，用户每秒可以请求任意数量的更新包（没有限制）。这样，高速网络用户可以每秒更新50次，只要用户愿意。半条命的默认设置是每秒每个用户（以及游戏中其它物体）发送20次更新，以100ms为时间窗口进行插&#20540;。【7】 To avoid the flattening of the bouncing ball problem, we employ a different algorithm for interpolation. In this method, we keep a more complete &quot;position history&quot; for each object that might be interpolated. 为了避免“反弹球&quot;平滑问题，我们在插&#20540;的过程中采用了一个不同的算法，这种算法中我们对每一个可能插&#20540;的物体记录了一个完整的“历史位置”信息。 The position history is the timestamp and origin and angles (and could include any other data we want to interpolate) for the object. Each update we receive from the server creates a new position history entry, including timestamp and origin/angles for that timestamp. To interpolate, we compute the target time as above, but then we search backward through the history of positions looking for a pair of updates that straddle the target time. We then use these to interpolate and compute the final position for that frame. This allows us to smoothly follow the curve that completely includes all of our sample points. If we are running at a higher framerate than the incoming update rate, we are almost assured of smoothly moving through the sample points, thereby minimizing (but not eliminating, of course, since the pure sampling rate of the world updates is the limiting factor) the flattening problem described above. 历史位置信息记录了物体的时间戳、远点、角度（以及其它我们需要插&#20540;计算的数据）。我们每收到一个服务器的更新，我们就创建一条包含时间戳的记录，其中包含原始位置、角度信息。在插&#20540;过程中，我们用上面的方法计算目标时间，然后搜索位置历史信息，找到包含目标时间的记录区间。然后用找到的信息插&#20540;计算当前帧的位置。这样我们就可以平滑跟踪到包含所有采样点的曲线。如果客户端帧率比服务器更新频率大，我们就可以将采样点平滑处理，减小上面提到的平滑处理带来的问题（当然没法避免，因为采用频率限制，而世界本身是连续的）。 The only consideration we have to layer on top of either interpolation scheme is some way to determine that an object has been forcibly teleported, rather than just moving really quickly. Otherwise we might &quot;smoothly&quot; move the object over great distances, causing the object to look like it's traveling way too fast. We can either set a flag in the update that says, &quot;don't interpolate&quot; or &quot;clear out the position history,&quot; or we can determine if the distance between the origin and one update and another is too big, and thereby presumed to be a teleportation/warp. In that case, the solution is probably to just move the object to the latest know position and start interpolating from there. 需要注意的是，上面提到的插&#20540;方法使用的时候，物体有时候会被服务器拉回，而不是快速移动。当然我们也可以平滑地将物体移动一段较长的距离，这样看起来物体移动很快。更新的过程中我们可以设一个标志表示不插&#20540;或清除历史记录，或者如果起始点与目标点距离过长，我们就认为数据不正常。这种情况我们就将物体直接拉过去。并以这个位置为起始点进行插&#20540;。 Lag Compensation 延迟补偿 Understanding interpolation is important in designing for lag compensation because interpolation is another type of latency in a user's experience. To the extent that a player is looking at other objects that have been interpolated, then the amount of interpolation must be taken into consideration in computing, on the server, whether the player's aim was true. 插&#20540;也会带来延迟，所以考虑延迟补偿的过程中需要理解插&#20540;过程。玩家看到的别的物体是经过插&#20540;计算出来的，所以插&#20540;过程中需要考虑在服务器上玩家的目标是否正确。 Lag compensation is a method of normalizing server-side the state of the world for each player as that player's user commands are executed. You can think of lag compensation as taking a step back in time, on the server, and looking at the state of the world at the exact instant that the user performed some action. The algorithm works as follows: 延迟补偿是服务器执行的一种策略，当服务器收到客户端命令并执行的过程中，根据客户端的具体情况进行归一。延迟补偿可以看做服务器处理用户命令的时候回退一段时间，退到客户端发送命令时候的准确时间。算法流程如下： 1.Before executing a player's current user command, the server: 1.服务器执行客户端命令之前执行以下操作： &nbsp; &nbsp; 1.Computes a fairly accurate latency for the player &nbsp; &nbsp; 1.为玩家计算一个相当精确的延迟时间 &nbsp; &nbsp; 2.Searches the server history (for the current player) for the world update that was sent to the player and received by the player just before the player would have issued the movement command &nbsp; &nbsp;&nbsp;2.对每个玩家，从服务器历史信息中找一个已发送给这个玩家并且这个玩家已收到的的world update, 这个world update是在这个玩家将要执行这个movement command之前的world update &nbsp; &nbsp; 3.From that update (and the one following it based on the exact target time being used), for each player in the update, move the other players backwards in time to &nbsp;exactly &nbsp;where they were when the current player's user command was created. This moving backwards must account for both connection latency andthe&nbsp;interpolation amount[8] the client was using that frame. &nbsp; &nbsp; 3. 对于每一个玩家，将其从上述的world update处拉回到这个玩家生成此user command的更新时间中执行用户命令。这个回退时间需要考虑到命令执行的时候的网络延时和插&#20540;量【8】 2.Allow the user command to execute (including any weapon firing commands, etc., that will run ray casts against all of the other players in their &quot;old&quot; positions). 2.执行玩家命令（包括武器开火等。） 3.Move all of the moved/time-warped players back to their correct/current positions 3.将所有移动的、错位的玩家移动到他们当前正确位置。 Note that in the step where we move the player backwards in time, this might actually require forcing additional state info backwards, too (for instance, whether the player was alive or dead or whether the player was ducking). The end result of lag compensation is that each local client is able to directly aim at other players without having to worry about leading his or her target in order to score a hit. Of course, this behavior is a game design tradeoff. 注意：我们把时间往后推算的时候，需要考虑那个时候玩家的状态，比如玩家是或者还是已经已经死掉，玩家是否处于躲避状态。执行运动补偿以后，玩家就可以直接瞄准目标进行设计，而不需要计算一个提前量。当然，这种方案是游戏中的权衡设计。 Game Design Implications of Lag Compensation 游戏涉及中延迟补偿的使用 The introduction of lag compensation allows for each player to run on his or her own clock with no apparent latency. In this respect, it is important to understand that certain paradoxes or inconsistencies can occur. Of course, the old system with the authoritative server and &quot;dumb&quot; or simple clients had it's own paradoxes. In the end, making this tradeoff is a game design decision. For Half-Life, we believe deciding in favor of lag compensation was a justified game design decision. 采用延迟补偿以后，每个玩家游戏的过程中感觉不到明显延迟。在这里需要理解可能会产生一些矛盾和不一致。当然，验证服务器和无逻辑的客户端老系统也会有自相矛盾的情况。最后，这个这种事游戏设计决定的。对于半条命，我们相信采用延迟补偿是正确的游戏决定。 The first problem of the old system was that you had to lead your target by some amount that was related to your latency to the server. Aiming directly at another player and pressing the fire button was almost assured to miss that player. The inconsistency here is that aiming is just not realistic and that the player controls have non-predictable responsiveness. 老系统的一个问题是，由于网络延迟，目标需要有一个提前量。瞄准敌人进行射击几乎总是不能击中。这种不一致导致射击很不真实，响应也不可控制。 With lag compensation, the inconsistencies are different. For most players, all they have to do is acquire some aiming skill and they can become proficient (you still have to be able to aim). Lag compensation allows the player to aim directly at his or her target and press the fire button (for instant hit weapons[9]). The inconsistencies that sometimes occur, however, are from the points of view of the players being fired upon. 采用延迟补偿以后带来的是另一种形式的不一致。对于大部分玩家，他们只需要专注于得到更多的射击技能来武装他们（当然他们也是需要瞄准的）。延时补偿使得玩家只需要直接瞄准他的目标并按下开火按钮即可（对于即时击中武器【9】）。不一致也时有发生，但是是在击中以后。 For instance, if a highly lagged player shoots at a less lagged player and scores a hit, it can appear to the less lagged player that the lagged player has somehow &quot;shot around a corner&quot;10. In this case, the lower lag player may have darted around a corner. But the lagged player is seeing everything in the past. To the lagged player, s/he has a direct line of sight to the other player. The player lines up the crosshairs and presses the fire button. In the meantime, the low lag player has run around a corner and maybe even crouched behind a crate. If the high lag player is sufficiently lagged, say 500 milliseconds or so, this scenario is quite possible. Then, when the lagged player's user command arrives at the server, the hiding player is transported backward in time and is hit. This is the extreme case, and in this case, the low ping player says that s/he was shot from around the corner. However, from the lagged player's point of view, they lined up their crosshairs on the other player and fired a direct hit. From a game design point of view, the decision for us was easy: let each individual player have completely responsive interaction with the world and his or her weapons. 例如，如果一个延时比较大的玩家击中一个延时比较小的玩家并且得到一分，低延时的玩家会感觉高延时玩家“在角落里被击中”【10】。这种情况下，低延迟玩家可能已经从角落里冲出，而高延时玩家看到的是过去的信息。每一个有延迟的玩家都有一个朝向别的玩家的直的视线，直的视线指向一个瞄准点然后开火。这个时候，低延时的玩家可能已经跑到角落里并且蹲在一个箱子后面，如果高延迟玩家延迟比较大，比如500ms，这是经常发生的；这样当高延时玩家的命令传到服务器的时候，已经隐藏起来的玩家需要取一个历史位置并计算是否击中，在这种极端情况下，低延时玩家会觉得他再角落里被击中了。然而，对于高延时玩家来说，他是正对着别的玩家开火的。从游戏设计的角度来讲，我们需要这样决定：让每个玩家即时与世界交互并开火。 In addition, the inconsistency described above is much less pronounced in normal combat situations. For first-person shooters, there are two more typical cases. First, consider two players running straight at each other pressing the fire button. In this case, it's quite likely that lag compensation will just move the other player backwards along the same line as his or her movement. The person being shot will be looking straight at his attacker and no &quot;bullets bending around corners&quot; feeling will be present. 此外，在正常战斗中，上面提到的不一致并不明显。对于第一人称射击游戏，有两种典型情况。第一、考虑两个玩家直线跑向对方并且开火；这种情况下，延时补偿只会把玩家在移动直线上往后拉。被击中的玩家看他的射击者在前方，这样就不会有“子弹拐到角落里”的情况发生。 The next example is two players, one aiming at the other while the other dashes in front perpendicular to the first player. In this case, the paradox is minimized for a wholly different reason. The player who is dashing across the line of sight of the shooter probably has (in first-person shooters at least) a field of view of 90 degrees or less. In essence, the runner can't see where the other player is aiming. Therefore, getting shot isn't going to be surprising or feel wrong (you get what you deserve for running around in the open like a maniac). Of course, if you have a tank game, or a game where the player can run one direction, and look another, then this scenario is less clear-cut, since you might see the other player aiming in a slightly incorrect direction. 第二种情况是两个玩家中的一个射击，另外一个玩家在垂直于第一个玩家视线的方向冲锋。这种情况下的解决问题的原理与刚才不同。刚才提到的冲锋的玩家视野差不多是90°（至少第一人称射击游戏是这样），因此，这个玩家看不到正在射击他的那个人。因此他被击中也不会感觉奇怪或者错误（谁让你在空旷区域狂奔呢，活该）。当然，如果你开发的是一个坦克游戏，或者在你的游戏中玩家朝一个方向跑的时候可以看到别的方向，错误可能就会比较明显，你可能发现玩家设计方向不对。 Conclusion 总结 Lag compensation is a tool to ameliorate the effects of latency on today's action games. The decision of whether to implement such a system rests with the game designer since the decision directly changes the feel of the game. For Half-Life, Team Fortress and Counter Strike, the benefits of lag compensation easily outweighed the inconsistencies noted above. 延迟补偿是当前动作游戏改善延迟影响的一种方法。是否采用这种方法取决于游戏设计者，因为如何设计直接影响到游戏的体验。对于把那条命、军团要塞、cs这样的游戏，延迟补偿所带来的效果提升显著大于其带来的错误。 Footnotes 脚注 [1]In the Half-Life engine, it is possible to ask the client-side prediction algorithm to account for some, but not all, of the latency in performing prediction. The user could control the amount of prediction by changing the value of the &quot;pushlatency&quot; console variable to the engine. This variable is a negative number indicating the maximum number of milliseconds of prediction to perform. If the number is greater (in the negative) than the user's current latency, then full prediction up to the current time occurs. In this case, the user feels zero latency in his or her movements. Based upon some erroneous superstition in the community, many users insisted that setting pushlatency to minus one-half of the current average latency was the proper setting. Of course, this would still leave the player's movements lagged (often described as if you are moving around on ice skates) by half of the user's latency. All of this confusion has brought us to the conclusion that full prediction should occur all of the time and that the pushlatency variable should be removed from the Half-Life engine.&nbsp; 【1】在半条命引擎中，预测的过程中允许一定的延迟，但不能容忍实际网络延迟这么大的延迟。通过调整参数，我们可以控制预测过程中的延迟，这个参数pushlatency是一个负数，以毫秒为单位表示预测过程中的延迟。如果这个&#20540;大于（绝对&#20540;）实际网络延迟，这时预测就是完全的预测（译注：客户端服务器完全同步）。这种情况下玩家感觉不到任何延迟。实际应用中，一些人错误地认为参数pushlatency应该设为实际网络延迟的一半，这种情况下玩家移动仍然有网络延迟一半的延迟（感觉类&#20284;于冰面移动）。基于这个原因，实际应用总应该总是采用完全预测，pushlatency这个变量应该从半条命引擎中移除 [2]http://www.quakeforge.net/files/q1source.zip (Return) [3]A discussion of cheating and what developers can do to deter it is beyond the scope of this paper. (Return) 【3】关于作弊和反作弊的问题超出了本篇文章讨论的范围 [4]Though hybrids and corrective methods are also possible. (Return) 【4】虽然混合纠正方法也可以使用 [5]&quot;Jerk&quot; is a measure of how fast accelerative forces are changing. (Return) 【5】“jerk”用来度量使玩家改变加速度的作用的快慢 [6]It is assumed in this paper that the client clock is directly synchronized to the server clock modulo the latency of the connection. In other words, the server sends the client, in each update, the value of the server's clock and the client adopts that value as its clock. Thus, the server and client clocks will always be matched, with the client running the same timing somewhat in the past (the amount in the past is equal to the client's current latency). Smoothing out discrepancies in the client clock can be solved in various ways. (Return) 【6】本文假设计算连接延时的时候客户端与服务器完全同步，也就是说，每次更新的时候客户端收到服务器发过来的时间被直接当做客户端的时间使用。这样，客户端跟服务器完全匹配，只是客户端稍微晚一点（晚多少取决于延时多少）。平滑客户端时钟差&#20540;可以有很多方法。 [7]The time spacing of these updates is not necessarily fixed. The reason why is that during high activity periods of the game (especially for users with lower bandwidth connections), it's quite possible that the game will want to send you more data than your connection can accommodate. If we were on a fixed update interval, then you might have to wait an entire additional interval before the next packet would be sent to the client. However, this doesn't match available bandwidth effectively. Instead, the server, after sending every packet to a player, determines when the next packet can be sent. This is a function of the user's bandwidth or &quot;rate&quot; setting and the number of updates requested per second. If the user asks for 20 updates per second, then it will be at least 50 milliseconds before the next update packet can be sent. If the bandwidth choke is active (and the server is sufficiently high framerate), it could be 61, etc., milliseconds before the next packet gets sent. Thus, Half-Life packets can be somewhat arbitrarily spaced. The simple move to latest goal interpolation schemes don't behave as well (think of the old anchor point for movement as being variable) under these conditions as the position history interpolation method (described below). (Return) 【7】更新时间间隔没必要是固定的。因为对于剧烈运动的游戏，如果带宽不够，很有可能客户端发过来的数据超过了处理能力。如果采用固定更新间隔，在发完一个更新包以后就需要等待一个固定更新周期时间以后再发下一个包。这种逻辑不能很好地使用带宽。因此，服务器发给每个客户端数据包以后，应该自己决定下一个包什么时候发，决定的依据是用户的带宽、用户设置的每秒更新频率。如果用户要求更新20次每秒，那么需要等待50ms以后下个更新包才能发送。如果激活了带宽限制（而服务器帧率又足够高），我们可能就需要等待比如61ms（或其他&#20540;）以后发送下一个更新包。因此，半条命游戏数据包发送间隔是随机的。基于服务器的这种情况，将启动点作为一个变量，移动到最新目标点进行插&#20540;这种方法效果欠佳。 [8]Which Half-Life encodes in the lerp_msec field of the usercmd_t structure described previously. (Return) 【8】半条命代码中usercmd_t结构中变量lerp_msec前面描述过。 [9]For weapons that fire projectiles, lag compensation is more problematic. For instance, if the projectile lives autonomously on the server, then what time space should the projectile live in? Does every other player need to be &quot;moved backward&quot; every time the projectile is ready to be simulated and moved by the server? If so, how far backward in time should the other players be moved? These are interesting questions to consider. In Half-Life, we avoided them; we simply don't lag compensate projectile objects (that's not to say that we don't predict the sound of you firing the projectile on the client, just that the actual projectile is not lag compensated in any way). (Return) 【9】对于发射导弹的武器，延迟补偿有更多需要解决的问题。假如导弹是由服务器处理的，那么导弹应该位于哪个时间区间？每次导弹准备发射的时候，是否需要把每个玩家往后拉一段时间的？如果是这样，那么需要往后拉多少？这些问题是需要考虑的。在半条命中，为了避免这种问题，我们对导弹不进行延迟补偿（这并不意味着客户端不进行声音预测，只是实际的导弹不进行延迟补偿）。 [10]This is the phrase our user community has adopted to describe this inconsistency. (Return) 【10】用户社区通常采用这种情况来描述不一致性。]]></content>
      <categories>
        <category>GS</category>
      </categories>
      <tags>
        <tag>Valve</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Source引擎多人游戏网络设计]]></title>
    <url>%2Fblog%2F2016%2F01%2F06%2FSource%E5%BC%95%E6%93%8E%E5%A4%9A%E4%BA%BA%E6%B8%B8%E6%88%8F%E7%BD%91%E7%BB%9C%E8%AE%BE%E8%AE%A1%2F</url>
    <content type="text"><![CDATA[原文原文出处 原文标题 : Source Multiplayer Networking Multiplayer games based on the Source Engine use a Client-Server networking architecture. Usually a server is a dedicated host that runs the game and is authoritative about world simulation, game rules, and player input processing. A client is a player's computer connected to a game server. The client and server communicate with each other by sending small data packets at a high frequency (usually 20 to 30 packets per second). A client receives the current world state from the server and generates video and audio output based on these updates. The client also samples data from input devices (keyboard, mouse, microphone, etc.) and sends these input samples back to the server for further processing. Clients only communicate with the game server and not between each other (like in a peer-to-peer application). In contrast with a single player game, a multiplayer game has to deal with a variety of new problems caused by packet-based communication. Network bandwidth is limited, so the server can't send a new update packet to all clients for every single world change. Instead, the server takes snapshots of the current world state at a constant rate and broadcasts these snapshots to the clients. Network packets take a certain amount of time to travel between the client and the server (i.e. half the ping time). This means that the client time is always a little bit behind the server time. Furthermore, client input packets are also delayed on their way back, so the server is processing temporally delayed user commands. In addition, each client has a different network delay which varies over time due to other background traffic and the client's framerate. These time differences between server and client causes logical problems, becoming worse with increasing network latencies. In fast-paced action games, even a delay of a few milliseconds can cause a laggy gameplay feeling and make it hard to hit other players or interact with moving objects. Besides bandwidth limitations and network latencies, information can get lost due to network packet loss. To cope with the issues introduced by network communication, the Source engine server employs techniques such as data compression and lag compensation which are invisible to the client. The client then performs prediction and interpolation to further improve the experience. Basic networking The server simulates the game in discrete time steps called ticks. By default, the timestep is 15ms, so 66.666... ticks per second are simulated, but mods can specify their own tickrate. During each tick, the server processes incoming user commands, runs a physical simulation step, checks the game rules, and updates all object states. After simulating a tick, the server decides if any client needs a world update and takes a snapshot of the current world state if necessary. A higher tickrate increases the simulation precision, but also requires more CPU power and available bandwidth on both server and client. The server admin may override the default tickrate with the -tickrate command line parameter, though tickrate changes done this way are not recommended because the mod may not work as designed if its tickrate is changed. Note:The -tickrate command line parameter is not available on CSS, DoD S, TF2, L4D and L4D2 because changing tickrate causes server timing issues. The tickrate is set to 66 in CSS, DoD S and TF2, and 30 in L4D and L4D2. Clients usually have only a limited amount of available bandwidth. In the worst case, players with a modem connection can't receive more than 5 to 7 KB/sec. If the server tried to send them updates with a higher data rate, packet loss would be unavoidable. Therefore, the client has to tell the server its incoming bandwidth capacity by setting the console variable rate (in bytes/second). This is the most important network variable for clients and it has to be set correctly for an optimal gameplay experience. The client can request a certain snapshot rate by changing cl_updaterate (default 20), but the server will never send more updates than simulated ticks or exceed the requested client rate limit. Server admins can limit data rate values requested by clients with sv_minrate and sv_maxrate (both in bytes/second). Also the snapshot rate can be restricted with sv_minupdaterate and sv_maxupdaterate (both in snapshots/second). The client creates user commands from sampling input devices with the same tick rate that the server is running with. A user command is basically a snapshot of the current keyboard and mouse state. But instead of sending a new packet to the server for each user command, the client sends command packets at a certain rate of packets per second (usually 30). This means two or more user commands are transmitted within the same packet. Clients can increase the command rate with cl_cmdrate. This will increase responsiveness but requires more outgoing bandwidth, too. Game data is compressed using delta compression to reduce network load. That means the server doesn't send a full world snapshot each time, but rather only changes (a delta snapshot) that happened since the last acknowledged update. With each packet sent between the client and server, acknowledge numbers are attached to keep track of their data flow. Usually full (non-delta) snapshots are only sent when a game starts or a client suffers from heavy packet loss for a couple of seconds. Clients can request a full snapshot manually with the cl_fullupdate command. Responsiveness, or the time between user input and its visible feedback in the game world, are determined by lots of factors, including the server/client CPU load, simulation tickrate, data rate and snapshot update settings, but mostly by the network packet traveling time. The time between the client sending a user command, the server responding to it, and the client receiving the server's response is called the latency or ping (or round trip time). Low latency is a significant advantage when playing a multiplayer online game. Techniques like prediction and lag compensation try to minimize that advantage and allow a fair game for players with slower connections. Tweaking networking setting can help to gain a better experience if the necessary bandwidth and CPU power is available. We recommend keeping the default settings, since improper changes may cause more negative side effects than actual benefits. Servers that Support Tickrate The tickrate can be altered by using the -tickrate parameter Counter Strike: Global Offensive Half-Life 2: Deathmatch The following servers tickrate cannot be altered as changing this causes server timing issues. Tickrate 66 Counter Strike: Source Day of Defeat: Source Team Fortress 2 Tickrate 30 Left 4 Dead Left 4 Dead 2 Entity interpolation By default, the client receives about 20 snapshot per second. If the objects (entities) in the world were only rendered at the positions received by the server, moving objects and animation would look choppy and jittery. Dropped packets would also cause noticeable glitches. The trick to solve this problem is to go back in time for rendering, so positions and animations can be continuously interpolated between two recently received snapshots. With 20 snapshots per second, a new update arrives about every 50 milliseconds. If the client render time is shifted back by 50 milliseconds, entities can be always interpolated between the last received snapshot and the snapshot before that. Source defaults to an interpolation period ('lerp') of 100-milliseconds (cl_interp 0.1); this way, even if one snapshot is lost, there are always two valid snapshots to interpolate between. Take a look at the following figure showing the arrival times of incoming world snapshots: The last snapshot received on the client was at tick 344 or 10.30 seconds. The client time continues to increase based on this snapshot and the client frame rate. If a new video frame is rendered, the rendering time is the current client time 10.32 minus the view interpolation delay of 0.1 seconds. This would be 10.22 in our example and all entities and their animations are interpolated using the correct fraction between snapshot 340 and 342. Since we have an interpolation delay of 100 milliseconds, the interpolation would even work if snapshot 342 were missing due to packet loss. Then the interpolation could use snapshots 340 and 344. If more than one snapshot in a row is dropped, interpolation can't work perfectly because it runs out of snapshots in the history buffer. In that case the renderer uses extrapolation (cl_extrapolate 1) and tries a simple linear extrapolation of entities based on their known history so far. The extrapolation is done only for 0.25 seconds of packet loss (cl_extrapolate_amount), since the prediction errors would become too big after that. Entity interpolation causes a constant view "lag" of 100 milliseconds by default (cl_interp 0.1), even if you're playing on a listenserver (server and client on the same machine). This doesn't mean you have to lead your aiming when shooting at other players since the server-side lag compensation knows about client entity interpolation and corrects this error. Tip:More recent Source games have the cl_interp_ratio cvar. With this you can easily and safely decrease the interpolation period by setting cl_interp to 0, then increasing the value of cl_updaterate (the useful limit of which depends on server tickrate). You can check your final lerp with net_graph 1. Note:If you turn on sv_showhitboxes (not available in Source 2009) you will see player hitboxes drawn in server time, meaning they are ahead of the rendered player model by the lerp period. This is perfectly normal! Input prediction Lets assume a player has a network latency of 150 milliseconds and starts to move forward. The information that the +FORWARD key is pressed is stored in a user command and send to the server. There the user command is processed by the movement code and the player's character is moved forward in the game world. This world state change is transmitted to all clients with the next snapshot update. So the player would see his own change of movement with a 150 milliseconds delay after he started walking. This delay applies to all players actions like movement, shooting weapons, etc. and becomes worse with higher latencies. A delay between player input and corresponding visual feedback creates a strange, unnatural feeling and makes it hard to move or aim precisely. Client-side input prediction (cl_predict 1) is a way to remove this delay and let the player's actions feel more instant. Instead of waiting for the server to update your own position, the local client just predicts the results of its own user commands. Therefore, the client runs exactly the same code and rules the server will use to process the user commands. After the prediction is finished, the local player will move instantly to the new location while the server still sees him at the old place. After 150 milliseconds, the client will receive the server snapshot that contains the changes based on the user command he predicted earlier. Then the client compares the server position with his predicted position. If they are different, a prediction error has occurred. This indicates that the client didn't have the correct information about other entities and the environment when it processed the user command. Then the client has to correct its own position, since the server has final authority over client-side prediction. If cl_showerror 1 is turned on, clients can see when prediction errors happen. Prediction error correction can be quite noticeable and may cause the client's view to jump erratically. By gradually correcting this error over a short amount of time (cl_smoothtime), errors can be smoothly corrected. Prediction error smoothing can be turned off with cl_smooth 0. Prediction is only possible for the local player and entities affected only by him, since prediction works by using the client's keypresses to make a "best guess" of where the player will end up. Predicting other players would require literally predicting the future with no data, since there's no way to instantaneously get keypresses from them. Lag compensation All source code for lag compensation and view interpolation is available in the Source SDK. See for implementation details. Let's say a player shoots at a target at client time 10.5. The firing information is packed into a user command and sent to the server. While the packet is on its way through the network, the server continues to simulate the world, and the target might have moved to a different position. The user command arrives at server time 10.6 and the server wouldn't detect the hit, even though the player has aimed exactly at the target. This error is corrected by the server-side lag compensation. The lag compensation system keeps a history of all recent player positions for one second. If a user command is executed, the server estimates at what time the command was created as follows: **Command Execution Time** = **Current Server Time** - **Packet Latency** - **Client View** &lt;a href=&quot;/wiki/Interpolation&quot; title=&quot;Interpolation&quot;&gt;**Interpolation**&lt;/a&gt;Then the server moves all other players - only players - back to where they were at the command execution time. The user command is executed and the hit is detected correctly. After the user command has been processed, the players revert to their original positions. Note:Since entity interpolation is included in the equation, failing to have it on can cause undesired results. On a listen server you can enable sv_showimpacts 1 to see the different server and client hitboxes: {% asset_img source_multiplayer_networking_img_3.jpg %} This screenshot was taken on a listen server with 200 milliseconds of lag (using net_fakelag), right after the server confirmed the hit. The red hitbox shows the target position on the client where it was 100ms + interp period ago. Since then, the target continued to move to the left while the user command was travelling to the server. After the user command arrived, the server restored the target position (blue hitbox) based on the estimated command execution time. The server traces the shot and confirms the hit (the client sees blood effects). Client and server hitboxes don't exactly match because of small precision errors in time measurement. Even a small difference of a few milliseconds can cause an error of several inches for fast-moving objects. Multiplayer hit detection is not pixel perfect and has known precision limitations based on the tickrate and the speed of moving objects. The question arises, why is hit detection so complicated on the server? Doing the back tracking of player positions and dealing with precision errors while hit detection could be done client-side way easier and with pixel precision. The client would just tell the server with a "hit" message what player has been hit and where. We can't allow that simply because a game server can't trust the clients on such important decisions. Even if the client is "clean" and protected by Valve Anti-Cheat, the packets could be still modified on a 3rd machine while routed to the game server. These "cheat proxies" could inject "hit" messages into the network packet without being detected by VAC (a "man-in-the-middle" attack). Network latencies and lag compensation can create paradoxes that seem illogical compared to the real world. For example, you can be hit by an attacker you can't even see anymore because you already took cover. What happened is that the server moved your player hitboxes back in time, where you were still exposed to your attacker. This inconsistency problem can't be solved in general because of the relatively slow packet speeds. In the real world, you don't notice this problem because light (the packets) travels so fast and you and everybody around you sees the same world as it is right now. Net graph The Source engine offers a couple of tools to check your client connection speed and quality. The most popular one is the net graph, which can be enabled with net_graph 2 (or +graph). Incoming packets are represented by small lines moving from right to left. The height of each line reflects size of a packet. If a gap appears between lines, a packet was lost or arrived out of order. The lines are color-coded depending on what kind of data they contain. Under the net graph, the first line shows your current rendered frames per second, your average latency, and the current value of cl_updaterate. The second line shows the size in bytes of the last incoming packet (snapshots), the average incoming bandwidth, and received packets per second. The third line shows the same data just for outgoing packets (user commands). {% asset_img source_multiplayer_networking_img_4.jpg %} Optimizations The default networking settings are designed for playing on dedicated server on the Internet. The settings are balanced to work well for most client/server hardware and network configurations. For Internet games the only console variable that should be adjusted on the client is "rate", which defines your available bytes/second bandwidth of your network connection. Good values for "rate" is 4500 for modems, 6000 for ISDN, 10000 DSL and above. In an high-performance network environment, where the server and all clients have the necessary hardware resources available, it's possible to tweak bandwidth and tickrate settings to gain more gameplay precision. Increasing the server tickrate generally improves movement and shooting precision but comes with a higher CPU cost. A Source server running with tickrate 100 generates about 1.5x more CPU load than a default tickrate 66. That can cause serious calculation lags, especially when lots of people are shooting at the same time. It's not suggested to run a game server with a higher tickrate than 66 to reserve necessary CPU resources for critical situations. Note:It is not possible to change tickrate on CSS, DoD S TF2, L4D and L4D2 because changing tickrate causes server timing issues. The tickrate is set to 66 in CSS, DoD S and TF2, and 30 in L4D and L4D2. If the game server is running with a higher tickrate, clients can increase their snapshot update rate (cl_updaterate) and user command rate (cl_cmdrate), if the necessary bandwidth (rate) is available. The snapshot update rate is limited by the server tickrate, a server can't send more then one update per tick. So for a tickrate 66 server, the highest client value for cl_updaterate would be 66. If you increase the snapshot rate and encounter packet loss or choke, you have to turn it down again. With an increased cl_updaterate you can also lower the view interpolation delay (cl_interp). The default interpolation delay is 0.1 seconds, which derives from the default cl_updaterate 20. View interpolation delay gives a moving player a small advantage over a stationary player since the moving player can see his target a split second earlier. This effect is unavoidable, but it can be reduced by decreasing the view interpolation delay. If both players are moving, the view lag delay is affecting both players and nobody has an advantage. This is the relation between snapshot rate and view interpolation delay is the following: interpolation period = max( cl_interp, cl_interp_ratio / cl_updaterate ) "Max(x,y)" means "whichever of these is higher". You can set cl_interp to 0 and still have a safe amount of interp. You can then increase cl_updaterate to decrease your interp period further, but don't exceed tickrate (66) or flood your connection with more data than it can handle. Tips Don't change console settings unless you are 100% sure what you are doing `Most "high-performance" setting cause exactly the opposite effect, if the server or network can't handle the load.` &lt;dt&gt;Don&apos;t turn off view interpolation and/or lag compensation&lt;/dt&gt; `It will not improve movement or shooting precision.` &lt;dt&gt;Optimized setting for one client may not work for other clients&lt;/dt&gt; `Do not just use settings from other clients without verifing them for your system.` &lt;dt&gt;If you follow a player in &quot;First-Person&quot; as a spectator in a game or &lt;a href=&quot;/wiki/SourceTV&quot; title=&quot;SourceTV&quot;&gt;SourceTV&lt;/a&gt;, you don&apos;t exactly see what the player sees&lt;/dt&gt; `Spectators see the game world without lag compensation.` 译文 译文出处 翻译：pluswu，审校：pluswuSource引擎的多人游戏使用基于UDP通信的C/S架构。游戏以服务器逻辑作为世界权威，客户端和服务器通过UDP协议(20~30packet/s）通信。客户端从服务器接收信息并基于当前世界状态渲染画面和输出音频。客户端以固定频率发送操作输入到服务器。客户端仅与游戏服务器，而不是彼此之间通信。多人游戏必须处理基于网络消息同步所带来的一系列问题。网络的带宽是有限的，所以服务器不能为每一个世界的变化发送新的更新数据包发送到所有客户端。相反，服务器以固定的频率取当前世界状态的快照并广播这些快照到客户端。网络数据包需要一定的时间量的客户端和服务器（RTT的一半）来往。这意味着客户端时间相对服务器时间总是稍有滞后。此外，客户端输入数据包同步到服务器也有一定网络传输时间，所以服务器处理客户端输入也存在延迟的。不同的客户端因为网络带宽和通信线路不同也会存在不同的网络延时。随着服务器和客户端之间的这些网络延迟增大, 网络延迟可能会导致逻辑问题。比如在快节奏的动作游戏中，在几毫秒的延迟甚至就会导致游戏卡顿的感觉，玩家会觉得很难打到对方玩家或运动的物体。此外除了带宽限制和网络延迟还要考虑网络传输中会有消息丢失的情况。为了解决网络通信引入的一系列问题，Source引擎在服务器同步时采用了数据压缩和延迟补偿的逻辑，客户端采用了预测运行和插值平滑处理等技术来获得更好的游戏体验。基本网络模型服务器以一个固定的时间间隔更新模拟游戏世界。默认情况下，时间步长为15ms，以66.66次每秒的频率更新模拟游戏世界，但不同游戏可以指定更新频率。在每个更新周期内服务器处理传入的用户命令，运行物理模拟步，检查游戏规则，并更新所有的对象状态。每一次模拟更新tick之后服务器会决定是否更新当前时间快照以及每个客户端当前是否需更新。较高的tickrate增加了模拟精度，需要服务器和客户端都有更多可用的CPU和带宽资源。客户通常只能提供有限的带宽。在最坏的情况下，玩家的调制解调器连接不能获得超过5-7KB /秒的流量。如果服务器的数据更新发送频率超过了客户端的带宽处理限制，丢包是不可避免的。因此客户端可以通过在控制台设置接受带宽限制，以告诉服务器其收到的带宽容量。这是客户最重要的网络参数，想要获得最佳的游戏体验的话必须正确的设置此参数。客户端可以通过设置cl_updaterate（默认20）来改变获得快照平的频率，但服务器永远不会发送比tickerate更多的更新或超过请求的客户端带宽限制。服务器管理员可以通过sv_minrate和sv_maxrate(byte/s)限制客户端的上行请求频率。当然快照更新同步频率都受到sv_minupdaterate和sv_maxupdaterate（快照/秒）的限制。客户端使用与服务端tickrate一样的频率采样操作输入创建用户命令。用户命令基本上是当前的键盘和鼠标状态的快照。客户端不会把每个用户命令都立即发送到服务器而是以每秒（通常是30）的速率发送命令包。这意味着两个或更多个用户的命令在同一包内传输。客户可以增加与的cl_cmdrate命令速率。这可以提高响应速度，但需要更多的出口带宽。游戏数据使用增量更新压缩来减少网络传输。服务器不会每次都发送一个完整的世界快照，而只会更新自上次确认更新(通过ACK确认)之后所发生的变化（增量快照)。客户端和服务器之间发送的每个包都会带有ACK序列号来跟踪网络数据流。当游戏开始时或客户端在发生非常严重的数据包丢失时, 客户可以要求全额快照同步。用户操作的响应速度(操作到游戏世界中的可视反馈之间的时间)是由很多因素决定的，包括服务器/客户端的CPU负载，更新频率，网络速率和快照更新设置，但主要是由网络包的传输时间确定。从客户端发送命令到服务器响应, 再到客户端接收此命令对应的服务器响应被称为延迟或ping（或RTT）。低延迟在玩多人在线游戏时有显著的优势。客户端本地预测和服务器的延迟补偿技术可以尽量为网络较差的游戏玩家提供相对公平的体验。如果有良好的带宽和CPU可用，可以通过调整网络设置以获得更好的体验, 反之我们建议保持默认设置，因为不正确的更改可能导致负面影响大于实际效益。Enitiy插值平滑通常情况下客户端接收每秒约20个快照更新。如果世界中的对象（实体）直接由服务器同步的位置呈现，物体移动和动画会看起来很诡异。网络通信的丢包也将导致明显的毛刺。解决这个问题的关键是要延迟渲染，玩家位置和动画可以在两个最近收到快照之间的连续插值。以每秒20快照为例，一个新的快照更新到达时大约每50毫秒。如果客户端渲染延迟50毫秒，客户端收到一个快照，并在此之前的快照之间内插(Source默认为100毫秒的插补周期)；这样一来，即使一个快照丢失，总是可以在两个有效快照之间进行平滑插值。如下图显示传入世界快照的到达时间： 在客户端接收到的最后一个快照是在tick 344或10.30秒。客户的时间将继续在此快照的基础上基于客户端的帧率增加。下一个视图帧渲染时间是当前客户端的时间10.32减去0.1秒的画面插值延迟10.20。在我们的例子下一个渲染帧的时间是10.22和所有实体及其动画都可以基于快照340和342做正确的插值处理。既然我们有一个100毫秒的延迟插值，如果快照342由于丢包缺失，插值可以使用快照340和344来进行平滑处理。如果连续多个快照丢失，插值处理可能表现不会很好，因为插值是基于缓冲区的历史快照进行的。在这种情况下，渲染器会使用外推法（cl_extrapolate 1），并尝试基于其已知的历史，为实体做一个基于目前为止的一个简单线性外推。外推只会快照更新包连续丢失（cl_extrapolate_amount）0.25秒才会触发，因为该预测之后误差将变得太大。实体内会插导致100毫秒默认（cl_interp 0.1）的恒定视图“滞后”，就算你在listenserver（服务器和客户端在同一台机器上）上玩游戏。这并不是说你必须提前预判动画去瞄准射击，因为服务器端的滞后补偿知道客户端实体插值并纠正这个误差。最近Source引擎的游戏有cl_interp_ratioCVaR的。有了这个，你可以轻松，安全地通过设置cl_interp为0，那么增加的cl_updaterate的值（这同时也会受限于服务器tickrate）来减少插补周期。你可以用net_graph 1检查您的最终线性插值。如果打开sv_showhitboxes，你会看到在服务器时间绘制的玩家包围盒，这意味着他们在前进的线性插值时期所呈现的播放器模式。输入预测让我们假设一个玩家有150毫秒的网络延迟，并开始前进。前进键被按下的信息被存储在用户命令，并发送至服务器。用户命令是由移动代码逻辑处理，玩家的角色将在游戏世界中向前行走。这个世界状态的变化传送到所有客户端的下一个快照的更新。因此玩家看到自己开始行动的响应会有150毫秒延迟，这种延迟对于高频动作游戏(体育，设计类游戏)会有明显的延迟感。玩家输入和相应的视觉反馈之间的延迟会产生一种奇怪的，不自然的感觉，使得玩家很难移动或精确瞄准。客户端的输入预测（cl_predict 1）执行是一种消除这种延迟的方法，让玩家的行动感到更即时。与其等待服务器来更新自己的位置，在本地客户端只是预测自己的用户命令的结果。因此，客户端准确运行相同的代码和规则服务器将使用来处理用户命令。预测完成后，当地的玩家会移动到新位置，而服务器仍然可以看到他在老地方。150毫秒后，客户会收到包含基于他早期预测用户命令更改服务器的快照。客户端会将预测位置同服务器的位置对比。如果它们是不同的，则发生了预测误差。这表明，在客户端没有关于其他实体的正确信息和环境时，它处理用户命令。然后，客户端必须纠正自己的位置，因为服务器拥有客户端预测最终决定权。如果cl_showerror 1开启，客户端可以看到，当预测误差发生。预测误差校正可以是相当明显的，并且可能导致客户端的视图不规则跳动。通过在一定时间（cl_smoothtime）逐渐纠正这个错误，错误可以顺利解决。预测误差平滑处理可以通过设置cl_smooth 0来关闭。预测只对本地玩家以及那些只收它影响的实体有效，因为预测的工作原理是使用客户端的操作来预测的。对于其他玩家没法做有效预测, 因为没有办法立即从他们身上得到操作信息。延迟补偿 比方说，一个玩家在10.5s的时刻射击了一个目标。射击信息被打包到用户命令，该命令通过网络的方式发送至服务器。服务器持续模拟游戏世界，目标可能已经移动到一个不同的位置。用户命令到达服务器时间10.6时服务器就无法检测到射击命中，即使玩家已经在目标准确瞄准。这个错误需要由服务器侧进行延迟补偿校正。延迟补偿系统使所有玩家最近位置的历史一秒。如果在执行用户的命令，服务器预计在命令创建什么时间如下：命令执行时间=当前服务器时间 - 数据包延迟 - 客户端查看插值然后服务器会将所有其他玩家回溯到命令执行时的位置，他们在命令执行时间。用户指令被执行，并正确地检测命中。用户命令处理完成后，玩家将会恢复到原来的位置。由于实体插值包含在公式中，可能会导致意外的结果。服务器端可以启用sv_showimpacts 1，显示服务器和客户端射击包围盒位置差异：该画面在主机上设置延迟200毫秒(net_fakelag设置)时获取的，射击真实命中玩家。红色命中包围盒显示了客户端那里是100毫秒+插补周期前的目标位置。此后，目标继续向左移动，而用户命令被行进到服务器。用户命令到达后，服务器恢复基于所述估计的命令执行时间目标位置（蓝色击中盒）。服务器回溯演绎，并确认命中（客户端看到流血效果）。因为在时间测量精度的误差客户端和服务器命中包围盒不完全匹配。对于快速移动的物体甚至几毫秒的误差也会导致几英寸的误差。多人游戏击中检测不是基于像素的完美匹配，此外基于tickrate模拟的运动物体的速度也有精度的限制。既然击中检测服务器上的逻辑如此复杂为什么不把命中检查放在客户端呢？如果在客户端进行命中检查, 玩家位置和像素命中处理检测都可以精准的进行。客户端将只告诉服务器用“打”的消息一直打到什么样的玩家。因为游戏服务器不能信任客户端这种重要决定。因为即使客户端是“干净”的，并通过了Valve反作弊保护，但是报文可以被截获修改然后发送到游戏服务器。这些“作弊代理”可以注入“打”的消息到网络数据包而不被VAC被检测。网络延迟和滞后补偿可能会引起真实的世界不可能的逻辑。例如，您可能被你看不到的目标所击中。服务器移到你的命中包围盒时光倒流，你仍然暴露给了攻击者。这种不一致问题不能通过一般化的防范解决，因为相对网络包传输的速度。在现实世界中，因为光传播如此之快，你，每个人都在你身边看到同一个世界，所以你才你没有注意到这个问题。网络视图Source引擎提供了一些工具来检查您的客户端连接速度和质量。使用net_graph 2可以启用相关的视图。下面的曲线图中，第一行显示每秒当前的渲染的帧，您的平均延迟时间，以及的cl_updaterate的当前值。第二行显示在最后进来的数据包（快照），平均传入带宽和每秒接收的数据包的字节大小。第三行显示刚刚传出的数据包（用户命令）相同的数据。优化默认的网络设置是专门为通过互联网连接的游戏服务器设计的。可以适用大多数客户机/服务器的硬件和网络配置工作。对于网络游戏，应该在客户端上进行调整，唯一的控制台变量是“rate”，它定义客户端可用的字节/网络连接带宽。 在一个良好的网络环境中，服务器和所有客户端都具有必要的硬件资源可用，可以调整带宽和更新频率设置，来获得更多的游戏精度。增加tickrate通常可以提高运动和射击精度，但会消耗更多的服务器CPU资源。tickrate 100运行的服务器的负载大概是tickrate 66运行时的约1.5倍, 因此如果CPU性能不足可能会导致严重的计算滞后，尤其是在玩家数量比较多的时候。建议对具有更高tickrate超的游戏服务器预留必要的CPU资源。 如果游戏服务器使用较高tickrate运行时，客户端可以在带宽可用的情况下增加他们的快照更新率（的cl_updaterate）和用户命令速率（的cl_cmdrate）。快照更新速率由服务器tickrate限制，一台服务器无法发送每个时钟周期的一个以上的更新。因此，对于一个tickrate66服务器，为的cl_updaterate最高的客户价值，将是66。如果你增加快照率遇到，你必须再次打开它。与增加的cl_updaterate你也可以降低画面插值延迟（cl_interp）。默认的插值延迟为0.1秒(默认的cl_updaterate为20) 视图内插延迟会导致移动的玩家会比静止不动的玩家更早发现对方。这种效果是不可避免的，但可以通过减小视图内插值延迟来减小。如果双方玩家正在移动，画面滞后会延迟影响双方玩家,双方玩家都不能获利。快照速率和视图延迟插值之间的关系如下：插补周期= MAX(cl_interp，cl_interp_ratio /cl_updaterate)可以设置cl_interp为0，仍然有插值的安全量。也可以把cl_updaterate增加，进一步降低你的插补周期，但不会超过更新tickrate(66)或客户端的网络处理能力。小贴士不要瞎改终端配置除非你完全确定你在干嘛 如果客户端和服务器没有足够CPU和网络资源，绝大多数所所谓高性能优化都是起负面作用不要关闭画面插值和延迟补偿 这样并不能代理移动和设计精准度提升优化设置可能不会对每个客户端都有效如果是你是在游戏里或者SourceTv里第一视角观看你看到的画面和玩家可能不一样观战者的画面没有延迟补偿【版权声明】原文作者未做权利声明，视为共享知识产权进入公共领域，自动获得授权；]]></content>
      <categories>
        <category>GS</category>
      </categories>
      <tags>
        <tag>Valve</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++很基础的易混淆点二]]></title>
    <url>%2Fblog%2F2015%2F12%2F09%2Fcplusplus_confused_points_two%2F</url>
    <content type="text"><![CDATA[位操作运算符的问题二进制的100 的第0位是 01(第2位) 0(第1位) 0(第0位)，所以把一个数的第三位进行清零、置位(把某个bit置为1)、取反的操作如下： 123456789101112131415161718192021#include &lt;stdio.h&gt;#define Bit3 (0X01&lt;&lt;3)/*对一个数的第三位进行清零、置位、取反*/int main()&#123; int a=15 ; // 0000 1111 printf("原大小：%d\n", a); a &amp;= ~Bit3; //清零, 0000 0111 printf("清零后：%d\n", a); a |= Bit3; //置位, 0000 1111 printf("置位后：%d\n", a); a ^= Bit3; //取反, 0000 0111 printf("取反后：%d\n", a); return 0;&#125; . . . 字符串分配的位置问题程序的存储区域分为：代码段、只读数据段、已初始化的读写数据段、未初始化的数据段、堆、栈。1、代码段、只读数据段、已初始化的读写数据段、未初始化的数据段都属于静态区域。2、堆内存只在程序运行时出现，一般有程序员分配和释放。3、栈内存只在程序运行时出现，在函数内部使用的变量，函数参数以及返回值将使用栈空间。 1234567891011121314151617char* get_str()&#123; char *str = "hello"; //第一种情况：分配在静态存储区上 //char str[] = "hello"; //第二种情况分配在栈上 return str;&#125;int main()&#123; char* p = get_str(); // 如果是第一种情况，下述打印可以打印出正确的值；但是第二种情况打印结果是错的。 printf("%s/n", p); *++p = 'a'; // 如果是第一种情况，运行时将会段错误，因为不能修改它； printf("%s/n", p); return 0;&#125; 复杂类型的声明和typedef定义 用变量a给出下面的定义:一个有10个指针的数组，该指针指向一个函数，该函数有一个整型参数并返回一个整型数 int (*a[10])(int); typedef表示一个长度为4的int数组;typedef int ARR[4]; typedef表示一个函数指针有一个整型参数并返回一个整型数：typedef int（*FUNC）（int）；]]></content>
      <categories>
        <category>CPP</category>
      </categories>
      <tags>
        <tag>CPP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++可变参数]]></title>
    <url>%2Fblog%2F2015%2F12%2F09%2Fcpp_vargs%2F</url>
    <content type="text"><![CDATA[可变参数的宏一般在调试打印Debug信息的时候, 需要可变参数的宏. 从C99开始可以使编译器标准支持可变参数宏(variadic macros), 另外GCC也支持可变参数宏, 但是两种在细节上可能存在区别. __VA_ARGS__ 将 “…” 传递给宏 . 如 #define debug(format, ...) fprintf(stderr, format, __VA_ARGS__) GCC使用一种不同的语法,从而可以给可变参数一个名字,如同其它参数一样. #define debug(format, args...) fprintf (stderr, format, args) 这和第一条的宏例子是完全一样的,但是这么写可读性更强并且更容易进行描述. 上面两个定义的宏,如果出现 debug(“A Message”)的时候,由于宏展开后有个多余的逗号,所以在GCC中将导致编译错误, 而VS则不会. 所以移植性更好的写法是使用一个特殊的”##”操作,格式如下: #define debug(format, ...) fprintf (stderr, format, ## __VA_ARGS__) 这里,如果可变参数被忽略或为空,”##”操作将使预处理器(preprocessor)去除掉它前面的那个逗号. 什么是可变形参函数 在 c++ 编程中，有时我们需要编写一些在源代码编写阶段无法确定参数个数，有时甚至无法确定参数类型的函数。 例如，一个求和函数。可以通过重载实现若干个数的和。 1234567int sum(int i1, int i2);int sum(int i1, int i2, int i3);...//还可以重载更多类似函数double sum(double d1, double d2);double sum(double d1, double d2, double d3);...//还可以重载更多类似函数 以上代码通过重载机制来解决变参问题。但很快我们就会发现这种方法存在的问题：必须确保所有可能的实参列表都有对应的重载声明和定义，如果上述方法如果参与运算的参数个数可能从 2——20 个不等，那么我们就需要重载 19 次同一个函数。 我们需要的是这样一类函数：它们可以在运行时取任意的实参个数并根据实参的个数自动处理不同实参的情形，或者至少可以在运行时指定任意的实参个数。 实现变参函数的三种方法 在 C++ 中实现一个变参函数的方法有三种：第一种方法，将函数形参声明为 C++11 新标准中的 initializer_list 标准库类型；第二种方法继承自 C 语言，形参声明为省略符，函数实现时用参数列表宏访问参数；最后一种方法利用 C++ 泛型特性，声明一个可变参数模板来实现。 1. 可变参数宏 实现步骤如下： １. 函数原型中使用省略号； ２. 函数定义中创建一个 va_list 变量； 3. 初始化 va_list 变量； 4. 访问参数列表； 5. 完成清理工作； 上述步骤的实现需要使用到四个宏：va_list、va_start(va_list, arg)、va_arg(va_list, type)、va_end(va_list) 这些宏在头文件 stdarg.h 中声明定义。因此使用时需要包含该头文件。 以下代码使用可变参数宏实现一个函数 sum，该函数接受任意个数的整形实参，返回这些实参的和。（忽略可能存在的整形溢出） 12345678910111213141516/* --sum.cpp-- 可变参数宏实现求任意个整形值得和 */#include &lt;stdarg.h&gt;int sum(int count, ...); //原型中使用省略号int sum(int count, ...)&#123; //count 表示可变参数个数 va_list ap; //声明一个va_list变量 va_start(ap, count); //初始化，第二个参数为最后一个确定的形参 int sum = 0; for(int i = 0; i &lt; count; i++) sum += va_arg(ap, int); //读取可变参数，的二个参数为可变参数的类型 va_end(ap); //清理工作 return sum;&#125; 使用这种方法需要注意一下几点： 1. 函数原型中，省略号必须在参数列表的末尾：也就是说，在函数原型中参数列表省略号的右边不能再出现确定参数； 2. 运行时，函数必须能够根据已有信息（既有约定，或确定实参）确定可变参数的具体个数与类型：函数定义需要知道可变参数的具体类型、个数，这些信息是在运行时确定的，那么显然应该由实参来确定。在上面的例子中 count 传递了可变参数的个数，而参数类型则是既有约定（整形）； 3. 使用完成时需要用 va_end() 做清理工作，可变参数宏可能使用了动态分配的内存，忘记执行清理操作有可能导致内存泄漏等问题； 4. 可变参数宏只能实现顺序访问可变参数，无法后退访问，但是可以在清理操作完成后重新使用 va_start 初始化 va_list 变量，重新遍历形参表； 5. 该方法是极不安全的，宏本身无法提供任何安全性保证，他总是按照既定代码 “自作多情” 的认为实参就应该是那么多，即使实参并不是那么多。这就要求所有安全性必须由程序员来保证。例如，在以上的示例代码中，如果调用时指定 count 为 10，但实际上只给出 9 个可变形参，那么函数还是会读取 10 个参数，显然第十次读取是多余的，多余的操作一般不会有什么好结果，当然如果实参过多，多余的实参也不会被读取而是被忽略。 使用这种方法的一个实例是 printf() 函数。printf() 函数通过分析第一个字符串参数中的占位符个数来确定形参的个数；通过占位符的不同来确定参数类型（%d 表示 int 类型、%s 表示 char *）；它也有上述提到的安全问题，如果不小心少提供了个实参，那么越界访问就会发生。 2. initializer_list 标准库类型 实现步骤如下： １. 函数原型中使用实例化 initializer_list 模板代表可变参数列表； ２. 使用迭代器访问 initializer_list 中的参数； 3. 传入实参写在 {} 之内。 以上步骤中使用到 initializer_list。这是 C++11 新标准中引入的一个标准库类型，与 vector 等容器一样 initializer_list 也支持 begin() 和 end() 操作，返回指向首元素的迭代器和尾后迭代器。initializer_list 在同名头文件中声明，其实现由编译器支持。 以下代码使用 initializer_list 实现函数 sum。（忽略可能存在的整形溢出） 1234567891011/* --sum.cpp-- 利用initializer_list模板实现求人一个整形值得和 */#include &lt;initializer_list&gt;int sum(initializer_list&lt;int&gt; il); //函数原型用int实例化initializer_list作为形参int sum(inttializer_list&lt;int&gt; il)&#123; int sum = 0; for(auto p = il.begin(); p != il.end(); p++) //使用迭代器访问参数 sum += *p; return sum;&#125; 使用这种方法需要注意一下几点： １. initializer_list 在 C++11 中才被引入，这意味着在编译时可能需要加上这个选项 -std=c++11 才能成功编译。上述代码中的 auto 关键字也是 C++11 的一部分； 2. 参数必须放在一组‘{}’（大括号）内，编译器通过大括号来将这组参数转化为 initializer_list. 大括号的的一组实参与 initializer_list 形参对应； 3. 函数原型 initializer_list 与普通形参无异。这表明形参列表中可以包含其他类型参数且位置不限，以下函数原型是正确的： 1void func(char c, initializer_list&lt;int&gt; il, double d); ４. 同一个 initializer_list 中的参数具有相同的类型。本质上来说 initializer_list 是一个编译器支持的容器类模板，同其他容器一样，容器中的元素具有相同的类型。 使用这种方法的一个实例是 C++11 中 vector 的列表初始化构造函数。 3. 可变参数模板 在介绍这种方法之前需要先介绍两个并不常用的概念：模板参数包和函数参数包。 模板参数包是零个或多个类型参数的集合。模板参数列表中，class… 或 typename… 表明其后的类型参数表示一个模板参数包； 函数参数包是零个或多个非类型参数的集合。函数形参列表中类型名加省略号表明其后的参数表示一个函数参数包；另外，类型为模板参数包的函数形参是一个函数参数包。 以下引用参考书目 2 中的示例代码来直观展现这两个概念： 12345//args是一个模板参数包；rest是一个函数参数包//args表示零个或多个模板类型参数//rest表示零个或多个函数参数template&lt;typename T, typename... args&gt;void foo(const T &amp;t, const args&amp;... rest); 与 sizeof() 运算符类似，sizeof…() 运算符用于参数包。sizeof…() 将返回参数包中参数个数。 利用可变参数模板实现可变参数函数的步骤如下： １. 编写含有模板参数包和函数参数包的模板函数； 2. 函数定义递归调用自己，每一步递归参数包中参数减一； 3. 编写处理边界情况（参数包含有零个参数）的模板。 以下引用参考书目２中示例代码： 123456789101112//用来终止递归并答应最后一个元素的函数//此函数必须在可变参数版本的print定义之前声明template &lt;typename T&gt;std::ostream &amp;print(std::ostream &amp;os, const T &amp;t)&#123; return os &lt;&lt; t; //包中最后一个元素&#125;//包中除最后一个元素之外的其他元素都会调用这个版本的pirnttemplate &lt;typename T, typename... Args&gt;std::ostream &amp;print(std::ostream &amp;os, const T &amp;t, cosnt Args &amp;... rest)&#123; os &lt;&lt; t &lt;&lt; &quot;,&quot;; //打印第一个实参，包中元素减一 return print(os, rest...); //递归调用，打印剩余实参&#125; 使用这种方法需要注意的是： 1. 必须处理边界情况。且如代码注释所示：应当首先定义处理边界情况的模板。 2. 参数包在参数列表最右侧，参数包只能从左至右展开？ 3. 参数包能够实现更加复杂的模板，更多内容参考 C++ Primer(第五版) 第 16 章相关内容。 这种实现方式的根本原理实际上与最初提到的重载是一致的。通过定义模板，让编译器根据实参类型自动生成对应的重载函数。 三种实现方法的比较 以上提到的三种方法都可以实现变参函数。但三种方法都有其各自的有点和局限性，在选择时可以从以下几个方面考虑： 1. 若非必要，不要使用可变参数函数。应该首先考虑函数重载等其他方法。 2. 除非需要兼容 C 语言编译器，否则不要使用可变参数宏。应为这种方法最不安全；尤其是当参数为对象时这种方法易产生各种问题。毕竟这些宏是为 C 语言设计的，C 语言中没有对象。 3. 如果参数类型相同且 C++11 可用，则通过声明形参为 initializer_list 往往是最简单、最有效的办法。 4. 变参模板看似最为强大。参数的类型可以不同、比可变参数宏更加安全并且可以自动推断参数类型和参数个数。但考虑到模板会为每一个不同的实例生成代码，如果函数的实例过多可能会使代码体积增大。另外，依靠递归使得功能具有局限性，并且效率也会受到影响。]]></content>
      <categories>
        <category>CPP</category>
      </categories>
      <tags>
        <tag>CPP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++与Lua本质原始交互API]]></title>
    <url>%2Fblog%2F2015%2F11%2F11%2Flua_cpp_bind%2F</url>
    <content type="text"><![CDATA[我们用一个例子来说明. . . . 创建c++主程序首先, 我们需要创建一个 C++ 的主程序，以便同 Lua 进行通信. 如下 : lua_test.cpp123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475extern "C" &#123; #include "lua.h" #include "lualib.h" #include "lauxlib.h"&#125;; // 注意 : 这个extern "C" &#123;&#125; 非常重要, 不然会找不到相关库函数#include &lt;iostream&gt;#include &lt;lua.hpp&gt;extern "C" &#123; static int l_cppfunction(lua_State *L) &#123; double arg = luaL_checknumber(L,1); lua_pushnumber(L, arg * 0.5); return 1; &#125;&#125;using namespace std;int main(int argc, const char * argv[])&#123; lua_State *L; L = luaL_newstate(); cout &lt;&lt; "&gt;&gt; 载入（可选）标准库，以便使用打印功能" &lt;&lt; endl; luaL_openlibs(L); cout &lt;&lt; "&gt;&gt; 载入文件，暂不执行" &lt;&lt; endl; if (luaL_loadfile(L, "luascript.lua")) &#123; cerr &lt;&lt; "载入文件出现错误" &lt;&lt; endl; cerr &lt;&lt; lua_tostring(L, -1) &lt;&lt; endl; lua_pop(L,1); &#125; cout &lt;&lt; "&gt;&gt; 从 C++ 写入数据 cppvar" &lt;&lt; endl; lua_pushnumber(L, 1.1); lua_setglobal(L, "cppvar"); cout &lt;&lt; "&gt;&gt; 执行 lua 文件" &lt;&lt; endl &lt;&lt; endl; if (lua_pcall(L,0, LUA_MULTRET, 0)) &#123; cerr &lt;&lt; "执行过程中出现错误" &lt;&lt; endl; cerr &lt;&lt; lua_tostring(L, -1) &lt;&lt; endl; lua_pop(L,1); &#125; cout &lt;&lt; "&gt;&gt; 从 Lua 读取全局变量 luavar 到 C++" &lt;&lt; endl; lua_getglobal(L, "luavar"); double luavar = lua_tonumber(L,-1); lua_pop(L,1); cout &lt;&lt; "C++ 从 Lua 读取到的 luavar = " &lt;&lt; luavar &lt;&lt; endl &lt;&lt; endl; cout &lt;&lt; "&gt;&gt; 从 C++ 执行 Lua 的方法 myfunction" &lt;&lt; endl; lua_getglobal(L, "myluafunction"); lua_pushnumber(L, 5); lua_pcall(L, 1, 1, 0); cout &lt;&lt; "函数返回值是：" &lt;&lt; lua_tostring(L, -1) &lt;&lt; endl &lt;&lt; endl; lua_pop(L,1); cout &lt;&lt; "&gt;&gt; 从 Lua 执行 C++ 的方法" &lt;&lt; endl; cout &lt;&lt; "&gt;&gt;&gt;&gt; 首先在 Lua 中注册 C++ 方法" &lt;&lt; endl; lua_pushcfunction(L,l_cppfunction); lua_setglobal(L, "cppfunction"); cout &lt;&lt; "&gt;&gt;&gt;&gt; 调用 Lua 函数以执行 C++ 函数" &lt;&lt; endl; lua_getglobal(L, "myfunction"); lua_pushnumber(L, 5); lua_pcall(L, 1, 1, 0); cout &lt;&lt; "函数返回值是：" &lt;&lt; lua_tonumber(L, -1) &lt;&lt; endl &lt;&lt; endl; lua_pop(L,1); cout &lt;&lt; "&gt;&gt; 释放 Lua 资源" &lt;&lt; endl; lua_close(L); return 0;&#125; 编译命令 : g++ lua_test.cpp -o ltest -llua -ldl 创建Lua文件其次，是 lua 文件，我们将它命名为 luascript.lua luascript.lua1234567891011print("Hello from Lua")print("Lua code is capable of reading the value set from C++", cppvar)luavar = cppvar * 3function myluafunction(times) return string.rep("(-)", times)endfunction myfunction(arg) return cppfunction(arg)end 打印结果运行 cpp 文件，结果如下： &gt;&gt; 载入（可选）标准库，以便使用打印功能 &gt;&gt; 载入文件，暂不执行 &gt;&gt; 从 C++ 写入数据 cppvar &gt;&gt; 执行 lua 文件 Hello from Lua Lua code is capable of reading the value set from C++ 1.1 &gt;&gt; 从 Lua 读取全局变量 luavar 到 C++ C++ 从 Lua 读取到的 luavar = 3.3 &gt;&gt; 从 C++ 执行 Lua 的方法 myfunction 函数返回值是：(-)(-)(-)(-)(-) &gt;&gt; 从 Lua 执行 C++ 的方法 &gt;&gt;&gt;&gt; 首先在 Lua 中注册 C++ 方法 &gt;&gt;&gt;&gt; 调用 Lua 函数以执行 C++ 函数 函数返回值是：2.5 &gt;&gt; 释放 Lua 资源参考参考]]></content>
      <categories>
        <category>Script</category>
      </categories>
      <tags>
        <tag>CPP</tag>
        <tag>Lua</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lua中ipairs和pairs的区别与使用]]></title>
    <url>%2Fblog%2F2015%2F11%2F11%2Flua_pairs_ipairs%2F</url>
    <content type="text"><![CDATA[关于ipairs()和pairs(),Lua官方手册是这样说明的： pairs (t) If t has a metamethod __pairs, calls it with t as argument and returns the first three results from the call. Otherwise, returns three values: the next function, the table t, and nil, so that the construction ` for k,v in pairs(t) do body end`will iterate over all key–value pairs of table t. See function next for the caveats of modifying the table during its traversal. ipairs (t) If t has a metamethod __ipairs, calls it with t as argument and returns the first three results from the call. Otherwise, returns three values: an iterator function, the table t, and 0, so that the construction ` for i,v in ipairs(t) do body end`will iterate over the pairs (1,t[1]), (2,t[2]), …, up to the first integer key absent from the table. 根据官方手册的描述，pairs会遍历表中所有的key-value值，而pairs会根据key的数值从1开始加1递增遍历对应的table[i]值，直到出现第一个不是按1递增的数值时候退出。 . . . 例子下面我们以例子说明一下吧 1234stars = &#123;[1] = "Sun", [2] = "Moon", [5] = 'Earth'&#125;for i, v in pairs(stars) do print(i, v)end 使用pairs()将会遍历表中所有的数据，输出结果是： 1 Sun 2 Moon 5 Earth如果使用ipairs（）的话， 12345for i, v in ipairs(stars) do print(i, v)end 当i的值遍历到第三个元素时，i的值为5，此时i并不是上一个次i值（2）的+1递增，所以遍历结束，结果则会是： 1 Sun 2 Moonipairs()和pairs()的区别就是这么简单。 还有一个要注意的是pairs()的一个问题，用pairs()遍历是[key]-[value]形式的表是随机的，跟key的哈希值有关系。看以下这个例子： 1234567stars = &#123;[1] = "Sun", [2] = "Moon", [3] = "Earth", [4] = "Mars", [5] = "Venus"&#125;for i, v in pairs(stars) do print(i, v)end 结果是： 2 Moon 3 Earth 1 Sun 4 Mars 5 Venus并没有按照其在表中的顺序输出。 但是如果是这样定义表stars的话 stars = {&quot;Sun&quot;, &quot;Moon&quot;, “Earth”, &quot;Mars&quot;, &quot;Venus&quot;} 结果则会是 1 Sun 2 Moon 3 Earth 4 Mars 5 Venus你清楚了吗？:)]]></content>
      <categories>
        <category>Script</category>
      </categories>
      <tags>
        <tag>Lua</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tolua++安装]]></title>
    <url>%2Fblog%2F2015%2F11%2F11%2Flua_cpp_toluapp_tutorial%2F</url>
    <content type="text"><![CDATA[我们用一个例子来说明. 本文环境为 : ubuntu1404 g++ 4.8.4 python git lua5.1( 因为tolua++只支持到5.1, 安装5.1教程看 Lua的win和linux环境简单安装 ) . . . 安装tolua++1. git clone git@github.com:LuaDist/toluapp.git2. sudo apt-get install scons3. cd toluapp/ 4. vi custom.py 然后添加内容 : custom.py12345678910# 自己通过命令 sudo find / -name "*lua.h*" 来查头文件.h在哪, 然后把路径填到下面CCFLAGS = ['-I/usr/include/lua5.1', '-O2', '-ansi']# 自己通过命令 sudo find / -name "*liblua*" 来查静态库.a文件在哪, 然后把路径填到下面LIBPATH = ['/usr/lib/x86_64-linux-gnu']LIBS = ['lua5.1', 'dl', 'm']#prefix = '/mingw'#build_dev=1tolua_bin = 'tolua++5.1'tolua_lib = 'tolua++5.1'TOLUAPP = 'tolua++5.1' 5. scons all6. scons install 测试用以下五个文件测试, 输入命令 : 1. tolua++5.1 -o lua_Student.cpp Student.pkg2. g++ *.cpp -I/usr/include/lua5.1 -llua5.1 -lm -ltolua++5.13. 如果执行 ./a.out 之后, 打印结果如下则为环境全部安装成功 : 1 2 3 4 5 6 7 8 9 10 Student Run Student Run10 1 2 3 4 5 6 7 8 9 10 Student Run Student Run10 1 2 3 4 5 6 7 8 9 10 Student Run Student Run10 ...五个测试文件Student.h123456789101112131415#pragma once #include&lt;iostream&gt;using namespace std; class Student&#123;public: Student(); ~Student(); void Run(); void Run(int a);&#125;; Student.cpp123456789101112131415161718192021#include "Student.h" Student::Student()&#123;&#125; void Student::Run()&#123; cout &lt;&lt; "Student Run" &lt;&lt; endl;&#125; void Student::Run(int a)&#123; cout &lt;&lt; "Student Run" &lt;&lt;a&lt;&lt; endl;&#125; Student::~Student()&#123;&#125; 1234567891011$#include&quot;Student.h&quot; class Student&#123;public: Student(); ~Student(); void Run(); void Run @ Run2(int a);&#125;; test.lua12345678910111213141516171819202122232425studentB=Student:new() --实例化Student全局对象 function Run() studentB:Run();end function Run2(a) studentB:Run2(a);end function show() local b = &#123;&#125; local index for index = 1,10,1 do print(index) end end show() Run() Run2(10) main.cpp1234567891011121314151617181920212223242526272829303132#include &lt;stdio.h&gt;#include &lt;unistd.h&gt;extern "C"&#123; #include "lua.h" #include "lualib.h" #include "lauxlib.h" #include "luaconf.h"&#125;#include "tolua++.h"#include"Student.h"extern int tolua_Student_open(lua_State* tolua_S);int main(int argc, char* argv[])&#123; while(1) &#123; sleep(2); lua_State* L = luaL_newstate(); luaL_openlibs(L); tolua_Student_open(L); luaL_dofile(L, "./test.lua"); lua_close(L); &#125; return 0;&#125; 在运行的时候把test.lua文件的Run2(10) 改为 Run2(99) 之后,打印也会跟着变为 : 1 2 3 4 5 6 7 8 9 10 Student Run Student Run10 1 2 3 4 5 6 7 8 9 10 Student Run Student Run99 1 2 3 4 5 6 7 8 9 10 Student Run Student Run99 ...]]></content>
      <categories>
        <category>Script</category>
      </categories>
      <tags>
        <tag>CPP</tag>
        <tag>Lua</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[KMP查找子字符串]]></title>
    <url>%2Fblog%2F2015%2F11%2F10%2Fkmp%2F</url>
    <content type="text"><![CDATA[KMP查找子字符串前言 KMP 算法是一种改进的字符串匹配算法，由 D.E.Knuth，J.H.Morris 和 V.R.Pratt 同时发现，因此人们称它为克努特——莫里斯——普拉特操作（简称 KMP 算法）。KMP 算法的关键是利用匹配失败后的信息（已经匹配的部分中的对称信息），尽量减少模式串（待搜索词）与文本串的匹配次数以达到快速匹配的目的。具体实现就是实现一个 next() 函数，该函数本身包含了模式串的局部匹配信息。 一，kmp 算法的原理：字符串匹配是计算机的基本任务之一。它所执行的任务就是在一个文本（较长的字符串）中查找是否包含着当前指定的模式字符串（较短的字符串），并找到其位置，这就是字符串匹配问题。注：在算法导论里面待搜索的词叫 “模式” 字符串，并用 P 来表示，本文有的地方叫他“待搜索词”。如无特殊说明数组下标以 0 为起始。举例来说，有一个长字符串 “BBC ABCDAB ABCDABCDABDE”（即文本串），我想知道里面是否包含另一个模式字符串 “ABCDABD”，它的位置在哪呢？ 1，朴素匹配算法过程：1）. 首先，长字符串 “BBC ABCDAB ABCDABCDABDE” 的第一个字符与搜索词 “ABCDABD” 的第一个字符，进行比较。因为 B 与 A 不匹配，所以搜索词后移一位。 2）. 因为 B 与 A 不匹配，搜索词再往后移。 3）. 就这样，直到长字符串有一个字符，与搜索词的第一个字符相同为止（准备开始位置不变搜索下一个字符）。 4）. 接着比较字符串和搜索词的下一个字符，还是相同。 5）. 但是后面的有可能是不完全匹配的，这时在长字符串中遇到一个字符与搜索词对应的字符不相同。 6）. 最自然的反应是，将搜索词整个后移一位，再从头逐个比较。这样做虽然可行，但是效率很差，因为你要把搜索位置 BCD 这一段又要重比一次，然而它的比较在直到了后面有 “AB” 与前面的 “AB” 对称的情况下是可以避免的，直到遇到 “AB” 才萌发新的可能使整个字符匹配的可能性。 2，KMP 匹配算法过程：1). 接着前面的匹配过程，现在来看：一个基本事实是，当空格与 D 不匹配时，你其实知道前面六个符 “ABCDAB” 已经匹配的部分。KMP 算法的想法是，设法利用这个搜索词的子串（就是搜索词的某个前缀）的已知对称值，注意这里的对称非中心对称，而是基于搜索词的某一个前缀的对称信息，在不匹配的时候跳过一些不必要的位置来加速匹配速度，这样就提高了效率。 2). 怎么做到这一点呢（如何跳过不必要的位置在不匹配时）？可以针对搜索词，算出一张模式字符串的前缀函数表。这张表是如何产生的，后面再介绍，这里只要知道就可以了。 3). 已知空格与 D 不匹配时，前面六个字符 “ABCDAB” 是匹配的。查模式串的《前缀函数表》可知，其前缀（即 ABCDAB）中的最后一个匹配字符 B 的位置对应的 “部分匹配值” 为 2，因此按照下面的公式算出向后移动的位数（直接将前面的对称信息调到后面的对称位置，跳过一些不必要的位置）： 移动位数 = 已匹配的字符数 - 当前已匹配字符串的部分匹配值 因为 6 - 2 等于 4，所以将搜索词向后移动 4 位（之所以移动 4，是因为搜索词的已匹配的前缀串 “ABCDAB” 的部分匹配值为 2，即有 “AB” 对称，因为当前已匹配字符串的 “AB” 始终要找到下一个 “AB” 的开始位置，而这个开始位置恰好就在本字符串中的后缀中，所以直接移动 4，我们可以最大减少匹配次数）。 4). 因为空格与Ｃ不匹配，搜索词还要继续往后移。这时，已匹配的字符数为 2（”AB”），对应的 “部分匹配值” 为 0。所以，移动位数 = 2 - 0，结果为 2，于是将搜索词向后移 2 位。 5). 因为空格与 A 不匹配，继续后移一位。 6). 逐位比较，直到发现 C 与 D 不匹配。于是，移动位数 = 6 - 2，继续将搜索词向后移动 4 位。 7). 逐位比较，直到搜索词的最后一位，发现完全匹配，于是搜索完成。如果还要继续搜索（即找出全部匹配），移动位数 = 7 - 0，再将搜索词向后移动 7 位，这里就不再重复了。 3, 关于前缀函数表1）前缀和后缀 首先，要了解两个概念：”前缀” 和 “后缀”。 “前缀” 指除了最后一个字符以外，一个字符串的全部头部组合；”后缀” 指除了第一个字符以外，一个字符串的全部尾部组合。 2）. 前缀函数表的产生 “前缀函数” 的对称值（也叫部分匹配值）就是模式串的对应前缀中的 “前缀” 和 “后缀” 的最长的共有元素的长度（实质是最大对称程度）。以 “ABCDABD” 为例， “A” 的前缀和后缀都为空集，共有元素的长度为 0； “AB” 的前缀为 [A]，后缀为 [B]，共有元素的长度为 0； “ABC” 的前缀为 [A, AB]，后缀为 [BC, C]，共有元素的长度 0； “ABCD” 的前缀为 [A, AB, ABC]，后缀为 [BCD, CD, D]，共有元素的长度为 0； “ABCDA” 的前缀为 [A, AB, ABC, ABCD]，后缀为 [BCDA, CDA, DA, A]，共有元素为 “A”，长度为 1； “ABCDAB” 的前缀为 [A, AB, ABC, ABCD, ABCDA]，后缀为[BCDAB, CDAB, DAB, AB, B]，共有元素为 “AB”，长度为 2（“AB” 是其最大对称串，长度为 2）； “ABCDABD” 的前缀为 [A, AB, ABC, ABCD, ABCDA, ABCDAB]，后缀为 [BCDABD, CDABD, DABD, ABD, BD, D]，共有元素的长度为 0。 3）. 前缀函数表的意义 A, 首相要搞清楚的是它是模式字符串的所有前缀产生的一张表，保存的值表征了最大对称度，我们一般用 next 数组来保存其某个前缀的对称值，例如 next[6]=2，代表的就是模式串的某个有 6 个字符的前缀 “ABCDAB”，这个前缀的对称度就是 2，即“AB” 是对称的。 B, 模式串的某前缀的对称值的实质是，有时候，搜索词已经部分匹配了（一定是某个前缀），其某个前缀（已经匹配部分）的头部和尾部会有重复。比如，”ABCDAB” 之中前缀有 “AB”，后缀也有 “AB”，那么它的 “前缀函数对称值” 就是 2（”AB” 的长度, 且 “AB” 是 “ABCDAB” 所有前缀和后缀中的最长公有元素）。搜索词移动的时候，第一个 “AB” 向后移动 4 位（字符串长度 - 部分匹配值），就可以来到第二个 “AB” 的位置，在这个新的位置我们跳过了不必要的比较位置，并且直接就有 “AB” 匹配。 二，前缀函数表实现原理 通过上文完全可以对 kmp 算法的原理有个清晰的了解，那么下一步就是编程实现了，其中最重要的就是如何根据待匹配的模式字符串求出其所有前缀函数表中的最大对称值，在下文中我们将其存储在 next 数组中。 1，编程策略： 1）、当前字符的前面所有字符的对称程度为 0 的时候，只要将当前字符与前面这个子串的第一个字符进行比较。这个很好理解啊，前面所有字符串的对称值都是 0，说明都不对称了，如果多加了一个字符，要对称的话只能是当前字符和前面字符串的第一个字符对称。比如 “ABCDA” 这个里面 “ABCD” 的最大对称值是 0，那么后面的 A 的对称程度只需要看它是不是等于前面字符串的第一个字符 A 相等，如果相等就增加 1，如果不相等那就保持不变，显然还是为 0。 2）、按照这个推理，我们就可以总结一个规律，不仅前面是 0 呀，如果前面字符串的的最大对称值是 1（k），那么我们就把当前字符与前面字符串的第二（k）个字符即 P[1]（P[k]）进行比较，因为前面的是 1（k），说明前面的字符已经和第一（k）个字符相等了，如果这个又与第二（k+1）个相等了，说明对称程度就是 2（k+1）了。有两（k+1）个字符对称了。比如上面 “ABCDA” 的最大对称值是 1，说明它只和第一个 A 对称了，接着我们就把下一个字符 “B” 与 P[1]（即第二个字符）比较，又相等，自然对称程度就累加了，就是 2 了。 但是如果不相等呢？那么这个对称值显然要减少，并且我们只能到前面去寻找对称值，而在找的过程我们同时也利用前缀函数表快速搜索找到与当前字符匹配的位置。比如假设是 “(AGCTAGC)(AGCTAGC)T”（请无视字符串中的括号，只为方便看出对称）， 模式字符串：AGCTAGCAGCTAGCT 模式字符串的前缀函数表： 000012312345674 显然最后一个 T 的前一个位置的对称度是 7, 说明 T 的前一个位置的 7 个字符的后缀必与 7 个字符的前缀相等，然而 T!=P[7]，说明 T 位置的对称度只能是比 7 小的长度的前缀，所以递减 k 值，递减为多少呢？那么我们应该利用已经得到的 next[0]···next[k-1] 来求 P[0]···P[k-1] 这个前缀中最大相同前后缀, 当前字符前一个位置的对称度为 k=next[13]=7，显然必须以 7 为基准减少，即在前缀长度为 7 以内的范围重新寻找以 T 结尾的前缀，所以 k=next[6] (即下面参考代码中的k = next[k-1];)，再接着判断是否相等 3）、按照上面的推理，我们总是在找当前字符 P[q]（q 为遍历到的位置下标，见下面程序）通过其前一个位置的对称值判断是否与 P[k] 相等，如果相等，那么加，如果不相等，那么就减少 k 值，重新寻找与与 P[q] 相等的元素位置 2，参考代码：1234567891011121314void MakeNext(const char P[],int next[]) &#123; int k;//k:最大对称长度 int m = strlen(P);//模版字符串长度 next[0] = 0;//模版字符串的第一个字符的最大对称值必为0 for (int q = 1,k = 0; q &lt; m; ++q)//for循环，从第二个字符开始，依次计算每一个字符对应的next值 &#123;//在前一个位置的k不为0，但是却不相等，那么减少k，重新寻找与P[q]相等的位置，让下面的if来增加k while(k &gt; 0 &amp;&amp; P[q] != P[k])// k = next[k-1]; //while循环是整段代码的精髓所在， if (P[q] == P[k])//如果相等，那么最大相同前后缀长度加1 k++;//增加k的唯一方式 next[q] = k; &#125; &#125; 附完整代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162#include "vector"#include "string"#include &lt;iostream&gt;#include "algorithm"using namespace std;//计算模式P的部分匹配值，保存在next数组中 void MakeNext(const string &amp;P, vector&lt;int&gt; &amp;next)&#123; int q, k;//k记录所有前缀的对称值 int m = P.size();//模式字符串的长度 next[0] = 0;//首字符的对称值肯定为0 for (q = 1, k = 0; q &lt; m; ++q)//计算每一个位置的对称值 &#123; //k总是用来记录上一个前缀的最大对称值 while (k &gt; 0 &amp;&amp; P[q] != P[k]) //k = next[k - 1];//k将循环递减，值得注意的是next[k]&lt;k总是成立 --k; if (P[q] == P[k]) k++;//增加k的唯一方法 next[q] = k;//获取最终值 &#125;&#125;void KmpMatch(const string &amp;T, const string &amp;P, vector&lt;int&gt; &amp;next)&#123; int n, m; n = T.size(); m = P.size(); MakeNext(P, next); for (int i = 0, q = 0; i &lt; n; ++i) &#123; while (q &gt; 0 &amp;&amp; P[q] != T[i]) q = next[q - 1]; if (P[q] == T[i]) q++; if (q == m) &#123; cout &lt;&lt; "模式文本的偏移为：" &lt;&lt; (i - m + 1) &lt;&lt; endl; q = next[q - 1];//寻找下一个匹配 &#125; &#125;&#125;int main()&#123; system("color 0A"); vector&lt;int&gt; next(20, 0);//保存待搜索字符串的部分匹配表（所有前缀函数的对称值） string T = "BBC ABCDAB ABCDABCDABDE";//文本 string P = "ABCDABD";//待搜索字符串 cout &lt;&lt; "文本字符串：" &lt;&lt; T &lt;&lt; endl; cout &lt;&lt; "模式字符串：" &lt;&lt; P &lt;&lt; endl; KmpMatch(T, P, next); cout &lt;&lt; "模式字符串的前缀函数表：" &lt;&lt; endl; for (int i = 0; i &lt; P.size(); i++) cout &lt;&lt; next[i]; cout &lt;&lt; endl; system("pause"); return 0;&#125; 参考资源：【1】网友，c_cloud，《KMP，深入讲解 next 数组的求解》，博客地址，http://www.cnblogs.com/c-cloud/p/3224788.html【2】网友，yearn520，《KMP 算法的前缀 next 数组最通俗的解释》，博客地址，http://blog.csdn.net/yearn520/article/details/6729426【3】《算法导论》，第三十二章，字符串匹配【4】网友，jBoxer，《The Knuth-Morris-Pratt Algorithm in my own words》，博客地址，http://jakeboxer.com/blog/2009/12/13/the-knuth-morris-pratt-algorithm-in-my-own-words/【5】九度 OJ，http://ac.jobdu.com/problemset.php?page=2【6】 https://blog.csdn.net/EbowTang/article/details/49129363]]></content>
      <categories>
        <category>CPP</category>
      </categories>
      <tags>
        <tag>CPP</tag>
        <tag>noodle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lua的win和linux环境搭建]]></title>
    <url>%2Fblog%2F2015%2F11%2F08%2Flua_install_tutorial%2F</url>
    <content type="text"><![CDATA[ubuntu环境. . . 测试文件a.cpp123456789101112131415extern "C" &#123; #include "lua.h" #include "lualib.h" #include "lauxlib.h"&#125;; // 注意 : 这个extern "C" &#123;&#125; 非常重要, 不然会找不到相关库函数//#include "lua.h"//#include "lauxlib.h"int main(int argc, char **argv)&#123; lua_State *L = luaL_newstate(); luaL_openlibs(L); luaL_dostring(L, "print('hello, '.._VERSION)"); return 0;&#125; lua5.1sudo apt-get install lua5.1 sudo apt-get install liblua5.1-0-dev编译命令 : gcc a.cpp -I/usr/include/lua5.1 -llua5.1 -lm 生成 a.out 之后, 运行 a.out, 若打印 hello, Lua 5.1 即为安装成功. lua5.3sudo apt-get install libreadline-dev sudo curl -R -O http://www.lua.org/ftp/lua-5.3.0.tar.gz sudo tar zxf lua-5.3.0.tar.gz sudo cd lua-5.3.0 sudo make linux test sudo make install编译命令 : g++ a.cpp -llua -ldl 生成 a.out 之后, 运行 a.out, 若打印 hello, Lua 5.3 即为安装成功. luajit //下载git clone http://luajit.org/git/luajit-2.0.gittar zxf LuaJIT-2.0.4.tar.gzcd LuaJIT-2.0.4//linux下编译make//安装sudo make installluajit -v出现版权信息即为安装成功。 luarocks到luarocks的官网下载luarocks, 直接apt-get的已经太老旧, 默认的配置文件有错 luarocks 命令： luarocks build XXX 建立/编译一个包 luarocks download XXX 从rocks服务器下载一个指定文件或者包 luarocks help luarocks帮助 luarocks install XXX 安装包 luarocks make XXX 下载并编译包 luarocks pack 打包 luarocks list 显示已安装的列表 luarocks path 返回包地址 luarocks remove XXX 删除 luarocks search Query the LuaRocks repositories luarocks show Shows information about an installed rock. luarocks unpack Unpack the contents of a rock. Install lua-socket如果有安装 Lua 模块的安装和部署工具 – luarocks， 那么一条指令就能安装部署好 LuaSocket： luarocks install luasocket 关于json如果想安装一个解析 JSON(JavaScript Object Notation) 的模块，可以用 search 参数先搜索一下有什么可安装的解析 JSON 的模块： luarocks search json 假设想安装一个名为 json4lua 模块，可以用 install 参数来安装： luarocks install json4lua Windows环境首先要安装一个微软依赖 : https://www.microsoft.com/en-us/download/details.aspx?id=3387&amp;fa43d42b-25b5-4a42-fe9b-1634f450f5ee=True 然后安装lua for windows : http://www.runoob.com/lua/lua-environment.html 或 http://luaforge.net/projects/luaforwindows/]]></content>
      <categories>
        <category>Script</category>
      </categories>
      <tags>
        <tag>Lua</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux常用命令笔记整理之tcpdump]]></title>
    <url>%2Fblog%2F2015%2F11%2F03%2FLinux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86%E4%B9%8Btcpdump%2F</url>
    <content type="text"><![CDATA[强大的抓包工具, 博大精深内容太多, 所以这篇博客整理得只说常用, 具体的参考tcpdump用户手册,tcpdump需要root权限, 所以记得加上sudo 常用参数 -nn选项：意思是说当tcpdump遇到协议号或端口号时，不要将这些号码转换成对应的协议名称或端口名称。比如，大家都知道80是http端口，tcpdump就不会将它显示成http了 -c选项：是Count的含义，这设置了我们希望tcpdump帮我们抓几个包。 -i : 指定哪一张网卡 -l : 使得输出变为行缓冲 -t : 输出时不打印时间戳 -v : 输出更详细的信息 -F : 指定过滤表达式所在的文件, 可以建立了一个filter.txt文本文件来存储过滤表达式，然后通过-F来指定filter.txt -w : 将流量保存到文件中 -r : 读取raw packets文件 . . . 常用过滤规则 通过eth0网卡的，且来源是qq.com服务器或者目的机器是qq.com服务器的网络包sudo tcpdump -i eth0 &#39;host qq.com&#39; 只看到目的机器dst(比如是qq.com)的网络包sudo tcpdump -i eth0 &#39;dst qq.com&#39; 也可以写成 sudo tcpdump -i eth0 &#39;dst host qq.com&#39; 注 : 上述的那个host可以省略 tcpdump支持如下的类型： host：指定主机名或IP地址，例如’host roclinux.cn’或’host 202.112.18.34′ net ：指定网络段，例如’arp net 128.3’或’dst net 128.3′ portrange：指定端口区域，例如’src or dst portrange 6000-6008′ port : 端口如果我们没有设置过滤类型，那么默认是host。 只抓udp的包sudo tcpdump -i eth0 &#39;udp&#39; tcpdump具有根据网络包的协议来进行过滤的能力，我们还可以把udp改为ether、ip、ip6、arp、tcp、rarp等 只抓目的机器的某个端口的包(比如只抓baidu.com的53或者80端口的包)sudo tcpdump -i eth0 &#39;dst baidu.com and (dst port 53 or dst port 80)&#39; 通过eth0网卡的，且qq.com和baidu.com之间通讯的网络包，或者qq.com和sina.cn之间通讯的网络包tcpdump -i eth0 &#39;host qq.com and (baidu.com or sina.cn)&#39; 获取和baidu.com之间建立TCP三次握手中第一个网络包，即带有SYN标记位的网络包sudo tcpdump -i eth0 &#39;tcp[tcpflags] &amp; tcp-syn != 0 and dst host baidu.com&#39; 注 : 因为用proto [ expr : size]语法在写过滤表达式时，你需要把协议格式完全背在脑子里，才能把表达式写对。可这对大多数人来说，可能有些困难。为了让tcpdump工具更人性化一些，有一些常用的偏移量，可以通过一些名称来代替，比如icmptype表示ICMP协议的类型域、icmpcode表示ICMP的code域，tcpflags则表示TCP协议的标志字段域。 更进一步的，对于ICMP的类型域，可以用这些名称具体指代：icmp-echoreply, icmp-unreach, icmp-sourcequench, icmp-redirect, icmp-echo, icmp-routeradvert, icmp-routersolicit, icmp-timxceed, icmp-paramprob, icmp-tstamp, icmp-tstampreply, icmp-ireq, icmp-ireqreply, icmp-maskreq, icmp-maskreply。 而对于TCP协议的标志字段域，则可以细分为tcp-fin, tcp-syn, tcp-rst, tcp-push, tcp-ack, tcp-urg。 输出内容解释以下是在ubuntu上用火狐打开了一个百度时候抓到的 1234567891011121314151617b@b-VirtualBox:~$ sudo tcpdump -i eth0 &apos;host baidu.com&apos;tcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on eth0, link-type EN10MB (Ethernet), capture size 65535 bytes06:46:17.487920 IP 192.168.1.57.60110 &gt; 111.13.101.208.http: Flags [S], seq 546310089, win 29200, options [mss 1460,sackOK,TS val 1221546 ecr 0,nop,wscale 7], length 006:46:17.530422 IP 111.13.101.208.http &gt; 192.168.1.57.60110: Flags [S.], seq 3245676077, ack 546310090, win 8192, options [mss 1440,sackOK,nop,nop,nop,nop,nop,nop,nop,nop,nop,nop,nop,wscale 5], length 006:46:17.530458 IP 192.168.1.57.60110 &gt; 111.13.101.208.http: Flags [.], ack 1, win 229, length 006:46:17.530982 IP 192.168.1.57.60110 &gt; 111.13.101.208.http: Flags [P.], seq 1:504, ack 1, win 229, length 50306:46:17.576476 IP 111.13.101.208.http &gt; 192.168.1.57.60110: Flags [.], ack 504, win 216, length 006:46:17.577447 IP 111.13.101.208.http &gt; 192.168.1.57.60110: Flags [P.], seq 1:291, ack 504, win 216, length 29006:46:17.577459 IP 192.168.1.57.60110 &gt; 111.13.101.208.http: Flags [.], ack 291, win 237, length 006:46:17.577482 IP 111.13.101.208.http &gt; 192.168.1.57.60110: Flags [P.], seq 291:452, ack 504, win 216, length 16106:46:17.577485 IP 192.168.1.57.60110 &gt; 111.13.101.208.http: Flags [.], ack 452, win 245, length 006:46:17.866950 IP 111.13.101.208.http &gt; 192.168.1.57.60110: Flags [P.], seq 291:452, ack 504, win 216, length 16106:46:17.866966 IP 192.168.1.57.60110 &gt; 111.13.101.208.http: Flags [.], ack 452, win 245, options [nop,nop,sack 1 &#123;291:452&#125;], length 006:46:27.865805 IP 192.168.1.57.60110 &gt; 111.13.101.208.http: Flags [.], ack 452, win 245, length 006:46:27.909962 IP 111.13.101.208.http &gt; 192.168.1.57.60110: Flags [.], ack 504, win 216, length 006:46:37.925624 IP 192.168.1.57.60110 &gt; 111.13.101.208.http: Flags [.], ack 452, win 245, length 0 可以参考tlpi的解释, 如下图 :]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>TcpDump</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux常用文本处理命令笔记整理之sed]]></title>
    <url>%2Fblog%2F2015%2F10%2F23%2Flinux%E5%B8%B8%E7%94%A8%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E5%91%BD%E4%BB%A4%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86%E4%B9%8Bsed%2F</url>
    <content type="text"><![CDATA[sed是一种流编辑器，它是文本处理中非常中的工具，能够完美的配合正则表达式使用，功能不同凡响。处理时，把当前处理的行存储在临时缓冲区中，称为“模式空间”（pattern space），接着用sed命令处理缓冲区中的内容，处理完成后，把缓冲区的内容送往屏幕。接着处理下一行，这样不断重复，直到文件末尾。文件内容并没有 改变，除非你使用重定向存储输出。Sed主要用来自动编辑一个或多个文件；简化对文件的反复操作；编写转换程序等。 . . . sed命令 a\ 在当前行下面插入文本。 i\ 在当前行上面插入文本。 c\ 把选定的行改为新的文本。 d 删除，删除选择的行。 D 删除模板块的第一行。 s 替换指定字符 h 拷贝模板块的内容到内存中的缓冲区。 H 追加模板块的内容到内存中的缓冲区。 g 获得内存缓冲区的内容，并替代当前模板块中的文本。 G 获得内存缓冲区的内容，并追加到当前模板块文本的后面。 l 列表不能打印字符的清单。 n 读取下一个输入行，用下一个命令处理新的行而不是用第一个命令。 N 追加下一个输入行到模板块后面并在二者间嵌入一个新行，改变当前行号码。 p 打印模板块的行。 P(大写) 打印模板块的第一行。 q 退出Sed。 b lable 分支到脚本中带有标记的地方，如果分支不存在则分支到脚本的末尾。 r file 从file中读行。 t label if分支，从最后一行开始，条件一旦满足或者T，t命令，将导致分支到带有标号的命令处，或者到脚本的末尾。 T label 错误分支，从最后一行开始，一旦发生错误或者T，t命令，将导致分支到带有标号的命令处，或者到脚本的末尾。 w file 写并追加模板块到file末尾。 W file 写并追加模板块的第一行到file末尾。 ! 表示后面的命令对所有没有被选定的行发生作用。 = 打印当前行号码。 # 把注释扩展到下一个换行符以前。 sed替换标记 g 表示行内全面替换。 p 表示打印行。 w 表示把行写入一个文件。 x 表示互换模板块中的文本和缓冲区中的文本。 y 表示把一个字符翻译为另外的字符（但是不用于正则表达式） \1 子串匹配标记 &amp; 已匹配字符串标记 sed元字符集 ^ 匹配行开始，如：/^sed/匹配所有以sed开头的行。 $ 匹配行结束，如：/sed$/匹配所有以sed结尾的行。 . 匹配一个非换行符的任意字符，如：/s.d/匹配s后接一个任意字符，最后是d。 * 匹配0个或多个字符，如：/*sed/匹配所有模板是一个或多个空格后紧跟sed的行。 [] 匹配一个指定范围内的字符，如/[ss]ed/匹配sed和Sed。 [^] 匹配一个不在指定范围内的字符，如：/[^A-RT-Z]ed/匹配不包含A-R和T-Z的一个字母开头，紧跟ed的行。 (..) 匹配子串，保存匹配的字符，如s/(love)able/\1rs，loveable被替换成lovers。 &amp; 保存搜索字符用来替换其他字符，如s/love/**&amp;**/，love改成**love**。 &lt; 匹配单词的开始，如:/\ 匹配单词的结束，如/love&gt;/匹配包含以love结尾的单词的行。 x{m} 重复字符x，m次，如：/0{5}/匹配包含5个0的行。 x{m,} 重复字符x，至少m次，如：/0{5,}/匹配至少有5个0的行。 x{m,n} 重复字符x，至少m次，不多于n次，如：/0{5,10}/匹配5~10个0的行。 直接编辑文件选项-i，否则并不会修改源文件 sed常用用法1：增记忆技巧 : *增为a, \这个符号是用来分隔a和具体要增加的字符串的, a代表的意思是在下一行插入,而i\是在上一行插入, 如果你用过vim的话, 应该很好记忆.* 123456b@b-VirtualBox:~/my_temp_test/abc$ cat abc3&amp;&amp; gg&amp;b@b-VirtualBox:~/my_temp_test/abc$ sed -i &apos;/gg/a\hello, my friend&apos; abc3b@b-VirtualBox:~/my_temp_test/abc$ cat abc3&amp;&amp; gg&amp;hello, my friend sed -i ‘/gg/a\hello, my friend’ abc3的含义是：在abc3文件中的“gg”字符串的下一行插入“hello， my friend” sed常用用法2：删123456b@b-VirtualBox:~/my_temp_test/abc$ cat abc3&amp;&amp; gg&amp;hello, my friendb@b-VirtualBox:~/my_temp_test/abc$ sed -i &apos;/gg/d&apos; abc3b@b-VirtualBox:~/my_temp_test/abc$ cat abc3hello, my friend sed -i ‘/gg/d’ abc3的含义是：将abc3文件中所有包含的“gg”字符串的行删除 sed常用用法3：改12345b@b-VirtualBox:~/my_temp_test/abc$ cat abc3hello, my friendb@b-VirtualBox:~/my_temp_test/abc$ sed -i &apos;s/hello/welcome/g&apos; abc3b@b-VirtualBox:~/my_temp_test/abc$ cat abc3welcome, my friend sed -i ‘s/hello/welcome/g’ abc3的含义是：将abc3文件中所有包含的“hello”字符串都修改为“welcome”]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Command</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux常用文本处理命令笔记整理之grep和awk]]></title>
    <url>%2Fblog%2F2015%2F10%2F21%2Flinux%E5%B8%B8%E7%94%A8%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E5%91%BD%E4%BB%A4%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86%E4%B9%8Bgrep%E5%92%8Cawk%2F</url>
    <content type="text"><![CDATA[linux常用文本处理的命令的使用率很高， 所以整理了一些之前的笔记，用markdown来记录备忘。首先抛出问题， 带着问题来学记忆知识更有动力： 如何通过一条命令取得eth0的IP4地址 ： 1ifconfig eth0 | grep -w &apos;inet&apos; | awk &apos;&#123;print $2&#125;&apos; | awk -F: &apos;&#123;print $2&#125;&apos; 如何通过一条命令替换当前路径下所有文件中的所有“xxx”为“yyy“ ： 1ls -alF | grep &apos;^-&apos; | awk &apos;&#123;print $NF&#125;&apos; | xargs sed -i &apos;s/xxx/yyy/g&apos; 如何通过一条命令杀掉占用端口34600的进程 ： 1sudo lsof -i:34600 | grep -v &apos;PID&apos; | awk &apos;&#123;print $2&#125;&apos; | xargs kill -9 这些命令它们分别具体是什么意思呢?为何能达到上述效果? . . . grep grep（global search regular expression(RE) and print out the line，全面搜索正则表达式并把行打印出来）是一种强大的文本搜索工具，它能使用正则表达式搜索文本，并把匹配的行打印出来。 -a 不要忽略二进制数据。 -A&lt;显示列数&gt; 除了显示符合范本样式的那一行之外，并显示该行之后的内容。 -b 在显示符合范本样式的那一行之外，并显示该行之前的内容。 -c 计算符合范本样式的列数。 -C&lt;显示列数&gt;或-&lt;显示列数&gt; 除了显示符合范本样式的那一列之外，并显示该列之前后的内容。 -d&lt;进行动作&gt; 当指定要查找的是目录而非文件时，必须使用这项参数，否则grep命令将回报信息并停止动作。 -e&lt;范本样式&gt; 指定字符串作为查找文件内容的范本样式。 -E 将范本样式为延伸的普通表示法来使用，意味着使用能使用扩展正则表达式。 -f&lt;范本文件&gt; 指定范本文件，其内容有一个或多个范本样式，让grep查找符合范本条件的文件内容，格式为每一列的范本样式。 -F 将范本样式视为固定字符串的列表。 -G 将范本样式视为普通的表示法来使用。 -h 在显示符合范本样式的那一列之前，不标示该列所属的文件名称。 -H 在显示符合范本样式的那一列之前，标示该列的文件名称。 -i 忽略字符大小写的差别。 -l 列出文件内容符合指定的范本样式的文件名称。 -L 列出文件内容不符合指定的范本样式的文件名称。 -n 在显示符合范本样式的那一列之前，标示出该列的编号。 -q 不显示任何信息。 -R/-r 此参数的效果和指定“-d recurse”参数相同。 -s 不显示错误信息。 -v 反转查找。 -w 只显示全字符合的列。 -x 只显示全列符合的列。 -y 此参数效果跟“-i”相同。 -o 只输出文件中匹配到的部分。 awk awk是一种编程语言，用于在linux/unix下对文本和数据进行处理。数据可以来自标准输入(stdin)、一个或多个文件，或其它命令的输出。它支持用户自定义函数和动态正则表达式等先进功能，是linux/unix下的一个强大编程工具。它在命令行中使用，但更多是作为脚本来使用。awk有很多内建的功能，比如数组、函数等，这是它和C语言的相同之处，灵活性是awk最大的优势。 常用命令选项 -F fs fs指定输入分隔符（awk默认的分隔符是空格），fs可以是字符串或正则表达式，如-F: -v var=value 赋值一个用户定义变量，将外部变量传递给awk 常用用法123456b@b-VirtualBox:~/my_temp_test/abc$ cat abc3klj;k uu&amp;&amp; ss&amp;b@b-VirtualBox:~/my_temp_test/abc$ cat abc3 | awk &apos;&#123;print $NF&#125;&apos;uuss&amp; cat abc3 | awk ‘{print $NF}’的含义是：输出abc3文件的每一行的最后一列 12b@b-VirtualBox:~/my_temp_test/abc$ cat abc3 | grep k | awk -F\; &apos;&#123;print $1&#125;&apos;klj cat abc3 | grep k | awk -F; ‘{print $1}’的含义是：先输入含有k的那一行（即klj；k）， 然后对那一行以；（;， 这个分号需要转义）分隔，打印出分隔后的第一列（即klj）]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Command</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[安装AndroidSDK的一些坑与注意点]]></title>
    <url>%2Fblog%2F2015%2F09%2F13%2Finstall_android_sdk_jdk%2F</url>
    <content type="text"><![CDATA[近来要用Unity打包到安卓上玩, 需要安装AndroidSDK. 安装教程基本上按照这篇文章A就可以, 遇到不明白的可以拿这篇B对照着看, 以A为准, 但是有几个点要注意 : jdk别装太高版本, 装个jdk-8u161的32位的即可, 别装64位, 也别装高版本的jdk10的64位, 不然 android sdk set up tool 不认识, sdk manager 也会闪退 jdk的环境变量很容易设置错, 比如环境变量JAVA_HOME应该填jdk的安装路径即 : JAVA_HOME=C:/Program Files/Java/jdk1.8.0_11而不是JAVA_HOME=C:/Program Files/Java, 填后者的话, sdk manager 会闪退 为了确保不必要的麻烦最好这样环境变量设置成类似如下 : 123JAVA_HOME=C:/Program Files/Java/jdk1.8.0_11JRE_HOME=C:/Program Files/Java/jre8Path=%JAVA_HOME%;C:... 打安卓包的时候, 如果报file not found debug.keystore 或 Unable to get debug signature key的错, 用管理员权限重新打开Unity即可.]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>Unity</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Unity中C#调用C++写的DLL之Swig篇]]></title>
    <url>%2Fblog%2F2015%2F09%2F13%2Funity_cpp_swig_csharp%2F</url>
    <content type="text"><![CDATA[近来要用Unity打包到安卓上玩, Unity那边需要用到服务器中用C++写的库,对比了 P/Invoke 和 C++/CLI 两种方式, 都不够省心省力, 决定使用 Swig来撸. 教程基本上按照这篇文章就可以, 文章写得非常详尽, 但文中关于设置 swiglib.i 自定义生成工具的命令行的时候, 他文中的下面一段要注意 : 在常规中选择命令行并且写入： echo on $(SolutionDir)/../../thirdpart/swigwin-3.0.12/swig.exe -c++ -csharp -outdir “$(SolutionDir)/../../../UnityProj/UnityCppLearn/Assets/SwigTools/Interface” “%(FullPath)” echo off应改成 : 我们在自己填的时候要记得改成自己项目中的路径, 以及把上面这段命令中的中文引号改成英文引号.]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>Unity</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hash索引btree索引聚簇索引非聚簇索引]]></title>
    <url>%2Fblog%2F2015%2F09%2F12%2Fhash%E7%B4%A2%E5%BC%95btree%E7%B4%A2%E5%BC%95%E8%81%9A%E7%B0%87%E7%B4%A2%E5%BC%95%E9%9D%9E%E8%81%9A%E7%B0%87%E7%B4%A2%E5%BC%95%2F</url>
    <content type="text"><![CDATA[索引概要索引是帮助mysql获取数据的数据结构。最常见的索引是 Btree索引 Hash索引 不同的引擎对于索引有不同的支持： Innodb和MyISAM默认的索引是Btree索引； Mermory默认的索引是Hash索引。 Hash索引Mermory默认的索引是Hash索引。 所谓Hash索引，当我们要给某张表某列增加索引时，将这张表的这一列进行哈希算法计算，得到哈希值， 排序在哈希数组上。所以Hash索引可以一次定位，其效率很高，而Btree索引需要经过多次的磁盘IO，但是innodb和myisam之所以没有采用它，是因为它存在着好多缺点. Hash索引的缺点 Hash 索引仅仅能满足”=”,”IN”和”&lt;=&gt;”查询，不能使用范围查询。由于 Hash 索引比较的是进行 Hash 运算之后的 Hash 值，所以它只能用于等值的过滤，不能用于基于范围的过滤，因为经过相应的 Hash 算法处理之后的 Hash 值的大小关系，并不能保证和Hash运算前完全一样。 Hash 索引无法被用来避免数据的排序操作。由于 Hash 索引中存放的是经过 Hash 计算之后的 Hash 值，而且Hash值的大小关系并不一定和 Hash 运算前的键值完全一样，所以数据库无法利用索引的数据来避免任何排序运算； Hash 索引不能利用部分索引键查询。对于组合索引，Hash 索引在计算 Hash 值的时候是组合索引键合并后再一起计算 Hash 值，而不是单独计算 Hash 值，所以通过组合索引的前面一个或几个索引键进行查询的时候，Hash 索引也无法被利用。 Hash 索引在任何时候都不能避免表扫描。前面已经知道，Hash 索引是将索引键通过 Hash 运算之后，将 Hash运算结果的 Hash 值和所对应的行指针信息存放于一个 Hash 表中，由于不同索引键存在相同 Hash 值，所以即使取满足某个 Hash 键值的数据的记录条数，也无法从 Hash 索引中直接完成查询，还是要通过访问表中的实际数据进行相应的比较，并得到相应的结果。 Hash 索引遇到大量Hash值相等的情况后性能并不一定就会比B-Tree索引高。对于选择性比较低的索引键，如果创建 Hash 索引，那么将会存在大量记录指针信息存于同一个 Hash 值相关联。这样要定位某一条记录时就会非常麻烦，会浪费多次表数据的访问，而造成整体性能低下 Btree索引Innodb和MyISAM默认的索引是Btree索引；至于Btree索引，它是以B+树为存储结构实现的。但是Btree索引的存储结构在Innodb和MyISAM中有很大区别。 btree索引在MyISAM中的实现在MyISAM中，我们如果要对某张表的某列建立Btree索引的话，如图： 所以我们经常会说MyISAM中数据文件和索引文件是分开的。因此MyISAM的索引方式也称为非聚集,至于辅助索引，类似于主索引，唯一区别就是主索引上的值不能重复，而辅助索引可以重复。 因此当我们根据Btree索引去搜索的时候，若key存在，在data域找到其地址，然后根据地址去表中查找数据记录。 btree索引在Innodb中的实现至于Innodb它跟上面又有很大不同，它的叶子节点存储的并不是表的地址，而是数据 我们可以看到这里并没有将地址放入叶子节点，而是直接放入了对应的数据， 这也就是我们平常说到的，Innodb的索引文件就是数据文件， 那么对于Innodb的辅助索引结构跟主索引也相差很多，如图： 我们可以发现，这里叶子节点存储的是主键的信息， 所以我们在利用辅助索引的时候，检索到主键信息， 然后再通过主键去主索引中定位表中的数据，这就可以说明Innodb中主键之所以不宜用过长的字段，由于所有的辅助索引都包含主索引， 所以很容易让辅助索引变得庞大。 我们还可以发现：在Innodb中尽量使用自增的主键， 这样每次增加数据时只需要在后面添加即可， 非单调的主键在插入时会需要维持B+tree特性而进行分裂调整，十分低效。 Btree索引中的最左匹配原则：Btree是按照从左到右的顺序来建立搜索树的。 比如索引是(name,age,sex)， 会先检查name字段，如果name字段相同再去检查后两个字段。 所以当传进来的是后两个字段的数据（age，sex）， 因为建立搜索树的时候是按照第一个字段建立的，所以必须根据name字段才能知道下一个字段去哪里查询。 所以传进来的是（name，sex）时，首先会根据name指定搜索方向，但是第二个字段缺失，所以将name字段正确的都找到后，然后才会去匹配sex的数据。 建立索引的规则： 利用最左前缀：Mysql会一直向右查找直到遇到范围操作（&gt;，&lt;，like、between）就停止匹配。比如a=1 and b=2 and c&gt;3 and d=6；此时如果建立了（a, b, c, d）索引，那么后面的d索引是完全没有用到，当换成了（a, b, d, c）就可以用到。 不能过度索引：在修改表内容的时候，索引必须更新或者重构，所以索引过多时，会消耗更多的时间。 尽量扩展索引而不要新建索引 最适合的索引的列是出现在where子句中的列或连接子句中指定的列。 不同值较少的列不必要建立索引（性别）。 练习题 数据索引的正确是(正确答案A, D) A、一个表只能有一个聚族索引，多个非聚族索引 B、字符串模糊查询不适合索引 C、哈希 索引有利于查询字段用于大小范围的比较查询 D、多余的索引字段会降低性能 Select A,B from Table1 where A between 60 and 100 order by B，下面哪些优化sql性能(正确答案B) A、字段A 建立hash索引，字段 B不建立索引 B、字段 A 建立btree索引，字段 B不建立索引 C、字段A 不建立 索引，字段 B建立btree索引]]></content>
      <categories>
        <category>DB</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GDB多进程多线程调试实战]]></title>
    <url>%2Fblog%2F2015%2F08%2F31%2Fgdb_multi_thread_and_multi_process%2F</url>
    <content type="text"><![CDATA[GDB多进程/多线程调试实战例子在gcc编译的时候，记得附加 -lpthread参数, 否则会出现 undefined reference to ‘pthread_create’ 的错误.(因为在链接的时候，无法找到phread库中哥函数的入口地址，于是链接会失败。) 例程123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263#include &lt;stdio.h&gt;#include &lt;pthread.h&gt;#include &lt;unistd.h&gt;void processA();void processB();void *processAworker(void *arg);int main(int argc, const char *argv[])&#123; int pid; pid = fork(); if (pid != 0) processA(); else processB(); return 0;&#125;void processA()&#123; pid_t pid = getpid(); char prefix[] = "ProcessA: "; char tprefix[] = "thread "; int tstatus; pthread_t pt; printf("%s%lu %s\n", prefix, pid, "step1"); tstatus = pthread_create(&amp;pt, NULL, processAworker, NULL); if (tstatus != 0) &#123; printf("ProcessA: Can not create new thread."); &#125; processAworker(NULL); sleep(1);&#125;void *processAworker(void *arg)&#123; pid_t pid = getpid(); pthread_t tid = pthread_self(); char prefix[] = "ProcessA: "; char tprefix[] = "thread "; printf("%s%lu %s%lu %s\n", prefix, pid, tprefix, tid, "step2"); printf("%s%lu %s%lu %s\n", prefix, pid, tprefix, tid, "step3"); return NULL;&#125;void processB()&#123; pid_t pid = getpid(); char prefix[] = "ProcessB: "; printf("%s%lu %s\n", prefix, pid, "step1"); printf("%s%lu %s\n", prefix, pid, "step2"); printf("%s%lu %s\n", prefix, pid, "step3");&#125; 输出[cnwuwil@centos c-lab]$ ./test ProcessA: 802 step1 ProcessB: 803 step1 ProcessB: 803 step2 ProcessB: 803 step3 ProcessA: 802 thread 3077555904 step2 ProcessA: 802 thread 3077555904 step3 ProcessA: 802 thread 3077553008 step2 ProcessA: 802 thread 3077553008 step3GDB多进程调试命令 set follow-fork-mode [parent|child] set detach-on-fork [on|off] follow-fork-mode detach-on-fork 说明 parent on 只调试主进程（GDB默认） child on 只调试子进程 parent off 同时调试两个进程，GDB跟主进程，子进程block在fork位置 child off 同时调试两个进程，GDB跟子进程，主进程block在fork位置 查询正在调试的进程：info inferiors 切换调试的进程： inferior +inferior number catch fork命令可以捕获进程的创建 attach + pid ， 可以附到一个正在运行的进程上进行调试 . . . GDB多线程调试命令 show scheduler-locking //显示当前scheduler-locking set scheduler-locking [on/off/step] //设置scheduler-locking on：只有当前调试线程运行，其他线程处于暂停状态。 off：当前调试线程外的其他线程一直在正常运行。 step：其他线程跟随当前调试线程运行，但具体怎么协同运行，测试中无法体现。 注意：set scheduler-locking要处于线程运行环境下才能生效，也就是程序已经运行并且暂停在某个断点处，否则会出现“Target ‘exec’ cannot support this command.”这样的错误；而且经过测试，设置后的scheduler-locking值在整个进程内有效，不属于某个线程。 查询线程：info threads 切换调试线程：thread + thread_number 实战调试11.调试主进程，block子进程。 (gdb) set detach-on-fork off (gdb) show detach-on-fork Whether gdb will detach the child of a fork is off. (gdb) catch fork Catchpoint 1 (fork) (gdb) r [Thread debugging using libthread_db enabled] Catchpoint 1 (forked process 3475), 0x00110424 in __kernel_vsyscall () Missing separate debuginfos, use: debuginfo-install glibc-2.12-1.47.el6.i686 (gdb) break test.c:14 Breakpoint 2 at 0x8048546: file test.c, line 14. (gdb) cont [New process 3475] [Thread debugging using libthread_db enabled] Breakpoint 2, main (argc=1, argv=0xbffff364) at test.c:14 Missing separate debuginfos, use: debuginfo-install glibc-2.12-1.47.el6.i686 (gdb) info inferiors Num Description Executable 2 process 3475 /home/cnwuwil/labs/c-lab/test * 1 process 3472 /home/cnwuwil/labs/c-lab/test2.切换到子进程： (gdb) inferior 2 [Switching to inferior 2 [process 3475] (/home/cnwuwil/labs/c-lab/test)] [Switching to thread 2 (Thread 0xb7fe86c0 (LWP 3475))] #0 0x00110424 in ?? () (gdb) info inferiors Num Description Executable * 2 process 3475 /home/cnwuwil/labs/c-lab/test 1 process 3472 /home/cnwuwil/labs/c-lab/test (gdb) inferior 1 [Switching to inferior 1 [process 3472] (/home/cnwuwil/labs/c-lab/test)] [Switching to thread 1 (Thread 0xb7fe86c0 (LWP 3472))] #0 main (argc=1, argv=0xbffff364) at test.c:14 (gdb) info inferiors Num Description Executable 2 process 3475 /home/cnwuwil/labs/c-lab/test * 1 process 3472 /home/cnwuwil/labs/c-lab/test3.设断点继续调试主进程，主进程产生两个子线程： (gdb) break test.c:50 Breakpoint 3 at 0x804867d: file test.c, line 50. (2 locations) (gdb) cont ProcessA: 3472 step1 [New Thread 0xb7fe7b70 (LWP 3562)] ProcessA: 3472 thread 3086911168 step2 Breakpoint 3, processAworker (arg=0x0) at test.c:50 (gdb) info inferiors Num Description Executable 2 process 3475 /home/cnwuwil/labs/c-lab/test * 1 process 3472 /home/cnwuwil/labs/c-lab/test (gdb) info threads 3 Thread 0xb7fe7b70 (LWP 3562) 0x00110424 in __kernel_vsyscall () 2 Thread 0xb7fe86c0 (LWP 3475) 0x00110424 in ?? () * 1 Thread 0xb7fe86c0 (LWP 3472) processAworker (arg=0x0) at test.c:504.切换到主进程中的子线程，注意：线程2为前面产生的子进程 (gdb) thread 3 [Switching to thread 3 (Thread 0xb7fe7b70 (LWP 3562))]#0 0x00110424 in __kernel_vsyscall () (gdb) cont ProcessA: 3472 thread 3086911168 step3 ProcessA: 3472 thread 3086908272 step2 [Switching to Thread 0xb7fe7b70 (LWP 3562)] Breakpoint 3, processAworker (arg=0x0) at test.c:50 (gdb) info threads * 3 Thread 0xb7fe7b70 (LWP 3562) processAworker (arg=0x0) at test.c:50 2 Thread 0xb7fe86c0 (LWP 3475) 0x00110424 in ?? () 1 Thread 0xb7fe86c0 (LWP 3472) 0x00110424 in __kernel_vsyscall () (gdb) thread 1实战调试2b@b-VirtualBox:~/Documents/temp_test$ sudo gdb ./o_multi_thread_process [sudo] password for b: GNU gdb (Ubuntu 7.7.1-0ubuntu5~14.04.2) 7.7.1 Copyright (C) 2014 Free Software Foundation, Inc. License GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt; This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. Type &quot;show copying&quot; and &quot;show warranty&quot; for details. This GDB was configured as &quot;x86_64-linux-gnu&quot;. Type &quot;show configuration&quot; for configuration details. For bug reporting instructions, please see: &lt;http://www.gnu.org/software/gdb/bugs/&gt;. Find the GDB manual and other documentation resources online at: &lt;http://www.gnu.org/software/gdb/documentation/&gt;. For help, type &quot;help&quot;. Type &quot;apropos word&quot; to search for commands related to &quot;word&quot;... Reading symbols from ./o_multi_thread_process...done. (gdb) attach 3027 Attaching to program: /home/b/Documents/temp_test/o_multi_thread_process, process 3027 Reading symbols from /lib/x86_64-linux-gnu/libpthread.so.0...Reading symbols from /usr/lib/debug//lib/x86_64-linux-gnu/libpthread-2.19.so...done. done. [New LWP 3029] [Thread debugging using libthread_db enabled] Using host libthread_db library &quot;/lib/x86_64-linux-gnu/libthread_db.so.1&quot;. Loaded symbols for /lib/x86_64-linux-gnu/libpthread.so.0 Reading symbols from /lib/x86_64-linux-gnu/libc.so.6...Reading symbols from /usr/lib/debug//lib/x86_64-linux-gnu/libc-2.19.so...done. done. Loaded symbols for /lib/x86_64-linux-gnu/libc.so.6 Reading symbols from /lib64/ld-linux-x86-64.so.2...Reading symbols from /usr/lib/debug//lib/x86_64-linux-gnu/ld-2.19.so...done. done. Loaded symbols for /lib64/ld-linux-x86-64.so.2 0x00007f5c9acb8dfd in nanosleep () at ../sysdeps/unix/syscall-template.S:81 81 ../sysdeps/unix/syscall-template.S: No such file or directory. (gdb) set follow-fork-mode parent (gdb) set detach-on-fork off (gdb) catch fork Catchpoint 1 (fork) (gdb) r Starting program: /home/b/Documents/temp_test/o_multi_thread_process [Thread debugging using libthread_db enabled] Using host libthread_db library &quot;/lib/x86_64-linux-gnu/libthread_db.so.1&quot;. Catchpoint 1 (forked process 3002), 0x00007ffff78b7ee4 in __libc_fork () at ../nptl/sysdeps/unix/sysv/linux/x86_64/../fork.c:130 130 ../nptl/sysdeps/unix/sysv/linux/x86_64/../fork.c: No such file or directory. (gdb) info inferiors Num Description Executable * 1 process 2998 /home/b/Documents/temp_test/o_multi_thread_process (gdb) b 14 Breakpoint 2 at 0x7ffff78b7f5b: file ../nptl/sysdeps/unix/sysv/linux/x86_64/../fork.c, line 14. (gdb) info breakpoints Num Type Disp Enb Address What 1 catchpoint keep y fork, process 3002 catchpoint already hit 1 time 2 breakpoint keep y 0x00007ffff78b7f5b in __libc_fork at ../nptl/sysdeps/unix/sysv/linux/x86_64/../fork.c:14 (gdb) d 2 (gdb) info breakpoints Num Type Disp Enb Address What 1 catchpoint keep y fork, process 3002 catchpoint already hit 1 time (gdb) b multi_thread_process.cpp : 14 Breakpoint 3 at 0x4007f4: file ./multi_thread_process.cpp, line 14. (gdb) c Continuing. [New process 3002] [Thread debugging using libthread_db enabled] Using host libthread_db library &quot;/lib/x86_64-linux-gnu/libthread_db.so.1&quot;. Reading symbols from /usr/lib/debug/lib/x86_64-linux-gnu/libpthread-2.19.so...done. Reading symbols from /usr/lib/debug/lib/x86_64-linux-gnu/libc-2.19.so...done. Reading symbols from /usr/lib/debug/lib/x86_64-linux-gnu/ld-2.19.so...done. Breakpoint 3, main (argc=1, argv=0x7fffffffe598) at ./multi_thread_process.cpp:15 15 if(pid != 0) (gdb) info inferiors Num Description Executable 2 process 3002 /home/b/Documents/temp_test/o_multi_thread_process * 1 process 2998 /home/b/Documents/temp_test/o_multi_thread_process (gdb) inferior 2 [Switching to inferior 2 [process 3002] (/home/b/Documents/temp_test/o_multi_thread_process)] [Switching to thread 2 (Thread 0x7ffff7fdf740 (LWP 3002))] 0 0x00007ffff78b7ee4 in __libc_fork () at ../nptl/sysdeps/unix/sysv/linux/x86_64/../fork.c:130 130 ../nptl/sysdeps/unix/sysv/linux/x86_64/../fork.c: No such file or directory. (gdb) set scheduler-locking on (gdb) b multi_thread_process.cpp : 50 Breakpoint 4 at 0x400916: multi_thread_process.cpp:50. (2 locations) (gdb) info threads Id Target Id Frame * 2 Thread 0x7ffff7fdf740 (LWP 3002) &quot;o_multi_thread_&quot; 0x00007ffff78b7ee4 in __libc_fork () at ../nptl/sysdeps/unix/sysv/linux/x86_64/../fork.c:130 1 Thread 0x7ffff7fdf740 (LWP 2998) &quot;o_multi_thread_&quot; main (argc=1, argv=0x7fffffffe598) at ./multi_thread_process.cpp:15 (gdb) c Continuing. Breakpoint 3, main (argc=1, argv=0x7fffffffe598) at ./multi_thread_process.cpp:15 15 if(pid != 0) (gdb) info threads Id Target Id Frame * 2 Thread 0x7ffff7fdf740 (LWP 3002) &quot;o_multi_thread_&quot; main (argc=1, argv=0x7fffffffe598) at ./multi_thread_process.cpp:15 1 Thread 0x7ffff7fdf740 (LWP 2998) &quot;o_multi_thread_&quot; main (argc=1, argv=0x7fffffffe598) at ./multi_thread_process.cpp:15 (gdb) c Continuing. ProcessB: 3002 step1 ProcessB: 3002 step2 ProcessB: 3002 step3 ^C Program received signal SIGINT, Interrupt. 0x00007ffff78b7de0 in __nanosleep_nocancel () at ../sysdeps/unix/syscall-template.S:81 81 ../sysdeps/unix/syscall-template.S: No such file or directory. (gdb) info threads Id Target Id Frame * 2 Thread 0x7ffff7fdf740 (LWP 3002) &quot;o_multi_thread_&quot; 0x00007ffff78b7de0 in __nanosleep_nocancel () at ../sysdeps/unix/syscall-template.S:81 1 Thread 0x7ffff7fdf740 (LWP 2998) &quot;o_multi_thread_&quot; main (argc=1, argv=0x7fffffffe598) at ./multi_thread_process.cpp:15 (gdb) info inferiors Num Description Executable * 2 process 3002 /home/b/Documents/temp_test/o_multi_thread_process 1 process 2998 /home/b/Documents/temp_test/o_multi_thread_process (gdb) inferior 1 [Switching to inferior 1 [process 2998] (/home/b/Documents/temp_test/o_multi_thread_process)] [Switching to thread 1 (Thread 0x7ffff7fdf740 (LWP 2998))] 0 main (argc=1, argv=0x7fffffffe598) at ./multi_thread_process.cpp:15 15 if(pid != 0) (gdb) list 10 { 11 int pid; 12 13 pid = fork(); 14 15 if(pid != 0) 16 processA(); 17 else 18 processB(); 19 (gdb) r The program being debugged has been started already. Start it from the beginning? (y or n) n Program not restarted. (gdb) c Continuing. ProcessA: 2998 step1 [New Thread 0x7ffff77f6700 (LWP 3017)] ^C Program received signal SIGINT, Interrupt. 0x00007ffff78b7dfd in nanosleep () at ../sysdeps/unix/syscall-template.S:81 81 ../sysdeps/unix/syscall-template.S: No such file or directory. (gdb) info threads Id Target Id Frame 3 Thread 0x7ffff77f6700 (LWP 3017) &quot;o_multi_thread_&quot; clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:81 2 Thread 0x7ffff7fdf740 (LWP 3002) &quot;o_multi_thread_&quot; 0x00007ffff78b7de0 in __nanosleep_nocancel () at ../sysdeps/unix/syscall-template.S:81 * 1 Thread 0x7ffff7fdf740 (LWP 2998) &quot;o_multi_thread_&quot; 0x00007ffff78b7dfd in nanosleep () at ../sysdeps/unix/syscall-template.S:81 (gdb) 参考参考]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>GDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Putty配置]]></title>
    <url>%2Fblog%2F2015%2F08%2F23%2Fputty_config%2F</url>
    <content type="text"><![CDATA[平时工作学习必须要使用Windows，在SSH远程连接软件里Putty算是用得比较顺手的，而且很小巧。但是每次输入密码很麻烦，还容易输错，OpenSSH可以利用密钥来自动登陆，如此一来方便了不少。配置过程分为三步：1、生成公钥和私钥先要下载一个叫puttygen的软件（下载见附件），在Windows端生成公钥和私钥。点击Generate开始生成在生成过程中用鼠标在进度条下面的空白处乱晃几下，产生随机性：生成完毕，将私钥保存起来：然后将公钥全选复制。2、远程主机配置我这里使用的是CentOS6.4，已经安装了OpenSSH，如果远程主机没有安装的，先要安装。先连接上远程主机，然后输入命令mkdir .sshchmod 700 .sshvim ~/.ssh/authorized_keys按“i”键进入编辑模式（用过vi/vim的都应该知道吧），然后点鼠标右键将刚才复制的公钥粘贴进去，然后按“Esc”，输入wq&lt;Enter&gt;保存。安全起见，设置验证文件为只读：chmod 400 ~/.ssh/authorized_keys3、Putty端配置先到Connection-Data项设置自己的登陆用户名，如图（我的是root）：再点SSH项下面的Auth，添加第一步保存的私钥然后很重要的是要回去Session项里保存！！！不然下次又得重新添加一遍然后再双击Default Settings里保存的任务，就直接登陆进去了：是不是很棒~最后再优化一下显示设置（转过来的）：字体大小设置Window-&gt;Appearance-&gt;Font settings—&gt;Change按钮设置（我的设置为12）字体颜色设置Window-&gt;Colours-&gt;Default Foreground-&gt;Modify设置（我喜欢绿色设置：R:0 G:255 B:64）此外在默认的黑色背景下 蓝色看不太清楚，可以把Window-&gt;Colours-&gt;ANSI Blue 更改一下设置（我设置为R：255 G：0 B：128）全屏/退出全屏的快捷键设置Window-&gt;Behaviour最下面有个Full screen on Alt-Enter 勾上就可以了。 putty配置导出的方法PuTTY 是一款小巧的开源 Telnet/SSH 客户端，但是它不提供设置的导入导出工具，PuTTY 将设置都保存在注册表中，所以要备份主要就是要备份注册表里的资料。 下面是备份步骤，实质上就是导出相应的注册表键值： 开始-&gt;运行(Win+R)-&gt;regedit 找到 HKEY_CURRENT_USER\Software\SimonTatham 在 SimonTatham 这个节点上点击右键，选择导出，保存。 如果你想恢复配置信息，只需要双击保存的这个文件，导入注册表信息即可。 说明：注册表PuTTY下的Sessions中保存设置连接的项目和设定值，SshHostKeys保存设置过的Remote Host Public Key。 KeepAlive很多远程主机当你一段时间没有输入, 他就会把你踢下线, 所以需要 KeepAlive 功能, 如果填写 0 , 就表示不需要 KeepAlive 功能,填写大于 0 的数, 比如 4, 就意味着每 4 秒就会发送一个空包到远程主机来 KeepAlive .所以建议填写8秒左右的数. SSH 证书登陆配置sudo vi /etc/ssh/sshd_config 取消注释 : #AuthorizedKeysFile .ssh/authorized_keys禁止密码登录 : 修改yes-&gt;no : PasswordAuthentication no 然后重启ssh : sudo service sshd restart Putty server refused our key的三种原因和解决方法server refused our key 是非常容易遇到的错误 1、.ssh文件夹权限错或authorized_keys权限错.ssh 以及其父文件夹（root为/root，普通用户为Home目录）都应该设置为只有该用户可写（比如700）。且 设置 authorized_keys 的权限为 400 chmod 700 .ssh chmod 400 ~/.ssh/authorized_keys以下为原因：ssh服务器的key方式登录对权限要求严格。 对于客户端: 私钥必须为600权限或者更严格权限(400), 一旦其他用户可读, 私钥就不起作用(如640), 表现为系统认为不存在私钥 对于服务器端: 要求必须公钥其他用户不可写, 一旦其他用户可写(如660), 就无法用key登录, 表现为:Permission denied (publickey). 同时要求.ssh目录其他用户不可写,一旦其他用户可写(如770), 就无法使用key登录, 表现为:Permission denied (publickey). 2、SElinux导致密钥文件不能通过SElinux认证，解决方法如下： # restorecon -R -v /home #root用户为/root 我遇到的就是这种情况，找了好久还找到是这个原因，因为是新装的虚拟机，SElinux还没关闭。这篇博文详细得说明了原因：http://www.toxingwang.com/linux-unix/linux-basic/846.html 3、sshd配置不正确正确配置方法如下：/etc/ssh/sshd_config 1、找到 #StrictModes yes 改成 StrictModes no （去掉注释后改成 no） 2、找到 #PubkeyAuthentication yes 改成 PubkeyAuthentication yes （去掉注释） 3、找到 #AuthorizedKeysFile .ssh/authorized_keys 改成 AuthorizedKeysFile .ssh/authorized_keys （去掉注释） 4、保存 5、/etc/rc.d/init.d/sshd reload 重新加载]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>VBox</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 常用SIG信号及其键值]]></title>
    <url>%2Fblog%2F2015%2F08%2F04%2FLinux%20%E5%B8%B8%E7%94%A8SIG%E4%BF%A1%E5%8F%B7%E5%8F%8A%E5%85%B6%E9%94%AE%E5%80%BC%2F</url>
    <content type="text"><![CDATA[01 SIGHUP 挂起（hangup） 02 SIGINT 中断，当用户从键盘按^c键或^break键时 03 SIGQUIT 退出，当用户从键盘按quit键时 04 SIGILL 非法指令 05 SIGTRAP 跟踪陷阱（trace trap），启动进程，跟踪代码的执行 06 SIGIOT IOT指令 07 SIGEMT EMT指令 08 SIGFPE 浮点运算溢出 09 SIGKILL 杀死、终止进程 10 SIGBUS 总线错误 11 SIGSEGV 段违例（segmentation violation），进程试图去访问其虚地址空间以外的位置 12 SIGSYS 系统调用中参数错，如系统调用号非法 13 SIGPIPE 向某个非读管道中写入数据 14 SIGALRM 闹钟。当某进程希望在某时间后接收信号时发此信号 15 SIGTERM 软件终止（software termination） 16 SIGUSR1 用户自定义信号1 17 SIGUSR2 用户自定义信号2 18 SIGCLD 某个子进程死 19 SIGPWR 电源故障]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++链接性之extern和static和const的用法]]></title>
    <url>%2Fblog%2F2015%2F07%2F19%2Fcplusplus_how_to_use_extern_static_const%2F</url>
    <content type="text"><![CDATA[基本解释基本解释：extern可以置于变量或者函数前，以标示变量或者函数的定义在别的文件中，提示编译器遇到此变量和函数时在其他模块中寻找其定义。此外extern也可用来进行链接指定。 也就是说extern有两个作用: 当它与”C”一起连用时，如: extern “C” void fun(int a, int b);则告诉编译器在编译fun这个函数名时按着C的规则去翻译相应的函数名而不是C++的，C++的规则在翻译这个函数名时会把fun这个名字变得面目全非，可能是fun@aBc_int_int#%$也可能是别的，这要看编译器的”脾气”了(不同的编译器采用的方法不一样)，为什么这么做呢，因为C++支持函数的重载啊，在这里不去过多的论述这个问题，如果你有兴趣可以去网上搜索，相信你可以得到满意的解释! 当extern不与”C”在一起修饰变量或函数时，如在头文件中: extern int g_Int; 它的作用就是声明函数或全局变量的作用范围的关键字，其声明的函数和变量可以在本模块或其他模块中使用，记住它是一个声明不是定义!也就是说B模块(编译单元)要是引用模块(编译单元)A中定义的全局变量或函数时，它只要包含A模块的头文件即可,在编译阶段，模块B虽然找不到该函数或变量，但它不会报错，它会在链接时从模块A生成的目标代码中找到此函数。 extern 变量在一个源文件里定义了一个数组：char a[6];在另外一个文件里用下列语句进行了声明：extern char *a；请问，这样可以吗？答案与分析： 不可以，程序运行时会告诉你非法访问。原因在于，指向类型T的指针并不等价于类型T的数组。extern char *a声明的是一个指针变量而不是字符数组，因此与实际的定义不同，从而造成运行时非法访问。应该将声明改为extern char a[ ]。 例子分析如下，如果a[] = “abcd”,则外部变量a=0x61626364 (abcd的ASCII码值)，*a显然没有意义显然a指向的空间（0x61626364）没有意义，易出现非法内存访问。 这提示我们，在使用extern时候要严格对应声明时的格式，在实际编程中，这样的错误屡见不鲜。 extern用在变量声明中常常有这样一个作用，你在.c文件中声明了一个全局的变量，这个全局的变量如果要被引用，就放在.h中并用extern来声明。 extern “c”在C++环境下使用C函数的时候，常常会出现编译器无法找到obj模块中的C函数定义，从而导致链接失败的情况，应该如何解决这种情况呢答案与分析： C++语言在编译的时候为了解决函数的多态问题，会将函数名和参数联合起来生成一个中间的函数名称，而C语言则不会，因此会造成链接时找不到对应函数的情况，此时C函数就需要用extern “C”进行链接指定，这告诉编译器，请保持我的名称，不要给我生成用于链接的中间函数名。下面是一个标准的写法： 1234567891011121314//在.h文件的头上#ifdef __cplusplus#if __cplusplusextern "C"&#123; #endif #endif /* __cplusplus */ … … //.h文件结束的地方 #ifdef __cplusplus #if __cplusplus&#125;#endif#endif /* __cplusplus */ 定义放在头文件还是源文件中?常常见extern放在函数的前面成为函数声明的一部分，那么，C语言的关键字extern在函数的声明中起什么作用？答案与分析： 如果函数的声明中带有关键字extern，仅仅是暗示这个函数可能在别的源文件里定义，没有其它作用。即下述两个函数声明没有明显的区别：extern int f(); 和int f();当然，这样的用处还是有的，就是在程序中取代include “*.h”来声明函数，在一些复杂的项目中，我比较习惯在所有的函数声明前添加extern修饰。关于这样做的原因和利弊可见下面的这个例子：“用extern修饰的全局变量” (1)在test1.h中有下列声明 : 12345#ifndef TEST1H#define TEST1Hextern char g_str[]; // 声明全局变量g_strvoid fun1();#endif (2)在test1.cpp中 : 123#include "test1.h"char g_str[] = "123456"; // 定义全局变量g_strvoid fun1() &#123; cout &lt;&lt; g_str &lt;&lt; endl; &#125; (3)以上是test1模块， 它的编译和链接都可以通过,如果我们还有test2模块也想使用g_str,只需要在原文件中引用就可以了 : 12#include "test1.h"void fun2() &#123; cout &lt;&lt; g_str &lt;&lt; endl; &#125; 以上test1和test2可以同时编译链接通过，如果你感兴趣的话可以用ultraEdit打开test1.obj,你可以在里面找到”123456”这个字符串,但是你却不能在test2.obj里面找到，这是因为g_str是整个工程的全局变量，在内存中只存在一份,test2.obj这个编译单元不需要再有一份了，不然会在链接时报告重复定义这个错误! *(4) *有些人喜欢把全局变量的声明和定义放在一起，这样可以防止忘记了定义，如把上面test1.h改为 extern char g_str[] = &quot;123456&quot;; // 这个时候相当于没有extern 然后把test1.cpp中的g_str的定义去掉,这个时候再编译链接test1和test2两个模块时，会报链接错误，这是因为你把全局变量g_str的定义放在了头文件之后， test1.cpp这个模块包含了test1.h所以定义了一次g_str,而test2.cpp也包含了test1.h所以再一次定义了g_str,这个时候链接器在链接test1和test2时发现两个g_str。如果你非要把g_str的定义放在test1.h中的话，那么就把test2的代码中#include “test1.h”去掉 换成: 12extern char g_str[];void fun2() &#123; cout &lt;&lt; g_str &lt;&lt; endl; &#125; 这个时候编译器就知道g_str是引自于外部的一个编译模块了，不会在本模块中再重复定义一个出来，但是我想说这样做非常糟糕，因为你由于无法在test2.cpp中使用#include “test1.h”,那么test1.h中声明的其他函数你也无法使用了，除非也用都用extern修饰，这样的话你光声明的函数就要一大串，而且头文件的作用就是要给外部提供接口使用的，所以 请记住， 只在头文件中做声明，真理总是这么简单。 extern 和 static extern 表明该变量在别的地方已经定义过了,在这里要使用那个变量. static 表示静态的变量，分配内存的时候, 存储在静态区,不存储在栈上面. static 作用范围是内部链接的关系, 和extern有点相反.它和对象本身是分开存储的,extern也是分开存储的,但是extern可以被其他的对象用extern 引用,而static 不可以,只允许对象本身用它. 具体差别首先，static与extern是一对“水火不容”的家伙，也就是说extern和static不能同时修饰一个变量；其次，static修饰的全局变量声明与定义同时进行，也就是说当你在头文件中使用static声明了全局变量后，它也同时被定义了；最后，static修饰全局变量的作用域只能是本身的编译单元，也就是说它的“全局”只对本编译单元有效，其他编译单元则看不到它,如: (1)test1.h : 12345#ifndef TEST1H#define TEST1Hstatic char g_str[] = "123456"; void fun1();#endif (2)test1.cpp : 12#include "test1.h"void fun1() &#123; cout &lt;&lt; g_str &lt;&lt; endl; &#125; (3)test2.cpp : 12#include "test1.h"void fun2() &#123; cout &lt;&lt; g_str &lt;&lt; endl; &#125; 以上两个编译单元可以链接成功, 当你打开test1.obj时，你可以在它里面找到字符串”123456”,同时你也可以在test2.obj中找到它们，它们之所以可以链接成功而没有报重复定义的错误是因为虽然它们有相同的内容，但是存储的物理地址并不一样，就像是两个不同变量赋了相同的值一样，而这两个变量分别作用于它们各自的编译单元。 也许你比较较真，自己偷偷的跟踪调试上面的代码,结果你发现两个编译单元（test1,test2）的g_str的内存地址相同，于是你下结论static修饰的变量也可以作用于其他模块，但是我要告诉你，那是你的编译器在欺骗你，大多数编译器都对代码都有优化功能，以达到生成的目标程序更节省内存，执行效率更高，当编译器在链接各个编译单元的时候，它会把相同内容的内存只拷贝一份，比如上面的”123456”, 位于两个编译单元中的变量都是同样的内容，那么在链接的时候它在内存中就只会存在一份了，如果你把上面的代码改成下面的样子，你马上就可以拆穿编译器的谎言: (1)test1.cpp: 123456#include "test1.h"void fun1()&#123; g_str[0] = 'a'; cout &lt;&lt; g_str &lt;&lt; endl;&#125; (2)test2.cpp : 12#include "test1.h"void fun2() &#123; cout &lt;&lt; g_str &lt;&lt; endl; &#125; (3) : 1234void main() &#123; fun1(); // a23456 fun2(); // 123456&#125; 这个时候你在跟踪代码时，就会发现两个编译单元中的g_str地址并不相同，因为你在一处修改了它，所以编译器被强行的恢复内存的原貌，在内存中存在了两份拷贝给两个模块中的变量使用。正是因为static有以上的特性，所以一般定义static全局变量时，都把它放在原文件中而不是头文件，这样就不会给其他模块造成不必要的信息污染，同样记住这个原则吧！ extern 和constC++中const修饰的全局常量据有跟static相同的特性，即它们只能作用于本编译模块中，但是const可以与extern连用来声明该常量可以作用于其他编译模块中, 如extern const char g_str[];然后在原文件中别忘了定义: const char g_str[] = “123456”; 所以当const单独使用时它就与static相同，而当与extern一起合作的时候，它的特性就跟extern的一样了！所以对const我没有什么可以过多的描述，我只是想提醒你，const char* g_str = &quot;123456&quot; 与 const char g_str[] =&quot;123465&quot;是不同的， 前面那个const 修饰的是 char * 而不是g_str,它的g_str并不是常量，它被看做是一个定义了的全局变量（可以被其他编译单元使用）， 所以如果你像让char*g_str遵守const的全局常量的规则，最好这么定义const char* const g_str=&quot;123456&quot;.]]></content>
      <categories>
        <category>CPP</category>
      </categories>
      <tags>
        <tag>CPP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PHP的超级全局变量小结]]></title>
    <url>%2Fblog%2F2015%2F07%2F13%2FPHP%E7%9A%84%E8%B6%85%E7%BA%A7%E5%85%A8%E5%B1%80%E5%8F%98%E9%87%8F%2F</url>
    <content type="text"><![CDATA[PHP 超级全局变量概绍 PHP中预定义了几个超级全局变量（superglobals） ， 这意味着它们在一个脚本的全部作用域中都可用。 你不需要特别说明，就可以在函数及类中使用。 PHP 超级全局变量列表: $GLOBALS $_SERVER $_REQUEST $_POST $_GET $_FILES $_ENV $_COOKIE $_SESSION $GLOBALS$GLOBALS 是PHP的一个超级全局变量组， 在一个PHP脚本的全部作用域中都可以访问。 $GLOBALS 是一个包含了全部变量的全局组合数组。变量的名字就是数组的键。 以下实例介绍了如何使用超级全局变量 $GLOBALS: 123456789101112&lt;?php $x = 75; $y = 25; function addition() &#123; $GLOBALS[&apos;z&apos;] = $GLOBALS[&apos;x&apos;] + $GLOBALS[&apos;y&apos;]; &#125; addition(); echo $z; ?&gt; 以上实例中 z 是一个$GLOBALS数组中的超级全局变量，该变量同样可以在函数外访问。 $_SERVER$_SERVER 是一个包含了诸如头信息(header)、路径(path)、以及脚本位置(script locations)等等信息的数组。 这个数组中的项目由 Web 服务器创建。 不能保证每个服务器都提供全部项目；服务器可能会忽略一些，或 者提供一些没有在这里列举出来的项目。 以下实例中展示了如何使用$_SERVER中的元素: 12345678910111213&lt;?php echo $_SERVER[&apos;PHP_SELF&apos;];echo &quot;&lt;br&gt;&quot;;echo $_SERVER[&apos;SERVER_NAME&apos;];echo &quot;&lt;br&gt;&quot;;echo $_SERVER[&apos;HTTP_HOST&apos;];echo &quot;&lt;br&gt;&quot;;echo $_SERVER[&apos;HTTP_REFERER&apos;];echo &quot;&lt;br&gt;&quot;;echo $_SERVER[&apos;HTTP_USER_AGENT&apos;];echo &quot;&lt;br&gt;&quot;;echo $_SERVER[&apos;SCRIPT_NAME&apos;];?&gt; $_REQUESTPHP $_REQUEST 用于收集HTML表单提交的数据。 以下实例显示了一个输入字段（input）及提交按钮(submit)的表单(form)。 当用户通过点击 “Submit” 按钮提交表单数据时, 表单数据将发送至标签中 action 属性中指定的脚本文件。 在这个实例中，我们指定文件来处理表单数据。 如果你希望其他的PHP文件来处理该数据，你可以修改该指定的脚本文件名。 然后，我们可以使用超级全局变量 $_REQUEST 来收集表单中的 input 字段数据: 123456789101112131415 &lt;html&gt;&lt;body&gt;&lt;form method=&quot;post&quot; action=&quot;&lt;?php echo $_SERVER[&apos;PHP_SELF&apos;];?&gt;&quot;&gt;Name: &lt;input type=&quot;text&quot; name=&quot;fname&quot;&gt;&lt;input type=&quot;submit&quot;&gt;&lt;/form&gt;&lt;?php $name = $_REQUEST[&apos;fname&apos;]; echo $name; ?&gt;&lt;/body&gt;&lt;/html&gt; $_POSTPHP $_POST 被广泛应用于收集表单数据， 在HTML form标签的指定该属性：”method=”post”。 以下实例显示了一个输入字段（input）及提交按钮(submit)的表单(form)。 当用户通过点击 “Submit” 按钮提交表单数据时, 表单数据将发送至标签中 action 属性中指定的脚本文件。 在这个实例中，我们指定文件来处理表单数据。 如果你希望其他的PHP文件来处理该数据，你可以修改该指定的脚本文件名。 然后，我们可以使用超级全局变量 $_POST 来收集表单中的 input 字段数据: 123456789101112131415&lt;html&gt;&lt;body&gt;&lt;form method=&quot;post&quot; action=&quot;&lt;?php echo $_SERVER[&apos;PHP_SELF&apos;];?&gt;&quot;&gt;Name: &lt;input type=&quot;text&quot; name=&quot;fname&quot;&gt;&lt;input type=&quot;submit&quot;&gt;&lt;/form&gt;&lt;?php $name = $_POST[&apos;fname&apos;]; echo $name; ?&gt;&lt;/body&gt;&lt;/html&gt; $_GETPHP $_GET 同样被广泛应用于收集表单数据， 在HTML form标签的指定该属性：”method=”get”。 $_GET 也可以收集URL中发送的数据。 假定我们有一个包含参数的超链接HTML页面： 1234567&lt;html&gt;&lt;body&gt;&lt;a href=&quot;test_get.php?subject=PHP&amp;web=runoob.com&quot;&gt;Test $GET&lt;/a&gt;&lt;/body&gt;&lt;/html&gt; 当用户点击链接 “Test $GET”, 参数 “subject” 和 “web” 将发送至”test_get.php”, 你可以在 “test_get.php” 文件中使用 $_GET 变量来获取这些数据。 以下实例显示了 “test_get.php” 文件的代码: 123456789&lt;html&gt;&lt;body&gt;&lt;?php echo &quot;Study &quot; . $_GET[&apos;subject&apos;] . &quot; at &quot; . $_GET[&apos;web&apos;];?&gt;&lt;/body&gt;&lt;/html&gt; $_REQUEST、$_POST、$_GET的区别和联系小结1. $_REQUESTphp中$_REQUEST可以获取以POST方法和GET方法提交的数据，但是速度比较慢 2. $_GET用来获取由浏览器通过GET方法提交的数据。GET方法他是通过把参数数据加在提交表单的action属性所指的URL中，值和表单内每个字段一一对应，然后在URL中可以看到，但是有如下缺点： 安全性不好，在URL中可以看得到 传送数据量较小，不能大于2KB。 3. $_POST用来获取由浏览器通过POST方法提交的数据。POST方法他是通过HTTP POST机制，将表单的各个字段放置在HTTP HEADER内一起传送到action属性所指的URL地址中，用户看不到这个过程。他提交的大小一般来说不受限制，但是具体根据服务器的不同，还是略有不同。相对于_GET方式安全性略高 4. $_REQUEST、$_POST、$_GET 的区别和联系$_REQUEST[“参数”]具用$_POST[“参数”] $_GET[“参数”]的功能,但是$_REQUEST[“参数”]比较慢。通过post和get方法提交的所有数据都可以通过$_REQUEST数组[“参数”]获得]]></content>
      <categories>
        <category>Script</category>
      </categories>
      <tags>
        <tag>PHP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python和lua数据类型的比较]]></title>
    <url>%2Fblog%2F2015%2F07%2F11%2Fpython%E5%92%8Clua%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E7%9A%84%E6%AF%94%E8%BE%83%2F</url>
    <content type="text"><![CDATA[Python比较特殊的数据类型：List []例如 : 123456789101112#!/usr/bin/python# -*- coding: UTF-8 -*- list = [ &apos;runoob&apos;, 786 , 2.23, &apos;john&apos;, 70.2 ]tinylist = [123, &apos;john&apos;] print list # 输出完整列表print list[0] # 输出列表的第一个元素print list[1:3] # 输出第二个至第三个的元素 print list[2:] # 输出从第三个开始至列表末尾的所有元素print tinylist * 2 # 输出列表两次print list + tinylist # 打印组合的列表 以上实例输出结果： 123456[&apos;runoob&apos;, 786, 2.23, &apos;john&apos;, 70.2]runoob[786, 2.23][2.23, &apos;john&apos;, 70.2][123, &apos;john&apos;, 123, &apos;john&apos;][&apos;runoob&apos;, 786, 2.23, &apos;john&apos;, 70.2, 123, &apos;john&apos;] Tuple（元祖）(),相当于只读列表，不可以二次赋值tuple = ( &#39;runoob&#39;, 786 , 2.23, &#39;john&#39;, 70.2 ), 除了元祖用()而list用[], 而且元祖只是可读的, 其他的跟list一毛一样 dictionary（字典）{}，key值对123456789101112131415#!/usr/bin/python# -*- coding: UTF-8 -*- dict = &#123;&#125;dict[&apos;one&apos;] = &quot;This is one&quot;dict[2] = &quot;This is two&quot; tinydict = &#123;&apos;name&apos;: &apos;john&apos;,&apos;code&apos;:6734, &apos;dept&apos;: &apos;sales&apos;&#125; print dict[&apos;one&apos;] # 输出键为&apos;one&apos; 的值print dict[2] # 输出键为 2 的值print tinydict # 输出完整的字典print tinydict.keys() # 输出所有键print tinydict.values() # 输出所有值 输出结果为: 12345This is oneThis is two&#123;&apos;dept&apos;: &apos;sales&apos;, &apos;code&apos;: 6734, &apos;name&apos;: &apos;john&apos;&#125;[&apos;dept&apos;, &apos;code&apos;, &apos;name&apos;][&apos;sales&apos;, 6734, &apos;john&apos;] lua比较特殊的数据类型lua变量 变量在使用前，必须在代码中进行声明，即创建该变量。 编译程序执行代码之前编译器需要知道如何给语句变量开辟存储区，用于存储变量的值。 Lua 变量有三种类型：全局变量、局部变量、表中的域。 Lua 中的变量全是全局变量，那怕是语句块或是函数里，除非用 local 显式声明为局部变量。 局部变量的作用域为从声明位置开始到所在语句块结束。 变量的默认值均为 nil。 test.lua 文件脚本123456789101112131415161718a = 5 -- 全局变量local b = 5 -- 局部变量function joke() c = 5 -- 全局变量 local d = 6 -- 局部变量endjoke()print(c,d) --&gt; 5 nildo local a = 6 -- 局部变量 b = 6 -- 全局变量 print(a,b); --&gt; 6 6endprint(a,b) --&gt; 5 6 执行以上实例输出结果为： 1234$ lua test.lua 5 nil6 65 6 lua的特有的东西table（表）在 Lua 里，table 的创建是通过”构造表达式”来完成， 最简单构造表达式是{}，用来创建一个空表。 也可以在表里添加一些数据，直接初始化表: 12345-- 创建一个空的 tablelocal tbl1 = &#123;&#125; -- 直接初始表local tbl2 = &#123;&quot;apple&quot;, &quot;pear&quot;, &quot;orange&quot;, &quot;grape&quot;&#125; Lua 中的表（table）其实是一个”关联数组”（associative arrays），数组的索引可以是数字或者是字符串。 123456789-- table_test.lua 脚本文件a = &#123;&#125;a[&quot;key&quot;] = &quot;value&quot;key = 10a[key] = 22a[key] = a[key] + 11for k, v in pairs(a) do print(k .. &quot; : &quot; .. v)end 脚本执行结果为： 123$ lua table_test.lua key : value10 : 33 不同于其他语言的数组把 0 作为数组的初始索引，在 Lua 里表的默认初始索引一般以 1 开始。 12345-- table_test2.lua 脚本文件local tbl = &#123;&quot;apple&quot;, &quot;pear&quot;, &quot;orange&quot;, &quot;grape&quot;&#125;for key, val in pairs(tbl) do print(&quot;Key&quot;, key)end 脚本执行结果为： 12345$ lua table_test2.lua Key 1Key 2Key 3Key 4 table 不会固定长度大小，有新数据添加时 table 长度会自动增长，没初始的 table 都是 nil。 12345678-- table_test3.lua 脚本文件a3 = &#123;&#125;for i = 1, 10 do a3[i] = ienda3[&quot;key&quot;] = &quot;val&quot;print(a3[&quot;key&quot;])print(a3[&quot;none&quot;]) 脚本执行结果为： 123$ lua table_test3.lua valnil]]></content>
      <categories>
        <category>Script</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Lua</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis和hiredis安装教程]]></title>
    <url>%2Fblog%2F2015%2F07%2F11%2Fredis_hiredis_install_tutorial%2F</url>
    <content type="text"><![CDATA[在ubuntu上 redis安装 : sudo apt-get install redis-server hiredis安装 : 先到 https://github.com/redis/hiredis 下载 hiredis , 然后 sudo make sudo make install sudo ldconfig]]></content>
      <categories>
        <category>DB</category>
      </categories>
      <tags>
        <tag>Redis</tag>
        <tag>NoSQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis概要之数据类型]]></title>
    <url>%2Fblog%2F2015%2F07%2F11%2Fredis%E6%A6%82%E8%A6%81%E4%B9%8B%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[Redis简介要义 Redis运行在内存中但是可以持久化到磁盘, 重启的时候可以再次加载进行使用 Redis的所有操作都是原子性的，同时Redis还支持对几个操作全并后的原子性执行 Redis支持二进制案例的 Strings, Lists, Hashes, Sets 及 Ordered Sets 数据类型操作 Redis支持数据的备份，即master-slave模式的数据备份 Redis数据类型Redis的数据类型很重要, 这是他做很多事情的基础, 不理解的话很难用好 . . . String(字符串)string是redis最基本的类型，你可以理解成与Memcached一模一样的类型， 一个key对应一个value。 string类型是二进制安全的。 二进制安全的意思是redis的string可以包含任何数据。 比如jpg图片或者序列化的对象 。 string类型是Redis最基本的数据类型，一个键最大能存储512MB。 实例 : 1234redis 127.0.0.1:6379&gt; SET name &quot;runoob&quot;OKredis 127.0.0.1:6379&gt; GET name&quot;runoob&quot; Hash（哈希表）Redis hash 是一个键名对集合。Redis hash是一个string类型的field和value的映射表，hash特别适合用于存储对象。 实例 : 1234567891011redis&gt; HSET people jack &quot;Jack Sparrow&quot;(integer) 1redis&gt; HSET people gump &quot;Forrest Gump&quot;(integer) 1redis&gt; HGETALL people1) &quot;jack&quot; # 域2) &quot;Jack Sparrow&quot; # 值3) &quot;gump&quot;4) &quot;Forrest Gump&quot; 1234567redis&gt; HMSET pet dog &quot;doudou&quot; cat &quot;nounou&quot; # 一次设置多个域OKredis&gt; HMGET pet dog cat fake_pet # 返回值的顺序和传入参数的顺序一样1) &quot;doudou&quot;2) &quot;nounou&quot;3) (nil) # 不存在的域返回nil值 12345678redis&gt; HMSET website google www.google.com yahoo www.yahoo.comOKredis&gt; HGET website google&quot;www.google.com&quot;redis&gt; HGET website yahoo&quot;www.yahoo.com&quot; 12345redis&gt; HSET website google &quot;www.g.cn&quot; # 设置一个新域(integer) 1redis&gt; HSET website google &quot;www.google.com&quot; # 覆盖一个旧域(integer) 0 12345678910111213# 域存在redis&gt; HSET site redis redis.com(integer) 1redis&gt; HGET site redis&quot;redis.com&quot;# 域不存在redis&gt; HGET site mysql(nil) 以上实例中 hash 数据类型存储了包含用户脚本信息的用户对象。每个 hash 可以存储 232 -1 键值对（40多亿）。 List（列表）Redis 列表是简单的字符串列表，按照插入顺序排序。 你可以添加一个元素到列表的头部（左边lpush）或者尾部（右边rpush）。 实例 : 1234567891011redis 127.0.0.1:6379&gt; lpush runoob redis(integer) 1redis 127.0.0.1:6379&gt; rpush runoob mongodb(integer) 2redis 127.0.0.1:6379&gt; lpush runoob rabitmq(integer) 3redis 127.0.0.1:6379&gt; lrange runoob 0 101) &quot;rabitmq&quot;2) &quot;redis&quot;3) &quot;mongodb&quot;redis 127.0.0.1:6379&gt; 列表最多可存储 232 - 1 元素 (4294967295, 每个列表可存储40多亿)。 Set（集合）Redis的Set是string类型的无序集合。 集合是通过哈希表实现的，所以添加，删除，查找的复杂度都是O(1)。 sadd 命令添加一个string元素到,key对应的set集合中，成功返回1, 如果元素已经在集合中返回0,key对应的set不存在返回错误。 sadd key member 实例 : 12345678910111213redis 127.0.0.1:6379&gt; sadd runoob redis(integer) 1redis 127.0.0.1:6379&gt; sadd runoob mongodb(integer) 1redis 127.0.0.1:6379&gt; sadd runoob rabitmq(integer) 1redis 127.0.0.1:6379&gt; sadd runoob rabitmq(integer) 0redis 127.0.0.1:6379&gt; smembers runoob1) &quot;rabitmq&quot;2) &quot;mongodb&quot;3) &quot;redis&quot; 注意：以上实例中 rabitmq 添加了两次， 但根据集合内元素的唯一性，第二次插入的元素将被忽略。 集合中最大的成员数为 232 - 1(4294967295, 每个集合可存储40多亿个成员)。 zset(sorted set：有序集合)Redis zset 和 set 一样也是string类型元素的集合,且不允许重复的成员。 不同的是每个元素都会关联一个double类型的分数。redis正是通过分数来为集合中的成员进行从小到大的排序。 zset的成员是唯一的,但分数(score)却可以重复。 zadd 命令添加元素到集合，元素在集合中存在则更新对应scorezadd key score member实例 : 12345678910111213redis 127.0.0.1:6379&gt; zadd runoob 1 redis(integer) 1redis 127.0.0.1:6379&gt; zadd runoob 3 mongodb(integer) 1redis 127.0.0.1:6379&gt; zadd runoob 4 rabitmq(integer) 1redis 127.0.0.1:6379&gt; zadd runoob 7 rabitmq(integer) 0redis 127.0.0.1:6379&gt; ZRANGEBYSCORE runoob 0 10001) &quot;redis&quot;2) &quot;mongodb&quot;3) &quot;rabitmq&quot;]]></content>
      <categories>
        <category>DB</category>
      </categories>
      <tags>
        <tag>Redis</tag>
        <tag>NoSQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[epoll扼要总结]]></title>
    <url>%2Fblog%2F2015%2F06%2F22%2Fepoll%E6%89%BC%E8%A6%81%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[epoll 编程接口epoll API是Linux系统专有的，在2.6版中新增。 epoll API的核心数据结构称作epoll实例，它和一个打开的文件描述符相关联。这个文件描述符不是用来做I/O操作的，相反，它是内核数据结构的句柄，这些内核数据结构实现了两个目的。 记录了在进程中声明过的感兴趣的文件描述符列表-interest list（兴趣列表）。 维护了处于I/O就绪态的文件描述符列表-ready list（就绪列表）。 ready list中的成员是interest list的子集。 对于由epoll检查的每一个文件描述符，我们可以指定一个位掩码来表示我们感兴趣的事件。这些位掩码同poll()所使用的位掩码有着紧密的关联。 . . . epoll概要需要包含epoll.h头文件, 即 : #include &lt;sys/epoll.h&gt;epoll只有epoll_create, epoll_ctl, epoll_wait 3个系统调用 : 系统调用epoll_create()创建一个epoll实例，返回代表该实例的文件描述符。 系统调用epoll_ctl()操作同epoll实例相关联的兴趣列表。通过epoll_ctl()，我们可以增加新的描述符到列表中，或者将已有的文件描述符从该列表中移除，也可以修改代表文件描述符七事件类型的位掩码。 系统调用epoll_wait()返回与epoll实例相关联的就绪列表中的成员。 epoll_createint epoll_create(int size);Returns file descriptor on success, or -1 on error. 创建一个epoll的句柄。自从linux2.6.8之后，size参数是被忽略的。函数返回代表新创建的 epoll 实例的文件描述符(一般用 epfd表示), 这个文件描述符在其他几个 epoll 系统调用中用来表示 epoll 实例.需要注意的是，当创建好epoll句柄后，它就是会占用一个fd值，在linux下如果查看/proc/进程id/fd/，是能够看到这个fd的，所以在使用完epoll后，必须调用close()关闭，否则可能导致fd被耗尽。 epoll_ctlint epoll_ctl(int epfd, int op, int fd, struct epoll_event *ev);Returns 0 on success, or -1 on error. epoll的事件注册函数，它不同于select()是在监听事件时告诉内核要监听什么类型的事件，而是在这里先注册要监听的事件类型。 第一个参数是epoll_create()的返回值, 也就是新创建的 epoll 实例的文件描述符。 第二个参数表示动作，用三个宏来表示： EPOLL_CTL_ADD：注册新的fd到epfd中的兴趣列表； EPOLL_CTL_MOD：修改已经注册的fd的设定事件； EPOLL_CTL_DEL：从epfd的兴趣列表中删除一个fd； 第三个参数指要修改兴趣列表中的哪一个文件描述符的设定。 第四个参数是告诉内核需要监听什么事，参数ev是指向结构体epoll_event的指针, struct epoll_event结构如下： 1234struct epoll_event &#123; __uint32_t events; /* Epoll events */ epoll_data_t data; /* User data variable */ &#125;; 结构体epoll_event中的data字段的类型为: 123456typedef union epoll_data &#123; void *ptr; /* Pointer to user-defined data */ int fd; __uint32_t u32; __uint64_t u64; &#125; epoll_data_t; 结构体epoll_event中的events字段是一个位掩码, 他指定了我们为待检查的描述符fd上所感兴趣的事件集合.可以是以下几个宏的集合： EPOLLIN ：表示对应的文件描述符可以读（包括对端SOCKET正常关闭）； EPOLLOUT：表示对应的文件描述符可以写； EPOLLPRI：表示对应的文件描述符有紧急的数据可读（这里应该表示有带外数据到来）； EPOLLERR：表示对应的文件描述符发生错误； EPOLLHUP：表示对应的文件描述符被挂断； EPOLLET： 将EPOLL设为边缘触发(Edge Triggered)模式，这是相对于水平触发(Level Triggered)来说的。(后文会说水平触发和边缘触发的区别) EPOLLONESHOT：只监听一次事件，当监听完这次事件之后，如果还需要继续监听这个socket的话，需要再次把这个socket加入到EPOLL队列里 data 字段是一个联合体, 当描述符 fd 稍后成为就绪态时, 联合体的成员可用来指定传回给调用进程的信息 使用 epoll_create() 和 epoll_ctl()的例子使用 epoll_create() 和 epoll_ctl() 1234567891011int epdf;struct epoll_event ev;epfd = epoll_create( 5 );if ( epfd == -1 ) errExit( "epoll_create" );ev.data.fd = fd;ev.events = EPOLLIN;if ( epoll_ctl( epofd, EPOLL_CTL_ADD, fd, ev ) == -1 ) errExit( "epoll_ctl" ); epoll_waitint epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout);Returns number of ready file descriptors, 0 on timeout, or -1 on error. epoll_wait收集在epoll监控的事件中已经发送的事件。 参数events是分配好的epoll_event结构体数组，epoll将会把发生的事件赋值到events数组中（events不可以是空指针，内核只负责把数据复制到这个events数组中，不会去帮助我们在用户态中分配内存）。 maxevents告之内核这个events有多大，这个 maxevents的值不能大于创建epoll_create()时的size， 参数timeout是超时时间, 用来确定 epoll_wait() 的阻塞行为, 有如下几种 : 如果 timeout 等于 -1, 调用将一直阻塞, 直到兴趣列表中的文件描述符有事件产生, 或者直到捕获到一个信号为止 如果 timeout 等于 0, 执行一次非阻塞的检查, 立即返回, 看兴趣列表中的文件描述符上产生了哪个事件 如果 timeout 大于 0, 调用将阻塞至多 timeout 毫秒, 知道文件描述符上有事件产生, 或者直到捕获到一个信号为止 TLPI书上的例子123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114/*************************************************************************\* Copyright (C) Michael Kerrisk, 2017. ** ** This program is free software. You may use, modify, and redistribute it ** under the terms of the GNU General Public License as published by the ** Free Software Foundation, either version 3 or (at your option) any ** later version. This program is distributed without any warranty. See ** the file COPYING.gpl-v3 for details. *\*************************************************************************//* Listing 63-5 *//* epoll_input.c Example of the use of the Linux epoll API. Usage: epoll_input file... This program opens all of the files named in its command-line arguments and monitors the resulting file descriptors for input events. This program is Linux (2.6 and later) specific.*/#include &lt;sys/epoll.h&gt;#include &lt;fcntl.h&gt;#include "tlpi_hdr.h"#define MAX_BUF 1000 /* Maximum bytes fetched by a single read() */#define MAX_EVENTS 5 /* Maximum number of events to be returned from a single epoll_wait() call */intmain(int argc, char *argv[])&#123; int epfd, ready, fd, s, j, numOpenFds; struct epoll_event ev; struct epoll_event evlist[MAX_EVENTS]; char buf[MAX_BUF]; if (argc &lt; 2 || strcmp(argv[1], "--help") == 0) usageErr("%s file...\n", argv[0]); epfd = epoll_create(argc - 1); if (epfd == -1) errExit("epoll_create"); /* Open each file on command line, and add it to the "interest list" for the epoll instance */ for (j = 1; j &lt; argc; j++) &#123; fd = open(argv[j], O_RDONLY); if (fd == -1) errExit("open"); printf("Opened \"%s\" on fd %d\n", argv[j], fd); ev.events = EPOLLIN; /* Only interested in input events */ ev.data.fd = fd; if (epoll_ctl(epfd, EPOLL_CTL_ADD, fd, &amp;ev) == -1) errExit("epoll_ctl"); &#125; numOpenFds = argc - 1; while (numOpenFds &gt; 0) &#123; /* Fetch up to MAX_EVENTS items from the ready list of the epoll instance */ printf("About to epoll_wait()\n"); ready = epoll_wait(epfd, evlist, MAX_EVENTS, -1); if (ready == -1) &#123; if (errno == EINTR) continue; /* Restart if interrupted by signal */ else errExit("epoll_wait"); &#125; printf("Ready: %d\n", ready); /* Deal with returned list of events */ for (j = 0; j &lt; ready; j++) &#123; printf(" fd=%d; events: %s%s%s\n", evlist[j].data.fd, (evlist[j].events &amp; EPOLLIN) ? "EPOLLIN " : "", (evlist[j].events &amp; EPOLLHUP) ? "EPOLLHUP " : "", (evlist[j].events &amp; EPOLLERR) ? "EPOLLERR " : ""); if (evlist[j].events &amp; EPOLLIN) &#123; s = read(evlist[j].data.fd, buf, MAX_BUF); if (s == -1) errExit("read"); printf(" read %d bytes: %.*s\n", s, s, buf); &#125; else if (evlist[j].events &amp; (EPOLLHUP | EPOLLERR)) &#123; /* After the epoll_wait(), EPOLLIN and EPOLLHUP may both have been set. But we'll only get here, and thus close the file descriptor, if EPOLLIN was not set. This ensures that all outstanding input (possibly more than MAX_BUF bytes) is consumed (by further loop iterations) before the file descriptor is closed. */ printf(" closing fd %d\n", evlist[j].data.fd); // 关闭一个文件描述符会自动的将其从所有的 epoll 实例的兴趣列表中移除 if (close(evlist[j].data.fd) == -1) errExit("close"); numOpenFds--; &#125; &#125; &#125; printf("All file descriptors closed; bye\n"); exit(EXIT_SUCCESS);&#125; 水平触发与边缘触发在深入讨论多种可选的机制之前，我们需要先区分两种文件描述符准备就绪的通知模式。 水平触发通知(epoll默认的通知方式)：如果文件描述符上可以非阻塞地执行I/O系统调用，此时认为它已经就绪。 边缘触发通知：如果文件描述符自上次状态检查以来有了新的I/O活动（比如新的输入），此时需要触发通知。 下图总结了I/O多路复用、信号驱动I/O以及epoll所采用的通知模型。epoll API同其他两种I/O模型的区别在于它对水平触发（默认）和边缘触发都支持。 水平触发与边缘触发的区别默认情况下 epoll 提供的是水平触发通知.要使用边缘触发通知，我们在调用epoll_ctl()时在ev．events字段中指定EPOLLET标志. 例如 : 12345struct epoll_event ev;ev.data.fd = fd;ev.events = EPOLLIN | EPOLLET;if (epoll_ctl(epfd, EPOLL_CTL_ADD, fd, ev) == -1) errExit("epoll_ctl"); 我们通过一个例子来说明epoll的水平触发和边缘触发通知之间的区别。假设我们使用epoll来监视一个套接字上的输入（EPOLLIN），接下来会发生如下的事件。 1．套接字上有输入到来。2．我们调用一次epoll_wait()。无论我们采用的是水平触发还是边缘触发通知，该调用都会告诉我们套接字已经处于就绪态了。3．再次调用epoll_wait()。 如果我们采用的是水平触发通知，那么第二个epoll_wait()调用将告诉我们套接字处于就绪态。而如果我们采用边缘触发通知，那么第二个epoll_wait()调用将阻塞，因为自从上一次调用epoll_wait()以来并没有新的输入到来。 边缘触发通知通常和非阻塞的文件描述符结合使用。因而，采用epoll的边缘触发通知机制的程序基本框架如下: 1．让所有待监视的文件描述符都成为非阻塞的。2．通过epoll_ctl()构建epoll的兴趣列表。3．通过如下的循环处理I/O事件 :(a)通过epoll_wait()取得处于就绪态的描述符列表。(b)针对每一个处于就绪态的文件描述符，不断进行I/O处理直到相关的系统调用( 例如read()、write()，recv()、send()或accept() )返回EAGAIN或EWOULDBLOCK错误。 epoll与select/poll的区别select函数，必须得清楚select跟linux特有的epoll的区别， 有三点(遍内树)： 遍历 ： 每次调用select都需要在内核遍历传递进来的所有fd，这个开销在fd很多时也很大；epoll只在epoll_ctl时为每个fd指定一个回调函数，当设备就绪，唤醒等待队列上的等待者时，就会调用这个回调函数，而这个回调函数会把就绪的fd加入一个就绪链表）。epoll_wait的工作实际上就是在这个就绪链表中查看有没有就绪的fd, 每次只需要简单的从列表里取出就行了 内存拷贝 ： select，poll每次调用都要把fd集合从用户态往内核态拷贝一次; epoll的解决方案在epoll_ctl函数中。每次注册新的事件到epoll句柄中时（在epoll_ctl中指定EPOLL_CTL_ADD），会把所有的fd拷贝进内核，而不是在epoll_wait的时候重复拷贝。epoll保证了每个fd在整个过程中只会拷贝一次 数量限制 ： select默认只支持1024个；epoll并没有最大数目限制 总结： （1）select，poll实现需要自己不断轮询所有fd集合，直到设备就绪，期间可能要睡眠和唤醒多次交替。而epoll其实也需要调用epoll_wait不断轮询就绪链表，期间也可能多次睡眠和唤醒交替，但是它是设备就绪时，调用回调函数，把就绪fd放入就绪链表中，并唤醒在epoll_wait中进入睡眠的进程。虽然都要睡眠和交替，但是select和poll在“醒着”的时候要遍历整个fd集合，而epoll在“醒着”的时候只要判断一下就绪链表是否为空就行了，这节省了大量的CPU时间。这就是回调机制带来的性能提升。 （2）select，poll每次调用都要把fd集合从用户态往内核态拷贝一次，并且要把current往设备等待队列中挂一次，而epoll只要一次拷贝，而且把current往等待队列上挂也只挂一次（在epoll_wait的开始，注意这里的等待队列并不是设备等待队列，只是一个epoll内部定义的等待队列）。这也能节省不少的开销。 常见的epoll编程模型我们知道，服务器并发模型通常可分为单线程和多线程模型，这里的线程通常是指“I/O线程”，即负责I/O操作，协调分配任务的“管理线程”，而实际的请求和任务通常交由所谓“工作者线程”处理。 通常多线程模型下，每个线程既是I/O线程又是工作者线程。 所以这里讨论的是，单I/O线程+多工作者线程的模型，这也是最常用的一种服务器并发模型。 我所在的项目中的server代码中，这种模型随处可见。 它还有个名字，叫“半同步/半异步“模型，同时，这种模型也是生产者/消费者（尤其是多消费者）模型的一种表现。 这种架构主要是基于I/O多路复用的思想（主要是epoll，select/poll已过时），通过单线程I/O多路复用，可以达到高效并发，同时避免了多线程I/O来回切换的各种开销，思路清晰，易于管理，而基于线程池的多工作者线程，又可以充分发挥和利用多线程的优势，利用线程池，进一步提高资源复用性和避免产生过多线程。 单I/O线程epoll实现单I/O线程的epoll模型是本架构的第一个技术要点，主要思想如下： 单线程创建epoll并等待，有I/O请求（socket）到达时，将其加入epoll并从线程池中取一个空闲工作者线程，将实际的任务交由工作者线程处理。 伪码： 1234567891011121314151617181920212223创建一个epoll实例;while(server running)&#123; epoll等待事件; if(新连接到达且是有效连接) &#123; accept此连接; 将此连接设置为non-blocking; 为此连接设置event(EPOLLIN | EPOLLET ...); 将此连接加入epoll监听队列; 从线程池取一个空闲工作者线程并处理此连接; &#125; else if(读请求) &#123; 从线程池取一个空闲工作者线程并处理读请求; &#125; else if(写请求) &#123; 从线程池取一个空闲工作者线程并处理写请求; &#125; else 其他事件; &#125; 伪码可能写的不太好，其实就是基本的epoll使用, 大概如下 : 1234567891011121314151617181920212223242526272829303132333435for( ; ; ) &#123; nfds = epoll_wait(epfd,events,20,500); for(i=0;i&lt;nfds;++i) &#123; if(events[i].data.fd==listenfd) //有新的连接 &#123; connfd = accept(listenfd,(sockaddr *)&amp;clientaddr, &amp;clilen); //accept这个连接 ev.data.fd=connfd; ev.events=EPOLLIN|EPOLLET; epoll_ctl(epfd,EPOLL_CTL_ADD,connfd,&amp;ev); //将新的fd添加到epoll的监听队列中 &#125; else if( events[i].events&amp;EPOLLIN ) //接收到数据，读socket &#123; n = read(sockfd, line, MAXLINE)) &lt; 0 //读 ev.data.ptr = md; //md为自定义类型，添加数据 ev.events=EPOLLOUT|EPOLLET; epoll_ctl(epfd,EPOLL_CTL_MOD,sockfd,&amp;ev);//修改标识符，等待下一个循环时发送数据，异步处理的精髓 &#125; else if(events[i].events&amp;EPOLLOUT) //有数据待发送，写socket &#123; struct myepoll_data* md = (myepoll_data*)events[i].data.ptr; //取数据 sockfd = md-&gt;fd; send( sockfd, md-&gt;ptr, strlen((char*)md-&gt;ptr), 0 ); //发送数据 ev.data.fd=sockfd; ev.events=EPOLLIN|EPOLLET; epoll_ctl(epfd,EPOLL_CTL_MOD,sockfd,&amp;ev); //修改标识符，等待下一个循环时接收数据 &#125; else &#123; //其他的处理 &#125; &#125; &#125; 但要注意和线程池的配合使用，如果线程池取不到空闲的工作者线程，还需要做一些处理。 线程池实现要点server启动时，创建一定数量的工作者线程加入线程池，如（20个），供I/O线程来取用； 每当I/O线程请求空闲工作者线程时，从池中取出一个空闲工作者线程，处理相应请求； 当请求处理完毕，关闭相应I/O连接时，回收相应线程并放回线程池中供下次使用； 若请求空闲工作者线程池时，没有空闲工作者线程，可作如下处理： 若池中”管理”的线程总数不超过最大允许值，可创建一批新的工作者线程加入池中，并返回其中一个供I/O线程使用； 若池中”管理”的线程总数已经达到最大值，不应再继续创建新线程， 则等待一小段时间并重试。注意因为I/O线程是单线程且不应被阻塞等待在此处，所以其实对线程池的管理应由一个专门的管理线程完成，包括创建新工作者线程等工作。此时管理线程阻塞等待（如使用条件变量并等待唤醒），一小段时间之后，线程池中应有空闲工作者线程可使用。否则server负荷估计是出了问题。 epoll代码实例代码来自互联网, 有疏漏, 也有命名不规范之处, 用于理解一下范式, 大概看看即可 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278#include &lt;iostream&gt;#include &lt;sys/socket.h&gt;#include &lt;sys/epoll.h&gt;#include &lt;netinet/in.h&gt;#include &lt;arpa/inet.h&gt;#include &lt;fcntl.h&gt;#include &lt;unistd.h&gt;#include &lt;stdio.h&gt;#include &lt;pthread.h&gt;#include &lt;errno.h&gt;#define MAXLINE 10#define OPEN_MAX 100#define LISTENQ 20#define SERV_PORT 8006#define INFTIM 1000//线程池任务队列结构体struct task&#123; int fd; //需要读写的文件描述符 struct task *next; //下一个任务&#125;;//用于读写两个的两个方面传递参数struct user_data&#123; int fd; unsigned int n_size; char line[MAXLINE];&#125;;//线程的任务函数void *readtask(void *args);void *writetask(void *args);//声明epoll_event结构体的变量,ev用于注册事件,数组用于回传要处理的事件struct epoll_event ev, events[20];int epfd;pthread_mutex_t mutex;pthread_cond_t cond1;struct task *readhead = NULL, *readtail = NULL, *writehead = NULL;void setnonblocking(int sock)&#123; int opts; opts = fcntl(sock, F_GETFL); if (opts &lt; 0) &#123; perror("fcntl(sock,GETFL)"); exit(1); &#125; opts = opts | O_NONBLOCK; if (fcntl(sock, F_SETFL, opts) &lt; 0) &#123; perror("fcntl(sock,SETFL,opts)"); exit(1); &#125;&#125;int main()&#123; int i, maxi, listenfd, connfd, sockfd, nfds; pthread_t tid1, tid2; struct task *new_task = NULL; struct user_data *rdata = NULL; socklen_t clilen; pthread_mutex_init(&amp;mutex, NULL); pthread_cond_init(&amp;cond1, NULL); //初始化用于读线程池的线程 pthread_create(&amp;tid1, NULL, readtask, NULL); pthread_create(&amp;tid2, NULL, readtask, NULL); //生成用于处理accept的epoll专用的文件描述符 epfd = epoll_create(256); struct sockaddr_in clientaddr; struct sockaddr_in serveraddr; listenfd = socket(AF_INET, SOCK_STREAM, 0); //把socket设置为非阻塞方式 setnonblocking(listenfd); //设置与要处理的事件相关的文件描述符 ev.data.fd = listenfd; //设置要处理的事件类型 ev.events = EPOLLIN | EPOLLET; //注册epoll事件 epoll_ctl(epfd, EPOLL_CTL_ADD, listenfd, &amp;ev); bzero(&amp;serveraddr, sizeof(serveraddr)); serveraddr.sin_family = AF_INET; serveraddr.sin_port = htons(SERV_PORT); serveraddr.sin_addr.s_addr = INADDR_ANY; bind(listenfd, (sockaddr *)&amp;serveraddr, sizeof(serveraddr)); listen(listenfd, LISTENQ); maxi = 0; for (;;) &#123; //等待epoll事件的发生 nfds = epoll_wait(epfd, events, 20, 500); //处理所发生的所有事件 for (i = 0; i &lt; nfds; ++i) &#123; if (events[i].data.fd == listenfd) &#123; connfd = accept(listenfd, (sockaddr *)&amp;clientaddr, &amp;clilen); if (connfd &lt; 0) &#123; perror("connfd&lt;0"); exit(1); &#125; setnonblocking(connfd); char *str = inet_ntoa(clientaddr.sin_addr); //std::cout&lt;&lt;"connec_ from &gt;&gt;"&lt;&lt;str&lt;&lt;std::endl; //设置用于读操作的文件描述符 ev.data.fd = connfd; //设置用于注册的读操作事件 ev.events = EPOLLIN | EPOLLET; //注册ev epoll_ctl(epfd, EPOLL_CTL_ADD, connfd, &amp;ev); &#125; else if (events[i].events &amp; EPOLLIN) // 读请求 &#123; //printf("reading!/n"); if ((sockfd = events[i].data.fd) &lt; 0) continue; new_task = new task(); new_task-&gt;fd = sockfd; new_task-&gt;next = NULL; //添加新的读任务 pthread_mutex_lock(&amp;mutex); if (readhead == NULL) &#123; readhead = new_task; readtail = new_task; &#125; else &#123; readtail-&gt;next = new_task; readtail = new_task; &#125; //唤醒所有等待cond1条件的线程 pthread_cond_broadcast(&amp;cond1); pthread_mutex_unlock(&amp;mutex); &#125; else if (events[i].events &amp; EPOLLOUT) // 写请求 &#123; // rdata=(struct user_data *)events[i].data.ptr; // sockfd = rdata-&gt;fd; // write(sockfd, rdata-&gt;line, rdata-&gt;n_size); // delete rdata; // //设置用于读操作的文件描述符 // ev.data.fd=sockfd; // //设置用于注测的读操作事件 // ev.events=EPOLLIN|EPOLLET; // //修改sockfd上要处理的事件为EPOLIN // epoll_ctl(epfd,EPOLL_CTL_MOD,sockfd,&amp;ev); &#125; &#125; &#125;&#125;static int count111 = 0;static time_t oldtime = 0, nowtime = 0;void *readtask(void *args)&#123; int fd = -1; unsigned int n; //用于把读出来的数据传递出去 struct user_data *data = NULL; while (1) &#123; pthread_mutex_lock(&amp;mutex); //等待到任务队列不为空 while (readhead == NULL) pthread_cond_wait(&amp;cond1, &amp;mutex); fd = readhead-&gt;fd; //从任务队列取出一个读任务 struct task *tmp = readhead; readhead = readhead-&gt;next; delete tmp; pthread_mutex_unlock(&amp;mutex); data = new user_data(); data-&gt;fd = fd; char recvBuf[1024] = &#123;0&#125;; int ret = 999; int rs = 1; while (rs) &#123; ret = recv(fd, recvBuf, 1024, 0); // 接受客户端消息 if (ret &lt; 0) &#123; //由于是非阻塞的模式,所以当errno为EAGAIN时,表示当前缓冲区已无数据可 //读在这里就当作是该次事件已处理过。 if (errno == EAGAIN) &#123; printf("EAGAIN\n"); break; &#125; else &#123; printf("recv error!\n"); close(fd); break; &#125; &#125; else if (ret == 0) &#123; // 这里表示对端的socket已正常关闭. rs = 0; &#125; if (ret == sizeof(recvBuf)) rs = 1; // 需要再次读取 else rs = 0; &#125; if (ret &gt; 0) &#123; //------------------------------------------------------------------------------- // 业务代码 data-&gt;n_size = n; count111++; struct tm *today; time_t ltime; time(&amp;nowtime); if (nowtime != oldtime) &#123; printf("%d\n", count111); oldtime = nowtime; count111 = 0; &#125; char buf[1000] = &#123;0&#125;; sprintf(buf, "HTTP/1.0 200 OK\r\nContent-type: text/plain\r\n\r\n%s", "Hello world!\n"); send(fd, buf, strlen(buf), 0); close(fd); &#125; &#125;&#125;]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[socket可读可写条件与非阻塞connect或accept浅析]]></title>
    <url>%2Fblog%2F2015%2F06%2F22%2Fsocket%E5%8F%AF%E8%AF%BB%E5%8F%AF%E5%86%99%E6%9D%A1%E4%BB%B6%E4%B8%8E%E9%9D%9E%E9%98%BB%E5%A1%9Econnect%E6%88%96accept%E6%B5%85%E6%9E%90%2F</url>
    <content type="text"><![CDATA[socket可读的条件: socket的接收缓冲区中的数据字节大于等于该socket的接收缓冲区低水位标记的当前大小。对这样的socket的读操作将不阻塞并返回一个大于0的值(也就是返回准备好读入的数据)。我们可以用SO_RCVLOWAT这个socket选项来设置该socket的低水位标记。对于TCP和UDP的socket而言，其缺省值为1. 该连接的读这一半关闭(也就是接收了FIN的TCP连接)。对这样的socket的读操作将不阻塞并返回0 给监听套接字准备好新连接 有一个socket有异常错误条件待处理.对于这样的socket的读操作将不会阻塞,并且返回一个错误(-1),errno则设置成明确的错误条件.这些待处理的错误也可通过指定socket选项SO_ERROR调用getsockopt来取得并清除; . . . socket可写的条件: socket的发送缓冲区中的可用空间字节数大于等于该socket的发送缓冲区低水位标记的当前大小。对这样的socket的写操作将不阻塞并返回一个大于0的值(也就是返回准备好写入的数据)。我们可以用SO_SNDLOWAT这个socket选项来设置该socket的低水位标记。对于TCP和UDP的socket而言，其缺省值为2048 该连接的写这一半关闭。对这样的socket的写操作将产生SIGPIPE信号，该信号的缺省行为是终止进程。 使用非阻塞connect的套接字已建立连接, 或者connect已经以失败告终 有一个socket异常错误条件待处理.对于这样的socket的写操作将不会阻塞并且返回一个错误(-1),errno则设置成明确的错误条件.这些待处理的错误也可以通过指定socket选项SO_ERROR调用getsockopt函数来取得并清除; 非阻塞connect/accept相关上述的各种条件可以大体总结为下图 注意 : 当socket异常错误的时候socket是可读并可写的, 所以在非阻塞connect(判断是否可写)/accept(判断是否可读)的时候要特别注意这种情况, 要用getsockopt函数, 使用SO_ERROR选项来检查处理.]]></content>
      <categories>
        <category>NP</category>
      </categories>
      <tags>
        <tag>NP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[const和volatile和mutable讲解]]></title>
    <url>%2Fblog%2F2015%2F05%2F24%2Fconst_mutable_volatile%2F</url>
    <content type="text"><![CDATA[const关键字const 是比较常见的关键字, 也是非常好的预防错误的手段. const 修饰普通变量和指针const 修饰变量，一般有两种写法： const TYPE value; TYPE const value;这两种写法在本质上是一样的。它的含义是：const 修饰的类型为 TYPE 的变量 value 是不可变的。对于一个非指针的类型 TYPE，无论怎么写，都是一个含义，即 value 值不可变。 例如： const int nValue； //nValue 是 const int const nValue； //nValue 是 const. . . 但是对于指针类型的 TYPE，不同的写法会有不同情况： （1） 指针本身是常量不可变 (char*) const pContent; （2）指针所指向的内容是常量不可变 const (char) *pContent; (char) const *pContent; （3） 两者都不可变 const char* const pContent; 识别 const 到底是修饰指针还是指针所指的对象，还有一个较为简便的方法，也就是沿着 * 号划一条线： 如果 const 位于 * 的左侧，则 const 就是用来修饰指针所指向的变量，即指针指向为常量； 如果 const 位于 * 的右侧，const 就是修饰指针本身，即指针本身是常量。 const 修饰函数参数const 修饰函数参数是它最广泛的一种用途，它表示在函数体中不能修改参数的值 (包括参数本身的值或者参数其中包含的值)： void function(const int Var); // 传递过来的参数在函数内不可以改变 (无意义，该函数以传值的方式调用) void function(const char* Var); // 参数指针所指内容为常量不可变 void function(char* const Var); // 参数指针本身为常量不可变 (也无意义，var 本身也是通过传值的形式赋值的) void function(const Class&amp; Var); // 引用参数在函数内不可以改变参数 const 通常用于参数为指针或引用的情况，若输入参数采用 “值传递” 方式，由于函数将自动产生临时变量用于复制该参数，该参数本就不需要保护，所以不用 const 修饰。 const 修饰类对象 / 对象指针 / 对象引用 const 修饰类对象表示该对象为常量对象，其中的任何成员都不能被修改。对于对象指针和对象引用也是一样。 const 修饰的对象，该对象的任何非 const 成员函数都不能被调用，因为任何非 const 成员函数会有修改成员变量的企图。 例如： 1234567891011121314151617181920class AAA&#123; void func1();void func2() const;&#125;const AAA aObj;aObj.func1(); 错误aObj.func2(); 正确const AAA* aObj = new AAA();aObj-&gt;func1(); 错误aObj-&gt;func2(); 正确 const 修饰数据成员 const 数据成员只在某个对象生存期内是常量，而对于整个类而言却是可变的。因为类可以创建多个对象，不同的对象其 const 数据成员的值可以不同。所以不能在类声明中初始化 const 数据成员，因为类的对象未被创建时，编译器不知道 const 数据成员的值是什么，例如： class A { const int size = 100; // 错误 int array[size]; // 错误，未知的 size }const 数据成员的初始化只能在类的构造函数的初始化列表中进行。要想建立在整个类中都恒定的常量，可以用类中的枚举常量来实现，例如： class A { … enum {size1=100, size2 = 200}; int array1[size1]; int array2[size2]; … }枚举常量不会占用对象的存储空间，他们在编译时被全部求值。但是枚举常量的隐含数据类型是整数，其最大值有限，且不能表示浮点数。 const 修饰成员函数const 修饰类的成员函数，用 const 修饰的成员函数不能改变对象的成员变量。一般把 const 写在成员函数的最后： class A { … void function()const; // 常成员函数, 它不改变对象的成员变量. 也不能调用类中任何非 const 成员函数。 }对于 const 类对象 / 指针 / 引用，只能调用类的 const 成员函数。 const 修饰成员函数的返回值 １、一般情况下，函数的返回值为某个对象时，如果将其声明为 const 时，多用于操作符的重载。通常，不建议用 const 修饰函数的返回值类型为某个对象或对某个对象引用的情况。原因如下：如果返回 const 对象，或返回 const 对象的引用，则返回值具有 const 属性，返回实例只能访问类 A 中的公有（保护）数据成员和 const 成员函数，并且不允许对其进行赋值操作，这在一般情况下很少用到。 2、如果给采用 “指针传递” 方式的函数返回值加 const 修饰，那么函数返回值（即指针所指的内容）不能被修改，该返回值只能被赋给加 const 修饰的同类型指针： const char * GetString(void); 如下语句将出现编译错误： char *str=GetString(); 正确的用法是： const char *str=GetString(); 3、函数返回值采用 “引用传递” 的场合不多，这种方式一般只出现在类的赙值函数中，目的是为了实现链式表达。如： class A {… A &amp;operate= (const A &amp;other); // 赋值函数 } A a,b,c; //a,b,c 为 A 的对象 … a=b=c; // 正常 (a=B)=c; // 不正常，但是合法 若赋值函数的返回值加 const 修饰，那么该返回值的内容不允许修改，上例中 a=b=c 依然正确。(a=b)=c 就不正确了。 const 常量与 define 宏定义的区别 （1） 编译器处理方式不同 define 宏是在预处理阶段展开。 const 常量是编译运行阶段使用。 （2）类型和安全检查不同 define 宏没有类型，不做任何类型检查，仅仅是展开。 const 常量有具体的类型，在编译阶段会执行类型检查。 （3） 存储方式不同 define 宏仅仅是展开，有多少地方使用，就展开多少次，不会分配内存。 const 常量会在内存中分配 (可以是堆中也可以是栈中)。 volatile关键字volatile 的本意是 “易变的”,volatile 关键字是一种类型修饰符，用它声明的类型变量表示可以被某些编译器未知的因素更改，比如操作系统、硬件或者其它线程等。遇到这个关键字声明的变量，编译器对访问该变量的代码就不再进行优化，从而可以提供对特殊地址的稳定访问。 当要求使用 volatile 声明的变量的值的时候，系统总是重新从它所在的内存读取数据，即使它前面的指令刚刚从该处读取过数据。而且读取的数据立刻被寄存。例如： volatile int i=10; int a = i; 。。。// 其他代码，并未明确告诉编译器，对 i 进行过操作 int b = i;volatile 指出 i 是随时可能发生变化的，每次使用它的时候必须从 i 的地址中读取，因而编译器生成的汇编代码会重新从 i 的地址读取数据放在 b 中。而优化做法是，由于编译器发现两次从 i 读数据的代码之间的代码没有对 i 进行过操作，它会自动把上次读的数据放在 b 中。而不是重新从 i 里面读。这样以来，如果 i 是一个寄存器变量或者表示一个端口数据就容易出错，所以说 volatile 可以保证对特殊地址的稳定访问。 注意，在 vc6 中，一般调试模式没有进行代码优化，所以这个关键字的作用看不出来。下面通过插入汇编代码，测试有无 volatile 关键字，对程序最终代码的影响。首先用 classwizard 建一个 win32 console 工程，插入一个 voltest.cpp 文件，输入下面的代码： 12345678910111213141516171819202122232425#include &lt;stdio.h&gt;void main()&#123;int i=10;int a = i;printf("i= %d/n",a);// 下面汇编语句的作用就是改变内存中 i 的值，但是又不让编译器知道__asm &#123; mov dword ptr [ebp-4], 20h&#125;int b = i;printf("i= %d/n",b);&#125; 然后，在调试版本模式运行程序，输出结果如下： i = 10 i = 32然后，在 release 版本模式运行程序，输出结果如下： i = 10 i = 10输出的结果明显表明，release 模式下，编译器对代码进行了优化，第二次没有输出正确的 i 值。下面，我们把 i 的声明加上 volatile 关键字，看看有什么变化： 1234567891011121314151617181920212223#include &lt;stdio.h&gt;void main()&#123;volatile int i=10;int a = i;printf("i= %d/n",a);__asm &#123; mov dword ptr [ebp-4], 20h&#125;int b = i;printf("i= %d/n",b);&#125; 分别在调试版本和 release 版本运行程序，输出都是： i = 10 i = 32这说明这个关键字发挥了它的作用！ 关于 volatile 的补充信息一个定义为 volatile 的变量是说这变量可能会被意想不到地改变，这样，编译器就不会去假设这个变量的值了。精确地说就是，优化器在用到这个变量时必须每次都小心地重新读取这个变量的值，而不是使用保存在寄存器里的备份。下面是 volatile 变量的几个例子： 1). 并行设备的硬件寄存器（如：状态寄存器） 2). 一个中断服务子程序中会访问到的非自动变量 (Non-automatic variables) 3). 多线程应用中被几个任务共享的变量 我认为这是区分 C 程序员和嵌入式系统程序员的最基本的问题。嵌入式系统程序员经常同硬件、中断、RTOS 等等打交道，所用这些都要求 volatile 变量。不懂得 volatile 内容将会带来灾难。假设被面试者正确地回答了这是问题（嗯，怀疑这否会是这样），我将稍微深究一下，看一下这家伙是不是直正懂得 volatile 的重要性： 1). 一个参数既可以是 const 还可以是 volatile 吗？解释为什么。 2). 一个指针可以是 volatile 吗？解释为什么。 3). 下面的函数有什么错误： int square(volatile int *ptr) { return *ptr * *ptr; } 下面是答案： 1). 是的。一个例子是只读的状态寄存器。它是 volatile 因为它可能被意想不到地改变。它是 const 因为程序不应该试图去修改它。 2). 是的。尽管这并不很常见。一个例子是当一个中服务子程序修该一个指向一个 buffer 的指针时。 3). 这段代码的有个恶作剧。这段代码的目的是用来返指针 *ptr 指向值的平方，但是，由于 *ptr 指向一个 volatile 型参数，编译器将产生类似下面的代码： int square(volatile int *ptr) { int a,b; a = *ptr; b = *ptr; return a * b; } 由于 *ptr 的值可能被意想不到地该变，因此 a 和 b 可能是不同的。结果，这段代码可能返不是你所期望的平方值！正确的代码如下： long square(volatile int *ptr) { int a; a = *ptr; return a * a; } mutable关键字mutalbe 的中文意思是 “可变的，易变的”，跟 constant（既 C++ 中的 const）是反义词。在 C++ 中，mutable 也是为了突破 const 的限制而设置的。被 mutable 修饰的变量 (mutable 只能由于修饰类的非静态数据成员)，将永远处于可变的状态，即使在一个 const 函数中。 我们知道，假如类的成员函数不会改变对象的状态，那么这个成员函数一般会声明为 const。但是，有些时候，我们需要在 const 的函数里面修改一些跟类状态无关的数据成员，那么这个数据成员就应该被 mutalbe 来修饰。下面是一个小例子： 12345678910111213141516171819202122232425class ClxTest&#123; public: void Output() const;&#125;;void ClxTest::Output() const&#123; cout &lt;&lt; "Output for test!" &lt;&lt; endl;&#125;void OutputTest(const ClxTest&amp; lx)&#123; lx.Output();&#125; 类 ClxTest 的成员函数 Output 是用来输出的，不会修改类的状态，所以被声明为 const。 函数 OutputTest 也是用来输出的，里面调用了对象 lx 的 Output 输出方法，为了防止在函数中调用成员函数修改任何成员变量，所以参数也被 const 修饰。 假如现在，我们要增添一个功能：计算每个对象的输出次数。假如用来计数的变量是普通的变量的话，那么在 const 成员函数 Output 里面是不能修改该变量的值的；而该变量跟对象的状态无关，所以应该为了修改该变量而去掉 Output 的 const 属性。这个时候，就该我们的 mutable 出场了，只要用 mutalbe 来修饰这个变量，所有问题就迎刃而解了。下面是修改过的代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061class ClxTest&#123; public: ClxTest(); ~ClxTest(); void Output() const; int GetOutputTimes() const; private: **mutable** int m_iTimes;&#125;;ClxTest::ClxTest()&#123; m_iTimes = 0;&#125;ClxTest::~ClxTest()&#123;&#125;void ClxTest::Output() const&#123; cout &lt;&lt; "Output for test!" &lt;&lt; endl; m_iTimes++;&#125;int ClxTest::GetOutputTimes() const&#123; return m_iTimes;&#125;void OutputTest(const ClxTest&amp; lx)&#123; cout &lt;&lt;lx.GetOutputTimes() &lt;&lt; endl; lx.Output(); cout &lt;&lt;lx.GetOutputTimes() &lt;&lt; endl;&#125; 计数器 m_iTimes 被 mutable 修饰，那么它就可以突破 const 的限制，在被 const 修饰的函数里面也能被修改。]]></content>
      <categories>
        <category>CPP</category>
      </categories>
      <tags>
        <tag>CPP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关系型数据库与NoSQL的爱恨情仇]]></title>
    <url>%2Fblog%2F2015%2F05%2F20%2F%E5%85%B3%E7%B3%BB%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%8ENoSQL%E7%9A%84%E7%88%B1%E6%81%A8%E6%83%85%E4%BB%87%2F</url>
    <content type="text"><![CDATA[NoSQL因关系数据库的不足而生随着互联网的不断发展，各种类型的应用层出不穷，所以导致在这个云计算的时代， 对技术提出了更多的需求，主要体现在下面这四个方面： 低延迟的读写速度：应用快速地反应能极大地提升用户的满意度; 支撑海量的数据和流量：对于搜索这样大型应用而言，需要利用PB级别的数据和能应对百万级的流量; 大规模集群的管理：系统管理员希望分布式应用能更简单的部署和管理; 庞大运营成本的考量：IT经理们希望在硬件成本、软件成本和人力成本能够有大幅度地降低; 目前世界上主流的存储系统大部分还是采用了关系型数据库，其主要有一下优点： 事务处理—保持数据的一致性； 由于以标准化为前提，数据更新的开销很小（相同的字段基本上只有一处）； 可以进行Join等复杂查询。 虽然关系型数据库已经在业界的数据存储方面占据不可动摇的地位，但是由于其天生的几个限制， 使其很难满足上面这几个需求： 扩展困难：由于存在类似Join这样多表查询机制，使得数据库在扩展方面很艰难; 读写慢：这种情况主要发生在数据量达到一定规模时由于关系型数据库的系统逻辑非常复杂，使得其非常容易发生死锁等的并发问题，所以导致其读写速度下滑非常严重; 成本高：企业级数据库的License价格很惊人，并且随着系统的规模，而不断上升; 有限的支撑容量：现有关系型解决方案还无法支撑Google这样海量的数据存储; 业界为了解决上面提到的几个需求，推出了多款新类型的数据库，并且由于它们在设计上和传统的NoSQL数据库相比有很大的不同，所以被统称为“NoSQL”系列数据库。 总的来说，在设计上，它们非常关注对数据高并发地读写和对海量数据的存储等，与关系型数据库相比，它们在架构和数据模型方量面做了“减法”， 而在扩展和并发等方面做了“加法”。 现在主流的NoSQL数据库有MongoDB和Redis以及BigTable、Hbase、Cassandra、SimpleDB、CouchDB、等。 接下来，将关注NoSQL数据库到底存在哪些优缺点。 NoSQL的优缺点在优势方面，主要体现在下面这三点： 简单的扩展：典型例子是Cassandra，由于其架构是类似于经典的P2P，所以能通过轻松地添加新的节点来扩展这个集群; 快速的读写：主要例子有redis，由于其逻辑简单，而且纯内存操作，使得其性能非常出色，单节点每秒可以处理超过10万次读写操作; 低廉的成本：这是大多数分布式数据库共有的特点，因为主要都是开源软件，没有昂贵的License成本; 但瑕不掩瑜，NoSQL数据库还存在着很多的不足，常见主要有下面这几个： 不提供对SQL的支持：如果不支持SQL这样的工业标准，将会对用户产生一定的学习和应用迁移成本; 支持的特性不够丰富：现有产品所提供的功能都比较有限，大多数NoSQL数据库都不支持事务，也不像MS SQL Server和Oracle那样能提供各种附加功能，比如BI和报表等; 现有产品的不够成熟：大多数产品都还处于初创期，和关系型数据库几十年的完善不可同日而语; 上面NoSQL产品的优缺点都是些比较共通的，在实际情况下，每个产品都会根据自己所遵从的数据模型和CAP理念而有所不同.]]></content>
      <categories>
        <category>DB</category>
      </categories>
      <tags>
        <tag>NoSQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[编译过程]]></title>
    <url>%2Fblog%2F2015%2F05%2F07%2F%E7%BC%96%E8%AF%91%E8%BF%87%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[通常我们使用gcc来生成可执行程序，命令为：gcc hello.c，默认生成可执行文件a.out 其实编译（包括链接）的命令：gcc hello.c 可分解为如下4个大的步骤： 预处理(Preprocessing)编译(Compilation)汇编(Assembly)链接(Linking) 预处理 1.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 预处理(Preproceessing) 预处理的过程主要处理包括以下过程： 将所有的#define删除，并且展开所有的宏定义处理所有的条件预编译指令，比如#if #ifdef #elif #else #endif等处理#include&nbsp;预编译指令，将被包含的文件插入到该预编译指令的位置。删除所有注释&nbsp;“//”和”/* */”.添加行号和文件标识，以便编译时产生调试用的行号及编译错误警告行号。保留所有的#pragma编译器指令，因为编译器需要使用它们 &nbsp; 通常使用以下命令来进行预处理： gcc -E hello.c -o hello.i 参数-E表示只进行预处理 或者也可以使用以下指令完成预处理过程 cpp hello.c &gt; hello.i &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;/* &nbsp;cpp – The C Preprocessor&nbsp; */ 直接cat hello.i 你就可以看到预处理后的代码 &nbsp; 编译 2.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 编译(Compilation) 编译过程就是把预处理完的文件进行一系列的词法分析，语法分析，语义分析及优化后生成相应的汇编代码。 $gcc –S hello.i –o hello.s 或者 $ /usr/lib/gcc/i486-linux-gnu/4.4/cc1 hello.c 注：现在版本的GCC把预处理和编译两个步骤合成一个步骤，用cc1工具来完成。gcc其实是后台程序的一些包装，根据不同参数去调用其他的实际处理程序，比如：预编译编译程序cc1、汇编器as、连接器ld 可以看到编译后的汇编代码(hello.s)如下： 1234567891011121314151617181920 .file &quot;hello.c&quot; .section .rodata.LC0: .string &quot;Hello, world.&quot; .text.globl main .type main, @functionmain: pushl %ebp movl %esp, %ebp andl $-16, %esp subl $16, %esp movl $.LC0, (%esp) call puts movl $0, %eax leave ret .size main, .-main .ident &quot;GCC: (Ubuntu 4.4.3-4ubuntu5) 4.4.3&quot; .section .note.GNU-stack,&quot;&quot;,@progbits &nbsp; 汇编 3.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 汇编(Assembly) 汇编器是将汇编代码转变成机器可以执行的命令，每一个汇编语句几乎都对应一条机器指令。汇编相对于编译过程比较简单，根据汇编指令和机器指令的对照表一一翻译即可。 $ gcc –c hello.c –o hello.o 或者 $ as hello.s –o hello.co 由于hello.o的内容为机器码，不能以普通文本形式的查看（vi 打开看到的是乱码）。 &nbsp; 链接4.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 链接(Linking) 通过调用链接器ld来链接程序运行需要的一大堆目标文件，以及所依赖的其它库文件，最后生成可执行文件。 ld -static crt1.o crti.o crtbeginT.o hello.o -start-group -lgcc -lgcc_eh -lc-end-group crtend.o crtn.o (省略了文件的路径名)。 &nbsp; 编译器和链接器具体做了什么 helloworld的大体编译和链接过程就是这样了，那么编译器和链接器到底做了什么呢？ &nbsp; 编译过程可分为6步： 词法分析：扫描器（Scanner）将源代的字符序列分割成一系列的记号（Token）。lex工具可实现词法扫描。 语法分析：语法分析器将记号（Token）产生语法树（Syntax Tree）。yacc工具可实现语法分析(yacc: Yet Another Compiler Compiler)。 语义分析：静态语义（在编译器可以确定的语义）、动态语义（只能在运行期才能确定的语义）。 源代码优化：源代码优化器(Source Code Optimizer)，将整个语法书转化为中间代码（Intermediate Code）（中间代码是与目标机器和运行环境无关的）。中间代码使得编译器被分为前端和后端。编译器前端负责产生机器无关的中间代码；编译器后端将中间代码转化为目标机器代码。 目标代码生成：代码生成器(Code Generator). 目标代码优化：目标代码优化器(Target Code Optimizer)。 &nbsp; 链接的主要内容是把各个模块之间相互引用的部分处理好，使得各个模块之间能够正确地衔接。 链接的主要过程包括：地址和空间分配（Address and Storage Allocation），符号决议（Symbol Resolution），重定位（Relocation）等。 链接分为静态链接和动态链接 静态链接是指在编译阶段直接把静态库加入到可执行文件中去，这样可执行文件会比较大。 动态链接则是指链接阶段仅仅只加入一些描述信息，而程序执行时再从系统中把相应动态库加载到内存中去。 静态链接的大致过程如下图所示：]]></content>
      <categories>
        <category>CPP</category>
      </categories>
      <tags>
        <tag>Compile</tag>
        <tag>Make</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于Valgrind所报的4种内存丢失]]></title>
    <url>%2Fblog%2F2015%2F05%2F02%2Fvalgrind_tutorial%2F</url>
    <content type="text"><![CDATA[官方解释及分析摘自http://valgrind.org/docs/manual/faq.html#faq.deflost *5.2.With Memcheck’s memory leak detector, what’s the difference between “definitely lost”, “indirectly lost”, “possibly lost”, “still reachable”, and “suppressed”?* The details are in the Memcheck section of the user manual.In short: “definitely lost” means your program is leaking memory – fix those leaks! “indirectly lost” means your program is leaking memory in a pointer-based structure. (E.g. if the root node of a binary tree is “definitely lost”, all the children will be “indirectly lost”.) If you fix the “definitely lost” leaks, the “indirectly lost” leaks should go away. “possibly lost” means your program is leaking memory, unless you’re doing unusual things with pointers that could cause them to point into the middle of an allocated block; see the user manual for some possible causes. Use –show-possibly-lost=no if you don’t want to see these reports. “still reachable” means your program is probably ok – it didn’t free some memory it could have. This is quite common and often reasonable. Don’t use –show-reachable=yes if you don’t want to see these reports. “suppressed” means that a leak error has been suppressed. There are some suppressions in the default suppression files. You can ignore suppressed errors. 分析 “definitely lost”：确认丢失。程序中存在内存泄露，应尽快修复。当程序结束时如果一块动态分配的内存没有被释放且通过程序内的指针变量均无法访问这块内存则会报这个错误。 “indirectly lost”：间接丢失。当使用了含有指针成员的类或结构时可能会报这个错误。这类错误无需直接修复，他们总是与”definitely lost”一起出现，只要修复”definitely lost”即可。例子可参考我的例程。 “possibly lost”：可能丢失。大多数情况下应视为与”definitely lost”一样需要尽快修复，除非你的程序让一个指针指向一块动态分配的内存（但不是这块内存起始地址），然后通过运算得到这块内存起始地址，再释放它。例子可参考我的例程。当程序结束时如果一块动态分配的内存没有被释放且通过程序内的指针变量均无法访问这块内存的起始地址，但可以访问其中的某一部分数据，则会报这个错误。 “still reachable”：可以访问，未丢失但也未释放。如果程序是正常结束的，那么它可能不会造成程序崩溃，但长时间运行有可能耗尽系统资源，因此笔者建议修复它。如果程序是崩溃（如访问非法的地址而崩溃）而非正常结束的，则应当暂时忽略它，先修复导致程序崩溃的错误，然后重新检测。 “suppressed”：已被解决。出现了内存泄露但系统自动处理了。可以无视这类错误。这类错误我没能用例程触发，看官方的解释也不太清楚是操作系统处理的还是valgrind，也没有遇到过。所以无视他吧~ 代码示例1234567891011121314151617181920212223242526#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;void *g_p1;int *g_p2;int ** fun1(void)&#123; //付给了局部变量, 函数结束而不释放,为肯定丢失. //把函数尾部语句return p; 改为return 0;更能说明这个问题. int **p=(int **)malloc(16); g_p1=malloc(20); //付给了全局变量, 内存可以访问 g_p2=(int*)malloc(30); g_p2++; //付给了全局变量, 内存可以访问,但是指针被移动过,为可能丢失 p[1]=(int *)malloc(40); //如果p丢失了,则p[1]为间接丢失. return p;&#125;int main(int argc, char *argv[])&#123; int **p=fun1();// free(g_p1); //如果不free, 将会有 still reachable 内存泄露// free(--g_p2);//如果不free, 将会有 possibly lost 内存泄露// free(p[1]); //如果不free, 将会有 indirectly lost 内存泄露// free(p); //如果不free, 将会有 definitely lost内存泄露 return 0;&#125; 执行编译命令g++ val_test.cpp -o v, 然后 当执行valgrind ./v 命令之后的简易内存错误报告 : ==4765== Memcheck, a memory error detector ==4765== Copyright (C) 2002-2013, and GNU GPL&apos;d, by Julian Seward et al. ==4765== Using Valgrind-3.10.1 and LibVEX; rerun with -h for copyright info ==4765== Command: ./v ==4765== ==4765== ==4765== HEAP SUMMARY: ==4765== in use at exit: 106 bytes in 4 blocks ==4765== total heap usage: 4 allocs, 0 frees, 106 bytes allocated ==4765== ==4765== LEAK SUMMARY: ==4765== definitely lost: 16 bytes in 1 blocks ==4765== indirectly lost: 40 bytes in 1 blocks ==4765== possibly lost: 30 bytes in 1 blocks ==4765== still reachable: 20 bytes in 1 blocks ==4765== suppressed: 0 bytes in 0 blocks ==4765== Rerun with --leak-check=full to see details of leaked memory ==4765== ==4765== For counts of detected and suppressed errors, rerun with: -v ==4765== ERROR SUMMARY: 0 errors from 0 contexts (suppressed: 0 from 0) b@b-VirtualBox:~/tc/valgrind_test$ valgrind --leak-check=full valgrind: no program specified valgrind: Use --help for more information.当执行valgrind --leak-check=full ./v 命令之后的详细内存错误报告 : ==4767== Memcheck, a memory error detector ==4767== Copyright (C) 2002-2013, and GNU GPL&apos;d, by Julian Seward et al. ==4767== Using Valgrind-3.10.1 and LibVEX; rerun with -h for copyright info ==4767== Command: ./v ==4767== ==4767== ==4767== HEAP SUMMARY: ==4767== in use at exit: 106 bytes in 4 blocks ==4767== total heap usage: 4 allocs, 0 frees, 106 bytes allocated ==4767== ==4767== 30 bytes in 1 blocks are possibly lost in loss record 2 of 4 ==4767== at 0x4C2AB80: malloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so) ==4767== by 0x40055E: fun1() (val_test.cpp:12) ==4767== by 0x4005AB: main (val_test.cpp:20) ==4767== ==4767== 56 (16 direct, 40 indirect) bytes in 1 blocks are definitely lost in loss record 4 of 4 ==4767== at 0x4C2AB80: malloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so) ==4767== by 0x40053F: fun1() (val_test.cpp:10) ==4767== by 0x4005AB: main (val_test.cpp:20) ==4767== ==4767== LEAK SUMMARY: ==4767== definitely lost: 16 bytes in 1 blocks ==4767== indirectly lost: 40 bytes in 1 blocks ==4767== possibly lost: 30 bytes in 1 blocks ==4767== still reachable: 20 bytes in 1 blocks ==4767== suppressed: 0 bytes in 0 blocks ==4767== Reachable blocks (those to which a pointer was found) are not shown. ==4767== To see them, rerun with: --leak-check=full --show-leak-kinds=all ==4767== ==4767== For counts of detected and suppressed errors, rerun with: -v ==4767== ERROR SUMMARY: 2 errors from 2 contexts (suppressed: 0 from 0)总结 由局部变量指向的内存,如果不释放为肯定丢失, 由此指针而引起的后续内存泄露,为间接丢失. 由全局变量指向的内存如果不被释放,为still reachable, 如果该变量改动过, 为可能丢失. 是啊,局部变量是栈变量,如果你不能把这个栈变量处理好,出了这个函数,指针地址就丢失了,这时那是肯定丢失了. 如果你付给的地址是全局变量,倒是可以访问,叫still reachable 但是如果你这个全局变量的值改动过, 那只有你知道怎样正确访问这块内存,别人可能就访问不到了,这叫可能丢失. 由肯定丢失而引起的进一步的内存丢失为间接丢失. 解决内存泄漏的顺序所以碰到问题你首先要解决什么问题? 肯定丢失,然后是可能丢失,然后间接丢失,然后still reachable!!!]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>Valgrind</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多线程开发的一些基本概念]]></title>
    <url>%2Fblog%2F2015%2F04%2F27%2Fmulti_thread_dev_base_concept%2F</url>
    <content type="text"><![CDATA[竞态（race condition）软件层面上，竞态是指多个线程或进程读写一个共享资源 (或共享设备) 时的输出结果依赖于线程或进程的先后执行顺序或者时间；（ 更权威的介绍可以看 wiki ) 至于为什么会发生竞态呢？很简单，因为并发，并发使多线程，多进程环境变成可能。 竞态具体场景：假如我们有 2 个进程会对一个全局变量进行 ++ 操作，理想时，程序会这样执行： Thread 1Thread 2 Integer value 0read value ←0increase value 0write back →1 read value←1 increase value 1 write back→2 然而，由于并发的普遍存在，使得情况有时” 不受控制”（不如工程师预期那样工作），可能会变成这样： Thread 1Thread 2 Integer value 0read value ←0 read value←0increase value 0 increase value 0write back →1 write back→1 并发（concurrency）并发 (concurrency) 指的是多个执行单元同时、并行被执行。而并发的执行单元对共享资源 (硬件资源和软件上的全局、静态变量) 的访问则容易导致竞态 (race conditions), 可能导致并发 (即竞态?) 的情况有： SMP（Symmetric Multi-Processing），对称多处理结构。SMP 是一种紧耦合、共享存储的系统模型，它的特点是多个 CPU 使用共同的系统总线，因此可访问共同的外设和存储器。 中断. 中断可以打断正在执行的进程 (哪怕是在中断上下文)，若中断处理程序对共享资源进程访问，则竞态也会发生. 内核抢占.2.6 以后内核提供了内核可抢占特性，虽然是作为一个配置选项，但我们写程序时还是要考虑周全，故内核抢占也是作为伪并发的表现，也可能发生竞态； 临界区（critical section）多个线程进程对共享资源进行访问在软件表现为一个程序片段，如何避免竞态的发生呢？ 一个执行路径在对共享资源进行访问时禁止其他执行路径进行访问，当有一个执行路径（A）对共享资源进行访问时，如有其他执行路径想访问共享资源，须睡眠等待 A 执行路径退出。 那么这时这个程序片段就是临界区。那么具体如何来实现临界区呢？linux 内核提供了多种同步互斥机制.（如信号量，互斥量，自旋锁，RCU，原子操作等）. 什么是RAII技术我们在C++中经常使用new申请了内存空间，但是却也经常忘记delete回收申请的空间，容易造成内存溢出，于是RAII技术就诞生了，来解决这样的问题。 RAII（Resource Acquisition Is Initialization）机制是Bjarne Stroustrup首先提出的，是一种利用对象生命周期来控制程序资源（如内存、文件句柄、网络连接、互斥量等等）的简单技术。 我们知道在函数内部的一些成员是放置在栈空间上的，当函数返回时，这些栈上的局部变量就会立即释放空间，于是Bjarne Stroustrup就想到确保能运行资源释放代码的地方就是在这个程序段（栈）中放置的对象的析构函数了，因为stack winding会保证它们的析构函数都会被执行。RAII就利用了栈里面的变量的这一特点。 RAII 的一般做法是这样的：在对象构造时获取资源，接着控制对资源的访问使之在对象的生命周期内始终保持有效，最后在对象析构的时候释放资源。 借此，我们实际上把管理一份资源的责任托管给了一个存放在栈空间上的局部对象。这种做法有两大好处： (1)不需要显式地释放资源。 (2)采用这种方式，对象所需的资源在其生命期内始终保持有效。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>CPP</tag>
        <tag>Linux</tag>
        <tag>MultiThread</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL入门三之GroupBy]]></title>
    <url>%2Fblog%2F2015%2F04%2F17%2FMySQL%E5%85%A5%E9%97%A8%E4%B8%89%E4%B9%8BGroupBy%2F</url>
    <content type="text"><![CDATA[SQL GROUP BY 实例我们拥有下面这个 “Orders” 表： O_Id OrderDate OrderPrice Customer 1 2008/12/29 1000 Bush 2 2008/11/23 1600 Carter 3 2008/10/05 700 Bush 4 2008/09/28 300 Bush 5 2008/08/06 2000 Adams 6 2008/07/21 100 Carter 现在，我们希望查找每个客户的总金额（总订单）。我们想要使用 GROUP BY 语句对客户进行组合。我们使用下列 SQL 语句： SELECT Customer,SUM(OrderPrice) FROM Orders GROUP BY Customer 结果集类似这样： Customer SUM(OrderPrice) Bush 2000 Carter 1700 Adams 2000 很棒吧，对不对？让我们看一下如果省略 GROUP BY 会出现什么情况： SELECT Customer,SUM(OrderPrice) FROM Orders 结果集类似这样： Customer SUM(OrderPrice) Bush 5700 Carter 5700 Bush 5700 Bush 5700 Adams 5700 Carter 5700 上面的结果集不是我们需要的。那么为什么不能使用上面这条 SELECT 语句呢？ 解释如下：上面的 SELECT 语句指定了两列（Customer 和 SUM(OrderPrice)）。“SUM(OrderPrice)” 返回一个单独的值（”OrderPrice” 列的总计），而 “Customer” 返回 6 个值（每个值对应 “Orders” 表中的每一行）。因此，我们得不到正确的结果。不过，您已经看到了，GROUP BY 语句解决了这个问题。]]></content>
      <categories>
        <category>DB</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[对象模型之内存对齐基础]]></title>
    <url>%2Fblog%2F2015%2F04%2F12%2Fsizeof_struct%2F</url>
    <content type="text"><![CDATA[本文不讨论类的虚函数, 请参考 C++对象模型之虚函数讲解 内存对齐规则首先我们明确内存对齐规则 我们设 A = #pragma pack()指定的数 B = 这个数据成员的自身长度 C = 结构(或联合)中最大数据成员长度在解释内存对齐的作用前，先来看下内存对齐的规则： 1. 对于结构的各个成员，第一个成员位于偏移为0的位置，以后每个数据成员的偏移量必须是 min( A，B ) 的倍数。 2. 在数据成员完成各自对齐之后，结构(或联合)本身也要进行对齐，对齐将按照 min( A, C) 进行。 问题32位机器上, 下列代码的sizeof(a)的值是多少? 12345678910111213141516171819202122#pragma pack(2)class A&#123; int i; union U &#123; char buff[13]; int i; &#125;u; void foo() &#123; &#125; typedef char* (*f)(void*); enum&#123; red, green, blue &#125; color;&#125;a;#pragma pack() 答案答案是sizeof(a)的值为22. void foo() { } ，typedef char* (f)(void);不占字节， 枚举占4个字节， union按最大的变量所占字节算，占14个字节， int占4个字节， 4+14+4=22。 如果把#pragma pack(2)改为 #pragma pack(4)， sizeof(a)的值就为 24。 解析分为三部分来解析: 枚举所占内存计算方法 #pragma pack用法 共用体(union)所占内存计算方法 枚举所占内存计算方法枚举变量，由枚举类型定义的变量。枚举变量的大小，即枚举类型所占内存的大小。 由于枚举变量的赋值，一次只能存放枚举结构中的某个常数。 所以枚举变量的大小，实质是常数所占内存空间的大小（常数为int类型，当前主流的编译器中一般是32位机器和64位机器中int型都是4个字节），枚举类型所占内存大小也是这样。 #pragma pack用法#pragma pack(a)规定的对齐长度（a可选值为1，2，4，8，16），实际使用的规则是：结构，联合，或者类的数据成员，第一个放在偏移为0的地方，以后每个数据成员的对齐，按照#pragma pack指定的数值和这个数据成员自身长度中，比较小的那个进行。也就是说，当#pragma pack的值等于或超过所有数据成员长度的时候，这个值的大小将不产生任何效果。而结构整体的对齐，则按照结构体中最大的数据成员 和 #pragma pack指定值 之间，较小的那个进行。而 #pragma pack() 表示恢复默认的内存对齐（与#pragma pack(a)指令配对使用） 12345678910111213141516#pragma pack(4)class TestB&#123;public: int aa; //第一个成员，放在[0,3]偏移的位置， char a; //第二个成员，自身长为1，#pragma pack(4),取小值，也就是1，所以这个成员按一字节对齐，放在偏移[4]的位置。 short b; //第三个成员，自身长2，#pragma pack(4)，取2，按2字节对齐，所以放在偏移[6,7]的位置。 char c; //第四个，自身长为1，放在[8]的位置。&#125;;#pragma pack() 这个类实际占据的内存空间是9字节类之间的对齐，是按照类内部最大的成员的长度，和#pragma pack规定的值之中较小的一个对齐的。所以这个例子中，类之间对齐的长度是min(sizeof(int),4)，也就是4。9按照4字节圆整的结果是12，所以sizeof(TestB)是12。 如果 12345678910#pragma pack(2)class TestB&#123;public: int aa; //第一个成员，放在[0,3]偏移的位置， char a; //第二个成员，自身长为1，#pragma pack(2),取小值，也就是1，所以这个成员按一字节对齐，放在偏移[4]的位置。 short b; //第三个成员，自身长2，#pragma pack(2)，取2，按2字节对齐，所以放在偏移[6,7]的位置。 char c; //第四个，自身长为1，放在[8]的位置。&#125;;#pragma pack() 可以看出，上面的位置完全没有变化，只是类之间改为按2字节对齐，9按2圆整的结果是10。所以 sizeof(TestB)是10。 现在去掉第一个成员变量为如下代码： 123456789#pragma pack(4)class TestC&#123;public: char a;//第一个成员，放在[0]偏移的位置， short b;//第二个成员，自身长2，#pragma pack(4)，取2，按2字节对齐，所以放在偏移[2,3]的位置。 char c;//第三个，自身长为1，放在[4]的位置。&#125;;#pragma pack() 整个类的大小是5字节，按照min(sizeof(short),4)字节对齐，也就是2字节对齐，结果是6所以sizeof(TestC)是6。 整个类的大小是5字节，按照min(sizeof(short),4)字节对齐，也就是2字节对齐，结果是6，所以sizeof(TestC)是6。 共用体(union)所占内存计算方法共用体又名”联合体”, 英文名为union. 当多个数据需要共享内存或者多个数据每次只取其一时，可以利用联合体(union)。在C Programming Language 一书中对于联合体是这么描述的： 联合体是一个结构； 它的所有成员相对于基地址的偏移量都为0； 此结构空间要大到足够容纳最”宽”的成员； 其对齐方式要适合其中所有的成员； 下面解释这四条描述： 由于联合体中的所有成员是共享一段内存的，因此每个成员的存放首地址相对于于联合体变量的基地址的偏移量为0，即所有成员的首地址都是一样的。为了使得所有成员能够共享一段内存，因此该空间必须足够容纳这些成员中最宽的成员。对于这句“对齐方式要适合其中所有的成员”是指其必须符合所有成员的自身对齐方式。 下面举例说明： 1234561 union U2 &#123;3 char s[9];4 int n;5 double d;6 &#125;; s占9字节，n占4字节，d占8字节，因此其至少需9字节的空间。然而其实际大小并不是9，用运算符sizeof测试其大小为16.这是因为这里存在字节对齐的问题，9既不能被4整除，也不能被8整除。 因此补充字节到16，这样就符合所有成员的自身对齐了。从这里可以看出联合体所占的空间不仅取决于最宽成员，还跟所有成员有关系，即其大小必须满足两个条件： 大小足够容纳最宽的成员； 大小能被其包含的所有基本数据类型的大小所整除。 若问题为#pragma pack(4)的情况 void foo() { } ，typedef char* (f)(void);不占字节， 枚举占4个字节， union按最大的变量buff[13]所占字节算为13, 在#pragma pack(2)的情况, 得补齐1个字节变为14才能被2整除, 而#pragma pack(4)的情况得补齐3个字节, 总占16个字节，才可以被4整除, int占4个字节 所以#pragma pack(4)的情况, sizeof(A)为4+16+4=24。 练习注意有陷阱, 32位环境下 1234567891011121314151617181920212223242526272829303132# pragma pack(2)class test_class&#123;public: static float i; union test_union &#123; int bb; char aa[13]; short cc; &#125;; enum test_enum &#123; monday, tuesday, sunday &#125;; virtual void testFunc() &#123;&#125; char xmly;&#125;;# pragma pack()int main()&#123; cout &lt;&lt; "sizeof(test_class) : " &lt;&lt; sizeof(test_class) &lt;&lt; endl; return 0;&#125; 请问打印结果? sizeof(test_class) : 6 为什么呢?注意看共用体 test_union 和枚举 test_enum其实并没有声明变量, 如果写成 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556#include &lt;iostream&gt;using namespace std;# pragma pack(2)class test_class&#123;public: static float i; union test_union &#123; int bb; char aa[13]; short cc; &#125;uVar; enum test_enum &#123; monday, tuesday, sunday &#125;eVar; virtual void testFunc() &#123;&#125; char xmly;&#125;;# pragma pack()enum enum_x &#123; x1=5, x2, x3, x4, &#125;; enum enum_x x=x3; int main()&#123; cout &lt;&lt; "sizeof(test_class) : " &lt;&lt; sizeof(test_class) &lt;&lt; endl; cout &lt;&lt; "x : " &lt;&lt; x &lt;&lt; endl; test_class::test_enum i; i = test_class::monday; cout &lt;&lt; "i : " &lt;&lt; i &lt;&lt; endl; test_class test_obj; test_obj.eVar = test_class::sunday; cout &lt;&lt; test_obj.monday &lt;&lt; endl; cout &lt;&lt; test_class::sunday &lt;&lt; endl; return 0;&#125; 打印结果就为 12345sizeof(test_class) : 24x : 7i : 002]]></content>
      <categories>
        <category>CPP</category>
      </categories>
      <tags>
        <tag>ObjectModel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GCC的分支预测优化__builtin_expect]]></title>
    <url>%2Fblog%2F2015%2F04%2F11%2Flinux_gcc_builtin_expect%2F</url>
    <content type="text"><![CDATA[1. 为什么需要分支预测优化 将流水线引入cpu，可以提高cpu的效率。更简单的说，让cpu可以预先取出下一条指令，可以提供cpu的效率。如下图所示： 取指令 执行指令 输出结果 取指令 执行 可见，cpu流水钱可以减少cpu等待取指令的耗时，从而提高cpu的效率。如果存在跳转指令，那么预先取出的指令就无用了。cpu在执行当前指令时，从内存中取出了当前指令的下一条指令。执行完当前指令后，cpu发现不是要执行下一条指令,而是执行offset偏移处的指令。cpu只能重新从内存中取出offset偏移处的指令。因此，跳转指令会降低流水线的效率，也就是降低cpu的效率。 综上，在写程序时应该尽量避免跳转语句。那么如何避免跳转语句呢？答案就是使用__builtin_expect。 这个指令是gcc引入的，作用是”允许程序员将最有可能执行的分支告诉编译器”。 这个指令的写法为：builtin_expect(EXP, N)。意思是：EXP==N的概率很大。一般的使用方法是将builtin_expect指令封装为LIKELY和UNLIKELY宏。这两个宏的写法如下。 12#define LIKELY(x) __builtin_expect(!!(x), 1) //x很可能为真#define UNLIKELY(x) __builtin_expect(!!(x), 0) //x很可能为假 在很多源码如Linux内核、Glib等,我们都能看到likely()和unlikely()这两个宏,通常这两个宏定义是下面这样的形式。可以看出这2个宏都是使用函数 __builtin_expect()实现的, __builtin_expect()函数是GCC的一个内建函数(build-in function). 2. 函数声明函数__builtin_expect()是GCC v2.96版本引入的, 其声明如下: long __builtin_expect(long exp, long c); 2.1. 功能描述由于大部分程序员在分支预测方面做得很糟糕，所以GCC 提供了这个内建函数来帮助程序员处理分支预测. 你期望 exp 表达式的值等于常量 c, 看 c 的值, 如果 c 的值为0(即期望的函数返回值), 那么 执行 if 分支的的可能性小, 否则执行 else 分支的可能性小(函数的返回值等于第一个参数 exp). GCC在编译过程中，会将可能性更大的代码紧跟着前面的代码，从而减少指令跳转带来的性能上的下降, 达到优化程序的目的. 通常，你也许会更喜欢使用 gcc 的一个参数 ‘-fprofile-arcs’ 来收集程序运行的关于执行流程和分支走向的实际反馈信息,但是对于很多程序来说,数据是很难收集的。 2.2. 参数详解 exp exp 为一个整型表达式, 例如: (ptr != NULL) c c 必须是一个编译期常量, 不能使用变量 2.3. 返回值 返回值等于 第一个参数 exp 2.4. 使用方法与关键字if一起使用.首先要明确一点就是 if (value) 等价于 if (__builtin_expert(value, x)), 与x的值无关. 例子如下: 例子1 : 期望 x == 0, 所以执行func()的可能性小 12345678if (__builtin_expect(x, 0))&#123; func();&#125;else&#123; //do someting&#125; 例子2 : 期望 ptr !=NULL这个条件成立(1), 所以执行func()的可能性小 12345678if (__builtin_expect(ptr != NULL, 1))&#123; //do something&#125;else&#123; func();&#125; 例子3 : 引言中的likely()和unlikely()宏 首先,看第一个参数!!(x), 他的作用是把(x)转变成”布尔值”, 无论(x)的值是多少 !(x)得到的是true或false, !!(x)就得到了原值的”布尔值” 使用 likely() ，执行 if 后面的语句 的机会更大，使用 unlikely()，执行 else 后面的语句的机会更大。 12345678910111213141516171819202122#define likely(x) __builtin_expect(!!(x), 1)#define unlikely(x) __builtin_expect(!!(x), 0)int main(char *argv[], int argc)&#123; int a; /* Get the value from somewhere GCC can't optimize */ a = atoi (argv[1]); if (unlikely (a == 2)) &#123; a++; &#125; else &#123; a--; &#125; printf ("%d\n", a); return 0;&#125; 3. RATIONALE(原理)if else 句型编译后, 一个分支的汇编代码紧随前面的代码,而另一个分支的汇编代码需要使用JMP指令才能访问到. 很明显通过JMP访问需要更多的时间, 在复杂的程序中,有很多的if else句型,又或者是一个有if else句型的库函数,每秒钟被调用几万次, 通常程序员在分支预测方面做得很糟糕, 编译器又不能精准的预测每一个分支,这时JMP产生的时间浪费就会很大, 函数 __builtin_expert() 就是用来解决这个问题的.]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>CPP</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[智能指针笔记]]></title>
    <url>%2Fblog%2F2015%2F04%2F11%2Fsmart_pointer_note%2F</url>
    <content type="text"><![CDATA[有些错误是编译器查不到的, 这种错误是最可怕的, 当项目大了之后,即使用 Valgrind 也很难定位,因为裸指针在团队合作中使用很容易导致其他成员忘记释放或多次释放, 所以在团队合作中一般使用智能指针. 而智能指针用的不好, 结果可能适得其反. 所以我们聊一下智能指针的几点注意事项. 总结自 C++ Primer. 一个简单的包含删除器的例子演示12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758#include&lt;iostream&gt;#include&lt;functional&gt;#include&lt;memory&gt;using std::cout;using std::endl;using std::bind;using namespace std::placeholders;int testBind( int* a, int b, int c )&#123; cout &lt;&lt; *a + b + c &lt;&lt; endl; return *a;&#125;struct Foo&#123; Foo() = default; Foo( const Foo &amp; a ) &#123; data = a.data; std::cout &lt;&lt; "复制构造" &lt;&lt; std::endl; &#125; void print_sum( int n1, int n2 ) &#123; std::cout &lt;&lt; n1 + n2 &lt;&lt; '\n'; &#125; int data = 10;&#125;;int main()&#123; //绑定类成员函数用对象的指针 Foo foo; auto f3 = std::bind( &amp;Foo::print_sum, &amp;foo, 95, _1 ); f3( 5 ); auto check_testBind = std::bind( testBind, std::placeholders::_1, 3, 9 ); int * p = new int( 7 ); cout &lt;&lt; check_testBind( p ) &lt;&lt; endl; shared_ptr&lt;int&gt; pi( new int(), check_testBind ); *pi = 88; shared_ptr&lt;int&gt; pii( new int( 12 ), std::bind( testBind, std::placeholders::_1, 32, 19 ) ); std::function&lt; int( int* ) &gt; test_function_bind = std::bind( testBind, std::placeholders::_1, 331, 9 ); cout &lt;&lt; "test_function_bind( p, 331, 9 ) = " &lt;&lt; test_function_bind( p ) &lt;&lt; endl;; shared_ptr&lt;int&gt; piii( new int( 112 ), test_function_bind ); return 0;&#125; 打印结果 : 100 119 7 347 test_function_bind( p, 331, 9 ) = 7 452 63 100 请按任意键继续. . .智能指针陷阱智能指针可以提供对动态分配的内存安全而又方便的管理，但这建立在正确使用的前提下 。 为了正确使用智能指针，我们必须坚持一些基本规范 : 不使用相同的内置指针值初始化(或 reset) 多个智能指针 。 不 delete get ( ) 返 回的指针 。 不使用 get () 初始化或 reset 另 一 个智能指针 。 如果你使用 get () 返 回的指针，记住当最后一个对应的智能指针销 毁 后， 你 的指 针就 变为无 效 了 。 如果你使用智能指针管理的资源不是 new 分配的内存 ， 记住传递给它一个删除器( 参见 12. 1.4 节 ， 第 415 页和 12. 1.5 节 ， 第 419 页)。 尽量用make_shared而非newshared_ptr 可以协调对象的析构 ， 但这仅限于其自身的拷贝 ( 也 是 shared_ptr)之间。 这也是为什么我们推荐使用 make_shared 而不是 new 的原因 。 这样 ， 我们就能在分配对象的同时就将 shared_ptr 与之绑定，从而避免了无意中将同一块内存绑定到多个独立创建的 shared_ptr 上 。(这是最容易犯的错) 总结 : 所以我们要尽量一开始就用make_shared来分配动态内存, 而不是先new一个出来, 再找机会将它转为智能指针. 考虑下面对 shared_ptr 进行操作的函数 : 12345// 在函数被调用时 ptr 被创建并初始化void process(shared_ptr&lt;int&gt; ptr)&#123; // 使用 ptr&#125; // ptr 离 开作用域，被销毁 process 的参数是传值方式传递的，因此实参会被拷贝到 ptr 巾 。 拷贝 一 个 shared_ptr会递增其引用讨数，因此， 在 process 运行过程中，引用七| 数值至少为 2 。 当 process结束时， ptr 的引用计数会边喊，但不会变为 0 。 因此，当用音11变 11 ptr 被销毁时， ptr指向的内存不会被释放 。 使用此函数的正确方法是传递给它一个 shared_ptr : 123shared_ptr&lt;int&gt; p(new int(42)) ; // 引用计数为 1process(p); // 拷贝 p 会递增它的引用计数 ;在 process 中引用计数位为 2int i = *p; // 正确:引用计数位为 1 虽然不能传递给 process 一 个内置指针，但可以传递 给它 一 个(临时的)shared_ptr ， 这个 shared_ptr 是用 一个内 置指针显式构造的 。 但是，这样做很可能会导致错误 : 1234int *x(new int(1024)); // 危险 x 是一个普通指针，不是一个智能指针process(x) ; // 错误 : 不能将 int* 转换为 一个 shared_ptr&lt;int&gt;process(shared_ptr&lt;int&gt;(x)); // 合法的，但内存会被释放!int j = *x ; //未定义的 x 是一个空悬指针! 在上面的调用中 ， 我们将一个临时 shared_ptr 传递给 process 。 当这个调用所在的表达式结束时，这个临时对象就被销毁了 。 销毁这个临时变量会递减引用计数，此时引用计数就变为 0 了 。 因此，当临时对象被销毁时 ， 它所指向的内存会被释放 。但 x 继续指 向 (已经释放的)内存，从而变成一个空悬指针。如果试图使用 x 的值，其行为是未定义的 。 当将一个 shared_ptr 绑定到一个普通指针时 ， 我们就将内存的管理责任交给了这个 shared_ptr 一旦这样做 了 ， 我们就不应该再使用内置指针来访问 shared_ptr 所指向的内存了 不要使用 get 初始化另一个智能指针或为智能指针赋值智能指针类型定义了 一个名为 get 的函数(参见表 1 2. J )，它返回一个内置指针，指向智能指针管理的对象 。 此函数是为了这样一种情况而设计的 : 我们需要向不能使用智能指针的代码传递一个内置指针。使用 get 返回的指告| 的代码不能 delete 此指针 。虽然编译器不会给出错误信息 ， 但将另一个智能指针也绑定到 get 返回的指针上是错误的 : 1234567shared_ptr&lt;int&gt; p(new int(42)) ; // 引 用计数为 1int *q = p . get() ; // 正确 · 但使用 q 时妥注意，不要让它管理的指针被释放&#123; // 新程序块// 未定义:两个独立的 shared_ptr 指 向 相同的内存 shared ptr&lt;int&gt; (q) ;&#125; // 程序块结束， q 被销毁 ， 它指向的内存被待放int foo = *p ; // 未定义 p 指向的内存 已 经被释放了 在本例中， p 和 q 指 向相同的内存。由于它们是相互独立创建的，因此各自的引用计数都是 1。 当 q 所在的程序块结束时 ， q 被销毁 ， 这会导致 q 指向的内存被释放 。 从而 p 变成一个空悬指针，意味着当我们试图使用 p 时，将发生未定义的行为 。 而且 ， 当 p 被销毁时 ，这块内存会被第二次 delete 。 get 用来将指针的访问权限传递给代码，你只有在确定代码不会 get.特别是，永远不要用 get 初始化另一个智能指针 del ete 指针或者为另一个智能指针赋值.]]></content>
      <categories>
        <category>CPP</category>
      </categories>
      <tags>
        <tag>CPP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GCC的原子操作函数]]></title>
    <url>%2Fblog%2F2015%2F04%2F11%2Flinux_gcc_atomic%2F</url>
    <content type="text"><![CDATA[linux支持的哪些操作是具有原子特性的？知道这些东西是理解和设计无锁化编程算法的基础。 下面的东西整理自网络。先感谢大家的分享！ 原子操作的api函数gcc从4.1.2以后提供了 __sync_* 系列的下面几类的内嵌函数，提供用于针对数字或布尔型变量的原子操作。 n++类这组返回更新前的值 type __sync_fetch_and_add (type *ptr, type value, ...) type __sync_fetch_and_sub (type *ptr, type value, ...) type __sync_fetch_and_or (type *ptr, type value, ...) type __sync_fetch_and_and (type *ptr, type value, ...) type __sync_fetch_and_xor (type *ptr, type value, ...) type __sync_fetch_and_nand (type *ptr, type value, ...)++n类这组返回更新后的值 type __sync_add_and_fetch (type *ptr, type value, ...) type __sync_sub_and_fetch (type *ptr, type value, ...) type __sync_or_and_fetch (type *ptr, type value, ...) type __sync_and_and_fetch (type *ptr, type value, ...) type __sync_xor_and_fetch (type *ptr, type value, ...) type __sync_nand_and_fetch (type *ptr, type value, ...)type可以是1,2,4或8字节长度的int类型，即： int8_t / uint8_t int16_t / uint16_t int32_t / uint32_t int64_t / uint64_t后面的可扩展参数(…)用来指出哪些变量需要memory barrier,因为目前gcc实现的是full barrier（类似于linux kernel 中的mb(),表示这个操作之前的所有内存操作不会被重排序到这个操作之后）,所以可以略掉这个参数。 CAS类CAS 即 compare-and-swap , 下面这两个函数提供原子的比较和交换，如果 *ptr == oldval,就将 newval 写入 *ptr 此函数在相等并写入的情况下返回 true bool __sync_bool_compare_and_swap (type *ptr, type oldval, type newval, ...) /* 对应的伪代码 */ { if (*ptr == oldval) { *ptr = newval; return true; } else { return false; } } 此函数在返回 oldval type __sync_val_compare_and_swap (type *ptr, type oldval, type newval, ...) /* 对应的伪代码 */ { if (*ptr == oldval) { *ptr = newval; } return oldval; } 其他原子操作type __sync_lock_test_and_set (type *ptr, type value, ...)将 *ptr 设为value并返回 *ptr 操作之前的值。 void __sync_lock_release (type *ptr, ...)将 *ptr 置 0 内存栅障内存栅障主要是处理不同cpu运作时（主要是执行不同线程代码时），一个cpu对内存的操作的原子性，也就保证其他cpu见到的内存单元数据的正确性。 内存栅障介绍如果有对某一变量上写锁， 就不能在不获得相应的锁时对其进行读取操作。内存栅的作用在于保证内存操作的相对顺序， 但并不保证内存操作的严格时序。 内存栅并不保证 CPU 将本地快取缓存或存储缓冲的内容刷写回内存， 而是在锁释放时确保其所保护的数据， 对于能看到刚释放的那个锁的 CPU 或设备可见。持有内存栅的 CPU 可以在其快取缓存或存储缓冲中将数据保持其所希望的、 任意长的时间， 但如果其它 CPU 在同一数据元上执行原子操作，则第一个 CPU 必须保证， 其所更新的数据值， 以及内存栅所要求的任何其它操作， 对第二个 CPU 可见。 __sync_synchronize (...)发出一个full barrier. 内存栅障应用对于执行一条指令，操作到4个寄存器时，如： write1(dev.register_size,size); write1(dev.register_addr,addr); write1(dev.register_cmd,READ); write1(dev.register_control,GO);最后一个寄存器是控制寄存器，在所有的参数都设置好之后向其发出指令，设备开始读取参数. 如果最后一条write1被换到了前几条语句之前，那么肯定不是我们所期望的，这时候我们可以在最后一条语句之前加入一个memory barrier,强制cpu执行完前面的写入以后再执行最后一条： write1(dev.register_size,size); write1(dev.register_addr,addr); write1(dev.register_cmd,READ); __sync_synchronize(); write1(dev.register_control,GO);memory barrier有几种类型： acquire barrier : 不允许将barrier之后的内存读取指令移到barrier之前（linux kernel中的wmb()）。 release barrier : 不允许将barrier之前的内存读取指令移到barrier之后 (linux kernel中的rmb())。 full barrier : 以上两种barrier的合集(linux kernel中的mb())。 原子操作应用范围原子操作只允许一次更新或读一个内存单元。 需要原子地更新多个单元时， 就必须使用锁来代替它了。 例如， 如果需要更新两个相互关联的计数器时， 就必须使用锁， 而不是两次单独的原子操作了。 原子操作例子例子代码： 123456789101112131415161718192021222324252627282930#include &lt;stdio.h&gt; #include &lt;pthread.h&gt; #include &lt;stdlib.h&gt; static int count = 0; void *test_func(void *arg) &#123; int i=0; for(i=0;i&lt;20000;++i)&#123; __sync_fetch_and_add(&amp;count,1); &#125; return NULL; &#125; int main(int argc, const char *argv[]) &#123; pthread_t id[20]; int i = 0; for(i=0;i&lt;20;++i)&#123; pthread_create(&amp;id[i],NULL,test_func,NULL); &#125; for(i=0;i&lt;20;++i)&#123; pthread_join(id[i],NULL); &#125; printf("%d\n",count); return 0; &#125; 原子操作封装使用根据常用的原子操作，封装一些实用的接口 : 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697//原子操作 //设置值 int lock_set(volatile int &amp;a, int value) &#123; __sync_val_compare_and_swap(&amp;a, a, value); return a; &#125; //加1 int lock_inc(volatile int &amp;n) &#123; return __sync_fetch_and_add(&amp;n, 1); &#125; //减1 int lock_dec(volatile int &amp;n) &#123; return __sync_fetch_and_sub(&amp;n, 1); &#125; //加值value int lock_add(volatile int &amp;n, int value) &#123; return __sync_fetch_and_add(&amp;n, value); &#125; //减值value int lock_sub(volatile int &amp;n, int value) &#123; return __sync_fetch_and_sub(&amp;n, value); &#125; //位或value int lock_or(volatile int &amp;n, int value) &#123; return __sync_fetch_and_or(&amp;n, value); &#125; //位与value int lock_and(volatile int &amp;n, int value) &#123; return __sync_fetch_and_and(&amp;n, value); &#125; //异或value int lock_xor(volatile int &amp;n, int value) &#123; return __sync_fetch_and_xor(&amp;n, value); &#125; //无符号类型（函数重载） //设置值 unsigned int lock_set(volatile unsigned int &amp;a, unsigned int value) &#123; __sync_val_compare_and_swap(&amp;a, a, value); return a; &#125; //加1 unsigned int lock_inc(volatile unsigned int &amp;n) &#123; return __sync_fetch_and_add(&amp;n, 1); &#125; //减1 unsigned int lock_dec(volatile unsigned int &amp;n) &#123; return __sync_fetch_and_sub(&amp;n, 1); &#125; //加值value unsigned int lock_add(volatile unsigned int &amp;n, unsigned int value) &#123; return __sync_fetch_and_add((int*)&amp;n, value); &#125; //减值value unsigned int lock_sub(volatile unsigned int &amp;n, unsigned int value) &#123; return __sync_fetch_and_sub((int*)&amp;n, value); &#125; //位或value unsigned int lock_or(volatile unsigned int &amp;n, unsigned int value) &#123; return __sync_fetch_and_or((int*)&amp;n, value); &#125; //位与value unsigned int lock_and(volatile unsigned int &amp;n, unsigned int value) &#123; return __sync_fetch_and_and((int*)&amp;n, value); &#125; //异或value unsigned int lock_xor(volatile unsigned int &amp;n, unsigned int value) &#123; return __sync_fetch_and_xor((int*)&amp;n, value); &#125;]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>CPP</tag>
        <tag>Linux</tag>
        <tag>MultiThread</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux一些不要想当然的事(一)之目录权限]]></title>
    <url>%2Fblog%2F2015%2F03%2F18%2Flinux_directory_permission%2F</url>
    <content type="text"><![CDATA[目录的可读/可写/可执行权限不要把目录的这几个权限和档案的这几个权限混淆了, 不要想当然的以为是差不多的, 差很多!记忆技巧 : 档案的rwx是针对于档案的内容来设计的, 而目录的rwx是针对于目录的文件名列表来设计的 . . . 目录可读r 目录可读权限r : 只能获得文件列表 特别注意:如果一个目录为非空, 却没有r权限, 即使你有wx的权限, 你用rm -r也是删不掉的, 因为没有r权限拿不到这个目录的文件列表, rm -r 自然也就不晓得要删除什么东西了.只有求助root了 123456789101112131415161718192021b@b-VirtualBox:~/my_temp_test/abc$ mkdir tempb@b-VirtualBox:~/my_temp_test/abc$ touch temp/ddb@b-VirtualBox:~/my_temp_test/abc$ ls tempddb@b-VirtualBox:~/my_temp_test/abc$ chmod 444 tempb@b-VirtualBox:~/my_temp_test/abc$ ls templs: cannot access temp/dd: Permission deniedddb@b-VirtualBox:~/my_temp_test/abc$ cd temp/bash: cd: temp/: Permission deniedb@b-VirtualBox:~/my_temp_test/abc$ cat temp/dd cat: temp/dd: Permission deniedb@b-VirtualBox:~/my_temp_test/abc$ touch temp/yytouch: cannot touch ‘temp/yy’: Permission deniedb@b-VirtualBox:~/my_temp_test/abc$ rm temp/dd rm: cannot remove ‘temp/dd’: Permission deniedb@b-VirtualBox:~/my_temp_test/abc$ rm -r temprm: descend into write-protected directory ‘temp’? yrm: cannot remove ‘temp/dd’: Permission deniedrm: remove write-protected directory ‘temp’? yrm: cannot remove ‘temp’: Directory not empty 目录可写w 目录可写权限w : 代表可以在目录下增加或删除档案和目录和改名(但是必须得有目录可执行权限x的支持才可以, 所以一般有w就会有x) 不要和档案的可写权限混淆了, 即使没有目录可写权限, 有目录可执行x也是可以修改目录下的档案的, 只要拥有要修改的那个档案的可写权限既可. 但也要注意的是: 档案的w是针对于档案的内容来说的, 你可以编辑修改他的内容, 但是如果想删除这个档案, 你需要这个档案所在的目录的w权限. 1234567891011121314b@b-VirtualBox:~/my_temp_test/abc$ chmod 222 tempb@b-VirtualBox:~/my_temp_test/abc$ mkdir temp/uumkdir: cannot create directory ‘temp/uu’: Permission deniedb@b-VirtualBox:~/my_temp_test/abc$ touch temp/ootouch: cannot touch ‘temp/oo’: Permission deniedb@b-VirtualBox:~/my_temp_test/abc$ chmod 333 temp b@b-VirtualBox:~/my_temp_test/abc$ mkdir temp/uub@b-VirtualBox:~/my_temp_test/abc$ touch temp/oob@b-VirtualBox:~/my_temp_test/abc$ rm -r temprm: cannot remove ‘temp’: Permission deniedb@b-VirtualBox:~/my_temp_test/abc$ rm -r temp/uub@b-VirtualBox:~/my_temp_test/abc$ rm temp/oob@b-VirtualBox:~/my_temp_test/abc$ ls templs: cannot open directory temp: Permission denied 目录可执行x 目录可执行权限x : 有进入目录的权限, 有在这个目录下执行命令的权限. 但不可以删除或者增加档案和目录(因为不具备目录的可写权限w) 1234567891011121314151617b@b-VirtualBox:~/my_temp_test/abc$ chmod 111 temp/b@b-VirtualBox:~/my_temp_test/abc$ ls templs: cannot open directory temp: Permission deniedb@b-VirtualBox:~/my_temp_test/abc$ echo &quot;xxd&quot; &gt; temp/ddb@b-VirtualBox:~/my_temp_test/abc$ cat temp/ddxxdb@b-VirtualBox:~/my_temp_test/abc$ touch temp/yytouch: cannot touch ‘temp/yy’: Permission deniedb@b-VirtualBox:~/my_temp_test/abc$ rm temp/ddrm: cannot remove ‘temp/dd’: Permission deniedb@b-VirtualBox:~/my_temp_test/abc$ rm -r temprm: descend into write-protected directory ‘temp’? yrm: remove write-protected directory ‘temp’? yrm: cannot remove ‘temp’: Permission deniedb@b-VirtualBox:~/my_temp_test/abc$ cd tempb@b-VirtualBox:~/my_temp_test/abc/temp$ lsls: cannot open directory .: Permission denied]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux常用运维命令(df和free和top)笔记整理(三)]]></title>
    <url>%2Fblog%2F2015%2F03%2F11%2Flinux_command_df_free_top%2F</url>
    <content type="text"><![CDATA[df df命令用于显示磁盘分区上的可使用的磁盘空间。默认显示单位为KB。可以利用该命令来获取硬盘被占用了多少空间，目前还剩下多少空间等信息。 -a或–all：包含全部的文件系统； –block-size=&lt;区块大小&gt;：以指定的区块大小来显示区块数目； -h或–human-readable：以可读性较高的方式来显示信息； -H或–si：与-h参数相同，但在计算时是以1000 Bytes为换算单位而非1024 Bytes； -i或–inodes：显示inode的信息； -k或–kilobytes：指定区块大小为1024字节； -l或–local：仅显示本地端的文件系统； -m或–megabytes：指定区块大小为1048576字节； –no-sync：在取得磁盘使用信息前，不要执行sync指令，此为预设值； -P或–portability：使用POSIX的输出格式； –sync：在取得磁盘使用信息前，先执行sync指令； -t&lt;文件系统类型&gt;或–type=&lt;文件系统类型&gt;：仅显示指定文件系统类型的磁盘信息； -T或–print-type：显示文件系统的类型； -x&lt;文件系统类型&gt;或–exclude-type=&lt;文件系统类型&gt;：不要显示指定文件系统类型的磁盘信息； –help：显示帮助； –version：显示版本信息 . . . df常用用法：df -h12345678910b@b-VirtualBox:~$ df -hFilesystem Size Used Avail Use% Mounted onudev 990M 4.0K 990M 1% /devtmpfs 201M 968K 200M 1% /run/dev/sda1 8.8G 4.1G 4.3G 49% /none 4.0K 0 4.0K 0% /sys/fs/cgroupnone 5.0M 0 5.0M 0% /run/locknone 1001M 76K 1001M 1% /run/shmnone 100M 36K 100M 1% /run/user/dev/sr0 57M 57M 0 100% /media/b/VBOXADDITIONS_5.1.22_115126 free free命令可以显示当前系统未使用的和已使用的内存数目，还可以显示被内核使用的内存缓冲区。 free常用用法：free -m或者free -g12345b@b-VirtualBox:~$ free -m total used free shared buffers cachedMem: 2000 1231 768 9 72 456-/+ buffers/cache: 702 1297Swap: 1021 0 1021 top PID：进程的ID USER：进程所有者 PR：进程的优先级别，越小越优先被执行 NInice：值 VIRT：进程占用的虚拟内存 RES：进程占用的物理内存 SHR：进程使用的共享内存 S：进程的状态。S表示休眠，R表示正在运行，Z表示僵死状态，N表示该进程优先值为负数 %CPU：进程占用CPU的使用率 %MEM：进程使用的物理内存和总内存的百分比 TIME+：该进程启动后占用的总的CPU时间，即占用CPU使用时间的累加值。 COMMAND：进程启动命令名称 另 : 1：使用 ps -ef|grep xxx 命令查找需要查看的进程，xxx是进程名字 2：top -p pid 查看程序的情况 3：ps -aux | grep process_name 4：cat /proc/pid/status这里会打印出当前进程详细的情况，其中，内存是 VmRSS。( 注：pid是要替换成一个id数字的。)]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Command</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux常用运维命令(netstat和lsof)笔记整理(二)]]></title>
    <url>%2Fblog%2F2015%2F03%2F09%2Flinux_command_netstat_lsof%2F</url>
    <content type="text"><![CDATA[netstat netstat命令用来打印Linux中网络系统的状态信息，可让你得知整个Linux系统的网络情况。 -a或–all：显示所有连线中的Socket； -A&lt;网络类型&gt;或–&lt;网络类型&gt;：列出该网络类型连线中的相关地址； -c或–continuous：持续列出网络状态； -C或–cache：显示路由器配置的快取信息； -e或–extend：显示网络其他相关信息； -F或–fib：显示FIB； -g或–groups：显示多重广播功能群组组员名单； -h或–help：在线帮助； -i或–interfaces：显示网络界面信息表单； -l或–listening：显示监控中的服务器的Socket； -M或–masquerade：显示伪装的网络连线； -n或–numeric：直接使用ip地址，而不通过域名服务器； -N或–netlink或–symbolic：显示网络硬件外围设备的符号连接名称； -o或–timers：显示计时器； -p或–programs：显示正在使用Socket的程序识别码和程序名称； -r或–route：显示Routing Table； -s或–statistice：显示网络工作信息统计表； -t或–tcp：显示TCP传输协议的连线状况； -u或–udp：显示UDP传输协议的连线状况； -v或–verbose：显示指令执行过程； -V或–version：显示版本信息； -w或–raw：显示RAW传输协议的连线状况； -x或–unix：此参数的效果和指定”-A unix”参数相同； –ip或–inet：此参数的效果和指定”-A inet”参数相同。 . . . netstat常用用法：netstat -anlpnetstat -anlpt的含义是 ： 列出所有处于使用tcp协议的 Sockets 123456789b@b-VirtualBox:~$ sudo netstat -anlptActive Internet connections (servers and established)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program nametcp 0 0 127.0.1.1:53 0.0.0.0:* LISTEN 1075/dnsmasq tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 935/sshd tcp 0 0 127.0.0.1:631 0.0.0.0:* LISTEN 2271/cupsd tcp6 0 0 :::22 :::* LISTEN 935/sshd tcp6 0 0 ::1:631 :::* LISTEN 2271/cupsd tcp6 1 0 ::1:50654 ::1:631 CLOSE_WAIT 1027/cups-browsed 查看udp的就是netstat -anlpu；只查看tcp和udp的就是netstat -anlptu lsof （list open files） lsof命令用于查看你进程开打的文件，打开文件的进程，进程打开的端口(TCP、UDP)。找回/恢复删除的文件。是十分方便的系统监视工具，因为lsof命令需要访问核心内存和各种文件，所以需要root用户执行。 在linux环境下，任何事物都以文件的形式存在，通过文件不仅仅可以访问常规数据，还可以访问网络连接和硬件。所以如传输控制协议 (TCP) 和用户数据报协议 (UDP) 套接字等，系统在后台都为该应用程序分配了一个文件描述符，无论这个文件的本质如何，该文件描述符为应用程序与基础操作系统之间的交互提供了通用接口。因为应用程序打开文件的描述符列表提供了大量关于这个应用程序本身的信息，因此通过lsof工具能够查看这个列表对系统监测以及排错将是很有帮助的。 -a：列出打开文件存在的进程； -c&lt;进程名&gt;：列出指定进程所打开的文件； -g：列出GID号进程详情； -d&lt;文件号&gt;：列出占用该文件号的进程； +d&lt;目录&gt;：列出目录下被打开的文件； +D&lt;目录&gt;：递归列出目录下被打开的文件； -n&lt;目录&gt;：列出使用NFS的文件； -i&lt;条件&gt;：列出符合条件的进程。（4、6、协议、:端口、 @ip ） -p&lt;进程号&gt;：列出指定进程号所打开的文件； -u：列出UID号进程详情； -h：显示帮助信息； -v：显示版本信息 -R: 显示PPID（父进程ID） lsof常用用法1：lsof -pps -ef |grep sshd|grep -v grep| awk ‘{print $2}’|xargs sudo lsof -p的含义是：列出sshd进程打开的所有文件描述符 123456789101112131415161718192021222324252627282930313233343536373839b@b-VirtualBox:~$ ps -ef |grep sshd|grep -v grep| awk &apos;&#123;print $2&#125;&apos;|xargs sudo lsof -plsof: WARNING: can&apos;t stat() fuse.gvfsd-fuse file system /run/user/1000/gvfs Output information may be incomplete.COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEsshd 935 root cwd DIR 8,1 4096 2 /sshd 935 root rtd DIR 8,1 4096 2 /sshd 935 root txt REG 8,1 770944 301274 /usr/sbin/sshdsshd 935 root mem REG 8,1 43616 136982 /lib/x86_64-linux-gnu/libnss_files-2.19.sosshd 935 root mem REG 8,1 47760 136992 /lib/x86_64-linux-gnu/libnss_nis-2.19.sosshd 935 root mem REG 8,1 39824 136978 /lib/x86_64-linux-gnu/libnss_compat-2.19.sosshd 935 root mem REG 8,1 101240 137033 /lib/x86_64-linux-gnu/libresolv-2.19.sosshd 935 root mem REG 8,1 14256 136950 /lib/x86_64-linux-gnu/libkeyutils.so.1.4sshd 935 root mem REG 8,1 43672 403209 /usr/lib/x86_64-linux-gnu/libkrb5support.so.0.1sshd 935 root mem REG 8,1 186824 403203 /usr/lib/x86_64-linux-gnu/libk5crypto.so.3.1sshd 935 root mem REG 8,1 31792 137035 /lib/x86_64-linux-gnu/librt-2.19.sosshd 935 root mem REG 8,1 141574 137027 /lib/x86_64-linux-gnu/libpthread-2.19.sosshd 935 root mem REG 8,1 252032 137010 /lib/x86_64-linux-gnu/libpcre.so.3.13.1sshd 935 root mem REG 8,1 14664 136924 /lib/x86_64-linux-gnu/libdl-2.19.sosshd 935 root mem REG 8,1 97296 136976 /lib/x86_64-linux-gnu/libnsl-2.19.sosshd 935 root mem REG 8,1 1840928 136907 /lib/x86_64-linux-gnu/libc-2.19.sosshd 935 root mem REG 8,1 14592 136916 /lib/x86_64-linux-gnu/libcom_err.so.2.1sshd 935 root mem REG 8,1 831616 403207 /usr/lib/x86_64-linux-gnu/libkrb5.so.3.3sshd 935 root mem REG 8,1 290520 403037 /usr/lib/x86_64-linux-gnu/libgssapi_krb5.so.2.2sshd 935 root mem REG 8,1 43368 136917 /lib/x86_64-linux-gnu/libcrypt-2.19.sosshd 935 root mem REG 8,1 100728 137070 /lib/x86_64-linux-gnu/libz.so.1.2.8sshd 935 root mem REG 8,1 10680 137062 /lib/x86_64-linux-gnu/libutil-2.19.sosshd 935 root mem REG 8,1 1934624 136919 /lib/x86_64-linux-gnu/libcrypto.so.1.0.0sshd 935 root mem REG 8,1 281552 136921 /lib/x86_64-linux-gnu/libdbus-1.so.3.7.6sshd 935 root mem REG 8,1 14536 440884 /usr/lib/x86_64-linux-gnu/libck-connector.so.0.0.0sshd 935 root mem REG 8,1 134296 137037 /lib/x86_64-linux-gnu/libselinux.so.1sshd 935 root mem REG 8,1 55856 136999 /lib/x86_64-linux-gnu/libpam.so.0.83.1sshd 935 root mem REG 8,1 104936 136897 /lib/x86_64-linux-gnu/libaudit.so.1.0.0sshd 935 root mem REG 8,1 36632 137067 /lib/x86_64-linux-gnu/libwrap.so.0.7.6sshd 935 root mem REG 8,1 149120 136883 /lib/x86_64-linux-gnu/ld-2.19.sosshd 935 root 0u CHR 1,3 0t0 6 /dev/nullsshd 935 root 1u CHR 1,3 0t0 6 /dev/nullsshd 935 root 2u CHR 1,3 0t0 6 /dev/nullsshd 935 root 3u IPv4 10479 0t0 TCP *:ssh (LISTEN)sshd 935 root 4u IPv6 10481 0t0 TCP *:ssh (LISTEN) ps -ef | grep sshd | grep -v grep : 获取ps打印出来的列表中的sshd进程所在的那一行（grep -v grep的含义是清除掉包含“grep”字符串的那一行）, 即为： 12b@b-VirtualBox:~$ ps -ef | grep sshd | grep -v greproot 935 1 0 17:37 ? 00:00:00 /usr/sbin/sshd -D awk ‘{print $2}’ : 获取上述命令打印出来结果的第2列（上述结果的第二列为sshd的pid， 是935） xargs sudo lsof -p ： 列出上述结果pid为935的进程打开的所有文件描述符， 等价于sudo lsof -p 935的结果 因为在 Linux 中一切皆为文件, socket 不也例外, 我们在上面的例子的最后两行可以看到 12sshd 935 root 3u IPv4 10479 0t0 TCP *:ssh (LISTEN)sshd 935 root 4u IPv6 10481 0t0 TCP *:ssh (LISTEN) 10479 和 10481 就是 ssh 打开的两个socket文件描述符了. 用命令 ls -l /proc/命令ID/fd , 也可查看所打开的文件.本例中pid为 935 , 则相应的命令为 ls -l /proc/935/fd lsof常用用法：lsof -i:sudo lsof -i:22含义为列出占用22的进程 1234b@b-VirtualBox:~$ sudo lsof -i:22COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEsshd 935 root 3u IPv4 10479 0t0 TCP *:ssh (LISTEN)sshd 935 root 4u IPv6 10481 0t0 TCP *:ssh (LISTEN)]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Command</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux常用运维命令(iostat)笔记整理(一)]]></title>
    <url>%2Fblog%2F2015%2F03%2F07%2Flinux_command_iostat%2F</url>
    <content type="text"><![CDATA[在linux服务器开发过程中， 经常需要各种命令配合来查看各种状态，所以整理了一些老的笔记来备忘。 iostat iostat主要用于监控系统设备的IO负载情况，iostat首次运行时显示自系统启动开始的各项统计信息，之后运行iostat将显示自上次运行该命令以后的统计信息。用户可以通过指定统计的次数和时间来获得所需的统计信息 -c 仅显示CPU统计信息.与-d选项互斥. -d 仅显示磁盘统计信息.与-c选项互斥. -k 以K为单位显示每秒的磁盘请求数,默认单位块. -t 在输出数据时,打印搜集数据的时间. -V 打印版本号和帮助信息. -x 输出扩展信息. . . . iostat常用用法1：iostat -d指定采样时间间隔与采样次数 我们可以以”iostat interval [count] ”形式指定iostat命令的采样间隔和采样次数： 12345678910linux # iostat -d 1 2Linux 2.6.16.60-0.21-smp (linux) 06/13/12Device: tps Blk_read/s Blk_wrtn/s Blk_read Blk_wrtnsda 0.55 8.93 36.27 6737086 27367728sdb 0.00 0.00 0.00 928 0Device: tps Blk_read/s Blk_wrtn/s Blk_read Blk_wrtnsda 2.00 0.00 72.00 0 72sdb 0.00 0.00 0.00 0 0 以上命令输出Device的信息，采样时间为1秒，采样2次，若不指定采样次数，则iostat会一直输出采样信息，直到按”ctrl+c”退出命令。注意，第1次采样信息与单独执行iostat的效果一样，为从系统开机到当前执行时刻的统计信息。 iostat常用用法2： iostat -xdk123456linux # iostat -xdk 1Linux 2.6.16.60-0.21-smp (linux) 06/13/12……Device: rrqm/s wrqm/s r/s w/s rkB/s wkB/s avgrq-sz avgqu-sz await svctm %utilsda 0.00 9915.00 1.00 90.00 4.00 34360.00 755.25 11.79 120.57 6.33 57.60 以上各列的含义如下： rrqm/s: 每秒对该设备的读请求被合并次数，文件系统会对读取同块(block)的请求进行合并 wrqm/s: 每秒对该设备的写请求被合并次数 r/s: 每秒完成的读次数 w/s: 每秒完成的写次数 rkB/s: 每秒读数据量(kB为单位) wkB/s: 每秒写数据量(kB为单位) avgrq-sz:平均每次IO操作的数据量(扇区数为单位) avgqu-sz: 平均等待处理的IO请求队列长度 await: 平均每次IO请求等待时间(包括等待时间和处理时间，毫秒为单位) svctm: 平均每次IO请求的处理时间(毫秒为单位) %util: 采用周期内用于IO操作的时间比率，即IO队列非空的时间比率 对于以上示例输出，我们可以获取到以下信息： 每秒向磁盘上写30M左右数据(wkB/s值)每秒有91次IO操作(r/s+w/s)，其中以写操作为主体平均每次IO请求等待处理的时间为120.57毫秒，处理耗时为6.33毫秒等待处理的IO请求队列中，平均有11.79个请求驻留]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Command</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python的struct模块]]></title>
    <url>%2Fblog%2F2015%2F03%2F02%2Fpython%E7%9A%84struct%E6%A8%A1%E5%9D%97%2F</url>
    <content type="text"><![CDATA[struct, 这玩意c/c++也有, 顾名思义, 能联想到这玩意是啥了 模块的主要作用就是对python基本类型值与 用python字符串格式表示的C struct类型间 的转化（This module performs conversions between Python values and C structs represented as Python strings.） 基本用法123456789101112import structimport binasciivalues = (1, 'abc', 2.7)s = struct.Struct('I3sf')packed_data = s.pack(*values)unpacked_data = s.unpack(packed_data) print 'Original values:', valuesprint 'Format string :', s.formatprint 'Uses :', s.size, 'bytes'print 'Packed Value :', binascii.hexlify(packed_data)print 'Unpacked Type :', type(unpacked_data), ' Value:', unpacked_data 输出为: 12345Original values: (1, &apos;abc&apos;, 2.7) Format string : I3sf Uses : 12 bytes Packed Value : 0100000061626300cdcc2c40 Unpacked Type : &lt;type &apos;tuple&apos;&gt; Value: (1, &apos;abc&apos;, 2.700000047683716) 代码中， 首先定义了一个元组数据， 包含int、string、float三种数据类型， 然后定义了struct对象，并制定了format‘I3sf’， I 表示int， 3s表示三个字符长度的字符串， f 表示 float。最后通过struct的pack和unpack进行打包和解包。通过输出结果可以发现， value被pack之后， 转化为了一段二进制字节串， 而unpack可以把该字节串再转换回一个元组， 但是值得注意的是对于float的精度发生了改变， 这是由一些比如操作系统等客观因素所决定的。打包之后的数据所占用的字节数与C语言中的struct十分相似。定义format可以参照官方api提供的对照表： 字节序设置另一方面，打包的后的字节顺序默认上是由操作系统的决定的， 当然struct模块也提供了自定义字节顺序的功能， 可以指定大端存储、小端存储等特定的字节顺序， 对于底层通信的字节顺序是十分重要的， 不同的字节顺序和存储方式也会导致字节大小的不同。在format字符串前面加上特定的符号即可以表示不同的字节顺序存储方式， 例如采用小端存储 s = struct.Struct(‘&lt;I3sf’)就可以了。官方api library 也提供了相应的对照列表：]]></content>
      <categories>
        <category>Script</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL入门二之一些小注意点]]></title>
    <url>%2Fblog%2F2015%2F03%2F02%2FMySQL%E5%85%A5%E9%97%A8%E4%BA%8C%2F</url>
    <content type="text"><![CDATA[distinct关键字distinct是应用于所有列的, 而不是某一个列 1234567891011121314151617181920212223242526272829mysql&gt; select * from test_table;+------+------+| one | two |+------+------+| 56 | 12 || 52 | 10 || 56 | 12 || 56 | 13 |+------+------+4 rows in set (0.00 sec)mysql&gt; select distinct one, two from test_table;+------+------+| one | two |+------+------+| 56 | 12 || 52 | 10 || 56 | 13 |+------+------+3 rows in set (0.00 sec)mysql&gt; select distinct one from test_table;+------+| one |+------+| 56 || 52 |+------+2 rows in set (0.00 sec) . . . and关键字and的组合优先级比or高 12345678910111213141516171819202122232425262728293031323334353637mysql&gt; select * from test_table;+------+------+| one | two |+------+------+| 56 | 12 || 52 | 10 || 56 | 12 || 56 | 13 || NULL | NULL |+------+------+5 rows in set (0.00 sec)mysql&gt; select one, two from test_table where one = 52 or one = 56 and two &gt; 12;+------+------+| one | two |+------+------+| 52 | 10 || 56 | 13 |+------+------+2 rows in set (0.00 sec)mysql&gt; select one, two from test_table where one = 52 or (one = 56 and two &gt; 12);+------+------+| one | two |+------+------+| 52 | 10 || 56 | 13 |+------+------+2 rows in set (0.00 sec)mysql&gt; select one, two from test_table where (one = 52 or one = 56) and two &gt; 12;+------+------+| one | two |+------+------+| 56 | 13 |+------+------+1 row in set (0.00 sec) NULLnull和空字符是不一样的, 找到他和删除他的方式也比较特别 1234567891011121314151617181920212223242526272829303132333435363738mysql&gt; insert into test_table(one , two) values (null, null);Query OK, 1 row affected (0.00 sec)mysql&gt; select * from test_table;+------+------+| one | two |+------+------+| NULL | NULL |+------+------+1 row in set (0.00 sec)mysql&gt; delete from test_table where one = NULL;Query OK, 0 rows affected (0.00 sec)mysql&gt; select * from test_table;+------+------+| one | two |+------+------+| NULL | NULL |+------+------+1 row in set (0.00 sec)mysql&gt; delete from test_table where one = &apos; &apos;;Query OK, 0 rows affected (0.00 sec)mysql&gt; select * from test_table;+------+------+| one | two |+------+------+| NULL | NULL |+------+------+1 row in set (0.00 sec)mysql&gt; delete from test_table where isnull(one);Query OK, 1 row affected (0.00 sec)mysql&gt; select * from test_table;Empty set (0.00 sec) rollback 并不是什么都可以回滚的, 典型的如创建表和删除表这些都是不能回退的. 事务是用来管理 insert,update,delete 语句的123456789101112131415161718192021222324252627282930313233343536mysql&gt; set autocommit = 0;Query OK, 0 rows affected (0.00 sec)mysql&gt; select * from test_tab;+-----+-----+-------+| one | two | three |+-----+-----+-------+| 3 | 4 | 5 |+-----+-----+-------+1 row in set (0.00 sec)mysql&gt; set autocommit = 0;Query OK, 0 rows affected (0.00 sec)mysql&gt; insert into test_tab value (4, 4, 5);Query OK, 1 row affected (0.00 sec)mysql&gt; rollback;Query OK, 0 rows affected (0.01 sec)mysql&gt; select * from test_tab;+-----+-----+-------+| one | two | three |+-----+-----+-------+| 3 | 4 | 5 |+-----+-----+-------+1 row in set (0.00 sec)mysql&gt; drop table test_tab;Query OK, 0 rows affected (0.02 sec)mysql&gt; rollback;Query OK, 0 rows affected (0.00 sec)mysql&gt; select * from test_tab;ERROR 1146 (42S02): Table &apos;b_test_database.test_tab&apos; doesn&apos;t exist]]></content>
      <categories>
        <category>DB</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL入门一之增删查改与关联]]></title>
    <url>%2Fblog%2F2015%2F02%2F27%2FMySQL%E5%85%A5%E9%97%A8%E4%B8%80%E4%B9%8B%E5%A2%9E%E5%88%A0%E6%9F%A5%E6%94%B9%E4%B8%8E%E5%85%B3%E8%81%94%2F</url>
    <content type="text"><![CDATA[增删改查 INSERT INTO table_name (列1, 列2,…) VALUES (值1, 值2,….) DELETE FROM 表名称 WHERE 列名称 = 值 UPDATE 表名称 SET 列名称 = 新值 WHERE 列名称 = 某值 SELECT 列名称 FROM 表名称 关联SQL join 用于根据两个或多个表中的列之间的关系，从这些表中查询数据。 Join和Key概绍有时为了得到完整的结果，我们需要从两个或更多的表中获取结果。我们就需要执行 join。数据库中的表可通过键将彼此联系起来。主键（Primary Key）是一个列，在这个列中的每一行的值都是唯一的。在表中，每个主键的值都是唯一的。这样做的目的是在不重复每个表中的所有数据的情况下，把表间的数据交叉捆绑在一起。请看 “Persons” 表： |Id_P| LastName| FirstName| Address |City| —–| —–| —-| —–||1| Adams| John| Oxford Street| London|2| Bush| George| Fifth Avenue| New York|3| Carter| Thomas| Changan Street| Beijing 请注意，”Id_P” 列是 Persons 表中的的主键。这意味着没有两行能够拥有相同的 Id_P。即使两个人的姓名完全相同，Id_P 也可以区分他们。接下来请看 “Orders” 表： Id_O OrderNo Id_P 1 77895 3 2 44678 3 3 22456 1 4 24562 1 5 34764 65 请注意，”Id_O” 列是 Orders 表中的的主键，同时，”Orders” 表中的 “Id_P” 列用于引用 “Persons” 表中的人，而无需使用他们的确切姓名。请留意，”Id_P” 列把上面的两个表联系了起来。 下面列出了您可以使用的 JOIN 类型，以及它们之间的差异。 JOIN(INNER JOIN): 如果左右表中都有至少一个匹配，则返回行 LEFT JOIN: 即使右表中没有匹配，也从左表返回所有的行 RIGHT JOIN: 即使左表中没有匹配，也从右表返回所有的行 FULL JOIN: 只要其中一个表中存在匹配，就返回行 注 : JOIN使用on的, 而不是where. 使用Join(INNER JOIN)除了上面的方法，我们也可以使用关键词 JOIN 来从两个表中获取数据。如果我们希望列出所有人的定购，可以使用下面的 SELECT 语句： 12345SELECT Persons.LastName, Persons.FirstName, Orders.OrderNoFROM PersonsINNER JOIN OrdersON Persons.Id_P = Orders.Id_PORDER BY Persons.LastName 结果集： LastName FirstName OrderNo Adams John 22456 Adams John 24562 Carter Thomas 77895 Carter Thomas 44678 使用Left Join现在，我们希望列出所有的人，以及他们的定购 - 如果有的话。您可以使用下面的 SELECT 语句： 12345SELECT Persons.LastName, Persons.FirstName, Orders.OrderNoFROM PersonsLEFT JOIN OrdersON Persons.Id_P=Orders.Id_PORDER BY Persons.LastName 结果集： LastName FirstName OrderNo Adams John 22456 Adams John 24562 Carter Thomas 77895 Carter Thomas 44678 Bush George - - - - LEFT JOIN 关键字会从左表 (Persons) 那里返回所有的行，即使在右表 (Orders) 中没有匹配的行。 使用Right Join现在，我们希望列出所有的定单，以及定购它们的人 - 如果有的话。您可以使用下面的 SELECT 语句： 12345SELECT Persons.LastName, Persons.FirstName, Orders.OrderNoFROM PersonsRIGHT JOIN OrdersON Persons.Id_P=Orders.Id_PORDER BY Persons.LastName 结果集： LastName FirstName OrderNo Adams John 22456 Adams John 24562 Carter Thomas 77895 Carter Thomas 44678 34764 RIGHT JOIN 关键字会从右表 (Orders) 那里返回所有的行，即使在左表 (Persons) 中没有匹配的行。 使用Full Join现在，我们希望列出所有的人，以及他们的定单，以及所有的定单，以及定购它们的人。您可以使用下面的 SELECT 语句： 12345SELECT Persons.LastName, Persons.FirstName, Orders.OrderNoFROM PersonsFULL JOIN OrdersON Persons.Id_P=Orders.Id_PORDER BY Persons.LastName 结果集： LastName FirstName OrderNo Adams John 22456 Adams John 24562 Carter Thomas 77895 Carter Thomas 44678 Bush George 34764 FULL JOIN 关键字会从左表 (Persons) 和右表 (Orders) 那里返回所有的行。如果 “Persons” 中的行在表 “Orders” 中没有匹配，或者如果 “Orders” 中的行在表 “Persons” 中没有匹配，这些行同样会列出。]]></content>
      <categories>
        <category>DB</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ProtoBuf的安装与使用]]></title>
    <url>%2Fblog%2F2015%2F02%2F23%2Fprotobuf_tutorial%2F</url>
    <content type="text"><![CDATA[介绍与 JSON 相比， Protobuf 的序列化和反序列化的速度更快，而且传输的数据会先压缩，使得传输的效率更高些 。Protobuf ， 全称 Protocol Buffer ， 是 Google 公司内部的混合语言数据标准，是一种轻便高效的结构化数据存储格式，可以用于结构化数据串行化，或者说序列化。 它很适合做数据存储或 RPC 数据交换格式 。 Protobuf是可用于通信协议、数据存储等领域的语言无关、平台无关、可扩展的序列化结构数据格式 。 安装谷歌的东西要想在大陆安装起来总是有点那啥, 你懂的. 需要的依赖sudo apt-get install curl sudo apt-get install autoconf autogen sudo apt-get install libtool安装步骤下载自github的代码需要首先执行 $ ./autogen.sh 生成configure文件 注意autogen.sh 需要gtest包，默认是从 googletest.googlecode.com下载，国内需要翻墙才能访问，很多人问autogen.sh运行失败，这里我补充一下 修改一下autogen.sh , 将这段: 123echo &quot;Google Test not present. Fetching gtest-1.5.0 from the web...&quot;curl http://googletest.googlecode.com/files/gtest-1.5.0.tar.bz2 | tar jxmv gtest-1.5.0 gtest 修改为: 123wget https://github.com/google/googletest/archive/release-1.5.0.tar.gztar xzvf release-1.5.0.tar.gzmv googletest-release-1.5.0 gtest 再执行 autogen.sh，这样就不会报错了 $ ./configure $ make $ make check $ make install默认是装在 usr/local/bin usr/local/lib, usr/local/include 检查是否安装成功protoc --version如果安装成功,会出现版本号 如 libprotoc 2.6.1如果有问题，会输出错误内容, 最后我安装完成,用上述命令检查版本号时出现如下问题 protoc: error while loading shared libraries: libprotocbuf.so.9: cannot open shared错误原因 protobuf的默认安装路径是/usr/local/lib,而/usr/local/lib不在ubuntu体系默认的LD_LIBRARY_PATH里,所以就找不到lib 解决办法 : 1 - 在 /etc/ld.so.conf.d/目录下创建文件 bprotobuf.conf文件,文件内容如下 /usr/local/lib 2 - 输入命令 sudo ldconfig这时,再输入protoc --version就可以正常看到版本号了 使用Writer.cpp123456789101112131415#include&lt;iostream&gt;#include&lt;fstream&gt;#include "Mymessage.pb.h"using namespace std;int main()&#123; Im::Content msg1; msg1.set_id(101); msg1.set_str("ggsmd"); fstream output("./log", ios::out | ios::trunc | ios::binary); if (!msg1.SerializeToOstream(&amp;output)) &#123; cerr &lt;&lt; "Failed to write msg." &lt;&lt; endl; return -1; &#125; return 0; &#125; Reader.cpp123456789101112131415161718#include&lt;iostream&gt;#include&lt;fstream&gt;#include "Mymessage.pb.h"using namespace std;void ListMsg(const Im::Content &amp; msg)&#123; cout &lt;&lt; msg.id() &lt;&lt; endl; cout &lt;&lt; msg.str() &lt;&lt; endl;&#125; int main(int argc, char* argv[])&#123; Im::Content msg1; fstream input("./log", ios::in | ios::binary); if (!msg1.ParseFromIstream(&amp;input)) &#123; cerr &lt;&lt; "Failed to parse address book." &lt;&lt; endl; return -1; &#125; ListMsg(msg1); return 0;&#125; makefile1234567891011121314151617181920INC=/usr/local/includeLIB=/usr/local/liblib=protobufall:Writer ReaderWriter.o:Writer.cpp g++ -g -c Writer.cpp -I$(INC) -L$(LIB) -l$(lib)Reader.o:Reader.cpp g++ -g -c Reader.cpp -I$(INC) -L$(LIB) -l$(lib) Writer:Writer.o Mymessage.pb.o g++ -g -o Writer Writer.o Mymessage.pb.o -I$(INC) -L$(LIB) -l$(lib)Reader:Reader.o Mymessage.pb.o g++ -g -o Reader Reader.o Mymessage.pb.o -I$(INC) -L$(LIB) -l$(lib)Mymessage.pb.o:Mymessage.pb.cc g++ -g -c Mymessage.pb.cc -I$(INC) -L$(LIB) -l$(lib) clean:Writer Reader Writer.o Reader.o Mymessage.pb.o rm Writer Reader Writer.o Reader.o Mymessage.pb.o Mymessage.proto1234567package Im; message Content &#123; required int32 id = 1; // ID required string str = 2; // str optional int32 opt = 3; //optional field &#125; 打印结果执行protoc -I=./ --cpp_out=./ Mymessage.proto 命令后，会生成 Mymessage.pb.h 和 Mymessage.pb.cc 文件。 再执行 make 命令，生成Writer 和 Reader 文件 。 执行 ./Writer 命令后，再执行./Reader 命令，终端上输出： b@b-VirtualBox:~/tc$ protoc -I=./ --cpp_out=./ Mymessage.proto b@b-VirtualBox:~/tc$ ll total 44 drwxrwxr-x 2 b b 4096 5月 19 22:43 ./ drwxr-xr-x 4 b b 4096 5月 19 22:35 ../ -rw-rw-r-- 1 b b 647 5月 19 22:36 makefile -rw-rw-r-- 1 b b 12214 5月 19 22:43 Mymessage.pb.cc -rw-rw-r-- 1 b b 7762 5月 19 22:43 Mymessage.pb.h -rw-rw-r-- 1 b b 161 5月 19 22:36 Mymessage.proto -rw-rw-r-- 1 b b 421 5月 19 22:36 Reader.cpp -rw-rw-r-- 1 b b 340 5月 19 22:35 Writer.cpp b@b-VirtualBox:~/tc$ make g++ -g -c Writer.cpp -I/home/sharexu/charpter13/1302/include -L/home/sharexu/charpter13/1302/lib -lprotobuf g++ -g -c Mymessage.pb.cc -I/home/sharexu/charpter13/1302/include -L/home/sharexu/charpter13/1302/lib -lprotobuf g++ -g -o Writer Writer.o Mymessage.pb.o -I/home/sharexu/charpter13/1302/include -L/home/sharexu/charpter13/1302/lib -lprotobuf g++ -g -c Reader.cpp -I/home/sharexu/charpter13/1302/include -L/home/sharexu/charpter13/1302/lib -lprotobuf g++ -g -o Reader Reader.o Mymessage.pb.o -I/home/sharexu/charpter13/1302/include -L/home/sharexu/charpter13/1302/lib -lprotobuf b@b-VirtualBox:~/tc$ ll total 772 drwxrwxr-x 2 b b 4096 5月 19 22:43 ./ drwxr-xr-x 4 b b 4096 5月 19 22:35 ../ -rw-rw-r-- 1 b b 647 5月 19 22:36 makefile -rw-rw-r-- 1 b b 12214 5月 19 22:43 Mymessage.pb.cc -rw-rw-r-- 1 b b 7762 5月 19 22:43 Mymessage.pb.h -rw-rw-r-- 1 b b 244112 5月 19 22:43 Mymessage.pb.o -rw-rw-r-- 1 b b 161 5月 19 22:36 Mymessage.proto -rwxrwxr-x 1 b b 188430 5月 19 22:43 Reader* -rw-rw-r-- 1 b b 421 5月 19 22:36 Reader.cpp -rw-rw-r-- 1 b b 57656 5月 19 22:43 Reader.o -rwxrwxr-x 1 b b 184244 5月 19 22:43 Writer* -rw-rw-r-- 1 b b 340 5月 19 22:35 Writer.cpp -rw-rw-r-- 1 b b 59232 5月 19 22:43 Writer.o b@b-VirtualBox:~/tc$ ./Writer b@b-VirtualBox:~/tc$ ./Reader 101 ggsmd]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>ProtoBuf</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++对象模型之虚函数实例讲解]]></title>
    <url>%2Fblog%2F2015%2F02%2F21%2Fcpp_object_model_conclusion%2F</url>
    <content type="text"><![CDATA[介绍因为c++只规定了 虚继承/ 虚函数/ 多继承/ 的行为, 但将实现方法留给编译器作者. 所以各个平台的实现并不相同, 得出的结果也不尽相同. 经测试, vs和gcc目前比较统一的情况只有2种 : 无继承+无虚函数 无继承+虚函数 故本文只讨论这2种, 以及了解虚函数和虚继承的含义. . . . 关于虚函数当类中声明了虚函数（不管是1个还是多个），那么在实例化对象时，编译器会自动在对象里安插一个指针vPtr指向虚函数表VTable； 关于虚继承当涉及到虚继承，会增加vbPtr指针指向虚基表vbTable 单继承对象模型类的继承关系为：class Derived : public Base 无继承但有虚函数示例测试环境为Windows/VS, 32位. 1234567891011121314151617181920212223242526class A &#123; &#125;; class B &#123; char ch; virtual void func0() &#123; &#125; &#125;; class C &#123; char ch1; char ch2; virtual void func() &#123; &#125; virtual void func1() &#123; &#125; &#125;; int main(void) &#123; cout&lt;&lt;"A="&lt;&lt;sizeof(A)&lt;&lt;endl; //result=1 cout&lt;&lt;"B="&lt;&lt;sizeof(B)&lt;&lt;endl; //result=8 cout&lt;&lt;"C="&lt;&lt;sizeof(C)&lt;&lt;endl; //result=8 return 0; &#125; 总结首先，平时所声明的类只是一种类型定义，它本身是没有大小可言的。 因此，如果用sizeof运算符对一个类型名操作，那得到的是具有该类型实体的大小。计算一个类对象的大小时的规律： 空类、单一继承的空类、多重继承的空类所占空间大小为：1（字节，下同）；一个类中，虚函数本身、成员函数（包括静态与非静态）和静态数据成员都是不占用类对象的存储空间的；因此一个对象的大小≥所有非静态成员大小的总和； 类对象的大小 =各非静态数据成员（包括父类的非静态数据成员但都不包括所有的成员函数）的总和 +vfptr指针(多继承下可能不止一个)+vbptr指针(多继承下可能不止一个) +编译器因为要内存对齐而额外增加的字节。 测试常用平台的不同测试了 Windows10 / VS2015 和 Ubuntu14.04.3 / gcc4.8.4 , 都是64位 测试有多继承的情况12345678910111213141516171819202122232425262728293031323334353637383940414243444546#include &lt;iostream&gt;using std::cout;using std::endl;class A&#123;&#125;;class B&#123; char ch; virtual void func0() &#123; &#125;&#125;;class C&#123; char ch1; char ch2; virtual void func() &#123; &#125; virtual void func1() &#123; &#125;&#125;;class D : public A, public C&#123; int d; virtual void func() &#123; &#125; virtual void func1() &#123; &#125;&#125;;class E : public B, public C&#123; int e; virtual void func0() &#123; &#125; virtual void func1() &#123; &#125;&#125;;int main(void)&#123; cout &lt;&lt; "A=" &lt;&lt; sizeof(A) &lt;&lt; endl; cout &lt;&lt; "B=" &lt;&lt; sizeof(B) &lt;&lt; endl; cout &lt;&lt; "C=" &lt;&lt; sizeof(C) &lt;&lt; endl; cout &lt;&lt; "D=" &lt;&lt; sizeof(D) &lt;&lt; endl; cout &lt;&lt; "E=" &lt;&lt; sizeof(E) &lt;&lt; endl; return 0;&#125; 打印对比如下 : Windows10/VS2015123456A=1B=16C=16D=24E=40请按任意键继续. . . Ubuntu14.04.3/gcc4.8.412345A=1B=16C=16D=16E=32 测试有虚拟继承的情况123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113#include &lt;iostream&gt;using std::cout;using std::endl;class Base&#123;public: Base() &#123; mBase = 11; &#125; virtual void funcA() &#123; cout &lt;&lt; "Base::funcA()" &lt;&lt; endl; &#125; virtual void funcX() &#123; cout &lt;&lt; "Base::funcX()" &lt;&lt; endl; &#125;protected: int mBase;&#125;;class Base1 : virtual public Base&#123;public: Base1() : Base() &#123; mBase1 = 101; &#125; virtual void funcA() &#123; cout &lt;&lt; "Base1::funcA()" &lt;&lt; endl; &#125; virtual void funcB() &#123; cout &lt;&lt; "Base1::funcB()" &lt;&lt; endl; &#125;private: int mBase1;&#125;;class Base2 : virtual public Base&#123;public: Base2() : Base() &#123; mBase2 = 102; &#125; virtual void funcA() &#123; cout &lt;&lt; "Base2::funcA()" &lt;&lt; endl; &#125; virtual void funcC() &#123; cout &lt;&lt; "Base2::funcC()" &lt;&lt; endl; &#125;private: int mBase2;&#125;;class Base3 : virtual public Base&#123;public: Base3() : Base() &#123; mBase3 = 102; &#125; virtual void funcA() &#123; cout &lt;&lt; "Base3::funcA()" &lt;&lt; endl; &#125; virtual void funcX() &#123; cout &lt;&lt; "Base3::funcC()" &lt;&lt; endl; &#125;private: int mBase3;&#125;;class Derived : public Base1, public Base2&#123;public: Derived() : Base1(), Base2() &#123; mDerived = 1001; &#125; virtual void funcD() &#123; cout &lt;&lt; "Derived::funcD()" &lt;&lt; endl; &#125; virtual void funcA() &#123; cout &lt;&lt; "Derived::funcA()" &lt;&lt; endl; &#125;private: int mDerived;&#125;;int main(void)&#123; cout &lt;&lt; "Derived's size is " &lt;&lt; sizeof(Derived) &lt;&lt; endl; cout &lt;&lt; "Base's size is " &lt;&lt; sizeof(Base) &lt;&lt; endl; cout &lt;&lt; "Base1's size is " &lt;&lt; sizeof(Base1) &lt;&lt; endl; cout &lt;&lt; "Base2's size is " &lt;&lt; sizeof(Base2) &lt;&lt; endl; cout &lt;&lt; "Base3's size is " &lt;&lt; sizeof(Base3) &lt;&lt; endl; return 0;&#125; 打印对比如下 : Windows10/VS2015123456Derived&apos;s size is 80Base&apos;s size is 16Base1&apos;s size is 48Base2&apos;s size is 48Base3&apos;s size is 40请按任意键继续. . . Ubuntu14.04.3/gcc4.8.412345Derived&apos;s size is 48Base&apos;s size is 16Base1&apos;s size is 32Base2&apos;s size is 32Base3&apos;s size is 32 测试总结这两个都还算是比较常用的平台了, 测试之后发现vs和gcc目前比较统一的情况只有2种 : 无继承+无虚函数 无继承+虚函数 参考 C++对象模型之详述C++对象的内存布局 C++对象模型之简述C++对象的内存布局]]></content>
      <categories>
        <category>CPP</category>
      </categories>
      <tags>
        <tag>ObjectModel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python中的__name__和__main()__]]></title>
    <url>%2Fblog%2F2015%2F02%2F10%2Fpython%E4%B8%AD%E7%9A%84__name__%E5%92%8C__main()__%2F</url>
    <content type="text"><![CDATA[12345678#hello.pydef sayHello(): str="hello" print(str);if __name__ == "__main__": print ('This is main of module "hello.py"') sayHello() python作为一种脚本语言，我们用python写的各个module都可以包含以上那么一个累死c中的main函数，只不过python中的这种__main__与c中有一些区别，类似于php的魔术那一套, 主要体现在： 1、当单独执行该module时，比如单独执行以上hello.py： python hello.py，则输出 12This is main of module &quot;hello.py&quot;hello 可以理解为&quot;if __name__==&quot;__main__&quot;:&quot;这一句与c中的main()函数所表述的是一致的，即作为入口； 2、当该module被其它module 引入使用时，其中的&quot;if __name__==&quot;__main__&quot;:&quot; 所表示的Block不会被执行, 这是因为此时module被其它module引用时， 其__name__的值将发生变化，__name__的值将会是module的名字。 比如在python shell中import hello后，查看hello.__name__： 123import hellohello.__name__'hello' 3、因此，在python中，当一个module作为整体被执行时,moduel.name的值将是&quot;__main__&quot;； 而当一个module被其它module引用时，module.__name__将是module自己的名字， 当然一个module被其它module引用时，其本身并不需要一个可执行的入口main了。]]></content>
      <categories>
        <category>Script</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[条件编译指令之#if和#ifdef和#if defined之间的区别]]></title>
    <url>%2Fblog%2F2015%2F02%2F09%2F%E6%9D%A1%E4%BB%B6%E7%BC%96%E8%AF%91%E6%8C%87%E4%BB%A4%E4%B9%8Bif%E5%92%8Cifdef%E5%92%8Cifdefined%E4%B9%8B%E9%97%B4%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[#if的使用说明#if的后面接的是表达式 : 123#if (MAX==10)||(MAX==20) code...#endif 它的作用是：如果(MAX==10)||(MAX==20)成立，那么编译器就会把其中的#if 与 #endif之间的代码编译进去（注意：是编译进去，不是执行！！） #if defined的使用#if后面接的是一个宏, 而#if define(x)的使用如下 : 123#if defined (x) ...code...#endif 这个#if defined它不管里面的“x”的逻辑是“真”还是“假”它只管这个程序的前面的宏定义里面有没有定义“x”这个宏，如果定义了x这个宏，那么，编译器会编译中间的…code…否则不直接忽视中间的…code…代码。另外 #if defined(x)也可以取反，也就用 #if !defined(x) #ifdef的使用 #ifdef的使用和#if defined()的用法一致 #ifndef又和#if !defined()的用法一致。 最后强调两点： 这几个宏定义只是决定代码块是否被编译！ 别忘了#endif]]></content>
      <categories>
        <category>CPP</category>
      </categories>
      <tags>
        <tag>CPP</tag>
        <tag>Compile</tag>
        <tag>Make</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lua特别之处笔记]]></title>
    <url>%2Fblog%2F2015%2F02%2F03%2Flua_special_part%2F</url>
    <content type="text"><![CDATA[2.2 Booleans两个取值 false 和 true。但要注意 Lua 中所有的值都可以作为条件。在控制结构的条件中除了 false 和 nil 为假，其他值都为真。所以 Lua 认为 0 和空串都是真。 2.5 table我们用一个疑问来引入table的介绍 传的是值还是引用?lua的函数调用传的是值还是引用? . . . 测试代码123456789101112131415161718192021222324252627282930313233tTableForTest = &#123;&#125;tTableForTest[1] = 9function testTable(tTable) tTable[1] = 11endprint("tTableForTest[1]".." : "..tTableForTest[1])testTable(tTableForTest)print("tTableForTest[1]".." : "..tTableForTest[1])print("\n==================\n")nNumberForTest = 1function TestNumber( nNumber ) nNumber = 99endprint("nNumberForTest".." : "..nNumberForTest)TestNumber(nNumberForTest)print("nNumberForTest".." : "..nNumberForTest)print("\n==================\n")nStringForTest = "hi"function TestNumber( nString ) nString = "hello"endprint("nStringForTest".." : "..nStringForTest)TestNumber(nStringForTest)print("nStringForTest".." : "..nStringForTest) 打印结果tTableForTest[1] : 9 tTableForTest[1] : 11 ================== nNumberForTest : 1 nNumberForTest : 1 ================== nStringForTest : hi nStringForTest : hi结论 table传引用 number传值 string传值 为什么lua中table会不一样在Lua中, table 既不是值也不是变量而是对象. 程序仅持有一个对他们的引用, Lua不会暗中产生table的副本或创建新的table. 事实上, table的创建是通过构造表达式完成的, 最简单的构造表达式就是{} 123456789101112131415161718a = &#123;&#125;k = "x"a[k] = 10a[20] = "great"print(a["x"]) --&gt; 10k = 20print(a[k]) --&gt; "great"a["x"] = a["x"] + 1print(a["x"]) --&gt; 11b = a -- b与a引用了同一个tableprint(b["x"]) --&gt; 10b["x"] = 20print(a["x"]) --&gt; 20a = nil -- 现在只有b还在引用tableb = nil -- 再也没有对table的引用了 当一个程序再也没有对一个table的引用时, Lua的垃圾收集器最终会删除该table, 并复用它的内存. a[x]和a.x是不同的a.x表示a[&quot;x&quot;] , 表示以字符串&quot;x&quot;来索引table a[x] 是以变量x的值来索引table 123456a = &#123;&#125;x = "y"a[x] = 10print(a[x]) --&gt; 表示a["y"], 即 10print(a.x) --&gt; 表示a["x"], 没有定义这个, 所以是 nilprint(a.y) --&gt; 表示a["y"], 即 10 用table作为数组时就Lua的习惯而言, 数组通常以1作为索引的起始值, 并且还有不少lua机制依赖于这个惯例, 大多数lua内置的函数都假设数组起始于索引1, 这跟c语言以0为起始是不同的. Lua5.1以上, 可以使用 # 来返回一个数组或者线性表的最后一个索引值或者其大小. 1234-- 打印所有的行for i=1, #a do print(a[i])end]]></content>
      <categories>
        <category>Script</category>
      </categories>
      <tags>
        <tag>Lua</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[单例模式的析构问题和线程安全问题]]></title>
    <url>%2Fblog%2F2015%2F02%2F02%2Fsingleton_pattern%2F</url>
    <content type="text"><![CDATA[在某些应用环境下面，一个类只允许有一个实例，这就是著名的单例模式。单例模式分为 懒汉模式 饿汉模式 饿汉模式在实例化 m_instance 变量时，直接调用类的构造函数。顾名思义，在还未使用变量时，已经对 m_instance 进行赋值，就像很饥饿的感觉。在main开始前就初始化好了， 所以是线程安全的。 没有考虑析构问题饿汉模式的示例代码首先给出没有考虑析构问题的饿汉模式的实现 1234567891011121314151617181920212223242526272829303132333435#include &lt;iostream&gt;using namespace std;class singleton&#123;protected: singleton() &#123;&#125;;private: singleton(const singleton&amp;) &#123;&#125;; singleton&amp; operator=(const singleton&amp;) &#123;&#125;; static singleton* m_instance;public: static singleton* GetInstance(); ~singleton() &#123; printf("Singleton destruction\n"); &#125;&#125;;singleton* singleton::GetInstance()&#123; return m_instance;&#125;singleton* singleton::m_instance = new singleton;int main()&#123; singleton *ct = singleton::GetInstance(); return 0;&#125; 饿汉模式的优点 线程安全 实现简单，容易维护 饿汉模式的缺点 不适合部分场景。如：因为性能问题，希望懒加载；需要运行时才能知道，是否生成实例 由于在main开始前就必须初始化，几乎不可能给类传入任何参数。 懒汉模式懒汉模式下，在定义m_instance变量时先等于NULL，在调用GetInstance()方法时，在判断是否要赋值。这种模式，并非是线程安全的，因为多个线程同时调用GetInstance()方法，就可能导致有产生多个实例。 没有考虑线程安全与析构问题的懒汉模式的示例代码下面是没有考虑线程安全以及析构问题的懒汉模式的代码实现 123456789101112131415161718192021222324252627282930313233343536#include &lt;iostream&gt;using namespace std;class singleton&#123;protected: singleton() &#123;&#125;;private: singleton(const singleton&amp;) &#123;&#125;; singleton&amp; operator=(const singleton&amp;) &#123;&#125;; static singleton* m_instance;public: static singleton* GetInstance(); ~singleton() &#123; printf("Singleton destruction\n"); &#125;&#125;;singleton* singleton::GetInstance()&#123; if (m_instance == NULL) &#123; m_instance = new singleton; &#125; return m_instance;&#125;singleton* singleton::m_instance = NULL;int main()&#123; singleton *ct = singleton::GetInstance(); return 0;&#125; 解决懒汉模式线程安全问题的几种方法有下面几种方法 : 使用局部静态变量 加锁 pthread_once DCL 使用局部静态变量使用局部静态变量。局部静态变量的初始化是线程安全的，这一点由编译器保证.（http://gcc.gnu.org/ml/gcc-patches/2004-09/msg00265.html，这是一个 GCC 的 patch，专门解决这个问题）。会在程序退出的时候自动销毁。见此处 这个方法适合 C++11，C++11保证静态局部变量的初始化是线程安全的。如果是 C++98 就不能用这个方法。 12345678910111213class S&#123; public: static S&amp; getInstance() &#123; static S instance; return instance; &#125; private: S() &#123;&#125; S(S const&amp;); // Don't Implement. void operator=(S const&amp;); // Don't implement &#125;; 加锁线程安全，但每次都有开销。 1234567891011121314151617181920// singleton.hclass Singleton &#123; public: static Singleton *GetInstance() &#123; lock(); if (p == NULL) &#123; p = new Singleton; &#125; unlock(); return p; &#125; private: static Singleon *p; Singleton() &#123;&#125; Singleton(const Singleton &amp;); Singleton&amp; operator= (const Singleton &amp;);&#125;;// singleton.ccSingleton *Singleton::p = NULL; pthread_once陈硕推荐的做法 123456789101112131415161718class Singleton &#123; public: static Singleton *GetInstance() &#123; pthread_once(&amp;ponce_, &amp;Singleton::init); return value_; &#125; private: Singleton() &#123;&#125; Singleton(const Singleton &amp;); Singleton&amp; operator= (const Singleton &amp;); static void init() &#123; value_ = new T(); &#125; static pthread_once_t ponce_; static Singleton *value_;&#125;;pthread_once_t SIngleton::ponce_ = PTHREAD_ONCE_INIT;Singleton* Singleton::value_ = NULL; DCLdouble check locking. 只能用内存屏障，其他做法都是有问题的。参见论文： http://www.aristeia.com/Papers/DDJ_Jul_Aug_2004_revised.pdf普通的 double check 之所以错，是因为乱序执行和多处理器下，不同 CPU 中间 cache 刷往内存并对其他 CPU 可见的顺序无法保障（cache coherency problem）。Singleton&lt;T&gt; *p = new Singleton&lt;T&gt;;, 那么实际有 3 步： 分配内存 构造 赋值给 p 2 和 3 的顺序是未定的（乱序执行！）。因此，如果直接赋值给 p 那么很可能构造还没完成。此时另一个线程调用 GetInstance，在 lock 外面 check 了一下，发现 p!=NULL，于是直接返回 p，使用了未初始化完成的实例，跪了。 那么，如果用中间变量转一下呢？用 tmp_p 转了下以后，tmp_p 赋值给 p 的时候，显然 p 指向的实例是构造完成了的。然而，这个 tmp_p 在编译器看来明显没什么用，会被优化掉。 关于不能自动调用析构的问题上面的两个示例代码( 没有考虑析构问题饿汉模式的示例代码 和 没有考虑线程安全与析构问题的懒汉模式的示例代码 ) 都有不能自动调用析构的问题. 当你运行这两个示例代码之后, 你都会发现并没有打印 “Singleton destruction”, 也就是说程序结束时并没有调用 singleton 类的析构函数的, 为什么呢? 因为 m_instance = new singleton;, new出来的东西需要delete掉, 如果加上一句 delete ct; ct = NULL;, 就会调用析构函数了.但这种手动调用很容易忘啊, 怎么才能自动调用它的析构呢? 我们想要的是 : 自动化的正常删除该实例。 有两种方法, 我给他划分为: 不需要加GC(垃圾回收)内嵌类的单例模式(推荐) 需要加GC(垃圾回收)内嵌类的单例模式 需要加GC内嵌类的单例模式我们先看第二种, 我们知道，程序在结束的时候，系统会自动析构所有的全局变量。事实上，系统也会析构所有的类的静态成员变量，就像这些静态成员也是全局变量一样。利用这个特征，我们可以在单例类中定义一个这样的静态成员变量，而它的唯一工作就是在析构函数中删除单例类的实例。那就是定义一个内部垃圾回收GC类，并且在 singleton 中定义一个此类的静态成员。程序结束时，系统会自动析构此静态成员，此时，在此类的析构函数中析构 singleton 实例，就可以实现 m_instance 的自动释放。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253#include &lt;iostream&gt;using namespace std;class singleton&#123;protected: singleton() &#123;&#125;;private: singleton(const singleton&amp;) &#123;&#125;; singleton&amp; operator=(const singleton&amp;) &#123;&#125;; static singleton* m_instance;public: static singleton* GetInstance(); ~singleton() &#123; printf("Singleton destruction\n"); &#125; class GC &#123; public: ~GC() &#123; printf("GC destruction\n"); if (m_instance) &#123; delete m_instance; m_instance = NULL; &#125; &#125; &#125;; static GC gc_singleton;&#125;;singleton::GC singleton::gc_singleton;singleton* singleton::GetInstance()&#123; if (m_instance == NULL) &#123; m_instance = new singleton(); &#125; return m_instance;&#125;singleton* singleton::m_instance = NULL;int main()&#123; singleton *ct = singleton::GetInstance(); return 0;&#125; 当然还有更好的方法.那就是下面这个不需要加GC内嵌类的单例模式. 不需要加GC内嵌类的单例模式在 GetInstance 方法里放一个 m_instance 的局部静态变量, 然后返回他的地址, 他就可以在程序结束自动调用析构函数.而且这种方法在C++11也能保证线程安全. 123456789101112131415161718192021222324252627282930#include &lt;iostream&gt;using namespace std;class singleton&#123;protected: singleton() &#123;&#125;;private: singleton(const singleton&amp;) &#123;&#125;; singleton&amp; operator=(const singleton&amp;) &#123;&#125;;public: static singleton* GetInstance(); ~singleton() &#123; printf("Singleton destruction\n"); &#125;&#125;;singleton* singleton::GetInstance()&#123; static singleton m_instance; return &amp;m_instance;&#125;int main()&#123; singleton *ct = singleton::GetInstance(); return 0;&#125; 总结既要考虑线程安全又要考虑析构问题的话, 有下面几种方法 : 饿汉模式+GC内嵌类 懒汉模式+GC内嵌类, 然后加锁，但每个线程缓存了返回的指针，调用一次有用缓存的指针即可。 懒汉模式+GC内嵌类, 然后 pthread_once 如果是C++11的话, 则可以使用局部静态变量, 因为C++11保证静态局部变量的初始化是线程安全的(C++98不保证), 而且也没有析构问题.]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>DesignPattern</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GDB基础教程]]></title>
    <url>%2Fblog%2F2015%2F02%2F02%2Fgdb_tutorial%2F</url>
    <content type="text"><![CDATA[GDB 操作提示在编译可执行文件时需要给 gcc 加上 “-g” 选项，这样它才会为生成的可执行文件加入额外的调试信息。不要使用编译器的优化选项，比如： “-O”，”-O2”。因为编译器会为了优化而改变程序流程，那样不利于调试。在 GDB 中执行 shell 命令可以使用：shell commandGDB 命令可以使用 TAB 键来补全。按两次 TAB 键可以看到所有可能的匹配。GDB 命令缩写：例如 info bre 中的 bre 相当于 breakpoints。 启动GDB gdb executable gdb -e executable -c core-file gdb executable -pid process-id （使用ps相关命令可以查看进程的 pid） GDB常用命令 help 列出 gdb 帮助信息。 info+subcommand , 比如 : info breakpoints : 列出断点。 info watchpoints : 列出观察点。 info threads : 列出当前的线程。 info locals : 列出Local variables of current stack frame step(简写一个s也可) 进入下一行代码的执行，会进入函数内部。 next(简写一个n也可) 执行下一行代码。但不会进入函数内部。 finish 跳出当前代码（之前跳入调试） continue(c) 继续执行直到下一个断点或观察点。 b 断点 kill 停止程序执行。 quit(q) 退出 GDB调试器 run(r) 从头开始执行程序，也允许进行重定向。 print(p) variable 打印指定变量的值。 p variable p file::variable p ‘file’::variables backtrace(简写bt), 显示函数调用栈 bt : 显示所有函数调用栈 bt n : 显示程序的调用栈信息，只显示栈顶n桢(frame) bt -n : 显示程序的调用栈信息，只显示栈底部n桢(frame) set backtrace limit n : 设置bt显示的最大桢层数 where 和 info stack ： 都是bt的别名，功能一样 细说断点给 test.c 的第10行设置一个断点 : b test.c:10 断点删除断点的删除与断点的设置同样的重要。删除断点的命令有两个: delete clear delete用法：delete [breakpoints num] [range…]delete可删除单个断点，也可删除一个断点的集合，这个集合用连续的断点号来描述。例如： delete 5 delete 1-10clear用法: clear 删除所选定的环境中所有的断点 clear location location描述具体的断点。 clear 删除断点是基于行的，不是把所有的断点都删除。例如： clear list_insert //删除函数的所有断点 clear list.c:list_delet //删除文件：函数的所有断点 clear 12 //删除行号的所有断点 clear list.c:12 //删除文件：行号的所有断点断点的使能和禁止对断点的控制除了建立和删除外，还可以通过使能和禁止来控制，后一种方法更灵活。 断点的四种使能操作： enable [breakpoints] [range…] 完全使能 enable //激活所有断点 enable 4 //激活4断点 enable 5-6 //激活5～6断点 disable [breakpoints] [range…] 禁止 enable once [breakpoints] [range…] 使能一次，触发后禁止 enable delete [breakpoints] [range…]使能一次，触发后删除 用法举例： diable //禁止所有断点 disble 2 //禁止第二个断点 disable 1-5 //禁止第1到第5个断点GDB帮助GDB的命令很多, 有些用得少的命令记不住的话, 可以在进入GDB之后敲 “help”, 然后再敲 “help + command_class”,比如 : (gdb) helpList of classes of commands: aliases – Aliases of other commandsbreakpoints – Making program stop at certain pointsdata – Examining datafiles – Specifying and examining filesinternals – Maintenance commandsobscure – Obscure featuresrunning – Running the programstack – Examining the stackstatus – Status inquiriessupport – Support facilitiestracepoints – Tracing of program execution without stopping the programuser-defined – User-defined commands Type “help” followed by a class name for a list of commands in that class.Type “help all” for the list of all commands.Type “help” followed by command name for full documentation.Type “apropos word” to search for commands related to “word”.Command name abbreviations are allowed if unambiguous. *(gdb) help running *Running the program. List of commands: advance – Continue the program up to the given location (same form as args for break command)attach – Attach to a process or file outside of GDBcontinue – Continue program being debuggeddetach – Detach a process or file previously attacheddetach checkpoint – Detach from a checkpoint (experimental)detach inferiors – Detach from inferior ID (or list of IDS)disconnect – Disconnect from a targetfinish – Execute until selected stack frame returnshandle – Specify how to handle signalsinferior – Use this command to switch between inferiorsinterrupt – Interrupt the execution of the debugged programjump – Continue program being debugged at specified line or addresskill – Kill execution of program being debuggedkill inferiors – Kill inferior ID (or list of IDs)next – Step programnexti – Step one instructionreverse-continue – Continue program being debugged but run it in reversereverse-finish – Execute backward until just before selected stack frame is calledreverse-next – Step program backwardreverse-nexti – Step backward one instructionreverse-step – Step program backward until it reaches the beginning of another source linereverse-stepi – Step backward exactly one instructionrun – Start debugged program…]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>GDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[new和delete详解]]></title>
    <url>%2Fblog%2F2015%2F01%2F22%2Fnew%E5%92%8Cdelete%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[c++中对new申请的内存的释放方式有delete和delete[]两种方式，到底这两者有什么区别呢？ 疑问我们通常从教科书上看到这样的说明：delete 释放new分配的单个对象指针指向的内存delete[] 释放new分配的对象数组指针指向的内存那么，按照教科书的理解，我们看下下面的代码： 123int *a = new int[10];delete a; //方式1delete [] a; //方式2 肯定会有很多人说方式1肯定存在内存泄漏，是这样吗？ 针对基本数据类型针对简单类型 使用new分配后的不管是数组还是非数组形式内存空间用两种方式均可 如： 123int *a = new int[10];delete a;delete [] a; 此种情况中的释放效果相同，原因在于：分配简单类型内存时，内存大小已经确定，系统可以记忆并且进行管理，在析构时，系统并不会调用析构函数，它直接通过指针可以获取实际分配的内存空间，哪怕是一个数组内存空间(在分配过程中 系统会记录分配内存的大小等信息，此信息保存在结构体_CrtMemBlockHeader中，具体情况可参看VC安装目录下CRT\SRC\DBGDEL.cpp) 针对复杂数据类型针对类Class，两种方式体现出具体差异当你通过下列方式分配一个类对象数组： 123456789101112class A&#123;private: char *m_cBuffer; int m_nLen;public: A()&#123; m_cBuffer = new char[m_nLen]; &#125; ~A() &#123; delete [] m_cBuffer; &#125;&#125;;A *a = new A[10];delete a; //仅释放了a指针指向的这个数组的全部内存空间, 而且只调用了a[0]对象的析构函数, 但是剩下的从a[1]到a[9]这9个用户自行分配的m_cBuffer对应内存空间没有释放 从而造成内存泄漏delete [] a; //调用使用类对象的析构函数释放用户自己分配内存空间并且释放了a指针指向的全部内存空间 总结所以总结下就是，关于 new[] 和 delete[]，其中又分为两种情况： 基本数据类型对于像int/char/long/int*/struct等等简单数据类型，由于对象没有destructor，所以用delete 和delete [] 是一样的！ 复杂数据类型类型 delete ptr 代表用来释放内存，且只用来释放ptr指向的内存。 delete[] rg 用来释放rg指向的内存！！还逐一调用数组中每个对象的destructor！！ 习题12345class A&#123;//...&#125;;A *pa = new A();A *pas = new A[NUM](); delete []pas; //详细流程答案见上文 delete []pa; //会发生什么答案是调用未知次数的A的析构函数. 因为delete[]会去通过pa+offset找一个基于pa的偏移量找一个内存里的数据, 他假定这个内存里放了要调用析构的次数n这个数据, 而这个内存里到底是多少是未知的. delete pas; //哪些指针会变成野指针答案是pas和A[0]中的指针会变成野指针. 因为只有这两个指针指向的内存被释放了, 也就是说, 仅释放了pas指针指向的这个数组的全部内存空间, 以及只调用了a[0]对象的析构函数]]></content>
      <categories>
        <category>CPP</category>
      </categories>
      <tags>
        <tag>CPP</tag>
        <tag>noodle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式系统设计概要笔记-四]]></title>
    <url>%2Fblog%2F2015%2F01%2F10%2Fdistributed_system_design_note_four%2F</url>
    <content type="text"><![CDATA[分布式系统设计实践基本的理论和策略简单介绍这么多，后面本人会从工程的角度，细化说一下”数据分布“、”副本控制”和”高可用协议” 在分布式系统中，无论是计算还是存储，处理的对象都是数据，数据不存在于一台机器或进程中， . . . 这就牵扯到如何多机均匀分发数据的问题，此小结主要讨论”哈希取模”，”一致性哈希“，”范围表划分“，”数据块划分“ 哈希取模：哈希方式是最常见的数据分布方式，实现方式是通过可以描述记录的业务的id或key(比如用户 id)， 通过Hash函数的计算求余。 余数作为处理该数据的服务器索引编号处理。 如图： 这样的好处是只需要通过计算就可以映射出数据和处理节点的关系，不需要存储映射。 难点就是如果id分布不均匀可能出现计算、存储倾斜的问题，在某个节点上分布过重。 并且当处理节点宕机时，这种”硬哈希“的方式会直接导致部分数据异常，还有扩容非常困难，原来的映射关系全部发生变更。 此处，如果是”无状态“型的节点，影响比较小，但遇到”有状态“的存储节点时，会发生大量数据位置需要变更，发生大量数据迁移的问题。 这个问题在实际生产中，可以通过按2的幂的机器数，成倍扩容的方式来缓解，如图： 不过扩容的数量和方式后收到很大限制。 下面介绍一种”自适应“的方式解决扩容和容灾的问题。 一致性哈希：一致性哈希 – Consistent Hash 是使用一个哈希函数计算数据或数据特征的哈希值，令该哈希函数的输出值域为一个封闭的环，最大值+1=最小值。 将节点随机分布到这个环上，每个节点负责处理从自己开始顺时针至下一个节点的全部哈希值域上的数据，如图： 一致性哈希的优点在于可以任意动态添加、删除节点，每次添加、删除一个节点仅影响一致性哈希环上相邻的节点。 为了尽可能均匀的分布节点和数据，一种常见的改进算法是引入虚节点的概念，系统会创建许多虚拟节点，个数远大于当前节点的个数，均匀分布到一致性哈希值域环上。 读写数据时，首先通过数据的哈希值在环上找到对应的虚节点，然后查找到对应的real节点。 这样在扩容和容错时，大量读写的压力会再次被其他部分节点分摊，主要解决了压力集中的问题。 如图： 数据范围划分：有些时候业务的数据id或key分布不是很均匀，并且读写也会呈现聚集的方式。 比如某些id的数据量特别大，这时候可以将数据按Group划分，从业务角度划分比如id为010000，已知8000以上的id可能访问量特别大，那么分布可以划分为[[08000],[80009000],[90001000]]。 将小访问量的聚集在一起。 这样可以根据真实场景按需划分，缺点是由于这些信息不能通过计算获取，需要引入一个模块存储这些映射信息。 这就增加了模块依赖，可能会有性能和可用性的额外代价。 数据块划分：许多文件系统经常采用类似设计，将数据按固定块大小(比如HDFS的64MB)，将数据分为一个个大小固定的块，然后这些块均匀的分布在各个节点，这种做法也需要外部节点来存储映射关系。 由于与具体的数据内容无关，按数据量分布数据的方式一般没有数据倾斜的问题，数据总是被均匀切分并分布到集群中。 当集群需要重新负载均衡时，只需通过迁移数据块即可完成。 如图：]]></content>
      <categories>
        <category>NP</category>
      </categories>
      <tags>
        <tag>Distributed</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式系统设计概要笔记-三]]></title>
    <url>%2Fblog%2F2015%2F01%2F07%2Fdistributed_system_design_note_three%2F</url>
    <content type="text"><![CDATA[分布式系统设计策略重试机制一般情况下，写一段网络交互的代码，发起rpc或者http，都会遇到请求超时而失败情况。 可能是网络抖动(暂时的网络变更导致包不可达，比如拓扑变更)或者对端挂掉。 这时一般处理逻辑是将请求包在一个重试循环块里，如下：[cpp] view plain copy print?int retry = 3;while(!request() &amp;&amp; retry–)sched_yield(); // or usleep(100) . . . 此种模式可以防止网络暂时的抖动，一般停顿时间很短，并重试多次后，请求成功！但不能防止对端长时间不能连接(网络问题或进程问题) 心跳机制心跳顾名思义，就是以固定的频率向其他节点汇报当前节点状态的方式。 收到心跳，一般可以认为一个节点和现在的网络拓扑是良好的。 当然，心跳汇报时，一般也会携带一些附加的状态、元数据信息，以便管理。 如下图： 但心跳不是万能的，收到心跳可以确认ok，但是收不到心跳却不能确认节点不存在或者挂掉了，因为可能是网络原因倒是链路不通但是节点依旧在工作。 所以切记，”心跳“只能告诉你正常的状态是ok，它不能发现节点是否真的死亡，有可能还在继续服务。 (后面会介绍一种可靠的方式 – Lease机制) 副本副本指的是针对一份数据的多份冗余拷贝，在不同的节点上持久化同一份数据，当某一个节点的数据丢失时，可以从副本上获取数据。 数据副本是分布式系统解决数据丢失异常的仅有的唯一途径。 当然对多份副本的写入会带来一致性和可用性的问题，比如规定副本数为3，同步写3份，会带来3次IO的性能问题。 还是同步写1份，然后异步写2份，会带来一致性问题，比如后面2份未写成功其他模块就去读了(下个小结会详细讨论如果在副本一致性中间做取舍)。 中心化/无中心化系统模型这方面，无非就是两种：中心节点，例如mysql的MSS单主双从、MongDB Master、HDFS NameNode、MapReduce JobTracker等，有1个或几个节点充当整个系统的核心元数据及节点管理工作，其他节点都和中心节点交互。 这种方式的好处显而易见，数据和管理高度统一集中在一个地方，容易聚合，就像领导者一样，其他人都服从就好。 简单可行。 但是缺点是模块高度集中，容易形成性能瓶颈，并且如果出现异常，就像群龙无首一样。 无中心化的设计，例如cassandra、zookeeper，系统中不存在一个领导者，节点彼此通信并且彼此合作完成任务。 好处在于如果出现异常，不会影响整体系统，局部不可用。 缺点是比较协议复杂，而且需要各个节点间同步信息。]]></content>
      <categories>
        <category>NP</category>
      </categories>
      <tags>
        <tag>Distributed</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[观察者模式]]></title>
    <url>%2Fblog%2F2015%2F01%2F07%2Fobserver_pattern%2F</url>
    <content type="text"><![CDATA[概述一些面向对象的编程方式，提供了一种构建对象间复杂网络互连的能力。当对象们连接在一起时，它们就可以相互提供服务和信息。 通常来说，当某个对象的状态发生改变时，你仍然需要对象之间能互相通信。但是出于各种原因，你也许并不愿意因为代码环境的改变而对代码做大的修改。也许，你只想根据你的具体应用环境而改进通信代码。或者，你只想简单的重新构造通信代码来避免类和类之间的相互依赖与相互从属。 问题当一个对象的状态发生改变时，你如何通知其他对象？是否需要一个动态方案――一个就像允许脚本的执行一样，允许自由连接的方案？ 解决方案观察者模式 ：定义对象间的一种一对多的依赖关系, 当一个对象的状态发生改变时, 所有依赖于它的对象都得到通知并被自动更新。 观察者模式允许一个对象关注其他对象的状态，并且，观察者模式还为被观测者提供了一种观测结构，或者说是一个主体和一个客体。主体，也就是被观测者，可以用来联系所有的观测它的观测者。客体，也就是观测者，用来接受主体状态的改变 观测就是一个可被观测的类（也就是主题）与一个或多个观测它的类（也就是客体）的协作。不论什么时候，当被观测对象的状态变化时，所有注册过的观测者都会得到通知。 观察者模式将被观测者（主体）从观测者（客体）种分离出来。这样，每个观测者都可以根据主体的变化分别采取各自的操作。（观察者模式和 Publish/Subscribe 模式一样，也是一种有效描述对象间相互作用的模式。）观察者模式灵活而且功能强大。对于被观测者来说，那些查询哪些类需要自己的状态信息和每次使用那些状态信息的额外资源开销已经不存在了。另外，一个观测者可以在任何合适的时候进行注册和取消注册。你也可以定义多个具体的观测类，以便在实际应用中执行不同的操作。 将一个系统分割成一系列相互协作的类有一个常见的副作用：需要维护相关对象间的一致性。我们不希望为了维持一致性而使各类紧密耦合，因为这样降低了它们的可重用性。 适用性在以下任一情况下可以使用观察者模式: 当一个抽象模型有两个方面, 其中一个方面依赖于另一方面。将这二者封装在独立的对象中以使它们可以各自独立地改变和复用。 当对一个对象的改变需要同时改变其它对象 , 而不知道具体有多少对象有待改变。 当一个对象必须通知其它对象，而它又不能假定其它对象是谁。换言之 , 你不希望这些对象是紧密耦合的。 结构 模式的组成观察者模式包含如下角色： 目标（Subject）: 目标知道它的观察者。可以有任意多个观察者观察同一个目标。 提供注册和删除观察者对象的接口。 具体目标（ConcreteSubject）: 将有关状态存入各 ConcreteObserver 对象。 观察者 (Observer): 为那些在目标发生改变时需获得通知的对象定义一个更新接口。当它的状态发生改变时, 向它的各个观察者发出通知。 具体观察者 (ConcreteObserver): 维护一个指向 ConcreteSubject 对象的引用。存储有关状态，这些状态应与目标的状态保持一致。实现 O b s e r v e r 的更新接口以使自身状态与目标的状态保持一致。 观察者模式的优缺点Observer 模式允许你独立的改变目标和观察者。你可以单独复用目标对象而无需同时复用其观察者, 反之亦然。它也使你可以在不改动目标和其他的观察者的前提下增加观察者。 下面是观察者模式其它一些优点 : 观察者模式可以实现表示层和数据逻辑层的分离, 并定义了稳定的消息更新传递机制，抽象了更新接口，使得可以有各种各样不同的表示层作为具体观察者角色。 在观察目标和观察者之间建立一个抽象的耦合 ：一个目标所知道的仅仅是它有一系列观察者 , 每个都符合抽象的 Observer 类的简单接口。目标不知道任何一个观察者属于哪一个具体的类。这样目标和观察者之间的耦合是抽象的和最小的。因为目标和观察者不是紧密耦合的, 它们可以属于一个系统中的不同抽象层次。一个处于较低层次的目标对象可与一个处于较高层次的观察者通信并通知它 , 这样就保持了系统层次的完整。如果目标和观察者混在一块 , 那么得到的对象要么横贯两个层次 (违反了层次性), 要么必须放在这两层的某一层中 (这可能会损害层次抽象)。 支持广播通信 : 不像通常的请求, 目标发送的通知不需指定它的接收者。通知被自动广播给所有已向该目标对象登记的有关对象。目标对象并不关心到底有多少对象对自己感兴趣 ; 它唯一的责任就是通知它的各观察者。这给了你在任何时刻增加和删除观察者的自由。处理还是忽略一个通知取决于观察者。 观察者模式符合 “开闭原则” 的要求。 观察者模式的缺点 : 如果一个观察目标对象有很多直接和间接的观察者的话，将所有的观察者都通知到会花费很多时间。 如果在观察者和观察目标之间有循环依赖的话，观察目标会触发它们之间进行循环调用，可能导致系统崩溃。 观察者模式没有相应的机制让观察者知道所观察的目标对象是怎么发生变化的，而仅仅只是知道观察目标发生了变化。 意外的更新 因为一个观察者并不知道其它观察者的存在 , 它可能对改变目标的最终代价一无所知。在目标上一个看似无害的的操作可能会引起一系列对观察者以及依赖于这些观察者的那些对象的更新。此外 , 如果依赖准则的定义或维护不当，常常会引起错误的更新 , 这种错误通常很难捕捉。 简单的更新协议不提供具体细节说明目标中什么被改变了 , 这就使得上述问题更加严重。如果没有其他协议帮助观察者发现什么发生了改变，它们可能会被迫尽力减少改变。 实现在 php 的 SPL 支持观察者模式，SPL 提供了 SplSubject 和 SplObserver 接口。 SplSubject 接口提供了 attach()、detach()、notify() 三个方法。而 SplObserver 接口则提供了 update() 方法。 SplSubject 派生类维护了一个状态，当状态发生变化时 - 比如属性变化等，就会调用 notify() 方法，这时，之前在 attach() 方法中注册的所有 SplObserver 实例的 update() 方法就会被调用。接口定义如下： 1234567891011121314&lt;?php /** * 这一模式的概念是 SplSubject 类维护了一个特定状态，当这个状态发生变化时，它就会调用 notify() 方法。 * 调用 notify() 方法时，所有之前使用 attach() 方法注册的 SplObserver 实例的 update 方法都会被调用。 * */ interface SplSubject&#123; public function attach(SplObserver $observer);// 注册观察者 public function detach(SplObserver $observer);// 释放观察者 public function notify();// 通知所有注册的观察者 &#125; interface SplObserver&#123; public function update(SplSubject $subject);// 观察者进行更新状态 &#125; 实现代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667&lt;?php /** * 具体目标 * */ class ConcreteSubject implements SplSubject &#123; private $observers, $value; public function __construct() &#123; $this-&gt;observers = array(); &#125; public function attach(SplObserver $observer) &#123; // 注册观察者 $this-&gt;observers[] = $observer; &#125; public function detach(SplObserver $observer) &#123; // 释放观察者 if($idx = array_search($observer,$this-&gt;observers,true)) &#123; unset($this-&gt;observers[$idx]); &#125; &#125; public function notify() &#123; // 通知所有观察者 foreach($this-&gt;observers as $observer) &#123; $observer-&gt;update($this); &#125; &#125; public function setValue($value) &#123; $this-&gt;value = $value; $this-&gt;notify(); &#125; public function getValue() &#123; return $this-&gt;value; &#125; &#125; /** * 具体观察者 * */ class ConcreteObserver1 implements SplObserver &#123; public function update(SplSubject $subject) &#123; echo 'ConcreteObserver1 value is',$subject-&gt;getValue(), '&lt;br&gt;'; &#125; &#125; /** * 具体观察者 * */ class ConcreteObserver2 implements SplObserver &#123; public function update(SplSubject $subject) &#123; echo 'ConcreteObserver2 value is', $subject-&gt;getValue(), '&lt;br&gt;'; &#125; &#125; $subject = new ConcreteSubject(); $observer1 = new ConcreteObserver1(); $observer2 = new ConcreteObserver2(); $subject-&gt;attach($observer1); $subject-&gt;attach($observer2); $subject-&gt;setValue(5); ?&gt; 我们扩展上面的例子，根据目标状态而更新不同的观察者： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136&lt;?php /** * 具体目标 * */ class ConcreteSubject implements SplSubject &#123; private $observers, $_state; public function __construct() &#123; $this-&gt;observers = array(); &#125; /** * 注册观察者 * * @param SplObserver $observer */ public function attach(SplObserver $observer) &#123; $this-&gt;observers[] = $observer; &#125; /** * // 释放观察者 * * @param SplObserver $observer */ public function detach(SplObserver $observer) &#123; if($idx = array_search($observer,$this-&gt;observers,true)) &#123; unset($this-&gt;observers[$idx]); &#125; &#125; /** * 通知所有观察者 * */ public function notify() &#123; /** * 只要状态改变，就通知观察者 */ foreach($this-&gt;observers as $observer) &#123; if ($observer-&gt;getState() == $this-&gt;_state) &#123; $observer-&gt;update($this); &#125; &#125; &#125; /** * 设置状态 * * @param unknown_type $state */ public function setState($state) &#123; $this-&gt;_state = $state; $this-&gt;notify(); &#125; public function getState() &#123; return $this-&gt;_state; &#125; &#125; /** * 抽象观摩者 * */ abstract class observer&#123; private $_state; function __construct($state) &#123; $this-&gt;_state = $state; &#125; public function setState($state) &#123; $this-&gt;_state = $state; $this-&gt;notify(); &#125; public function getState() &#123; return $this-&gt;_state; &#125; &#125; /** * 具体观察者 1 * */ class ConcreteObserver1 extends observer implements SplObserver &#123; function __construct($state) &#123; parent::__construct($state); &#125; public function update(SplSubject $subject) &#123; echo 'ConcreteObserver1 state is',$subject-&gt;getState(), '&lt;br&gt;'; &#125; &#125; /** * 具体观察者 2 * */ class ConcreteObserver2 extends observer implements SplObserver &#123; function __construct($state) &#123; parent::__construct($state); &#125; public function update(SplSubject $subject) &#123; echo 'ConcreteObserver2 state is', $subject-&gt;getState(), '&lt;br&gt;'; &#125; &#125; /** * 具体观察者 3 * */ class ConcreteObserver3 extends observer implements SplObserver &#123; function __construct($state) &#123; parent::__construct($state); &#125; public function update(SplSubject $subject) &#123; echo 'ConcreteObserver3 state is', $subject-&gt;getState(), '&lt;br&gt;'; &#125; &#125; $subject = new ConcreteSubject(); $observer1 = new ConcreteObserver1(1); $observer2 = new ConcreteObserver2(1); $observer3 = new ConcreteObserver3(2); $subject-&gt;attach($observer1); $subject-&gt;attach($observer2); $subject-&gt;attach($observer3); echo 'Subject state is 1', '&lt;br&gt;'; $subject-&gt;setState(1); echo 'Subject state is 2', '&lt;br&gt;'; $subject-&gt;setState(2); ?&gt; 与其他相关模式 终结者模式 Mediator: 通过封装复杂的更新语义 , ChangeManager 充当目标和观察者之间的中介者。 单例模式 Singleton: ChangeManager 可使用 Singleton 模式来保证它是唯一的并且是可全局访问的。 总结与分析通过 Observer 模式，把一对多对象之间的通知依赖关系的变得更为松散，大大地提高了程序的可维护性和可扩展性，也很好的符合了开放 - 封闭原则。]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>DesignPattern</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[工厂模式]]></title>
    <url>%2Fblog%2F2015%2F01%2F07%2Ffactory_pattern%2F</url>
    <content type="text"><![CDATA[分类工厂模式主要是为创建对象提供过渡接口，以便将创建对象的具体过程屏蔽隔离起来，达到提高灵活性的目的。 工厂模式可以分为三类： 简单工厂模式 （Simple Factory, 简单工厂模式可看为工厂方法模式的一种特例，两者归为一类。 ） 工厂方法模式 （Factory Method） 抽象工厂模式 （Abstract Factory） 这三种模式从上到下逐步抽象，并且更具一般性。 GOF在《设计模式》一书中将工厂模式分为两类：工厂方法模式（Factory Method）与抽象工厂模式（Abstract Factory）。 区别 简单工厂模式 ： 一个工厂类, 这个工厂类能创建多个具体产品类的实例。 一个抽象产品类，可以派生出多个具体产品类。 工厂方法模式 ： 一个抽象工厂类，可以派生出多个具体工厂类。 一个抽象产品类，可以派生出多个具体产品类。 每个具体工厂类只能创建一个具体产品类的实例。 抽象工厂模式 ： 一个抽象工厂类，可以派生出多个具体工厂类。 多个抽象产品类，每个抽象产品类可以派生出多个具体产品类。 每个具体工厂类可以创建多个具体产品类的实例。 区别： 工厂方法模式只有一个抽象产品类，而抽象工厂模式有多个。 工厂方法模式的具体工厂类只能创建一个具体产品类的实例，而抽象工厂模式可以创建多个。 简单工厂模式产品类1234567891011121314151617181920&lt;?php/** 车子系列 */abstract Class BWM&#123; function construct($pa) &#123; &#125;&#125;Class BWM320 extends BWM&#123; function construct($pa) &#123; &#125;&#125;Class BMW523 extends BWM&#123; function construc($pb)&#123; &#125;&#125; 工厂类12345678910111213141516/** 工厂创建车 */class Factory &#123; static function createBMW($type)&#123; switch ($type) &#123; case 320: return new BWM320(); case 523: return new BMW523(); //…. &#125;&#125; 客户类12345678910/** 客户通过工厂获取车 */class Customer &#123; private $BMW; function getBMW($type)&#123; $this¬-&gt; BMW = Factory::createBMW($type); &#125;&#125; 工厂方法模式产品类1234567891011121314151617181920&lt;?php/** 车子系列 /abstract Class BWM&#123;function construct($pa) &#123;&#125;&#125;Class BWM320 extends BWM&#123;function construct($pa) &#123;&#125;&#125;Class BMW523 extends BWM&#123; function construc($pb)&#123;&#125;&#125; 创建工厂类123456789101112131415161718192021222324252627282930/** 创建工厂的接口 */interface FactoryBMW &#123; function createBMW();&#125;/** 创建BWM320车 **/class FactoryBWM320 implements FactoryBMW &#123; function createBMW($type)&#123; return new BWM320(); &#125;&#125;/** 创建BWM523车 **/class FactoryBWM523 implements FactoryBMW &#123; function createBMW($type)&#123; return new BMW523(); &#125;&#125; 客户类12345678910111213141516171819/** 客户得到车 */class Customer &#123; private $BMW; function getBMW($type)&#123; switch ($type) &#123; case 320: $BWM320 = new FactoryBWM320(); return $BWM320-&gt;createBMW(); case 523: $BWM523 = new FactoryBWM523(); return $BWM320-&gt;createBMW(); //…. &#125; &#125;&#125; 抽象工厂模式产品类123456789101112131415161718192021222324252627&lt;?php/** 车子系列以及型号 */abstract class BWM&#123;&#125;class BWM523 extends BWM &#123;&#125;class BWM320 extends BWM &#123;&#125;/* 空调 */abstract class aircondition&#123;&#125;class airconditionBWM320 extends aircondition &#123;&#125;class airconditionBWM52 extends aircondition &#123;&#125; 创建工厂类123456789101112131415161718192021222324252627282930313233343536/* 创建工厂的接口 */interface FactoryBMW &#123; function createBMW(); function createAirC();&#125;/** 创建BWM320车 /class FactoryBWM320 implements FactoryBMW &#123; function createBMW()&#123; return new BWM320();&#125;function createAirC()&#123; //空调 return new airconditionBWM320();&#125;&#125;/* 创建BWM523车 */class FactoryBWM523 implements FactoryBMW &#123; function createBMW()&#123; return new BWM523();&#125;function createAirC()&#123; return new airconditionBWM523();&#125;&#125; 客户1234567891011121314/* 客户得到车 */class Customer &#123; private $BMW; private $airC; function getBMW($type)&#123; $class = new ReflectionClass(‘FactoryBWM’ .$type );//建立 Person这个类的反射类 $instance = $class-&gt;newInstanceArgs();//相当于实例化Person 类 $this-&gt;BMW = $instance-&gt;createBMW(); $this-&gt;airC = $instance-&gt;createAirC(); &#125;&#125;]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>DesignPattern</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式系统设计概要笔记-二]]></title>
    <url>%2Fblog%2F2015%2F01%2F05%2Fdistributed_system_design_note_two%2F</url>
    <content type="text"><![CDATA[分布式系统特性CAP是分布式系统里最著名的理论，wiki百科如下 Consistency(all nodes see the same data at the same time) Availability (a guarantee that every request receives a response about whether it was successful or failed) Partition tolerance (the system continues to operate despite arbitrary message loss or failure of part of the system)(摘自 ：http://en.wikipedia.org/wiki/CAP_theorem) . . . 早些时候，国外的大牛已经证明了CAP三者是不能兼得，很多实践也证明了。 本人就不挑战权威了，感兴趣的同学可以自己Google。 本人以自己的观点总结了一下： 一致性可以参考陈皓的博文&lt;&lt;分布式事务处理&gt;&gt; 描述当前所有节点存储数据的统一模型，分为强一致性和弱一致性：强一致性描述了所有节点的数据高度一致，无论从哪个节点读取，都是一样的。 无需担心同一时刻会获得不同的数据。 是级别最高的，实现的代价比较高如图： 弱一致性又分为单调一致性和最终一致性： 1、单调一致性强调数据是按照时间的新旧，单调向最新的数据靠近，不会回退，如： 数据存在三个版本v1-&gt;v2-&gt;v3，获取只能向v3靠近(如取到的是v2，就不可能再次获得v1) 2、最终一致性强调数据经过一个时间窗口之后，只要多尝试几次，最终的状态是一致的，是最新的数据 如图： 强一致性的场景，就好像交易系统，存取钱的+/-操作必须是马上一致的，否则会令很多人误解。 弱一致性的场景，大部分就像web互联网的模式，比如发了一条微博，改了某些配置，可能不会马上生效，但刷新几次后就可以看到了，其实弱一致性就是在系统上通过业务可接受的方式换取了一些系统的低复杂度和可用性。 可用性保证系统的正常可运行性，在请求方看来，只要发送了一个请求，就可以得到恢复无论成功还是失败（不会超时）! 分区容忍性在系统某些节点或网络有异常的情况下，系统依旧可以继续服务。 这通常是有负载均衡和副本来支撑的。 例如计算模块异常可通过负载均衡引流到其他平行节点，存储模块通过其他几点上的副本来对外提供服务。 扩展性扩展性是融合在CAP里面的特性，我觉得此处可以单独讲一下。 扩展性直接影响了分布式系统的好坏，系统开发初期不可能把系统的容量、峰值都考虑到，后期肯定牵扯到扩容，而如何做到快而不太影响业务的扩容策略，也是需要考虑的。 (后面在介绍数据分布时会着重讨论这个问题)]]></content>
      <categories>
        <category>NP</category>
      </categories>
      <tags>
        <tag>Distributed</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式系统设计概要笔记-一]]></title>
    <url>%2Fblog%2F2015%2F01%2F04%2Fdistributed_system_design_note_one%2F</url>
    <content type="text"><![CDATA[分布式系统中的概念最简单的分布式系统分布式可繁也可以简，最简单的分布式就是大家最常用的， 在负载均衡服务器后加一堆web服务器，然后在上面搞一个缓存服务器来保存临时状态， 后面共享一个数据库，其实很多号称分布式专家的人也就停留于此， 大致结构如下图所示： 这种环境下真正进行分布式的只是web server而已， 并且web server之间没有任何联系，所以结构和实现都非常简单。 最完备的分布式体系的模块组成有些情况下，对分布式的需求就没这么简单， 在每个环节上都有分布式的需求， 比如Load Balance、DB、Cache和文件等等， 并且当分布式节点之间有关联时， 还得考虑之间的通讯， 另外， 节点非常多的时候， 得有监控和管理来支撑。这样看起来， 分布式是一个非常庞大的体系， 只不过你可以根据具体需求进行适当地裁剪。按照最完备的分布式体系来看， 可以由以下模块组成： 分布式任务处理服务：负责具体的业务逻辑处理 分布式节点注册和查询：负责管理所有分布式节点的命名和物理信息的注册与询，是节点之间联系的桥梁 分布式DB：分布式结构化数据存取 分布式Cache：分布式缓存数据（非持久化）存取 分布式文件：分布式文件存取 网络通信：节点之间的网络数据通信 监控管理：搜集、监控和诊断所有节点运行状态 分布式编程语言：用于分布式环境下的专有编程语言，比如Elang、Scala 分布式算法：为解决分布式环境下一些特有问题的算法，比如解决一致性问题的Paxos算法 三元组其实，分布式系统说白了，就是很多机器组成的集群，靠彼此之间的网络通信，担当的角色可能不同，共同完成同一个事情的系统。 . . . 如果按”实体“来划分的话，就是如下这几种： 1、节点 – 系统中按照协议完成计算工作的一个逻辑实体，可能是执行某些工作的进程或机器 2、网络 – 系统的数据传输通道，用来彼此通信。 通信是具有方向性的。 3、存储 – 系统中持久化数据的数据库或者文件存储。 状态特性各个节点的状态可以是“无状态”或者“有状态的”, 一般认为，节点是偏计算和通信的模块，一般是无状态的。 这类应用一般不会存储自己的中间状态信息，比如Nginx，一般情况下是转发请求而已，不会存储中间信息。 另一种“有状态”的，如MySQL等数据库，状态和数据全部持久化到磁盘等介质。 “无状态”的节点一般我们认为是可随意重启的，因为重启后只需要立刻工作就好。 “有状态”的则不同，需要先读取持久化的数据，才能开始服务。 所以，“无状态”的节点一般是可以随意扩展的，“有状态”的节点需要一些控制协议来保证扩展。 系统异常异常，可认为是节点因为某种原因不能工作，此为节点异常。 还有因为网络原因，临时、永久不能被其他节点所访问，此为网络异常。 在分布式系统中，要有对异常的处理，保证集群的正常工作。 分布式系统与单节点的不同从linux write()系统调用说起众所周知，在unix/linux/mac(类Unix)环境下，两个机器通信，最常用的就是通过socket连接对方。 传输数据的话，无非就是调用write()这个系统调用，把一段内存缓冲区发出去。 但是可以进一步想一下，write()之后能确认对方收到了这些数据吗？ 答案肯定是不能，原因就是发送数据需要走内核-&gt;网卡-&gt;链路-&gt;对端网卡-&gt;内核，这一路径太长了，所以只能是异步操作。 write()把数据写入内核缓冲区之后就返回到应用层了，具体后面何时发送、怎么发送、TCP怎么做滑动窗口、流控都是tcp/ip协议栈内核的事情了。 所以在应用层，能确认对方受到了消息只能是对方应用返回数据，逻辑确认了这次发送才认为是成功的。 这就却别与单系统编程，大部分系统调用、库调用只要返回了就说明已经确认完成了。 TCP/IP协议是“不可靠”的教科书上明确写明了互联网是不可靠的，TCP实现了可靠传输。 何来“不可靠”呢？先来看一下网络交互的例子，有A、B两个节点，之间通过TCP连接，现在A、B都想确认自己发出的任何一条消息都能被对方接收并反馈，于是开始了如下操作：A-&gt;B发送数据，然后A需要等待B收到数据的确认，B收到数据后发送确认消息给A，然后B需要等待A收到数据的确认，A收到B的数据确认消息后再次发送确认消息给B，然后A又去需要等待B收到的确认。 死循环了！！ 其实，这就是著名的“拜占庭将军”问题 所以，通信双方是“不可能”同时确认对方受到了自己的信息。 而教科书上定义的其实是指“单向”通信是成立的，比如A向B发起Http调用， 收到了HttpCode 200的响应包，这只能确认，A确认B收到了自己的请求，并且B正常处理了，不能确认的是B确认A受到了它的成功的消息。 不可控的状态在单系统编程中，我们对系统状态是非常可控的。 比如函数调用、逻辑运算，要么成功，要么失败，因为这些操作被框在一个机器内部，cpu/总线/内存都是可以快速得到反馈的。 开发者可以针对这两个状态很明确的做出程序上的判断和后续的操作。 而在分布式的网络环境下，这就变得微妙了。 比如一次rpc、http调用，可能成功、失败，还有可能是“超时”，这就比前者的状态多了一个不可控因素，导致后面的代码不是很容易做出判断。 试想一下，用A用支付宝向B转了一大笔钱，当他按下“确认”后，界面上有个圈在转啊转，然后显示请求超时了，然后A就抓狂了，不知道到底钱转没转过去，开始确认自己的账户、确认B的账户、打电话找客服等等。 所以分布式环境下，我们的其实要时时刻刻考虑面对这种不可控的“第三状态”设计开发，这也是挑战之一。 视异常为正常单系统下，进程/机器的异常概率十分小。 即使出现了问题，可以通过人工干预重启、迁移等手段恢复。 但在分布式环境下，机器上千台，每几分钟都可能出现宕机、死机、网络断网等异常，出现的概率很大。 所以，这种环境下，进程core掉、机器挂掉都是需要我们在编程中认为随时可能出现的，这样才能使我们整个系统健壮起来，所以”容错“是基本需求。 异常可以分为如下几类： 节点错误：一般是由于应用导致，一些coredump和系统错误触发，一般重新服务后可恢复。 硬件错误：由于磁盘或者内存等硬件设备导致某节点不能服务，需要人工干预恢复。 网络错误：由于点对点的网络抖动，暂时的访问错误，一般拓扑稳定后或流量减小可以恢复。 网络分化： 网络中路由器、交换机错误导致网络不可达，但是网络两边都正常，这类错误比较难恢复，并且需要在开发时特别处理。 【这种情况也会比较前面的问题较难处理】]]></content>
      <categories>
        <category>NP</category>
      </categories>
      <tags>
        <tag>Distributed</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[为什么不推荐递归以及什么是尾递归]]></title>
    <url>%2Fblog%2F2015%2F01%2F02%2Fwhy_not_recursion_and_what_is_tail_recursion%2F</url>
    <content type="text"><![CDATA[为什么不推荐递归递归的调试难度奇高，就决定了实际项目中很少用递归。 而且递归确实运行效率低，因为函数一层一层调用存在调用栈， 在切换到更深层的函数时要产生断点，为了保证回来时继续运行， 必须保存现在所处函数的各种状态，回来时恢复状态，这样一层层下去性能损失就不断增加。 大量开辟在栈区的内存 ，直到每一层的递归结束或整个递归结束才释放 且这个内存空间可能呈几何级数增加， 空间效率不佳， 有可能会栈溢出 而要知道什么是尾递归， 首先得指到什么是尾调用 . . . 尾调用尾调用的概念非常简单，一句话就能说清楚，就是指某个函数的最后一步是调用另一个函数 123function f(x)&#123; return g(x);&#125; 上面代码中，函数f的最后一步是调用函数g，这就叫尾调用。 以下两种情况，都不属于尾调用。 12345678910// 情况一function f(x)&#123; let y = g(x); return y;&#125;// 情况二function f(x)&#123; return g(x) + 1;&#125; 上面代码中，情况一是调用函数g之后，还有别的操作，所以不属于尾调用，即使语义完全一样。情况二也属于调用后还有操作，即使写在一行内。 尾调用不一定出现在函数尾部，只要是最后一步操作即可。 123456function f(x) &#123; if (x &gt; 0) &#123; return m(x) &#125; return n(x);&#125; 上面代码中，函数m和n都属于尾调用，因为它们都是函数f的最后一步操作。 为什么推荐尾调用Lua 中函数的另一个有趣的特征是可以正确的处理尾调用（proper tail recursion，一些书使用术语“尾递归”，虽然并未涉及到递归的概念）。尾调用是一种类似在函数结尾的 goto 调用，当函数最后一个动作是调用另外一个函数时，我们称这种调用尾调用。例如： 123function f(x) return g(x)end g 的调用是尾调用。例子中 f 调用 g 后不会再做任何事情，这种情况下当被调用函数 g 结束时程序不需 要返回到调用者 f； 所以尾调用之后程序不需要在栈中保留关于调用者的任何信息。一些编译器比如 Lua 解释器利用这种特性在处理尾调用时不使用额外的栈，我们称这种语言支持正确的尾调用. 由于尾调用不需要使用栈空间，那么尾调用递归的层次可以无限制的。例如下面调用不论 n 为何值不会导致栈溢出。 123function foo (n) if n &gt; 0 then return foo(n - 1) endend 什么是尾递归函数调用自身，称为递归。如果尾调用自身，就称为尾递归。 尾递归具体例子112345long Rescuvie(long n) &#123; return (n == 1) ? 1 : n Rescuvie(n - 1);&#125; 尾递归: 123456789101112long TailRescuvie(long n, long a) &#123; return (n == 1) ? a : TailRescuvie(n - 1, a * n);&#125;long TailRescuvie(long n) &#123;//封装用的 return (n == 0) ? 1 : TailRescuvie(n, 1);&#125; 当n = 5时 对于线性递归, 他的递归过程如下: Rescuvie(5) {5 Rescuvie(4)} {5 {4 Rescuvie(3)}} {5 {4 {3 Rescuvie(2)}}} {5 {4 {3 {2 Rescuvie(1)}}}} {5 {4 {3 {2 1}}}} {5 {4 {3 2}}} {5 {4 6}} {5 * 24} 120对于尾递归, 他的递归过程如下: TailRescuvie(5) TailRescuvie(5, 1) TailRescuvie(4, 5) TailRescuvie(3, 20) TailRescuvie(2, 60) TailRescuvie(1, 120) 120很容易看出, 普通的线性递归比尾递归更加消耗资源, 在实现上说, 每次重复的过程 调用都使得调用链条不断加长. 系统不得不使用栈进行数据保存和恢复.而尾递归就 不存在这样的问题, 因为他的状态完全由n和a保存. 尾递归具体例子2具体事例2 快速排序算法实施尾递归优化 1234567891011121314151617void quickSort(SqList * list , int low ,int high)&#123; int pivot; while(low&lt;high) &#123; pivot=Partition(list,low,high); quickSort(list, low,pivot - 1); //quickSort(list,low,pivot-1); 原递归调用 //quickSort(list,pivot+1,high); low = pivot+1; /*尾递归*/ &#125;&#125;]]></content>
      <categories>
        <category>Misc</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[数据结构四之链表进阶]]></title>
    <url>%2Fblog%2F2014%2F12%2F22%2Fadvanced_linked_list%2F</url>
    <content type="text"><![CDATA[只谈一下单链表, 链表实在是太重要, 是前面两篇说算法博客的基础, 了解了其应用和衍生, 再去了解其本身就有动力了 这是一篇偏向单链表进阶的博客, 并不会讲单链表的建立/增加/删除等等, 而且这篇博客大多数只说思想不写代码(因为其实蛮简单的..) 存储结构12345typedef struct Node&#123; DataType data; struct Node *next;&#125;Node, *Node_Ptr; . . . 在O(1)时间删除链表结点给定单链表的头指针和一个结点指针, 定义一个函数在 O(1)时间删除该结点. 思路 : 如果要遍历找到该结点的前一个结点p, 来改变结点p的下一个结点指向的这种解法肯定是O(n)时间复杂度了.那是不是一定需要得到被删除的结点的前一个结点呢？答案是否定的。我们可以很方便地得到要删除的结点的下一个结点。如果我们把下一个结点的内容复制到需要删除的结点上覆盖原有的内容，再把下一个结点删除，那是不是就相当于把当前需要删除的结点删除了？ 找一个单链表的中间结点算法思想 : (快慢指针的使用)设置两个指针，一个每次移动两个位置，一个每次移动一个位置，当第一个指针到达尾节点时，第二个指针就达到了中间节点的位置 判断链表中是否有环算法思想 : (快慢指针的使用)链表中有环，其实也就是自相交. 用两个指针pslow和pfast从头开始遍历链表，pslow每次前进一个节点，pfast每次前进两个结点，若存在环，则pslow和pfast肯定会在环中相遇，若不存在，则pslow和pfast能正常到达最后一个节点 判断两个链表是否相交, 假设两个链表均不带环算法思想 : 如果两个链表相交于某一节点，那么在这个相交节点之后的所有节点都是两个链表所共有的。也就是说，如果两个链表相交，那么最后一个节点肯定是共有的。先遍历第一个链表，记住最后一个节点，然后遍历第二个链表，到最后一个节点时和第一个链表的最后一个节点做比较，如果相同，则相交，否则不相交。 从尾到头打印链表有两种解法 : 反转链表解法 : 反转链表之后再从头到尾打印 (这样会改变原来的链表) 栈存储解法(比较简单, 本文不详讲了) : 用栈存储之后再逐个出栈一一打印 (这样不会改变原来的链表) 反转链表解法比如一个链表:头指针-&gt;A-&gt;B-&gt;C-&gt;D-&gt;E反转成为:头指针-&gt;E-&gt;D-&gt;C-&gt;B-&gt;A 算法思想 :第一轮 : 头指针-&gt;A-&gt;B-&gt;C-&gt;D-&gt;E第二轮 : 头指针-&gt;B-&gt;A-&gt;C-&gt;D-&gt;E第三轮 : 头指针-&gt;C-&gt;B-&gt;A-&gt;D-&gt;E第四轮 : 头指针-&gt;D-&gt;C-&gt;B-&gt;A-&gt;E第五轮 : 头指针-&gt;E-&gt;D-&gt;C-&gt;B-&gt;A 算法cpp实现：手写的代码， 已经跑过了，可直接用下面代码中反转函数为 ReverseList ， 且有详细注释以及总结 LinkedList.h12345678910111213141516#pragma oncestruct TList&#123; struct TList *pNext; void *pData;&#125;;typedef struct TList *LPTLIST;void AppendElem(LPTLIST *ppstHead);void ReverseList(LPTLIST *ppstHead);void PrintList(LPTLIST *ppstHead);void DestroyList(LPTLIST *ppstHead); LinkedList.cpp123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162#include "LinkedList.h"#include &lt;iostream&gt;using std::cout;using std::endl;using std::cin;void AppendElem(LPTLIST *ppstHead)&#123; if (!ppstHead) &#123; cout &lt;&lt; "ppstHead is null" &lt;&lt; endl; return; &#125; if (!*ppstHead) &#123; *ppstHead = new TList; if (!*ppstHead) &#123; cout &lt;&lt; "*ppstHead malloc error" &lt;&lt; endl; return; &#125; (*ppstHead)-&gt;pData = nullptr; (*ppstHead)-&gt;pNext = nullptr; &#125; LPTLIST temp_elem_ptr = *ppstHead; cout &lt;&lt; "input '.' to finish" &lt;&lt; endl; char key_data = '.'; while (1) &#123; cin &gt;&gt; key_data; if (key_data != '.') &#123; char * temp_key_data = new char; if (!temp_key_data) &#123; cout &lt;&lt; "temp_key_data malloc error" &lt;&lt; endl; return; &#125; *temp_key_data = key_data; LPTLIST new_elem_ptr = new TList; if (!new_elem_ptr) &#123; cout &lt;&lt; "new_elem malloc error" &lt;&lt; endl; return; &#125; new_elem_ptr-&gt;pData = temp_key_data; new_elem_ptr-&gt;pNext = nullptr; temp_elem_ptr-&gt;pNext = new_elem_ptr; temp_elem_ptr = new_elem_ptr; &#125; else &#123; break; &#125; &#125;&#125;/* 将单链表反转, 要求只能扫描链表一次.*@param ppstHead 指向链表首节点的指针*/void ReverseList(LPTLIST *ppstHead)&#123; if (!ppstHead) &#123; cout &lt;&lt; "ppstHead is null" &lt;&lt; endl; return; &#125; if (!*ppstHead) &#123; cout &lt;&lt; "*ppstHead is null" &lt;&lt; endl; return; &#125; // 我们只用上述算法思想中第二轮来说明一下此算法, 即为 "第二轮 : 头指针-&gt;B-&gt;A-&gt;C-&gt;D-&gt;E" // origin_first_elem_ptr指针一直指向着原来链表头指针后面的那个元素 //（即原第一个元素， 这个指针的指向一直都不会变， 一直都是指向A） LPTLIST origin_first_elem_ptr = (*ppstHead)-&gt;pNext; LPTLIST temp_elem_ptr = nullptr; // 需要两个判断, 不然当 origin_first_elem_ptr 为NULL的时候会出错, // 且 origin_first_elem_ptr -&gt;pNext为NULL的时候也没必要继续循环了 while (origin_first_elem_ptr &amp;&amp; origin_first_elem_ptr-&gt;pNext) &#123; // 临时保存一下元素A后面的后面那个元素C temp_elem_ptr = origin_first_elem_ptr-&gt;pNext-&gt;pNext; // 让B指向A : B-&gt;A (第1步) origin_first_elem_ptr-&gt;pNext-&gt;pNext = (*ppstHead)-&gt;pNext; // 把目前第一个元素A替换为原第一个元素的后面那个元素B : 头指针-&gt;B (第2步) (*ppstHead)-&gt;pNext = origin_first_elem_ptr-&gt;pNext; // 原第一个元素A的pnext指到它后面的后面那个元素C : A-&gt;C (第3步) origin_first_elem_ptr-&gt;pNext = temp_elem_ptr; &#125; // 综上所述只需要3步, 链表反转需要两个指针， // 见上面两个指针, 一个 origin_first_elem_ptr, 一个 temp_elem_ptr // 且要注意while条件中循环的是origin_first_elem_ptr, 而非ppstHead&#125;void PrintList(LPTLIST *ppstHead)&#123; if (!ppstHead) &#123; cout &lt;&lt; "ppstHead is null" &lt;&lt; endl; return; &#125; if (!*ppstHead) &#123; cout &lt;&lt; "*ppstHead is null" &lt;&lt; endl; return; &#125; LPTLIST temp_elem_ptr = *ppstHead; while (temp_elem_ptr = temp_elem_ptr-&gt;pNext) &#123; cout &lt;&lt; *(char *)(temp_elem_ptr-&gt;pData) &lt;&lt; "-&gt;"; &#125; cout &lt;&lt; endl;&#125;void DestroyList(LPTLIST *ppstHead)&#123; if (!ppstHead) &#123; cout &lt;&lt; "ppstHead is null" &lt;&lt; endl; return; &#125; if (!*ppstHead) &#123; cout &lt;&lt; "*ppstHead is null" &lt;&lt; endl; return; &#125; LPTLIST temp_elem_ptr = *ppstHead, temp_next_ptr = nullptr; while (temp_elem_ptr) &#123; temp_next_ptr = temp_elem_ptr-&gt;pNext; delete (char *)(temp_elem_ptr-&gt;pData); temp_elem_ptr-&gt;pData = nullptr; delete temp_elem_ptr; temp_elem_ptr = temp_next_ptr; &#125; cout &lt;&lt; "DestroyList finished." &lt;&lt; endl;&#125; main.cpp123456789101112131415#include "LinkedList.h"int main()&#123; TList *test_list = nullptr; AppendElem(&amp;test_list); PrintList(&amp;test_list); ReverseList(&amp;test_list); PrintList(&amp;test_list); DestroyList(&amp;test_list); return 0;&#125;]]></content>
      <categories>
        <category>CPP</category>
      </categories>
      <tags>
        <tag>CPP</tag>
        <tag>DataStructure</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SVN的UpdateItemToThisRevision和RevertToThisRevision和UpdateItemToRevision的区别]]></title>
    <url>%2Fblog%2F2014%2F12%2F13%2FSVN%E7%9A%84UpdateItemToThisRevision%E5%92%8CRevertToThisRevision%E5%92%8CUpdateItemToRevision%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[前言使用SVN在管理代码的时候免不了进行代码的合并和还原，特别是当前版本的修改发现有重大问题的时候，还原是避免不了的，那么究竟应该怎样操作呢？ 内容使用SVN查看文件或目录的日志的时候，右键单击日志记录会弹出下面这个界面，今天我们来着重了解一下被红圈标记的三个选项——“Update item to this version”，“Revert to this version”，“Revert changes from this version”，这三个选项对于刚接触SVN的人确实不太好区分，一开始我也搞不懂，直到亲自试验一下才搞清楚这三个选项的用法。 在讲解这三个选项的作用之前，我们还是先来假定一个使用情景，假设我们的项目文件一共有8个版本，它版本号分别是1，2，3，4，5，6，7，8。 Update item to this version这个选项的作用是将文件版本更新到对应所选的版本（当然内容也修改到了相应的版本）。如果我们是在版本4这里点击“Update item to this version”，表示5~8版本所作的修改全部作废，这个文件的历史回退到了版本4那个时代，但是需要注意的是，此时文件的版本是4，并不是最新的。我们知道SVN工具中如果文件不是最新版本就无法上传，所以说这个功能只是用来暂时还原一下版本，来查询某个问题的，不能将还原后的文件上传。 Revert to this version这个选项的作用是将文件的内容更新到对应的版本，版本号没有发生变化。如果我们是在版本4这里点击“Revert to this version”，表示5~8版本所作的修改全部被还原，文件和版本4的文件一模一样，但需要注意的是这项操作相当于我们把版本4这个文件拷贝了一份赋值给了当前目录下的文件，此时的文件版本还是8，并且是可以提交的，提交以后文件的版本变成了9，增加了一个新的版本，虽然这个版本和版本4的内容是一样的。 Revert changes from this version这个选项的作用是将对应版本的修改还原，文件的版本号不发生变化，相当于在当前本版本上剔除某些版本所作的改变。如果我们是在版本4这里点击“Revert changes from this version”，表示版本4所作的修改被抹杀了，只剩下除版本4以外的7个修改了，但是此时文件是可以上传的，并且会生成新的版本9，只是版本9只包括除版本4以外的7次修改。这个选项是可以选择多个版本的，如果我们选择4,5,6,7这四个版本点击“Revert changes from this revision”，那么这几次修改都会被抹杀。如果我们选择5,6,7,8这四个版本点击“Revert changes from this revision”，表示取消这几个版本的修改，实际上和在版本4这里点击“Revert to this version”的作用是一样的。 总结 Update item to this version：回退文件的内容和版本到指定的版本A，文件内容与版本A一致，此时文件的版本也为A，但是无法上传文件。 Revert to this version：只是回退文件的内容到指定版本A，文件版本还是最新版本，此时文件会提示有所更改，可以上传，并且会在最新的版本号上加1，形成新的版本。 Revert changes from this version：还原对应版本所作的改变，会将所指定的版本所作的修改直接抹杀，可以对多个版本操作，注意很可能会出现冲突，需要手动解决。]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>SVN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一些常见的笔试题]]></title>
    <url>%2Fblog%2F2014%2F09%2F29%2Fsome_common_examination%2F</url>
    <content type="text"><![CDATA[求最大公约数求 a 和 b 的最大公约数 123456789101112131415161718192021int measure(int a, int b)&#123; int product = a * b; if (a == 0 || b == 0) &#123; return -1; &#125; if (a &lt; b) &#123; int temp = a; a = b; b = temp; &#125; while (int mod = a % b) &#123; a = b; b = mod; &#125; //return b; // 最大公约数 return product / b; // 记住这个公式： a*b=最小公倍数*最大公约数&#125; 棋盘/格子问题在如下8*6的矩阵中，请计算从A移动到B一共有多少走法？要求每次只能向上或向右移动一格，并且不能经过P。(答案为492) 给定一个M*N的格子或棋盘，从左下角走到右上角的走法总数（每次只能向右或向上移动一个方格边长的距离） 运用动态规划来解答 :我们可以把棋盘的左下角看做二维坐标的原点(0,0)，把棋盘的右上角看做二维坐标(M,N)(坐标系的单位长度为小方格的变长)用f(i,j)表示移动到坐标f(i,j)的走法总数，其中0=&lt;i,j&lt;=n，设f(m,n)代表从坐标（0,0）到坐标（m,n）的移动方法，则 f(m,n)=f(m-1,n)+f(m,n-1).于是状态f(i,j)的状态转移方程为： f(i,j)=f(i-1,j)+f(i,j-1) if i,j&gt;0 f(i,j)=f(i,j-1) if i=0 f(i,j)=f(i-1,j) if j=0初始情况就为：f(0,0)=0, f(0,1)=1, f(1,0)=1，这个问题可以在时间O(n^2)，空间O(n^2)内求解，非递归解. 所以答案为 492 =SumWaysOfMoveOnChessBoard(7, 5) - SumWaysOfMoveOnChessBoard(3, 3) * SumWaysOfMoveOnChessBoard(7 - 3, 5 - 3) 递归解12345678int SumWaysOfMoveOnChessBoard_Recursion(int m, int n) &#123; if (m == 0 &amp;&amp; n == 0) return 0; if (m==0 || n==0) return 1; return process(m, n - 1) + process(m - 1, n);&#125; 非递归解12345678910111213141516171819202122232425262728293031323334353637383940414243444546int SumWaysOfMoveOnChessBoard_NonRecursion_RawArray(int m, int n)&#123; if (m == 0 || n == 0) return 1; if (m == 0 &amp;&amp; n == 0) return 0; int xSize = m + 1; int ySize = n + 1; int** arr = new int*[xSize]; for (int i = 0; i &lt; xSize; ++i) arr[i] = new int[ySize]; arr[0][0] = 0; for (int i = 0; i &lt; xSize; ++i) arr[i][0] = 1; for (int i = 0; i &lt; ySize; ++i) arr[0][i] = 1; for (int i = 1; i &lt; xSize; ++i) for (int j = 1; j &lt; ySize; ++j) arr[i][j] = arr[i - 1][j] + arr[i][j - 1]; for (int i = 0; i &lt; xSize; ++i) delete[] arr[i]; delete[] arr; return arr[m][n];&#125;int SumWaysOfMoveOnChessBoard_NonRecursion_STL(int m, int n)&#123; if (m == 0 &amp;&amp; n == 0) return 0; int xSize = m + 1; int ySize = n + 1; std::vector&lt; vector&lt;int&gt; &gt; ChessBoardArray(xSize, vector&lt;int&gt;(ySize));; ChessBoardArray[0][0] = 0; for (int i = 0; i &lt; xSize; ++i) ChessBoardArray[i][0] = 1; for (int j = 0; j &lt; ySize; ++j) ChessBoardArray[0][j] = 1; for (int i = 1; i &lt; xSize; ++i) for (int j = 1; j &lt; ySize; ++j) ChessBoardArray[i][j] = ChessBoardArray[i][j - 1] + ChessBoardArray[i - 1][j]; return ChessBoardArray[m][n];&#125; 大数加法/乘法大数加法思路 :模拟小学列竖式 9 8 + 2 1 ------------- (1)(1)(9)大数乘法思路 : 模拟乘法累加 - 改进简单来说，方法二就是先不算任何的进位，也就是说，将每一位相乘，相加的结果保存到同一个位置，到最后才计算进位。 例如：计算98×21,步骤如下 9 8 × 2 1 ------------- (9)(8) &lt;---- 第1趟: 98×1的每一位结果 (18)(16) &lt;---- 第2趟: 98×2的每一位结果 ------------- (18)(25)(8) &lt;---- 这里就是相对位的和，还没有累加进位这里唯一要注意的便是进位问题，我们可以先不考虑进位，当所有位对应相加，产生结果之后，再考虑。从右向左依次累加，如果该位的数字大于10，那么我们用取余运算，在该位上只保留取余后的个位数，而将十位数进位（通过模运算得到）累加到高位便可，循环直到累加完毕。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129void BigIntAddition(char* bigIntA, char* bigIntB)&#123; if (!bigIntA || !bigIntB) return; size_t strlenA = strlen(bigIntA); size_t strlenB = strlen(bigIntB); size_t biggerStrlen = strlenA &gt; strlenB ? strlenA : strlenB; int* reversedA = new int[biggerStrlen]; int* reversedB = new int[biggerStrlen]; // 先将例子中的 1234 和 98765 逆序存储, 不够的补零, 方便计算 for (size_t i = 0; i &lt; biggerStrlen; ++i) &#123; //cout &lt;&lt; int(strlenA - 1 - i) &lt;&lt; endl; reversedA[i] = (int(strlenA - 1 - i) &gt;= 0) ? (bigIntA[strlenA - 1 - i] - '0') : 0; reversedB[i] = (int(strlenB - 1 - i) &gt;= 0) ? (bigIntB[strlenB - 1 - i] - '0') : 0; &#125; for (size_t i = 0; i &lt; biggerStrlen; ++i) cout &lt;&lt; reversedA[i]; cout &lt;&lt; endl; // --&gt; 43210 for (size_t i = 0; i &lt; biggerStrlen; ++i) cout &lt;&lt; reversedB[i]; cout &lt;&lt; endl; // --&gt; 98765 int* bigIntSum = new int[biggerStrlen + 1]; int x = 0; // 进位 // 模拟小学的列竖式加法, 满10进1 for (size_t i = 0; i &lt; biggerStrlen; ++i) &#123; bigIntSum[i] = reversedA[i] + reversedB[i] + x; x = bigIntSum[i] / 10; bigIntSum[i] %= 10; &#125; size_t printLen = biggerStrlen; // 查看最后一个进位是否 &gt; 0, 大于零则最高位为1 if (x &gt; 0) &#123; bigIntSum[biggerStrlen] = x; printLen = biggerStrlen + 1; &#125; for (size_t i = 0; i &lt; printLen; ++i) cout &lt;&lt; bigIntSum[printLen - 1 - i]; // --&gt; 58023 cout &lt;&lt; endl; delete[] bigIntSum;&#125;void BigIntMultiplication(char* bigIntA, char* bigIntB)&#123; if (!bigIntA || !bigIntB) return; int strlenA = static_cast&lt;int&gt;(strlen(bigIntA)); int strlenB = static_cast&lt;int&gt;(strlen(bigIntB)); cout &lt;&lt; strlenA &lt;&lt; ", " &lt;&lt; strlenB &lt;&lt; endl; int biggerStrlen = strlenA &gt; strlenB ? strlenA : strlenB; int* reversedA = new int[biggerStrlen]; int* reversedB = new int[biggerStrlen]; // 先将例子中的 1234 和 98765 逆序存储, 不够的补零, 方便计算 for (int i = 0; i &lt; biggerStrlen; ++i) &#123; reversedA[i] = (int(strlenA - 1 - i) &gt;= 0) ? (bigIntA[strlenA - 1 - i] - '0') : 0; reversedB[i] = (int(strlenB - 1 - i) &gt;= 0) ? (bigIntB[strlenB - 1 - i] - '0') : 0; &#125; for (int i = 0; i &lt; biggerStrlen; ++i) cout &lt;&lt; reversedA[i]; cout &lt;&lt; endl; // --&gt; 43210 for (int i = 0; i &lt; biggerStrlen; ++i) cout &lt;&lt; reversedB[i]; cout &lt;&lt; endl; // --&gt; 98765 // 分配一个空间，用来存储运算的结果，num1长的数 * num2长的数， // 结果不会超过num1+num2长 int* bigIntProduct = new int[strlenA + strlenB]; // 比如防止下面执行 bigIntSum[i + j] += reversedA[i] * reversedB[j]; 这句的时候 // i+j = 0 时 出错, 因为 bigIntSum[0] 为一个未初始化的值 for (int i = 0; i &lt; strlenA + strlenB; ++i) bigIntProduct[i] = 0; int carry = 0; // 进位 // 先不考虑进位问题，根据竖式的乘法运算， // num1的第i位与num2的第j位相乘，结果应该存放在结果的第i+j位上 for (int i = 0; i &lt; strlenA; ++i) for (int j = 0; j &lt; strlenB; ++j) bigIntProduct[i + j] += reversedA[i] * reversedB[j]; for (int i = 0; i &lt; strlenA + strlenB; ++i) cout &lt;&lt; bigIntProduct[i] &lt;&lt; ", "; // --&gt; 3659707060341650 cout &lt;&lt; endl; //单独处理进位 for (int i = 0; i &lt; strlenA + strlenB - 1; ++i) &#123; bigIntProduct[i] += carry; carry = bigIntProduct[i] / 10; bigIntProduct[i] %= 10; &#125; for (int i = 0; i &lt; strlenA + strlenB; ++i) cout &lt;&lt; bigIntProduct[i] &lt;&lt; ", "; // --&gt; 626770070 cout &lt;&lt; endl; int printLen = strlenA + strlenB - 1; // 查看最后一个进位是否 &gt; 0, 大于零则最高位为1 if (carry &gt; 0) &#123; bigIntProduct[strlenA + strlenB - 1] = carry; printLen = strlenA + strlenB; &#125; for (int i = 0; i &lt; printLen; ++i) cout &lt;&lt; bigIntProduct[printLen - 1 - i]; // --&gt; 70077626 cout &lt;&lt; endl; delete[] bigIntProduct;&#125;int main()&#123; char *bigIntA = "1234"; char *bigIntB = "56789"; BigIntAddition(bigIntA, bigIntB); BigIntMultiplication(bigIntA, bigIntB); return 0;&#125; 最长公共子串问题：有两个字符串str和str2，求出两个字符串中最长公共子串长度。 比如：str=acbcbcef，str2=abcbced，则str和str2的最长公共子串为bcbce，最长公共子串长度为5。 算法思路： 1、把两个字符串分别以行和列组成一个二维矩阵。2、比较二维矩阵中每个点对应行列字符中否相等，相等的话值设置为1，否则设置为0。3、通过查找出值为1的最长对角线就能找到最长公共子串。 针对于上面的两个字符串我们可以得到的二维矩阵如下： 从上图可以看到，str和str2共有5个公共子串，但最长的公共子串长度为5。 为了进一步优化算法的效率，我们可以再计算某个二维矩阵的值的时候顺便计算出来当前最长的公共子串的长度，即某个二维矩阵元素的值由 item[i][j]=1 演变为 item[i][j]=1 +item[i-1][j-1] ，这样就避免了后续查找对角线长度的操作了。修改后的二维矩阵如下： 故状态转移方程 X[i] == Y[j]，dp[i][j] = dp[i-1][j-1] + 1 X[i] != Y[j]，dp[i][j] = 012345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061int LongestCommonSubstring(char* strA, char* strB)&#123; if (!strA || !strB) return -1; int maxLen = 0; int strlenA = static_cast&lt;int&gt;(strlen(strA)); int strlenB = static_cast&lt;int&gt;(strlen(strB)); int biggerStrlen = strlenA &gt; strlenB ? (strlenA + 1) : (strlenB + 1); char * lcs = new char[biggerStrlen]; int lcsMaxIndex = 0; int** temp = new int*[strlenA]; for (int i = 0; i &lt; strlenA; ++i) temp[i] = new int[strlenB]; for (int i = 0; i &lt; strlenA; ++i) &#123; for (int j = 0; j &lt; strlenB; ++j) &#123; if (strA[i] == strB[j]) &#123; if (i &gt; 0 &amp;&amp; j &gt; 0) temp[i][j] = temp[i - 1][j - 1] + 1; else temp[i][j] = 1; &#125; else &#123; temp[i][j] = 0; &#125; if (temp[i][j] &gt; maxLen) &#123; maxLen = temp[i][j]; lcsMaxIndex = i; &#125; &#125; &#125; for (int i = 0;i &lt; maxLen; ++i) *(lcs + maxLen - i - 1) = strA[lcsMaxIndex--]; *(lcs+maxLen) = '\0'; cout &lt;&lt; lcs &lt;&lt; endl; for (int i = 0; i &lt; strlenA; ++i) delete[] temp[i]; delete[] temp; delete[] lcs; return maxLen;&#125;int main()&#123; cout &lt;&lt; "maxLen = " &lt;&lt; LongestCommonSubstring("wwwadfabcdeasdf", "wwweoruqpeorqabcdezcvnz") &lt;&lt; endl; return 0;&#125;]]></content>
      <categories>
        <category>CPP</category>
      </categories>
      <tags>
        <tag>CPP</tag>
        <tag>noodle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[STL之队列和双端队列和栈的比较]]></title>
    <url>%2Fblog%2F2014%2F09%2F25%2Fdeque_queue_stack_comparison%2F</url>
    <content type="text"><![CDATA[队列和双端队列的比较队列(queue)是一种是相对于栈的一种数据结构，它是先进先出(First In First Out)。它只可以在尾部添加元素。 双端队列(deque double ended queue（双端队列）)是一种相对于队列的一种数据结构。它可以在尾部和头部插入、移除和获取。 三者成员函数的比较通过他们各自的成员函数我们能一目了然的看出区别 栈的成员函数 stack&lt;Type&gt; s : 定义一个stack的变量 s.push(x) : 入栈，如例 s.pop() : 出栈，如例 . 注意，出栈操作只是删除栈顶元素，并不返回该元素。 s.top() : 访问栈顶，如例 s.empty() : 判断栈空，如例，当栈空时，返回true。 s.size() : 访问栈中的元素个数，如例 队列的成员函数 queue&lt;Type&gt; M : 定义一个queue的变量 M.empty() : 查看是否为空范例 是的话返回1，不是返回0; M.push() : 从已有元素后面增加元素 M.size() : 输出现有元素的个数 M.front() : 显示第一个元素 M.back() : 显示最后一个元素 M.pop() : 清除第一个元素 双端队列的成员函数 deque&lt;Type&gt; c : 定义一个deque的变量 c.pop_back() : 删除最后一个数据。 c.pop_front() : 删除头部数据。 c.push_back(elem) : 在尾部加入一个数据。 c.push_front(elem) : 在头部插入一个数据。 c.clear() : 移除容器中所有数据。 c.front() : 传回地一个数据。 c.back() : 传回最后一个数据，不检查这个数据是否存在。 c.size() : 返回容器中实际数据的个数。 c.empty() : 判断容器是否为空。]]></content>
      <categories>
        <category>CPP</category>
      </categories>
      <tags>
        <tag>CPP</tag>
        <tag>STL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构三之二叉搜索树的增删查]]></title>
    <url>%2Fblog%2F2014%2F09%2F24%2Fbinary_search_tree_insert_delete_search%2F</url>
    <content type="text"><![CDATA[有了二叉树的基础, 我们继续学习二叉搜索树. 二叉搜索树的定义二叉查找树(Binary Search Tree, 简称”BST”), 又名”二叉搜索树”或”二叉排序树”:它或者是一棵空树，或者是具有下列性质的二叉树： 若它的左子树不空，则左子树上所有结点的值均小于它的根结点的值； 若它的右子树不空，则右子树上所有结点的值均大于它的根结点的值； 它的左、右子树也分别为二叉排序树。 . . . *注 : *我们本文中二叉树的二叉链式存储方案的代码表示为 123456typedef struct BinaryTreeNode&#123; void *data; BinaryTreeNode *LeftNode; BinaryTreeNode *RightNode;&#125;BTN, *BTNP; 二叉搜索树的查询123456789101112131415161718192021BTNP SearchBST(BTNP btnp, char key_data)&#123; while (btnp) &#123; if ( key_data &lt; (*(char *)(btnp-&gt;data)) ) &#123; btnp = btnp-&gt;LeftNode; &#125; else if ( key_data &gt;(*(char *)(btnp-&gt;data)) ) &#123; btnp = btnp-&gt;RightNode; &#125; else &#123; cout &lt;&lt; "found" &lt;&lt; endl; return btnp; &#125; &#125; cout &lt;&lt; "not found" &lt;&lt; endl; return nullptr;&#125; 二叉搜索树的插入 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849void InsertBST(BTNP &amp;btnp, char key_data)&#123; char * temp_key_data = new char; *temp_key_data = key_data; BTNP new_btnp = new BTN; new_btnp-&gt;data = temp_key_data; new_btnp-&gt;LeftNode = nullptr; new_btnp-&gt;RightNode = nullptr; if (!btnp) &#123; btnp = new_btnp; return; &#125; BTNP temp_btnp = btnp; BTNP parent_btnp = nullptr; bool is_left = true; while (temp_btnp) &#123; parent_btnp = temp_btnp; if (key_data &lt; (*(char *)(temp_btnp-&gt;data))) &#123; temp_btnp = temp_btnp-&gt;LeftNode; is_left = true; &#125; else if (key_data &gt; (*(char *)(temp_btnp-&gt;data))) &#123; temp_btnp = temp_btnp-&gt;RightNode; is_left = false; &#125; else &#123; cout &lt;&lt; "already has a same key_data" &lt;&lt; endl; return; &#125; &#125; if (is_left == true) &#123; parent_btnp-&gt;LeftNode = new_btnp; &#125; else &#123; parent_btnp-&gt;RightNode = new_btnp; &#125;&#125; 二叉搜索树之删除某个结点 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173void DeleteBinaraySearchTree(BTNP &amp;btnp, char key)&#123; // 第一步 : 查找是否有这个key BTNP parentBTNP = NULL; bool isLeft = true; BTNP tempBNTP = btnp; while (tempBNTP) &#123; if (*(char*)tempBNTP-&gt;data &gt; key) &#123; parentBTNP = tempBNTP; tempBNTP = tempBNTP-&gt;LeftNode; isLeft = true; &#125; else if (*(char*)tempBNTP-&gt;data &lt; key) &#123; parentBTNP = tempBNTP; tempBNTP = tempBNTP-&gt;RightNode; isLeft = false; &#125; else &#123; break; &#125; &#125; if (!tempBNTP) &#123; cout &lt;&lt; "not found this key!!" &lt;&lt; endl; return; &#125; cout &lt;&lt; "found this key!!" &lt;&lt; endl; // 第二步 : 我们得处里key结点没有父结点的情况 if (!parentBTNP) &#123; delete (char *)btnp-&gt;data; btnp-&gt;data = NULL; delete btnp; btnp = NULL; cout &lt;&lt; "test" &lt;&lt; endl; return; &#125; else &#123; // 第三步 : 我们得处里key结点有父结点的4种情况 // 情况1 if (tempBNTP-&gt;LeftNode == NULL &amp;&amp; tempBNTP-&gt;RightNode == NULL) &#123; if (isLeft) &#123; parentBTNP-&gt;LeftNode = NULL; &#125; else &#123; parentBTNP-&gt;RightNode = NULL; &#125; &#125; // 情况2 else if (tempBNTP-&gt;LeftNode == NULL) &#123; if (isLeft) &#123; parentBTNP-&gt;LeftNode = tempBNTP-&gt;RightNode; &#125; else &#123; parentBTNP-&gt;RightNode = tempBNTP-&gt;RightNode; &#125; &#125; // 情况3 else if (tempBNTP-&gt;RightNode == NULL) &#123; if (isLeft) &#123; parentBTNP-&gt;LeftNode = tempBNTP-&gt;LeftNode; &#125; else &#123; parentBTNP-&gt;RightNode = tempBNTP-&gt;LeftNode; &#125; &#125; // 情况4 else if (tempBNTP-&gt;LeftNode != NULL &amp;&amp; tempBNTP-&gt;RightNode != NULL) &#123; // 情况4比较复杂, 我们得找到key结点的后继 // (后继 : 一个结点x的后继是大于x.key的最小关键字的结点) // 因为情况4中的key结点左孩子和右孩子都不为空, // 所以key结点的后继 successorBTNP 肯定位于key结点的右子树中, // 且 successorBTNP 没有左孩子( 不然 successorBTNP 的左孩子就是key结点的后继了嘛) BTNP tempTempBNTP = tempBNTP-&gt;RightNode; // key结点的后继 BTNP successorBTNP = NULL; // key结点的后继的父结点 BTNP successorParentBTNP = NULL; while (tempTempBNTP) &#123; successorBTNP = tempTempBNTP; if (tempTempBNTP &amp;&amp; tempTempBNTP-&gt;LeftNode &amp;&amp; tempTempBNTP-&gt;LeftNode-&gt;LeftNode == NULL) &#123; successorParentBTNP = tempTempBNTP; &#125; tempTempBNTP = tempTempBNTP-&gt;LeftNode; &#125; // 情况4又分两种情况, 如下 : // 第一种情况 A 是 successorBTNP 是 key 的右孩子; // 第二种情况 B 是 successorBTNP 在 key 的右子树中, 但并不是 successorBTNP 本身并不是 key 的右孩子 // 情况 A : if (successorBTNP == tempBNTP-&gt;RightNode) &#123; // 用 key 结点的后继 successorBTNP 来替代 key 结点 if (isLeft) &#123; parentBTNP-&gt;LeftNode = successorBTNP; &#125; else &#123; parentBTNP-&gt;RightNode = successorBTNP; &#125; &#125; // 情况 B : else &#123; // 如果 successorBTNP 有右子树, 则用 successorBTNP 的右子树 代替 原来 successorBTNP 的位置. if (successorBTNP-&gt;RightNode) &#123; successorParentBTNP-&gt;LeftNode = successorBTNP-&gt;RightNode; &#125; // 用 key 结点的后继 successorBTNP 的data来替换 key 结点的data char * tempChar = new char; *tempChar = *(char *)successorBTNP-&gt;data; tempBNTP-&gt;data = tempChar; tempBNTP = successorBTNP; // 下面注释的这块代码可以用上面这两4行代替 // if (isLeft) // &#123; // parentBTNP-&gt;LeftNode = successorBTNP; // successorBTNP-&gt;RightNode = tempBNTP-&gt;RightNode; // successorBTNP-&gt;LeftNode = tempBNTP-&gt;LeftNode; // &#125; // else // &#123; // parentBTNP-&gt;RightNode = successorBTNP; // successorBTNP-&gt;RightNode = tempBNTP-&gt;RightNode; // successorBTNP-&gt;LeftNode = tempBNTP-&gt;LeftNode; // &#125; &#125; &#125; // 记得释放 key 结点 delete (char *)tempBNTP-&gt;data; tempBNTP-&gt;data = NULL; delete tempBNTP; tempBNTP = NULL; &#125;&#125; 二叉搜索树之找最低公共祖先给定二叉搜索树（BST）中两节点，找出他们的最低公共祖先(LeastCommonAncestors, 简称LCA)。 例如对于本文第一张图的LCA为 : LCA(4， 14)=8; LCA(8， 10)=8. 思路: 利用BST的性质, 假设n1,n2都在BST中，并且n1 &lt; n2。则有 : 在遍历过程中，遇到的第一个值介于n1和n2之间的节点n，也即n1 =&lt; n &lt;= n2, 就是n1和n2的LCA。 在遍历过程中，如果节点的值比n1和n2都大，那么LCA在节点的左子树。 在遍历过程中，如果节点的值比n1和n2都小，那么LCA在节点的右子树。 123456789101112131415161718192021222324252627282930313233343536373839404142434445void LeastCommonAncestorsBinaraySearchTree(BTNP btnp, char key1, char key2)&#123; if (!btnp) &#123; cout &lt;&lt; "the BST is null" &lt;&lt; endl; return; &#125; if (key1 == key2) &#123; cout &lt;&lt; "key1 == key2, so this is not a BST" &lt;&lt; endl; return; &#125; if (!SearchBinarySearchTree(btnp, key1)) &#123; cout &lt;&lt; "key1 not found" &lt;&lt; endl; return; &#125; if (!SearchBinarySearchTree(btnp, key2)) &#123; cout &lt;&lt; "key2 not found" &lt;&lt; endl; return; &#125; while (btnp) &#123; char curChar = *(char *)(btnp-&gt;data); if (curChar &lt; key1 &amp;&amp; curChar &lt; key2) &#123; btnp = btnp-&gt;RightNode; &#125; else if (curChar &gt; key1 &amp;&amp; curChar &gt; key2) &#123; btnp = btnp-&gt;LeftNode; &#125; else &#123; cout &lt;&lt; "LeastCommonAncestors is " &lt;&lt; curChar &lt;&lt; endl; return; &#125; &#125;&#125; 参考&lt;&lt; 算法导论 &gt;&gt;]]></content>
      <categories>
        <category>CPP</category>
      </categories>
      <tags>
        <tag>CPP</tag>
        <tag>DataStructure</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构二之二叉树的遍历和交换左右孩子]]></title>
    <url>%2Fblog%2F2014%2F09%2F23%2Fbinary_tree_traverse_and_swap%2F</url>
    <content type="text"><![CDATA[二叉树的二叉链式存储方案的代码表示： 123456typedef struct BinTreeNode&#123; BinTreeNode( char Data ) : data_( Data ), left_( nullptr ), right_( nullptr ) &#123;&#125; char data_; struct BinTreeNode *left_, *right_;&#125;btn, *btnp; 二叉树的遍历. . . 二叉树的广度优先遍历 12345678910111213141516171819202122void BreadthFirstTraverse( btnp bTreeNode )&#123; if ( !bTreeNode ) &#123; return; &#125; std::queue&lt;btnp&gt; tempQueue; tempQueue.push( bTreeNode ); while ( !tempQueue.empty() ) &#123; cout &lt;&lt; tempQueue.front()-&gt;data_ &lt;&lt; endl; if ( tempQueue.front()-&gt;left_ ) &#123; tempQueue.push( tempQueue.front()-&gt;left_ ); &#125; if ( tempQueue.front()-&gt;right_ ) &#123; tempQueue.push( tempQueue.front()-&gt;right_ ); &#125; tempQueue.pop(); &#125;&#125; 深度优先的递归式遍历递归式遍历的代码实现非常简洁, 但生产环境一般不允许递归, 因为怕栈溢出. 12345678910111213141516171819202122232425262728293031323334353637typedef struct BinaryTreeNode&#123; void *data; BinaryTreeNode *LeftNode; BinaryTreeNode *RightNode;&#125;BTN, *BTNP;void PreOrderTraverse_Recursion(BTNP btnp)&#123; if (btnp) &#123; cout &lt;&lt; *(char *)btnp-&gt;data &lt;&lt; endl; PreOrderTraverse_Recursion(btnp-&gt;LeftNode); PreOrderTraverse_Recursion(btnp-&gt;RightNode); &#125;&#125;void InOrderTraverse_Recursion(BTNP btnp)&#123; if (btnp) &#123; PreOrderTraverse_Recursion(btnp-&gt;LeftNode); cout &lt;&lt; *(char *)btnp-&gt;data &lt;&lt; endl; PreOrderTraverse_Recursion(btnp-&gt;RightNode); &#125;&#125;void PostOrderTraverse_Recursion(BTNP btnp)&#123; if (btnp) &#123; PreOrderTraverse_Recursion(btnp-&gt;LeftNode); PreOrderTraverse_Recursion(btnp-&gt;RightNode); cout &lt;&lt; *(char *)btnp-&gt;data &lt;&lt; endl; &#125;&#125; 深度优先的迭代式遍历递归的本质就是出栈入栈, 所以我们用栈来模拟递归, 写出以下三种迭代式遍历 迭代的二叉树三种遍历方式其实思想是统一的 : 都是从左子树的各个结点依次入栈, 当左边已经走到头了, 就开始走右边, 在适当的条件就出栈, 只是每个遍历方式的出栈条件不一样而已, 或者是打印结点的时机不同而已. 迭代式先序遍历代码实现123456789101112131415161718192021222324void PreOrderTraverseNonRecursion( btnp bTreeNode )&#123; if ( !bTreeNode ) &#123; return; &#125; std::stack&lt;btnp&gt; tempStack; while ( !tempStack.empty() || bTreeNode ) &#123; while ( bTreeNode ) &#123; cout &lt;&lt; bTreeNode-&gt;data_ &lt;&lt; endl; tempStack.push( bTreeNode ); bTreeNode = bTreeNode-&gt;left_; &#125; if ( !tempStack.empty() ) &#123; bTreeNode = tempStack.top()-&gt;right_; tempStack.pop(); &#125; &#125;&#125; 迭代式中序遍历代码实现123456789101112131415161718192021222324void InOrderTraverseNonRecursion( btnp bTreeNode )&#123; if ( !bTreeNode ) &#123; return; &#125; std::stack&lt;btnp&gt; tempStack; while ( !tempStack.empty() || bTreeNode ) &#123; while ( bTreeNode ) &#123; tempStack.push( bTreeNode ); bTreeNode = bTreeNode-&gt;left_; &#125; if ( !tempStack.empty() ) &#123; cout &lt;&lt; tempStack.top()-&gt;data_ &lt;&lt; endl; bTreeNode = tempStack.top()-&gt;right_; tempStack.pop(); &#125; &#125;&#125; 迭代式后序遍历代码实现后序遍历的出栈条件有点不一样, 因为后序是先左后右再中的, 比如某个结点p要出栈, 需要遍历完了p的所有右子树之后才能出栈, 而不能第一次就出栈, 所以专门构造了一个结构体PostOrderBT来记录他是否是第一次出栈 (PostOrderBT结构体里有个 isFirstTime_ 的数据来记录) *所以我们代码中的思路就是 : *把每个将要入栈的结点的 isFirstTime_ 标志都置为 true , 当第一次遍历到结点p的时候, 不使p出栈, 但使p的 isFirstTime_ 标志变为 false, 然后 “bTreeNode = 栈顶-&gt;右孩子” 开始遍历他的右子树. 当p的右子树都遍历完了之后(也就是p的右子树都依次出栈了之后)又会遍历到p自己, 不过这一次他的 isFirstTime_ 标志已经为 false 了, 我们通过这个标志知道不是第一次遍历到p了, 所以这时我们使p出栈( 此时p的右子树都已经遍历完了, 所以不用像之前一样再 “bTreeNode = 栈顶-&gt;右孩子” 了 ) 12345678910111213141516171819202122232425262728293031323334353637383940void PostOrderTraverseNonRecursion( btnp bTreeNode )&#123; if ( !bTreeNode ) &#123; return; &#125; typedef struct PostOrderBTreeNode &#123; PostOrderBTreeNode( btnp OriginBT, bool IsFirstTime ) : bt_( OriginBT ), isFirstTime_( IsFirstTime ) &#123;&#125; btnp bt_; bool isFirstTime_; &#125;pobtn, *pobtnp; std::stack&lt;pobtn&gt; tempStack; while ( !tempStack.empty() || bTreeNode ) &#123; while ( bTreeNode ) &#123; pobtn np( bTreeNode, true ); tempStack.push( np ); bTreeNode = bTreeNode-&gt;left_; &#125; if ( !tempStack.empty() ) &#123; if ( tempStack.top().isFirstTime_ ) &#123; tempStack.top().isFirstTime_ = false; bTreeNode = tempStack.top().bt_-&gt;right_; &#125; else &#123; cout &lt;&lt; tempStack.top().bt_-&gt;data_ &lt;&lt; endl; tempStack.pop(); &#125; &#125; &#125;&#125; 遍历测试代码 如上图得到的相应的三种深度优先遍历的序列分别为 ： 先(根)序遍历 ： ABCDEGF 中(根)序遍历 ： CBEGDFA 后(根)序遍历 ： CGEFDBA 而得到的广度优先遍历的序列为 : ABCDEFG 参照上图构建如下二叉树 : 12345678910111213141516171819202122232425262728293031323334353637#include &lt;iostream&gt;#include &lt;queue&gt;#include &lt;stack&gt;using std::cout;using std::endl;using std::stack;using std::queue;int main()&#123; btn testbt = btn( 'A' ); auto testB = btn( 'B' ); auto testC = btn( 'C' ); auto testD = btn( 'D' ); auto testE = btn( 'E' ); auto testG = btn( 'G' ); auto testF = btn( 'F' ); testbt.left_ = &amp;testB; testbt.left_-&gt;left_ = &amp;testC; testbt.left_-&gt;right_ = &amp;testD; testbt.left_-&gt;right_-&gt;left_ = &amp;testE; testbt.left_-&gt;right_-&gt;left_-&gt;right_ = &amp;testG; testbt.left_-&gt;right_-&gt;right_ = &amp;testF; cout &lt;&lt; "BreadthFirstTraverse : \n"; BreadthFirstTraverse( &amp;testbt ); cout &lt;&lt; "PreOrderTraverseNonRecursion : \n"; PreOrderTraverseNonRecursion( &amp;testbt ); cout &lt;&lt; "InOrderTraverseNonRecursion : \n"; InOrderTraverseNonRecursion( &amp;testbt ); cout &lt;&lt; "PostOrderTraverseNonRecursion : \n"; PostOrderTraverseNonRecursion( &amp;testbt ); return 0;&#125; 深度优先的迭代式遍历之总结请仔细看完上述代码再看总结, 才更有体会. 出栈条件的不同: 前序遍历的出栈条件都是左边走到头了就让栈顶元素出栈 中序遍历同上 后序遍历的出栈条件是遍历到同一个栈顶元素第二次要出栈的时候才让他出栈 打印结点时机的不同: 前序遍历的打印结点时机是入栈时就打印 中序遍历的打印结点时机是第一次出栈时就打印 中序遍历的打印结点时机是第二次出栈时就打印 交换所有左右孩子非递归方式Swap1234567891011121314151617181920212223242526272829303132void SwapBT( btnp bTreeNode )&#123; if (!bTreeNode) return; std::stack&lt;btnp&gt; tempStack; tempStack.push( bTreeNode ); btnp tempForTop = nullptr; btnp tempForSwap = nullptr; while ( !tempStack.empty() ) &#123; tempForTop = tempStack.top(); // swap tempForSwap = tempForTop-&gt;left_; tempForTop-&gt;left_ = tempForTop-&gt;right_; tempForTop-&gt;right_ = tempForSwap; tempStack.pop(); if ( tempForTop-&gt;left_ ) &#123; tempStack.push( tempForTop-&gt;left_ ); &#125; if ( tempForTop-&gt;right_ ) &#123; tempStack.push( tempForTop-&gt;right_ ); &#125; &#125;&#125; 递归方式Swap交换左右孩子用递归很容易做到 123456789101112void SwapBinaryTree(BTNP btnp)&#123; if (btnp) &#123; BTNP tempBTNP = btnp-&gt;LeftNode; btnp-&gt;LeftNode = btnp-&gt;RightNode; btnp-&gt;RightNode = tempBTNP; SwapBinaryTree(btnp-&gt;LeftNode); SwapBinaryTree(btnp-&gt;RightNode); &#125;&#125;]]></content>
      <categories>
        <category>CPP</category>
      </categories>
      <tags>
        <tag>CPP</tag>
        <tag>DataStructure</tag>
        <tag>noodle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构一之二叉树的创建和销毁]]></title>
    <url>%2Fblog%2F2014%2F09%2F22%2Fbinary_tree_create_and_destroy%2F</url>
    <content type="text"><![CDATA[接着上一篇， 上一篇主要说了各种排序算法， 但对几个常用的数据结构还未提及，所以这一篇主要讲二叉树, 二叉树已经包括很多链表的知识了。所有代码都是测试过的, 可以直接撸. 二叉树这里不举太多数字方面的东西， 我们直接看图， 直观感性的认识满二叉树和完全二叉树： 有一点性质需要牢记：具有n个结点的完全二叉树的最大高度为log2n+1 二叉树的二叉链式存储二叉树的二叉链式存储方案的代码表示： 123456typedef struct BinaryTreeNode&#123; void *data; BinaryTreeNode *LeftNode; BinaryTreeNode *RightNode;&#125;BTN, *BTNP; . . . 二叉树的创建下面代码写法是基于二叉树的先序遍历来创建二叉树的.基于中序或者后序的写法都类似. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950void CreateBT(BTNP &amp;btnp)&#123; char input_data = 0; cin &gt;&gt; input_data; // 检查是否为叶子结点, 我们把输入为'.'的字符认为是叶子结点 if (input_data == '.') &#123; btnp = nullptr; return; &#125; char * temp_char = new char; if (!temp_char) &#123; return; &#125; *temp_char = input_data; btnp = new BTN; if (!btnp) &#123; return; &#125; // 注意这里不能写成 btnp-&gt;data = &amp;inputData; 因为inputData是分配在栈上的 btnp-&gt;data = temp_char; CreateBT(btnp-&gt;LeftNode); CreateBT(btnp-&gt;RightNode);&#125;void DestroyBT(BTNP btnp)&#123; if (!btnp) &#123; return; &#125; DestroyBT(btnp-&gt;LeftNode); DestroyBT(btnp-&gt;RightNode); // 安全释放void指针 : 将void *转换为原来类型的指针，然后再调用delete释放指针 delete (char *)btnp-&gt;data; btnp-&gt;data = nullptr; delete btnp; btnp = nullptr;&#125;]]></content>
      <categories>
        <category>CPP</category>
      </categories>
      <tags>
        <tag>CPP</tag>
        <tag>DataStructure</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面向对象的四大特征以及五大原则]]></title>
    <url>%2Fblog%2F2014%2F09%2F12%2F%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E7%9A%84%E5%9B%9B%E5%A4%A7%E7%89%B9%E5%BE%81%E4%BB%A5%E5%8F%8A%E4%BA%94%E5%A4%A7%E5%8E%9F%E5%88%99%2F</url>
    <content type="text"><![CDATA[面向对象的四大特征 封装：封装是把过程和数据包围起来，对数据的访问只能通过已定义的界面，面向对象计算始于这个基本概念，即现实世界可以被描绘成一系列完全自治，封装的对象，这些对象通过一个受保护的接口访问其他对象 继承：继承是一种联结类的层次模型，并且允许和鼓励类的重用，它提供了一种明确表达共性的方法，对象的一个新类可以从现在的类中派生，这个过程成为继承，新类继承了原始类的特性，新类成为原始类的派生类，而原始类称为新类的基类，派生类可以从他的基类那里继承方法和实例变量，并且类可以修改或增加新的方法使之更加适合特殊的需求 抽象：抽象就是忽略一个主题中与当前目标无关的那些方面，以便充分的注意与当前目标有关的方面，抽象包括两个方面，一个是过程抽象，二是数据抽象 多态性：多态性是指允许不同类的对象对同一消息作出响应，多态性包括参数化多态性和包含多态性，多态性语言具有灵活，抽象，行为共享，代码共享的优势，很好的解决了应用程序函数同名的问题 面向对象的五大原则记忆口诀 : 替开依(“凯隐”, 一个英雄联盟英雄名字)接单 单一职责原则（Single-Resposibility Principle）其核心思想为：一个类，最好只做一件事，只有一个引起它的变化。单一职责原则可以看做是低耦合、高内聚在面向对象原则上的引申，将职责定义为引起变化的原因，以提高内聚性来减少引起变化的原因。职责过多，可能引起它变化的原因就越多，这将导致职责依赖，相互之间就产生影响，从而大大损伤其内聚性和耦合度。通常意义下的单一职责，就是指只有一种单一功能，不要为类实现过多的功能点，以保证实体只有一个引起它变化的原因。专注，是一个人优良的品质；同样的，单一也是一个类的优良设计。交杂不清的职责将使得代码看起来特别别扭牵一发而动全身，有失美感和必然导致丑陋的系统错误风险。 开放封闭原则（Open-Closed principle）其核心思想是：软件实体应该是可扩展的，而不可修改的。也就是，对扩展开放，对修改封闭的。开放封闭原则主要体现在两个方面1、对扩展开放，意味着有新的需求或变化时，可以对现有代码进行扩展，以适应新的情况。2、对修改封闭，意味着类一旦设计完成，就可以独立完成其工作，而不要对其进行任何尝试的修改。实现开开放封闭原则的核心思想就是对抽象编程，而不对具体编程，因为抽象相对稳定。让类依赖于固定的抽象，所以修改就是封闭的；而通过面向对象的继承和多态机制，又可以实现对抽象类的继承，通过覆写其方法来改变固有行为，实现新的拓展方法，所以就是开放的。“需求总是变化”没有不变的软件，所以就需要用封闭开放原则来封闭变化满足需求，同时还能保持软件内部的封装体系稳定，不被需求的变化影响。 替换原则（Liskov-Substituion Principle）其核心思想是：子类必须能够替换其基类。这一思想体现为对继承机制的约束规范，只有子类能够替换基类时，才能保证系统在运行期内识别子类，这是保证继承复用的基础。在父类和子类的具体行为中，必须严格把握继承层次中的关系和特征，将基类替换为子类，程序的行为不会发生任何变化。同时，这一约束反过来则是不成立的，子类可以替换基类，但是基类不一定能替换子类。Liskov替换原则，主要着眼于对抽象和多态建立在继承的基础上，因此只有遵循了Liskov替换原则，才能保证继承复用是可靠地。实现的方法是面向接口编程：将公共部分抽象为基类接口或抽象类，通过Extract Abstract Class，在子类中通过覆写父类的方法实现新的方式支持同样的职责。Liskov替换原则是关于继承机制的设计原则，违反了Liskov替换原则就必然导致违反开放封闭原则。Liskov替换原则能够保证系统具有良好的拓展性，同时实现基于多态的抽象机制，能够减少代码冗余，避免运行期的类型判别。 依赖倒置原则（Dependecy-Inversion Principle）其核心思想是：依赖于抽象。具体而言就是高层模块不依赖于底层模块，二者都同依赖于抽象；抽象不依赖于具体，具体依赖于抽象。我们知道，依赖一定会存在于类与类、模块与模块之间。当两个模块之间存在紧密的耦合关系时，最好的方法就是分离接口和实现：在依赖之间定义一个抽象的接口使得高层模块调用接口，而底层模块实现接口的定义，以此来有效控制耦合关系，达到依赖于抽象的设计目标。抽象的稳定性决定了系统的稳定性，因为抽象是不变的，依赖于抽象是面向对象设计的精髓，也是依赖倒置原则的核心。依赖于抽象是一个通用的原则，而某些时候依赖于细节则是在所难免的，必须权衡在抽象和具体之间的取舍，方法不是一层不变的。依赖于抽象，就是对接口编程，不要对实现编程。 接口隔离原则（Interface-Segregation Principle）其核心思想是：使用多个小的专门的接口，而不要使用一个大的总接口。具体而言，接口隔离原则体现在：接口应该是内聚的，应该避免“胖”接口。一个类对另外一个类的依赖应该建立在最小的接口上，不要强迫依赖不用的方法，这是一种接口污染。接口有效地将细节和抽象隔离，体现了对抽象编程的一切好处，接口隔离强调接口的单一性。而胖接口存在明显的弊端，会导致实现的类型必须完全实现接口的所有方法、属性等；而某些时候，实现类型并非需要所有的接口定义，在设计上这是“浪费”，而且在实施上这会带来潜在的问题，对胖接口的修改将导致一连串的客户端程序需要修改，有时候这是一种灾难。在这种情况下，将胖接口分解为多个特点的定制化方法，使得客户端仅仅依赖于它们的实际调用的方法，从而解除了客户端不会依赖于它们不用的方法。分离的手段主要有以下两种：1、委托分离，通过增加一个新的类型来委托客户的请求，隔离客户和接口的直接依赖，但是会增加系统的开销。2、多重继承分离，通过接口多继承来实现客户的需求，这种方式是较好的。]]></content>
      <categories>
        <category>CPP</category>
      </categories>
      <tags>
        <tag>CPP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++很基础的易混淆点一]]></title>
    <url>%2Fblog%2F2014%2F09%2F09%2Fcplusplus_confused_points_one%2F</url>
    <content type="text"><![CDATA[2.1.1 : C++标准规定的各种算术类型的尺寸的最小值, 同时允许编译器赋予这些类型更大的尺寸. 比如char的最小尺寸为8位 执行浮点数运算选用double ，这是因为float 通常精度不够而且双精度浮点数和单精度浮点数的计算代价相差无儿。事实上， 对于某些机器来说，双精度运算甚至比单精度还快 2.1.2 : 当我们赋给无符号类型一个超出它表示范围的值时，结果是初始值对无符号类型表示数值总数取模后的余数。例如， 8 比特大小的 unsigned char 可以表示0 至255 区间内的值，如果我们赋了一个区间以外的值，则实际的结果是该值对256取模后所得的余数。因此，把 -1 赋给8 比特大小的 unsigned char 所得的结果是255 当我们赋给带符号类型一个超出它表示范围的值时，结果是未定义的( undefined )。此时， 程序，可能继续工作、可能崩溃，也可能生成垃圾数据。 如果表达式里既有带符号类型又有无符号类型， 当带符号类型取值为负时会出现异常结果， 这是因为带符号数会自动地转换成无符号数。例如，在一个形如 a*b 的式子中，如果a = -1 , b = 1 ，而且a 和b 都是int ，则表达式的值显然为- 1. 然而，如果a 是int ， 而b 是unsigned ， 则结果须视在当前机器上int 所占位数而定。在32环境里，因为2的32次方是4294967296, 所以a*b结果是4294967295 . . . 2.2.1 : 如果是内置类型的变量未被显式初始化，它的值由定义的位置决定。定义于任何函数体之外的变量被初始化为0 。然而如6. 1. 1 节(第1 85 页)所示， 一种例外情况是，定义在函数体内部的内置类型变量将不被初始化。一个未被初始化的内置类型变量的值是未定义的(参见2. 1. 2 节， 第33 页) ，如果试图拷贝或以其他形式访问此类值将引发错误 定义于函数体内的内置类型的对象如果没有初始化，值未定义。类的对象如果没有显式地初始化，则由类确定。 2.2.2 :为了支持分离式编译， C++语言将声明和定义区分开来。声明( declaration ) 使得名字为程序所知， 一个文件如果想使用别处定义的名字则必须包含对那个名字的声明。而定义( definition ) 负责创建与名字关联的实体。变量声明规定了变量的类型和名字， 在这一点上定义与之相同。但是除此之外，定义还申请存储空间，也可能会为变量赋一个初始值。如果想声明一个变量;而非定义它，就在变量名前添加关键字extern ，而且不要显式地初始化变量: 12extern int i ; // 声明i 而非定义iint j ; / / 声明并定义j 任何何包含了显式初始化的声明即成为定义。我们能给由extern 关键字标记的变量赋一个初始值，但是这么做也就抵消了extern 的作用。extern 语句如果包含初始值就不再是声明，而变成定义了:extern doub1e pi = 3 . 1416 ; // 定义 变量能且只能定义一次, 但可以多次声明. 在函数体内部，如果试图初始化一个由extern 关键字标记的变量， 将引发错误. 2.2.3 : C++也为标准库保留了一些名字。用户在自定义的标识符中不能连续出现两个下划线，也不能以下划线紧连大写字句开头。此外，定义在函数体外的标识稍不能以下划线开头。 比如: int _ = 3;是合法的 2.3.1 : 其他所有引用的类型都要和与之绑定的对象严格匹配。而且，引用只能绑定在对象上，而不能与字面值或某个表达式的计算结果绑定在一起， 相关原因将在2 .4 . 1 节详述: 123int &amp;refVa14 = 10 ; //错误· 引用类型的初始值必须是一个对象double dval = 3.14 ;int &amp;refVa15 = dva1 ; // 错误: 此处引用类型的初始位必须是int 型对象 2.3.2 : 因为引用不是对象， 没有实际地址，所以不能定义指向引用的指针。 2.4 :默认状态下， const 对象仅在文件内有效 当以编译时初始化的方式定义一个const 对象时，就如对bufSize 的定义一样: const int bufSize = 512; //输入缓冲区大小 编译器将在编译过程中把用到该变量的地方都替换成对应的值。也就是说，编译器会找到代码中所有用到 bufSize 的地方，然后用512 替换。为了执行上述替换， 编译器必须知道变量的初始值。如果程序包含多个文件，则每个用了const 对象的文件都必须得能访问到它的初始值才行。要做到这一点，就必须在每一个用到变量的文件中都有对它的定义(参见2.2.2 节， 第4 1 页)。为了支持这一用法，同时避免对同一变量的重复定义，默认情况下， const 对象被设定为仅在文件内有效。多个文件中出现了同名的const 变量时，其实等同于在不同文件中分别定义了独立的变量。某些时候有这样一种const 变量，它的初始值不是一个常量表达式，但又确实有必要在文件间共享。这种情况， 我们不希望编译器为每个文件分别生成独立的变量。相反，我们想让这类const 对象像其他(非常量)对象一样工作，也就是说，只在一个文件中定义const ，而在其他多个文件中声明并使用它。解决的办法是，对于 const 变量不管是声明还是定义都添加extern 关键字， 这样只需定义一次就可以了: 1234// file 1 . cc 定义并初始化了一个常量，该常量能被其他文件访问extern const int bufSize = fcn();// file 1 . h 头文件extern const int bufSize ; /1 与f ile 1. cc 中定义的bufSize 是同一个 如上述程序所示， file 1. cc 定义并初始化了bufSize 。因为这条语句包含了初始值，所以它(显然〉是一次定义。然而，因为bufSize 是个常量，必须用extern 加以限定使其被其他文件使用。 file 1. h 头文件中的声明也由extern 做了限定，其作用是指明bufSize 并非file 1独有，它的定义将在别处出现。 2.4.1初始化和对const 的引用 2.3.1节(第46 页)提到， 号| 用的类型必须与其所引用对象的类型→致，但是有两个例外。第一种例外情况就是在初始化常量引用时允许用任意表达式作为初始值，只要该表达式的结果能转换成(参见2.1 .2 节，第3 2 页)引用的类型叩可。尤其，允许为一个常量引用绑定非常量的对象、字面值， 甚至是个-般表达式: 12345int i= 42 ; .const int &amp;r1 = i ; // ft许将const int &amp;r1 定到一个普通int 对象上const int &amp;r2 = 42; // 正确r1 是一个常量引用const int &amp;r3 = r1 * 2 ; // 正确r3 是一个常量引用int &amp;r4 = r1 * 2 ; // 错误r4 是一个普通的非常量引用 要想、理解这种例外情况的原因，陆简单的办法是弄清楚当一个常量引用被绑定到另外一种类型上时到底发生了什么: 12doub1e dval = 3 . 14 ;const int &amp;ri = dva1 ; 此处ri 引用了一个int 型的数。对口的操作应该是整数运算，但dval 却是一个双精度浮点数而非整数。因此为了确保让rl 绑定一个整数，编译器把上述代码变成了如下形式: 12const int temp = dval; 1/ 由双精度浮点数生成一个临时的整型常量const int &amp;ri = temp ; 11 让rl 绑定这个临时受 在这种情况下， ri 绑定了一个临时量对象。所谓临时量对象就是当编译器而要一个空间来暂存表达式的求值结果时临时创建的一个未命名的对象。c++程序员们常常把临时量对象简称为临时量。接下来探讨当ri 不是常量时，如果执行了类似于上面的初始化过程将带来什么样的后果。如果且不是常量，就允许对ri 赋值，这样就会改变ri 所引用对象的值。注意，此时绑定的对象是一个临时量;而dvalo 程序员既然让rl 引用dval ， 就肯定想通过ri 改变dval 的值，否则干什么要给ri 赋值l呢?如此看来， 既然大家基本上不会想着把引用绑定到临时量上， c++语言也就把这种行为归为非法。 2.5.2 和原来另一些只对应一种特定类型的说明符(比如double) 不|司. auto 让编译器通过初始值来推算变量的类型。显然. auto 定义的变量必须有初始值: 使用auto 也能在一条语句中声明多个变量。因为一条声明语句只能有一个基本数据类型，所以该语句中所有变量的初始基本数据类型都必须一样: 123int i = 0; const int ci = i;auto &amp;n = i, *p = &amp;ci // 错误 : i 的类型是int 而&amp; ci 的类型是const int]]></content>
      <categories>
        <category>CPP</category>
      </categories>
      <tags>
        <tag>CPP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[排序算法四之谈一谈堆排序]]></title>
    <url>%2Fblog%2F2014%2F08%2F24%2Fheap_sort%2F</url>
    <content type="text"><![CDATA[此文用的是最大堆, 最大堆的堆排序之后的数组是升序, 最小堆反之.堆排序 HeapSort 由 以下两部分组成 : 堆化 MaxHeapify 建堆 BuildMaxHeap. . . . 堆化MaxHeapify具体过程如下图 : 注意 :在调用MaxHeapify的时候, 我们假定索引为index的元素的左子树和右子树都是最大堆, 不然你如果注意看的话, 你会发现上图中index为10的那个元素其实是没有计算到的, 因为我们假定以index=5为根节点的二叉树都是最大堆了, 所以无需计算他.那为何要作如此假设呢?因为要跟建堆 BuildMaxHeap 配合来完成堆排序, 而建堆 BuildMaxHeap是从下至上的. 123456789101112131415161718192021222324252627282930313233343536373839404142void MaxHeapify(int arr[], int index, int length)&#123; if (!arr || index &lt; 0 || length &lt;= 0) &#123; cout &lt;&lt; "error" &lt;&lt; endl; return; &#125; int tempData = 0; int leftChildIndex = 0; int rightChildIndex = 0; int largestIndex = 0; while (1) &#123; leftChildIndex = index * 2 + 1; rightChildIndex = index * 2 + 2; largestIndex = index; if (leftChildIndex &lt; length &amp;&amp; arr[largestIndex] &lt; arr[leftChildIndex]) &#123; largestIndex = leftChildIndex; &#125; if (rightChildIndex &lt; length &amp;&amp; arr[largestIndex] &lt; arr[rightChildIndex]) &#123; largestIndex = rightChildIndex; &#125; if (largestIndex != index) &#123; tempData = arr[index]; arr[index] = arr[largestIndex]; arr[largestIndex] = tempData; index = largestIndex; &#125; else &#123; break; &#125; &#125;&#125; 建堆BuildMaxHeap伪代码如下图 : 具体过程如下图 : 注意 : 因为 c++ 数组的 index 是从0开始的(跟上图中有所不同, 图中的 index 是从1开始的),所以根据算法导论中的结论我们可以知道 c++数组中 index 大于等于 length/2 的元素都是树的叶结点,所以我们对每一个不是叶结点的元素(即为 index 小于等于 length/2 - 1 的元素 )自底向上调用一次 MaxHeapify 就可以把一个大小为 length 的数组转换为最大堆. 123456789101112void BuildMaxHeap(int arr[], int length)&#123; if (!arr || length &lt;= 0) &#123; cout &lt;&lt; "error" &lt;&lt; endl; return; &#125; for (int index = length / 2 - 1; index &gt;= 0; --index) &#123; MaxHeapify(arr, index, length); &#125;&#125; 堆排序HeapSort伪代码如下图 : 具体过程如下图 : 12345678910111213141516171819void HeapSort(int arr[], int length)&#123; if (!arr || length &lt;= 0) &#123; cout &lt;&lt; "error" &lt;&lt; endl; return; &#125; BuildMaxHeap(arr, length); int tempData = 0; for (int index = length - 1; index &gt;= 1; --index) &#123; tempData = arr[index]; arr[index] = arr[0]; arr[0] = tempData; MaxHeapify(arr, 0, --length); &#125;&#125; 堆排序的复杂度时间复杂度 : MaxHeapify : O(logN). BuildMaxHeap : O(N).看起来像是O(NlogN), 其实是O(N), 因为不同结点运行 MaxHeapify 的 时间和该结点的树高相关, 而大部分结点的高度都很小, &lt;&lt;算法导论&gt;&gt;中有相关证明 HeapSort : O(NlogN).初始化堆 BuildMaxHeap 的时间复杂度为O(N); 之后因为每次交换结点然后从堆中去掉最后一个结点后都要重建堆 BuildMaxHeap (上述 HeapSort 函数代码中的倒数第三行 MaxHeapify(arr, 0, --length) 其实就是个重建堆的过程) ,重建堆 BuildMaxHeap 的时间复杂度为O(N), 而 length - 1 次调用了 MaxHeapify, MaxHeapify 的时间复杂度为O(lgN). 所以为 O(N + NlogN), 即为O(Nlogn) 空间复杂度 : O(1), 因为没有用辅助内存.]]></content>
      <categories>
        <category>CPP</category>
      </categories>
      <tags>
        <tag>CPP</tag>
        <tag>Sort</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[排序算法三之谈一谈快排优化和二分查找]]></title>
    <url>%2Fblog%2F2014%2F08%2F22%2Fquick_sort_and_binary_search%2F</url>
    <content type="text"><![CDATA[快速排序与归并排序一样， 快排也是用了分治的思想。 特别注意 : 快排的核心模块是Partition, 而Partition的复杂度为O(N). 你可以想象一个两副牌然后随意取出一张牌pivot，其他的所有牌都跟这张pivot牌比较， 大的放右边那一摞A，小的放左边B。接着再从左边这一摞B再随意取出一张牌pivot，其他的所有牌都跟这张pivot牌比较， 大的放右边那一摞，小的放左边，递归下去。A也重复上述步骤递归。 递归结束之后， 左边的都比右边的小， 而且是有序的。 算法导论的快排C++实现版本 . . . 这个c++实现版本主要用于说明算法思想, 而对于代码鲁棒性有太多关注,下面有个我自己手写的命名清晰版本会比较多的关注鲁棒性以及易读性 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657void swap(int *a, int *b)&#123; int temp = 0; temp = *a; *a = *b; *b = temp;&#125;int partition(int *array, int p, int r)&#123; int i = 0, j = 0, pivot = 0; pivot = array[r]; i = p-1; for(j=p; j&lt;=r-1; j++) &#123; if(array[j] &lt;= pivot) &#123; i++; swap(&amp;array[i], &amp;array[j]); &#125; &#125; swap(&amp;array[i+1], &amp;array[r]); return i+1;&#125;/*通常，我们可以向一个算法中加入随机化成分，以便对于所有输入，它均能获得较好的平均情况性能。将这种方法用于快速排序时，不是始终采用A[r]作为主元，而是从子数组A[p..r]中随机选择一个元素，即将A[r]与从A[p..r]中随机选出的一个元素交换。*/ int rand_patition(int test_arr[], int p, int r) &#123; srand(static_cast&lt;unsigned&gt;(time(nullptr))); int rand_index = (rand() % (r - p) ) + p + 1; swap(&amp;test_arr[rand_index], &amp;test_arr[r]); return partition(test_arr, p, r); &#125;void quick_sort(int *array, int p, int r)&#123; int q = 0; /*if(p &lt; r) &#123; q = rand_patition(array, p, r); quick_sort(array, p, q-1); quick_sort(array, q+1, r); &#125;*/ // 以上的注释部分可以写成尾递归的方式来优化 while (p &lt; r) &#123; q = rand_patition(array, p, r); quick_sort(array, p, q-1); p = q + 1; &#125;&#125; 命名清晰的递归版本自己手写的, 已测试. 写得略啰嗦, 但只是希望能一目了然吧. QuickSort.h12345678910111213141516171819202122#pragma oncevoid DoQuickSort(int arr[], int start_index, int end_index);int partition(int arr[], int start_index, int end_index);void swap(int &amp;a, int &amp;b);template &lt;unsigned N&gt;void QuickSort(int(&amp;arr)[N], int start_index, int end_index)&#123; if (!arr || start_index &lt; 0 || end_index &lt; 0 || end_index &lt;= start_index || end_index &gt;= sizeof(arr) / sizeof(int)) &#123; return; &#125; DoQuickSort(arr, start_index, end_index);&#125; QuickSort.cpp1234567891011121314151617181920212223242526272829303132333435#include "QuickSort.h"void DoQuickSort(int arr[], int start_index, int end_index)&#123; int partition_index = 0; if (start_index &lt; end_index) &#123; partition_index = partition(arr, start_index, end_index); DoQuickSort(arr, start_index, partition_index - 1); DoQuickSort(arr, partition_index + 1, end_index); &#125;&#125;int partition(int arr[], int start_index, int end_index)&#123; int partition_index = start_index - 1; for (int i = start_index; i &lt; end_index; ++i) &#123; if (arr[i] &lt; arr[end_index]) &#123; swap(arr[++partition_index], arr[i]); &#125; &#125; swap(arr[++partition_index], arr[end_index]); return partition_index;&#125;void swap(int &amp;a, int &amp;b)&#123; int temp = a; a = b; b = temp;&#125; main.cpp12345678910111213#include &lt;iostream&gt;#include "QuickSort.h"int main()&#123; int test_arr[] = &#123; 4, 2, 3, 5, 6, 1 &#125;; QuickSort(test_arr, 2, 5); for (auto i : test_arr) &#123; std::cout &lt;&lt; i &lt;&lt; std::endl; &#125; return 0;&#125; 快排的非递归版本我们知道快递排序大部分的版本都是递归的方式来实现的：通过Pritation来实现划分，并递归实现前后的划分。 因为我们大部分看到的都是递归方式来实现快速排序。并没有关注非递归的方式。 但是仔细想想也是可以做的，因为递归的本质是栈，因此我们非递归实现的过程中，借助栈来保存中间变量就可以实现非递归了。 在这里中间变量也就是通过Pritation函数划分之后分成左右两部分的首尾指针，只需要保存这两部分的首尾指针即可。 123456789101112131415161718192021222324252627282930313233343536373839void QuickSort_NonRecursion( int *arr, int startIndex, int endIndex )&#123; if ( !arr ) &#123; return; &#125; std::stack&lt;int&gt; tempStack; //（注意保存顺序）先将初始状态的左右Index压栈 tempStack.push( endIndex );//先存尾Index tempStack.push( startIndex );//再存首Index int tempStartIndex = 0; int tempEndIndex = 0; int tempPartitionIndex = 0; while ( !tempStack.empty() ) &#123; tempStartIndex = tempStack.top(); tempStack.pop(); tempEndIndex = tempStack.top(); tempStack.pop(); if ( tempStartIndex &lt; tempEndIndex ) &#123; tempPartitionIndex = Partition( arr, tempStartIndex, tempEndIndex ); &#123; // 块1 tempStack.push( tempPartitionIndex - 1 ); tempStack.push( tempStartIndex ); &#125; // 块1 和 块2 调换顺序也可以 &#123; // 块2 tempStack.push( tempEndIndex ); tempStack.push( tempPartitionIndex + 1 ); &#125; &#125; &#125;&#125; 从上面的代码可以看出，保存中间变量的时候需要注意保存的顺序，因为栈是后进先出的方式。 快速排序的优化优化方式有一部分已经在上面的代码里体现比如： 尾递归优化、随机选取枢轴（pivot）， 主要的优化方案有 ： 聚集相等元素 三数取中(选择分区的第一个、中间、最后一个元素的中值作为枢轴) 当待排序序列的长度分割到一定大小后，使用插入排序 多线程 随机选取枢轴（pivot） 尾递归化 具体方案查看这篇博客 快速排序思想的应用 问题 : 查找数组中第k大的数字算法思想 :有两种思路思路： 直接从大到小排序，排好序后，第k大的数就是arr[k-1]。 只需找到第k大的数，不必把所有的数排好序。我们借助快速排序中partition过程，一般情况下，在把所有数都排好序前，就可以找到第k大的数。我们依据的逻辑是，经过一次partition后，数组被pivot分成左右两部分：S左、S右。当S左的元素个数|S左|等于k-1时，pivot即是所找的数；当|S左|小于k-1，所找的数位于S右中；当|S左|&gt;k-1，所找的数位于S左中。显然，后两种情况都会使搜索空间缩小。 二分查找二分查找也叫”折半查找” 二分查找的复杂度计算方法： 时间复杂度可以视为while循环的次数。总共有n个元素，渐渐跟下去就是n,n/2,n/4,….n/2^k（接下来操作元素的剩余个数），其中k就是循环的次数由于你n/2^k取整后&gt;=1（接下来操作元素的剩余个数至少为一个）即令n/2^k=1可得k=log2n,（是以2为底，n的对数）所以时间复杂度可以表示O(h)=O(log2n) 递归版本：12345678910111213141516171819202122232425262728int BinarySearch_Recursion(int arr[], int key, int startIndex, int endIndex)&#123; if (!arr || startIndex &lt; 0 || endIndex &lt; 0 ) &#123; return -1; &#125; if (startIndex &lt;= endIndex) &#123; int mid = (startIndex + endIndex) / 2; if (key &lt; arr[mid]) &#123; BinarySearch(arr, key, startIndex, mid - 1); &#125; else if (key &gt; arr[mid]) &#123; BinarySearch(arr, key, mid + 1, endIndex); &#125; else &#123; return mid; &#125; &#125; else &#123; return -1; &#125;&#125; 迭代版本：1234567891011121314151617181920212223242526int BinarySearch_Iteration(int arr[], int key, int startIndex, int endIndex)&#123; if (!arr || startIndex &lt; 0 || endIndex &lt; 0) &#123; return -1; &#125; while (startIndex &lt;= endIndex ) &#123; int mid = (startIndex + endIndex) / 2; if (key == arr[mid]) &#123; return mid; &#125; else if (key &lt; arr[mid]) &#123; endIndex = mid -1; &#125; else &#123; startIndex = mid + 1; &#125; &#125; return -1;&#125;]]></content>
      <categories>
        <category>CPP</category>
      </categories>
      <tags>
        <tag>CPP</tag>
        <tag>noodle</tag>
        <tag>Sort</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[排序算法二之谈一谈冒泡插入归并]]></title>
    <url>%2Fblog%2F2014%2F08%2F20%2Fbubble_insert_merge_sort%2F</url>
    <content type="text"><![CDATA[注：以下所有代码皆可以直接运行， 都已经测试过。 冒泡排序(可略过, 不实用)想象这里有很多泡泡，最大的泡泡每次循环之后浮到数组的最后面 123456789101112131415161718192021void BubbleSort(int arr[], int startIndex, int endIndex)&#123; if (!arr || startIndex &lt; 0 || endIndex &lt; 0 || endIndex - startIndex &lt;= 1) &#123; return; &#125; int temp = 0; for (int outerIndex = startIndex; outerIndex &lt;= endIndex; ++outerIndex) &#123; for (int innerIndex = startIndex; innerIndex &lt;= endIndex - outerIndex - 1; ++innerIndex) &#123; if (arr[innerIndex] &gt; arr[innerIndex + 1] ) &#123; temp = arr[innerIndex]; arr[innerIndex] = arr[innerIndex + 1]; arr[innerIndex + 1] = temp; &#125; &#125; &#125;&#125; . . . 插入排序想象手上有几张牌， 现在你抽了一张牌， 然后需要从手上最右边的牌开始比较，然后插入到相应位置 1234567891011121314151617181920void InsertSort( int arr[], int startIndex, int endIndex, int arrLen )&#123; if ( !arr || arrLen &lt;= 0 || endIndex - startIndex &gt; arrLen || startIndex &lt; 0 || startIndex &gt;= endIndex ) &#123; return; &#125; for ( int outerIndex = startIndex; outerIndex &lt;= endIndex; ++outerIndex ) &#123; for ( int innerIndex = outerIndex; innerIndex - 1 &gt;= startIndex &amp;&amp; arr[innerIndex] &lt; arr[innerIndex - 1]; --innerIndex ) &#123; auto temp = arr[innerIndex]; arr[innerIndex] = arr[innerIndex - 1]; arr[innerIndex - 1] = temp; &#125; &#125;&#125; 归并排序归并排序用了分治的思想，有很多算法在结构上是递归的：为了解决一个给定的问题，算法要一次或多次地递归调用其自身来解决相关的子问题。这些算法通常采用分治策略（divide-and-conquier）：将原问题划分成n个规模较小而结构与原问题相似的子问题；递归地解决这些子问题，然后再合并其结果，就得到原问题的解。 介绍分治思想分治模式在每一层递归上都有三个步骤： 分解（divide）：将原问题分解成一系列子问题； 解决（conquer）：递归地解各子问题。若子问题足够小，则直接求解； 合并：将子问题的结果合并成原问题的解。 归并的具体思路归并排序算法的关键操作是“合并”步骤中两个已排序序列的合并。我们通过调用一个辅助过程MERGE(A，p，q，r)来完成合并，其中A是一个数组，p、q和r是数组下标，满足p&lt;=q&lt;r。该过程假设子数组A[p．．q]和A[q+l．．r]都已排好序。它合并这两个子数组形成单一的已排好序的子数组并代替当前的子数组A[p.．r]。 回到我们玩扑克牌的例子，假设桌上有两堆牌面朝上的牌，每堆都已排序，最小的牌在顶上。我们希望把这两堆牌合并成单一的排好序的输出堆，牌面朝下地放在桌上。我们的基本步骤包括在牌面朝上的两堆牌的顶上两张牌中选取较小的一张，将该牌从其堆中移开（该堆的顶上将显露一张新牌）并牌面朝下地将该牌放置到输出堆。重复这个步骤，直到一个输入堆为空，这时，我们只是拿起剩余的输入堆并牌面朝下地将该堆放置到输出堆。 归并的算法实现下面是手写的一个比较直白明了的归并排序的 c++ 实现： MergeSort.h 1234567891011121314151617181920#pragma oncetemplate &lt;unsigned N&gt;void MergeSort(int (&amp;arr)[N], int start_index, int end_index)&#123; // check if (!arr || start_index &lt; 0 || end_index &lt; 0 || end_index &lt;= start_index || end_index &gt;= sizeof(arr) / sizeof(int)) &#123; return; &#125; DoMergeSort(arr, start_index, end_index);&#125;void DoMergeSort(int arr[], int start_index, int end_index);void Merge(int arr[], int start_index, int mid_index, int end_index); MergeSort.cpp 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172#include "MergeSort.h"void DoMergeSort(int arr[], int start_index, int end_index)&#123; if (start_index &gt;= end_index) &#123; return; &#125; int mid_index = (start_index + end_index) / 2; DoMergeSort(arr, start_index, mid_index); DoMergeSort(arr, mid_index + 1, end_index); Merge(arr, start_index, mid_index, end_index);&#125;void Merge(int arr[], int start_index, int mid_index, int end_index)&#123; int arr_left_len = mid_index - start_index + 1; int arr_right_len = end_index - mid_index; int *arr_left = new int[arr_left_len]; int *arr_right = new int[arr_right_len]; if (!arr_left || !arr_right) &#123; return; &#125; for (int i = start_index; i &lt;= end_index; ++i) &#123; if ( i &gt; mid_index ) &#123; arr_right[i - mid_index - 1] = arr[i]; &#125; else &#123; arr_left[i - start_index] = arr[i]; &#125; &#125; int arr_left_index = 0; int arr_right_index = 0; int arr_index = start_index; while (arr_left_index &lt; arr_left_len &amp;&amp; arr_right_index &lt; arr_right_len) &#123; if (arr_left[arr_left_index] &lt; arr_right[arr_right_index]) &#123; arr[arr_index++] = arr_left[arr_left_index++]; &#125; else &#123; arr[arr_index++] = arr_right[arr_right_index++]; &#125; &#125; while(arr_left_index &lt; arr_left_len) &#123; arr[arr_index++] = arr_left[arr_left_index++]; &#125; while(arr_right_index &lt; arr_right_len) &#123; arr[arr_index++] = arr_right[arr_right_index++]; &#125; delete[] arr_left; arr_left = nullptr; delete[] arr_right; arr_right = nullptr;&#125; main.cpp 1234567891011121314#include &lt;iostream&gt;#include "MergeSort.h"int main()&#123; int test_arr[] = &#123; 4, 2, 3, 5, 6, 1 &#125;; MergeSort(test_arr, 2, 5); for (auto i : test_arr) &#123; std::cout &lt;&lt; i &lt;&lt; std::endl; &#125; return 0;&#125;]]></content>
      <categories>
        <category>CPP</category>
      </categories>
      <tags>
        <tag>CPP</tag>
        <tag>noodle</tag>
        <tag>Sort</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[排序算法一之谈一谈各类算法的复杂度和常用数据结构]]></title>
    <url>%2Fblog%2F2014%2F08%2F19%2Fintroduction_of_sort_algorithm_complexity_and_data_structure%2F</url>
    <content type="text"><![CDATA[因为之前的笔记和书籍相关知识都是零零散散的， 没有一个汇总， 所以写了这篇博客。有些算法很简单，复杂度一眼都能看得出来， 几乎不需要记忆 ， 但是有些算法或者数据结构的操作的复杂度就不是一眼可以看得出来， 推导也是很费时间的， 所谓常识就是应该熟记于心且被认可的知识。 必须掌握的知识 DataStructure 链表 二叉树 栈 队列 散列表 算法 二分查找 快速排序 归并排序 堆排序 插入排序 树的插入/查找/删除 广度优先搜索 深度优先搜索 该注意的点 实用的排序算法有四种：插入、快速、归并、堆。其余的都不值得深究。这几个排序算法都有其特点，涵盖了常见的使用场景，在其特定的使用场景下是效率最高的。 插入排序 : 在小数据量比起归并和快速排序有更佳的时间效率 快速排序 : 时间复杂度依赖数据打乱的程度 快排最差情形的时间复杂度是O(n2), 平均是O(nlogn) 就地快速排序使用的空间是O(1)的，也就是个常数级；而真正消耗空间的就是递归调用了，因为每次递归就要保持一些数据； 最优的情况下空间复杂度为：O(logn) ；每一次都平分数组的情况 最差的情况下空间复杂度为：O( n ) ；退化为冒泡排序的情况 归并排序 : 时间复杂度稳定但是占用2N的内存 归并的空间复杂度就是那个临时的数组和递归时压入栈的数据占用的空间：n + logn；所以空间复杂度为: O(n) 还有一种空间复杂度为O(1)的归并排序的实现 堆排序 : 在不能一次排序N个数据并要求选出前M个数据时使用。 插入排序、堆排序、快速排序等都是原址排序。归并排序是非原址的。 插入排序、归并排序是稳定的, 堆排序、快速排序是不稳定的。 为什么在平均情况下快速排序比堆排序要优秀堆排序是渐进最优的比较排序算法，达到了O(nlgn)这一下界，而快排有一定的可能性会产生最坏划分，时间复杂度可能为O(n^2)，那为什么快排在实际使用中通常优于堆排序？ 虽然quick_sort会n^2（其实有稳定的nlgn的版本），但这毕竟很少出现。heap_sort大多数情况下比较次数都多于quick_sort，尽管大家都是nlgn。那就让倒霉蛋倒霉好了，大多数情况下快才是硬道理。 堆排比较的几乎都不是相邻元素，对cache极不友好，这才是很少被采用的原因。数学上的时间复杂度不代表实际运行时的情况.快排是分而治之，每次都在同一小段进行比较，最后越来约接近局部性。反观堆排，每次都是从堆地拿元素来替换，局部性破坏很严重。(局部性原理是指CPU访问存储器时，无论是存取指令还是存取数据，所访问的存储单元都趋于聚集在一个较小的连续区域中。) . . . 各类算法的复杂度汇总表]]></content>
      <categories>
        <category>CPP</category>
      </categories>
      <tags>
        <tag>CPP</tag>
        <tag>Sort</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[三种编程命名规范]]></title>
    <url>%2Fblog%2F2014%2F05%2F09%2F%E4%B8%89%E7%A7%8D%E7%BC%96%E7%A8%8B%E5%91%BD%E5%90%8D%E8%A7%84%E8%8C%83%2F</url>
    <content type="text"><![CDATA[匈牙利命名法：开头字母用变量类型的缩写，其余部分用变量的英文或英文的缩写，要求单词第一个字母大写。 ex:int iMyAge; “i”是int类型的缩写；char cMyName[10]; “c”是char类型的缩写；float fManHeight; “f”是float类型的缩写； 其他：前缀类型 a b by c cb cr cx,cy dw fn h i l lp m_ n np p s sz w （一一对应关系） 数组 (Array) 布尔值 (Boolean) 字节 (Byte) 有符号字符 (Char) 无符号字符 (Char Byte，没有多少人用) 颜色参考值 (ColorRef) 坐标差（长度 ShortInt） Double Word 函数 Handle（句柄） 整型 长整型 (Long Int) Long Pointer 类的成员 短整型 (Short Int) Near Pointer Pointer 字符串型 以 null 做结尾的字符串型 (String with Zero End) Word . . . 驼峰式命名法：又叫小驼峰式命名法。第一个单词首字母小写，后面其他单词首字母大写。 ex:int myAge;char myName[10];float manHeight; 帕斯卡命名法：又叫大驼峰式命名法。每个单词的第一个字母都大写。 ex:int MyAge;char MyName[10];float ManHeight; 下划线命名法还有些许其他的命名规范，如：下划线命名法，但是不是太常用，个人感觉可能是因为下划线位置太偏的事，不方便大量使用。 总结综合各方面考虑，匈牙利命名法比较好，优势明显，不过书写较为繁琐，但阅读相当便利, 目前使用驼峰式命名法的人比较多些, 个人推荐匈牙利命名法。]]></content>
      <categories>
        <category>CPP</category>
      </categories>
      <tags>
        <tag>CPP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Bash定制]]></title>
    <url>%2Fblog%2F2014%2F02%2F01%2Fbash_enhance%2F</url>
    <content type="text"><![CDATA[profile和bashrc和bash_profile的区别 /etc/profile:此文件为系统的每个用户设置环境信息,当用户第一次登录时,该文件被执行.并从 /etc/profile.d 目录的配置文件中搜集shell的设置.所以如果你有对/etc/profile有修改的话必须得重启你的修改才会生效，此修改对每个用户都生效。 /etc/bashrc:为每一个运行bash shell的用户执行此文件.当bash shell被打开时,该文件被读取.如果你想对所有的使用bash的用户修改某个配置并在以后打开的bash都生效的话可以修改这个文件，修改这个文件不用重启，重新打开一个bash即可生效。 ~/.bash_profile:每个用户都可使用该文件输入专用于自己使用的shell信息,当用户登录时,该文件仅仅执行一次!默认情况下,他设置一些环境变量,执行用户的.bashrc文件.此文件类似于/etc/profile，也是需要需要重启才会生效，/etc/profile对所有用户生效，~/.bash_profile只对当前用户生效。 ~/.bashrc:该文件包含专用于你的bash shell的bash信息,当登录时以及每次打开新的shell时,该文件被读取.（每个用户都有一个.bashrc文件，在用户目录下）此文件类似于/etc/bashrc，不需要重启生效，重新打开一个bash即可生效， /etc/bashrc对所有用户新打开的bash都生效，但~/.bashrc只对当前用户新打开的bash生效。 ~/.bash_logout:当每次退出系统(退出bash shell)时,执行该文件. 另外,/etc/profile中设定的变量(全局)的可以作用于任何用户,而~/.bashrc等中设定的变量(局部)只能继承/etc/profile中的变量,他们是”父子”关系. ~/.bash_profile 是交互式、login 方式进入bash 运行的；~/.bashrc 是交互式 non-login 方式进入bash 运行的； 通常二者设置大致相同，所以通常 ~/.bash_profile 会在其文件中加入以下代码来调用 ~/.bashrc : 1234# Get the aliases and functionsif [ -f ~/.bashrc ]; then . ~/.bashrcfi . . . 设置core文件格式/proc/sys/kernel/core_pattern可以设置格式化的core文件保存位置或文件名，比如原来文件内容是core-%e , 可以这样修改: echo &quot;/corefile/core-%e-%p-%t&quot; &gt; /proc/sys/kernel/core_pattern 将会控制所产生的core文件会存放到/corefile目录下，产生的文件名为core-命令名-pid-时间戳 ,以下是参数列表: %p - insert pid into filename 添加pid %u - insert current uid into filename 添加当前uid %g - insert current gid into filename 添加当前gid %s - insert signal that caused the coredump into the filename 添加导致产生core的信号 %t - insert UNIX time that the coredump occurred into filename 添加core文件生成时的unix时间 %h - insert hostname where the coredump happened into filename 添加主机名 %e - insert coredumping executable name into filename 添加命令名安全的rmrm -rf 慎用命令敲得多了，常在河边走，难免会湿鞋昨天，一个手误，敲错了命令，把原本想要留的文件夹给rm -rf掉了几天心血全木有了，靠，死的心都有了 几点教训： rm 特别是rm -rf之前，小心，三思，或者直接将命令改写掉 做好备份，有便捷的备份脚本 做好定时备份，有个前辈搞定时脚本，每天定时自个执行，即使删错了也不会那么悲催 首先，搞个回收站在~下 .bashrc或者.bash_profile加入 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364alias ll='ls -alF'mkdir -p ~/.trash# original rmalias or='/bin/rm'alias rm=trashalias r=trashalias lr='ls -alF ~/.trash'alias cr='cd ~/.trash'alias ur=undelfilealias er=emptytrashundelfile()&#123; mv -i ~/.trash/$@ ./&#125;trash()&#123; for TARGET_NAME in $@ do TARGET_WITH_NO_LAST_SLASH=~/.trash/$&#123;TARGET_NAME%*/&#125; if [ -d $&#123;TARGET_WITH_NO_LAST_SLASH&#125; ]; then # or -rf $&#123;TARGET_WITH_NO_LAST_SLASH&#125; mv $&#123;TARGET_WITH_NO_LAST_SLASH&#125; $&#123;TARGET_WITH_NO_LAST_SLASH&#125;_`date '+%x%X'` echo "rename the old one with the sameName_currentTime successfully." fi if [ -f $&#123;TARGET_WITH_NO_LAST_SLASH&#125; ]; then # or -rf $&#123;TARGET_WITH_NO_LAST_SLASH&#125; mv $&#123;TARGET_WITH_NO_LAST_SLASH&#125; $&#123;TARGET_WITH_NO_LAST_SLASH&#125;_`date '+%x%X'` echo "rename the old one with the sameName_currentTime successfully." fi if [ -L $&#123;TARGET_WITH_NO_LAST_SLASH&#125; ]; then # or -rf $&#123;TARGET_WITH_NO_LAST_SLASH&#125; mv $&#123;TARGET_WITH_NO_LAST_SLASH&#125; $&#123;TARGET_WITH_NO_LAST_SLASH&#125;_`date '+%x%X'` echo "rename the old one with the sameName_currentTime successfully." fi done ORIGIN_TARGET=$@ mv $&#123;ORIGIN_TARGET%*/&#125; ~/.trash/ #$&#123;ORIGIN_TARGET%*/&#125; for removing the last slash&#125;emptytrash()&#123; read -p "clear sure?[n]" confirm [ $confirm == 'y' ] || [ $confirm == 'Y' ] &amp;&amp; or -rf ~/.trash/*&#125;# mkdir and enter italias mc=mkdircdmkdircd()&#123; mkdir -p "$1" &amp;&amp; cd "$1"&#125;# cd and ls -alF diralias cd=cdllcdll()&#123; builtin cd "$@" &amp;&amp; ll;&#125;ulimit -c unlimited 如果想清空回收站彻底删除所有, 用er就可以了. 上文中的 alias or=&#39;/bin/rm&#39;中的 /bin/rm 因系统而异,你可以敲 whereis rm 命令来查看你的系统的rm在哪儿,比如我的就是 [b@host ~]$ whereis rm rm: /bin/rm /usr/share/man/man1/rm.1.gz而有些人的却是 /usr/bin/rm , 那就要改成 alias or=&#39;/usr/bin/rm&#39; 了 命令补全增强首先找到 .inputrc 文件, 通过 sudo find / -name inputrc来找到它, 如果没找到就在自己的home目录下新建一个 .inputrc 文件, 然后在.inputrc文件末尾加上常用的Bash定制 : set completion-ignore-case on #For single press Tab results for when a partial or no completion is possible set show-all-if-ambiguous on #For results when no completion is possible set show-all-if-unmodified on #History completion bound to arrow keys (down, up) &quot;\e[A&quot;: history-search-backward &quot;\e[B&quot;: history-search-forward 解释 : show-all-if-ambiguous : 默认情况下，按下两次 才会出现提示，现在只需要一次了。 completion-ignore-case : 在自动补全时忽略大小写 history-search-* : 输入某个命令的一部分时，按上下箭头，会匹配关于这个这命令最近的使用历史。比如：输入 vim ，然后按”上“键，此时，可以显示上一次运行vim时的那条命令，非常的方便！ Bash中快速移动光标bash有两种输入模式vi模式和emacs模式，其中emacs是默认模式，而且操作起来也比vi模式要快捷。可以通过 set -o vi和set -o emacs来转换。 命令行中移动Alt+f和Alt+b 是前后移动一个单词的距离这个很快比如你输入了 删除 Alt+d 往右边删除一个单词 Alt+Backspace 往左删除一个单词 Ctrl+u 往左删除到行首 Ctrl+k 往右删除到行末 其中这些删除都放入了删除环里面，可以使用Ctrl+y找回，Alt+y在删除环里面移动也就是说命令行里面可以使用剪切和粘贴了。上面的几条如果用熟练了效率能提高很多。 而如果还想了解更多的快捷键绑定，敲如下命令 bind -P 发现有些你需要的功能而没有快捷键绑定的话可以如下绑定，比如我绑定了两个函数 bind -m emacs ‘“/M-w”: kill-region’ bind -m emacs ‘“/M-W”: copy-region-as-kill’ 在命令历史中查找PS : 其实如果使用了上述的Bash定制中的 history-search-* 就不需要这个 ctrl + r 了 使用 Ctrl+r， 这个键组合是反向增量查找消息历史。很好用。 比如你很久以前输入过某个命令如。 gcc -c -DKKT - Dnnn 等等，一长串， 用上下方向键来找比较困难，这时候可以Ctrl+r，然后输入gcc很快找到该命令，重复按Ctrl+r将查找更早的历史。 bash脚本基础 -e filename 如果 filename存在，则为真 [ -e /var/log/syslog ] -d filename 如果 filename为目录，则为真 [ -d /tmp/mydir ] -f filename 如果 filename为常规文件，则为真 [ -f /usr/bin/grep ] -L filename 如果 filename为符号链接，则为真 [ -L /usr/bin/grep ] -r filename 如果 filename可读，则为真 [ -r /var/log/syslog ] -w filename 如果 filename可写，则为真 [ -w /var/mytmp.txt ] -x filename 如果 filename可执行，则为真 [ -L /usr/bin/grep ] filename1-nt filename2 如果 filename1比 filename2新，则为真 [ /tmp/install/etc/services -nt /etc/services ] filename1-ot filename2 如果 filename1比 filename2旧，则为真 [ /boot/bzImage -ot arch/i386/boot/bzImage ] 一个例子 : 1234567891011121314151617181920212223#!/bin/shBUILD_DIR=../buildif [ ! -d $&#123;BUILD_DIR&#125; ]; then mkdir $&#123;BUILD_DIR&#125;else echo "BUILD_DIR is already exist."fiRTS_PATH=`cd .. &amp;&amp; pwd`RTS_SYMBOLIC_LINK=~/rtsif [ ! -L "$RTS_SYMBOLIC_LINK" ]; then ln -s $RTS_PATH $RTS_SYMBOLIC_LINK;else echo "RTS_SYMBOLIC_LINK is already exist."fiexit 0]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Bash</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从零开始搭建一台简易Ubuntu服务器三]]></title>
    <url>%2Fblog%2F2013%2F08%2F27%2Fhow_to_build_a_simple_ubuntu_server_three%2F</url>
    <content type="text"><![CDATA[安装MySQL执行命令安装MySQL：sudo apt-get install mysql-server安装的时候会提示填入一个root的初始密码，先输入个8做初始密码吧 导入客户数据库base_accountmysql -uroot -p &lt; *****.sql(某个sql文件) 安装svn并checkout一个svn服务器上的目录 进入 /data/www目录下 ：cd /data/www 执行命令安装：sudo apt-get install subversion checkout一个目录（比如svn://112.124.26.188/myapp/td/01CServer_PHP/errorMsg），执行命令：svn checkout svn://112.124.26.188/myapp/td/01CServer_PHP/errorMsg （或者 svn co svn://112.124.26.188/myapp/td/01CServer_PHP/errorMsg） . . . 测试是否可以访问并将相应数据传入数据库中访问 localhost/errorMsg/ErrorMsg.php?data=3_2&amp;error_msg=zhangnimashuai然后查看数据库相应表里是否增加了数据 证书登陆 (可选)#对dev 将id_rsa私钥和id_rsa.pub公钥以及authorized_keys授权文件拷贝至~dev/.ssh/目录chmod 600 id_rsa; chmod 644 id_rsa.pub; chmod 644 authorized_keys SSH 证书登陆配置sudo vi /etc/ssh/sshd_config取消注释 : #AuthorizedKeysFile .ssh/authorized_keys修改yes-&gt;no : PasswordAuthentication nosudo service ssh restart 重启服务 测试登陆sudo service nginx restartsudo service php5-fpm restart 测试成功后去除dev用户的sudo权限 （可选）sudo visudo 删除：dev ALL=(ALL:ALL) ALL]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>VBox</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从零开始搭建一台简易Ubuntu服务器二]]></title>
    <url>%2Fblog%2F2013%2F08%2F25%2Fhow_to_build_a_simple_ubuntu_server_two%2F</url>
    <content type="text"><![CDATA[安装nginx如若有有不明白，还可以前往参考 执行命令安装nginx：sudo apt-get install nginx测试是否安装成功：在本机的浏览器里访问 localhost ;如果现实”Welcome to nginx!”，表明你的 Nginx 服务器安装成功！启动 Nginx：sudo /etc/init.d/nginx start关闭 Nginx：sudo /etc/init.d/nginx stop重启 nginx：sudo /etc/init.d/nginx restart 或者 sudo service nginx restartsudo service apache2 stop (如果之前装了apache2则需要sudo apt-get remove apache2 卸载掉apache2然后执行这个stop命令) . . . 修改nginx的server配置文件 执行命令：sudo vi /etc/nginx/sites-available/default 然后将default文件里的内容全部删除，把下面的内容粘贴进去： 12345678910111213141516171819202122232425262728server &#123; listen 80 default_server; root /data/www; #这里表示nginx根目录 index index.php index.html index.htm; server_name localhost; chunked_transfer_encoding off; location / &#123; try_files $uri $uri/ =404; &#125; error_page 404 /index.html; location ~ \.php$ &#123; #加上这个代码块就可以用php访问了，这个代码块还有fastcgi的相关内容 try_files $uri =404; fastcgi_split_path_info ^(.+\.php)(/.+)$; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; include fastcgi_params; &#125;&#125; 安装以及启动fastcgi 执行命令安装：sudo apt-get install spawn-fcgi 启动fastcgi ：spawn-fcgi -a 127.0.0.1 -p 9000 -C 10 -u www-data -f /usr/bin/php-cgi 为了让php-cgi开机自启动： Ubuntu开机之后会执行/etc/rc.local文件中的脚本 所以我们可以直接在/etc/rc.local中添加启动脚本。将 spawn-fcgi -a 127.0.0.1 -p 9000 -C 10 -u www-data -f /usr/bin/php-cgi 添加到语句：exit 0 前面才行 安装PHP执行命令安装PHP：sudo apt-get install php5-cli php5-cgi php5-fpm php5-mcrypt php5-mysql php设置sudo vi /etc/php5/fpm/php.ini #设置cgi.fix_pathinfo=0sudo service php5-fpm restart sudo vi /etc/php5/fpm/pool.d/www.conflisten.owner = www-datalisten.group = www-datalisten.mode = 0660 （去掉原www.conf里“listen.mode = 0660”前的分号，那个分号是注释的意思）sudo service php5-fpm restart 测试是否可以访问 在nginx根目录也就是 上面server配置文件里的 /data/www(若没有这个目录就建一个，并改变权限，执行sudo chown dev:dev data/)文件夹里新建index.html（不建此文件将不能在本机访问localhost）将下面的内容粘贴到index.html文件里 12345678&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;&lt;/head&gt;&lt;body bgcolor=&quot;white&quot; text=&quot;black&quot;&gt;&lt;center&gt;&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;&lt;/center&gt;&lt;/body&gt;&lt;/html&gt; 第1步完成后将可以在浏览器访问 http://localhost 再在 /data/www文件夹里新建test.php将下面的内容粘贴到test.php文件里&lt;?php phpinfo(); ?&gt; 第3步完成后将可以在浏览器访问 http://localhost/test.php 注：如果没有启动fastcgi，访问之后将会下载此php文件。若启动了fastcgi，则访问phpinfo的网页;如果出现No input file specified就在上面的server配置文件里的下述地方加入这条语句：fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;如下： 12345678location ~ \.php$ &#123; try_files $uri =404; fastcgi_split_path_info ^(.+\.php)(/.+)$; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include fastcgi_params;&#125;]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>VBox</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从零开始搭建一台简易Ubuntu服务器一]]></title>
    <url>%2Fblog%2F2013%2F08%2F23%2Fhow_to_build_a_simple_ubuntu_server_one%2F</url>
    <content type="text"><![CDATA[VirtualBox安装Ubuntu以及分区请参考后来写的文章 创建dev用户sudo adduser dev 增加dev权限sudo visudo 添加：dev ALL=(ALL:ALL) ALL . . . 关于Ubuntu的root密码Ubuntu的默认root密码是随机的，即每次开机都有一个新的root密码。我们可以在终端输入命令 sudo passwd，然后输入当前用户的密码，enter，终端会提示我们输入新的密码并确认，此时的密码就是root新密码。修改成功后，输入命令 su root，再输入新的密码就ok了。 安装ssh服务端先更换源， 然后执行sudo apt-get update执行命令：sudo apt-get install openssh-server测试是否安装成功：ssh localhost]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>VBox</tag>
      </tags>
  </entry>
</search>
